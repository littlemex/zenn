---
title: "[翻訳] AWS TrainiumとInferentiaでAWS Neuron Profilerを使用したモデルプロファイリングの開始"
emoji: "🐥"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["AWS Neuron"]
published: true
---

https://builder.aws.com/content/33FrJL2E97pPBNrqmYmvCRIHMew/getting-started-with-model-profiling-on-aws-trainium-and-inferentia-using-aws-neuron-profiler の翻訳


# AWS TrainiumとInferentiaでAWS Neuron Profilerを使用したモデルプロファイリングの開始

このブログ投稿では、AWS TrainiumとInferentiaチップ上でMLモデルのパフォーマンスを分析するためにAWS Neuron Profilerを使用する方法について、インスタンスのセットアップからプロファイルのキャプチャと可視化まで、ステップバイステップで概説します。

![Scott Perry](https://avatars.builderprofile.aws.dev/34RuDQSJkCmT9pVjz59mIKuUgDh.webp)

[Scott Perry](/community/@sperry)

**公開日: 2025年10月23日**

---

このブログシリーズの[パート1](https://builder.aws.com/content/2x3ldAuia9gjjUixpOt162ttkK2/unlocking-machine-learning-model-performance-from-bottlenecks-to-breakthroughs)で述べたように、この投稿では**Neuron Profiler**を使用して機械学習モデルをプロファイリングする方法を説明します。Neuron Profilerがコンピュートエンジンの活用状況とメモリアクセスパターンを強調することでワークロードに関する洞察をどのように提供し、パフォーマンス特性をより深く理解し、最適化の機会を特定できるようにするかを示します。

AWS Trainium/InferentiaチップでNeuron Profilerを使用してモデルのプロファイリングを開始するには、以下の手順に従います:

#### 1. 最新のNeuron DLAMIでインスタンスを起動

まず、[最新のNeuron Deep Learning AMI (DLAMI) for Trainium/Inferentia](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/setup/neuron-setup/multiframework/multi-framework-ubuntu22-neuron-dlami.html#setup-ubuntu22-multi-framework-dlami)を使用して[EC2インスタンスを起動](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/LaunchingAndUsingInstances.html)します。インスタンスが起動したら、インスタンスにSSH接続し、仮想環境をアクティベートします。

**注意:** この例は、AWS TrainiumとInferentiaの両方のインスタンスで動作します。デモンストレーションの目的で、`trn1.2xlarge`インスタンスを使用します。このモデルのデバイスレベルのプロファイルをキャプチャします。

```
$ ssh <user>@<ip> -L 3001:localhost:3001 -L 8086:localhost:8086
source /opt/aws_neuronx_venv_pytorch_2_7/bin/activate
```

AWS Neuron Profilerツールは`aws-neuronx-tools`パッケージに含まれており、`/opt/aws/neuron/bin`ディレクトリに自動的にインストールされます。

Neuron webプロファイルビューアは、プロファイリングされたワークロードからの時系列データを後処理後に保存するためにInfluxDB OSS 2.xを使用します。InfluxDBをインストールするには、以下の手順に従います:

```
# Install InfluxDB
wget -q https://repos.influxdata.com/influxdata-archive_compat.key
echo '393e8779c89ac8d958f81f942f9ad7fb82a25e133faddaf92e15b16e6ac9ce4c influxdata-archive_compat.key' | sha256sum -c && cat influxdata-archive_compat.key | gpg --dearmor | sudo tee /etc/apt/trusted.gpg.d/influxdata-archive_compat.gpg > /dev/null
echo 'deb [signed-by=/etc/apt/trusted.gpg.d/influxdata-archive_compat.gpg] https://repos.influxdata.com/debian stable main' | sudo tee /etc/apt/sources.list.d/influxdata.list

sudo apt-get update && sudo apt-get install influxdb2 influxdb2-cli -y
sudo systemctl start influxdb
influx setup
# Fill in the information to finish the setup
```

#### 2. 多層パーセプトロンモデル (MLP) をダウンロード

この例では、簡単なMLPモデルを例として使用します。必要なサンプルコードファイルを取得するためにリポジトリをクローンします。

```
git clone https://github.com/aws-neuron/aws-neuron-samples.git
cd aws-neuron-samples/torch-neuronx/training/mnist_mlp
```

#### 3. モデルをコンパイル

`trn1.2xlarge`インスタンスタイプで実行するためにトレーニングスクリプト`train.py`をコンパイルします。[neuron_parallel_compile](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/api-reference-guide/training/pytorch-neuron-parallel-compile.html)は、モデルのトライアル実行からグラフを抽出し、グラフの並列事前コンパイルを実行し、[Neuron Persistent Cache](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/arch/neuron-features/neuron-caching.html#neuron-caching)に格納します。

```
neuron_parallel_compile python3 train.py
```

![neuron_parallel_compile output](https://prod-assets.cosmic.aws.dev/a/34RzVqPUUiDQ0g7BtkoTqLVFzmc/new_.webp?imgSize=1000x269 "neuron_parallel_compile output")

コンパイルが完了すると、コンパイルされたアーティファクトが`neuron-compile-cache`ディレクトリで利用可能になります。Neuronコンパイルキャッシュのデフォルトパスは`/var/tmp/neuron-compile-cache`です。

**注意:** Neuronコンパイルは1回限りのプロセスです。モデルがコンパイルされると、その後の実行では再コンパイルは必要ありません。代わりに、トレーニングジョブはコンパイラキャッシュから事前コンパイルされたNeuron実行可能ファイル(NEFF)を直接ロードして実行します。

#### 4. プロファイルのキャプチャ

モデルがコンパイルされたら、新しく生成された`neff`(Neuron実行可能ファイル形式)ファイルを使用してこのモデルのプロファイルをキャプチャします。これを実現するには、Neuronコンパイルキャッシュパスに移動し、以下のコマンドを使用します:

```
cd /var/tmp/neuron-compile-cache/neuronxcc-<neuron-compiler-version>/MODULE_<module-id>
neuron-profile capture -n model.neff -s profile.ntff
```

これにより、最近コンパイルされたグラフのNeuronプロファイルが`profile.ntff`として生成されます。

#### 5. Neuronプロファイルの表示

生成されたプロファイルは以下のコマンドで表示できます:

```
neuron-profile view -n model.neff -s profile.ntff
```

![neuron-profile view](https://prod-assets.cosmic.aws.dev/a/34Rw2hoJyl0aHg7DT6Se5Al40IG/imag.webp?imgSize=1000x60 "neuron-profile view")

このモデルのプロファイルを表示するには、Webブラウザを起動して`localhost:3001`にアクセスします。

注意: 以下のコマンドを使用してインスタンスのポートフォワーディングが有効になっていることを確認してください:

```
$ ssh <user>@<ip> -L 3001:localhost:3001 -L 8086:localhost:8086
```

![Neuron Profiler UI](https://prod-assets.cosmic.aws.dev/a/34RwEO0bseJeff4Px5ztOy2esTH/imag.webp?imgSize=1000x532 "Neuron Profiler UI")

#### 6. マルチワーカージョブのプロファイルキャプチャ

次に、32ワーカージョブのNeuronプロファイルをキャプチャしましょう。この例で使用するインスタンスは`trn1.2xlarge`で、2つのNeuronコアを備えており、それぞれが個別のワーカーとして機能します。32個のNeuronコアを持つ`trn1.32xlarge`を使用することもできます。まず、32ワーカージョブ用にMLPモデルをコンパイルします。これは以下のコマンドで実現できます:

```
neuron_parallel_compile torchrun --nproc_per_node=32 train_torchrun.py
```

モデルがコンパイルされたら、Neuron Profilerを使用してコンパイルされたNEFFの1つからプロファイルを生成します:

```
neuron-profile capture -n model.neff --collectives-workers-per-node 32 -s output/profile.ntff
```

以下を使用してプロファイリング結果を可視化します:

```
neuron-profile view -n model.neff -s output/profile.ntff
```

![Multi-worker profile in Neuron Profiler UI](https://prod-assets.cosmic.aws.dev/a/34RwQuZwOG8fupamS2dJFXY8UsP/imag.webp?imgSize=1000x554 "Multi-worker profile in Neuron Profiler UI")

上部にすべてのワーカーがタブとして表示されていることに注目してください。それらのいずれかをクリックして、それぞれのワーカー上のトレースを確認できます。

## Neuronプロファイルの理解

**実行タイムラインの概要**

![Execution timeline view](https://prod-assets.cosmic.aws.dev/a/34RwZsdAsyED9r3QOdCA5Or56Iw/imag.webp?imgSize=1000x572 "Execution timeline view")

タイムラインの一部を拡大することは、目的のウィンドウ全体にカーソルをドラッグするだけで簡単にでき、その期間中の実行をより詳細に表示できます。

![Zoomed-in view of execution timeline](https://prod-assets.cosmic.aws.dev/a/34RwhOIqMehZQW45rTLd68BMPER/imag.webp?imgSize=1000x470 "Zoomed-in view of execution timeline")

実行タイムラインは、実行開始からの経過時間に基づいてイベントをプロットすることで可視化されます。これにより、さまざまなプロセスが互いに対していつ発生するかを明確に表現でき、パフォーマンスの分析や潜在的なボトルネックの特定に役立ちます。

**NeuronCoreエンジンの実行**

上に移動すると、タイムラインは個々のNeuronCoreエンジンの実行詳細を表示します。このセクションの各行は、TensorMatrixエンジン、Vectorエンジン、Scalarエンジン、GPSIMDエンジンなどの特定のエンジンに対応しています([Neuron Documentation](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/arch/trainium_inferentia2_arch.html#neuroncore-v2-compute-engines)を参照して、Neuronコアで利用可能なさまざまなコンピュートエンジンについて詳しく学んでください)。TensorEトレースはLoadStationary(重みの読み込み)を示し、TensorMatrixEはMultiplyMoving(行列積)を示します。両方の操作は同じTensorEエンジン上で実行されますが、明確にするためにタイムラインで2つの行に分けています。この分離により、バックグラウンドのload-stationary機能が強調されます。ここでは、次の行列積の静止行列が現在の行列積の実行と並行してロードされます。

これらのバーをクリックすると、実行依存関係やワークロード分散などのより詳細な詳細にアクセスできます。集合計算操作を組み込むモデルの場合、CC-coreというラベルの付いた追加の行が表示されます。これらの行は、複数のコアにわたる分散実行に不可欠なCC(Collective Compute)操作の同期を特に追跡します。

![Instruction details](https://prod-assets.cosmic.aws.dev/a/34RwrJ3h7TaBOhlSgtaTkaz7YDF/imag.webp?imgSize=1000x256 "Instruction details")

**メモリ転送とオンチップSRAMアクティビティ**

タイムラインの上部には、実際のDMAアクティビティが表示されます。このセクションは、入力テンソルと出力テンソル、中間テンソル、オンチップSRAMメモリとの間のスピルまたはロード/アンロード操作の移動を含む、メモリ転送の内訳を提供します。これらのメモリトランザクションは、計算ユニット間のスムーズなデータフローを確保する上で重要な役割を果たし、そのパターンを監視することでモデルのパフォーマンスを最適化できます。

![DMA activity](https://prod-assets.cosmic.aws.dev/a/34RwyITsEJajtGdudQYJmx5gNc1/imag.webp?imgSize=1000x270 "DMA activity")

**DMAメトリクス**

タイムラインの下部には、Direct Memory Access (DMA)スループットとPending DMA Countの行があり、DMA操作のアクティビティを表しています。これらの行は、任意の時点で発生しているメモリ転送の量を示します。理想的には、これらのセクションの値が高いほど、メモリ帯域幅の効率的な活用を示唆し、偏差はワークロードが主にメモリバウンドかコンピュートバウンドかについての洞察を提供する可能性があります。

![DMA metrics](https://prod-assets.cosmic.aws.dev/a/34Rx3qTgKjrfgrBUK9WwO8PACPb/imag.webp?imgSize=1000x94 "DMA metrics")

さらに、モデルのすべてのパフォーマンス関連データを表形式で表示するサマリータブがあります。このビューにより、HFU、MFU、演算強度などの全体的な統計を分析しやすくなります。また、DMAを介したデータ移動と各個別エンジンのパフォーマンスメトリクスに関する詳細な洞察も提供します。

![Summary tab](https://prod-assets.cosmic.aws.dev/a/34Rx7BG3mJ6HdULbYF2XKbDUvOL/imag.webp?imgSize=1000x429 "Summary tab")

実行タイムラインのこれらのさまざまなセクションを分析することで、ユーザーは計算とメモリ操作の相互作用について貴重な洞察を得ることができ、最終的にパフォーマンスチューニングとデバッグに役立ちます。

この投稿では、AWS NeuronデバイスでNeuron Profilerを使用してモデルのパフォーマンス特性を分析する方法を示しました。[このシリーズの次のパート](https://builder.aws.com/content/34Ru44lIq9QrlPgr16BvDm78F3G/decoding-nki-kernel-performance-using-aws-neuron-profiler)では、NKIカーネルレベルでのプロファイリングに焦点を移し、Neuron Profilerがそこでどのようにより深い洞察を提供するかを探ります。

---

このブログは、Esha Lakhotia、Sadaf Rasool、Scott Perryによって執筆されました。

**Esha Lakhotia**は、AWSのAnnapurna Labsのプロダクトマネージャーです。彼女は、AWSのAIチップ用の開発者ツールを構築することで、顧客がMLパフォーマンス最適化目標を達成できるようにしています。AWS TrainiumとAWS Inferentia上で顧客が機械学習ワークロードをプロファイル、デバッグ、加速するのに役立つ直感的なツール体験の作成に注力しています。

**Sadaf Rasool**は、AWSのAnnapurna Labsのソリューションアーキテクトです。Sadafは顧客と協力して、重要なビジネス課題に対処する機械学習ソリューションを設計しています。顧客がAWS TrainiumまたはAWS Inferentiaチップを活用して機械学習モデルをトレーニングおよび展開し、イノベーションの旅を加速するのを支援しています。

**Scott Perry**は、AWSのAnnapurna MLアクセラレータチームのソリューションアーキテクトです。カナダを拠点とし、AWS InferentiaとAWS Trainiumを使用したディープラーニングのトレーニングと推論ワークロードの展開と最適化を顧客が行うのを支援しています。彼の関心分野には、大規模言語モデル、深層強化学習、IoT、ゲノミクスが含まれます。
