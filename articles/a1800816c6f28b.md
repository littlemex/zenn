---
title: "202411.News"
emoji: "ðŸ˜Ž"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["TechNews"]
published: false
---

## [Template]()

- MM/DD
- **Reading** / Investigation / Verification

----

# 20241125

## [Accelerating Mixtral MoE fine-tuning on Amazon SageMaker with QLoRA](https://aws.amazon.com/blogs/machine-learning/accelerating-mixtral-moe-fine-tuning-on-amazon-sagemaker-with-qlora/)

- 11/22
- **Reading** / Investigation / Verification

### Features
- Mixtral employs a sparse mixture of experts (SMoE) architecture

### Notes
- 


## [Amazon SageMaker Inference now supports G6e instances](https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-inference-now-supports-g6e-instances/)

- 11/22
- **Reading** / Investigation / Verification

### Features
- G6e instances powered by NVIDIAâ€™s L40S Tensor Core GPUs
- each GPU providing 48 GB of high bandwidth memory
- Up to 400 Gbps of networking throughput
- Up to 384 GB GPU Memory
- G6e instances are ideal for fine-tuning and deploying open large language models

### Notes
- 