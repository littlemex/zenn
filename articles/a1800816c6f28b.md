---
title: "202412.News (Part1)"
emoji: "😎"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["TechNews", "AWS"]
published: true
---

自分用のまとめ、re:invent 2024 特集

zenn markdown って80000文字制限とかあったんか・・

# まとめ系記事とか

- [Top announcements of AWS re:Invent 2024](https://aws.amazon.com/jp/blogs/aws/top-announcements-of-aws-reinvent-2024/)
- [AWS re:Invent 2024 の記事一覧(Classmethod)](https://dev.classmethod.jp/referencecat/aws-reinvent-2024/?utm_source=dev.classmethod.jp&utm_medium=banner&utm_campaign=contentsbanner&utm_content=aws-reinvent-2024)

# 20241204(JST) 編集

## Related to Amazon Bedrock

- [Introducing latency-optimized inference for foundation models in Amazon Bedrock](https://aws.amazon.com/jp/about-aws/whats-new/2024/12/latency-optimized-inference-foundation-models-amazon-bedrock/)

## Related to Amazon Q

- [Announcing Amazon Q Developer transformation capabilities for .NET, mainframe, and VMware workloads in a web experience (preview)](https://aws.amazon.com/jp/blogs/aws/announcing-amazon-q-developer-transformation-capabilities-for-net-mainframe-and-vmware-workloads-preview/)
- [Amazon Q Business is adding new workflow automation capability and 50+ action integrations](https://aws.amazon.com/jp/blogs/aws/amazon-q-business-is-adding-new-workflow-automation-capability-and-50-action-integrations/)
- [Amazon Q Business adds support to extract insights from visual elements within documents](https://aws.amazon.com/about-aws/whats-new/2024/12/amazon-q-business-extract-insights-visual-elements-documents)
  - This new feature enables users to query information embedded in various types of visual, including diagrams, infographics, charts, and image-based content
- [Announcing Amazon Q Developer transformation capabilities for .NET in IDE (preview)](https://aws.amazon.com/blogs/aws/announcing-amazon-q-developer-transformation-capabilities-for-net-preview/)
- [New Amazon Q Developer agent capabilities include generating documentation, code reviews, and unit tests](https://aws.amazon.com/jp/blogs/aws/new-amazon-q-developer-agent-capabilities-include-generating-documentation-code-reviews-and-unit-tests/)

### Notes

## [Introducing multi-agent collaboration capability for Amazon Bedrock (preview)](https://aws.amazon.com/jp/blogs/aws/introducing-multi-agent-collaboration-capability-for-amazon-bedrock/)

- **機能概要**:  
Amazon Bedrockの新機能である「マルチエージェントコラボレーション」により、複数のAIエージェントが協力して、専門的なスキルを活かしながら複雑なマルチステップのタスクを効率的に処理することが可能になります。従来、開発者はオープンソースソリューションでエージェントのオーケストレーションやメモリ管理などを手動で実装する必要がありましたが、この機能により、これらの作業が大幅に簡略化されます。

- **アップデート日付**: 2024/12/03

- **特徴**:  
  - **簡単なセットアップ**: 複雑なコーディングを必要とせずに、数分でエージェントを作成、デプロイ、管理可能。  
  - **コンポーザビリティ**: 既存のエージェントをサブエージェントとして統合し、シームレスに協力できる。  
  - **効率的なエージェント間通信**: スーパーバイザーエージェントが一貫したインターフェースを介してサブエージェントと通信し、タスク完了を効率化。  
  - **最適化されたコラボレーションモード**:  
    - スーパーバイザーモード: 複雑な問題を分析し、サブエージェントを順次または並列に呼び出す。  
    - スーパーバイザーとルーティングモード: 単純なリクエストは直接サブエージェントにルーティングし、複雑なものはスーパーバイザーモードに切り替える。  
  - **統合トレース＆デバッグコンソール**: マルチエージェントの相互作用を可視化・分析できる。  

- **利用ケース**:  
  - 投資アドバイザリーシステム: 財務データ分析、調査、予測、投資推奨などの専門エージェントが協力してタスクを遂行する。  
  - 小売業の運用システム: 需要予測、在庫配分、サプライチェーンの調整、価格最適化などを複数エージェントで管理する。  
  - ソーシャルメディアキャンペーン管理: コンテンツ戦略エージェントとエンゲージメント予測エージェントを組み合わせて最適な投稿内容とタイミングを提案する。  

- **注意事項**:  
  - プレビュー期間中はリアルタイムチャットアシスタントなどの同期型ユースケースに対応。  
  - サブエージェントのコラボレーションは最大3階層までのエージェントチーム構成が可能。  
  - エージェント間の会話履歴共有機能は、簡単なタスクを処理する際に無効化することが推奨される。  

- **対応リージョン**:  
  東京リージョンを含むAmazon Bedrock AgentsがサポートされているすべてのAWSリージョンで利用可能（AWS GovCloud（US-West）は除く）。  

- **簡単な利用手順の概要**:  
  - **手順1**: Amazon Bedrockコンソールで「エージェント作成」から専門エージェント（例: コンテンツ戦略エージェント、エンゲージメント予測エージェント）を個別に作成。  
  - **手順2**: スーパーバイザーエージェントを作成し、マルチエージェントコラボレーションを有効化。  
  - **手順3**: 作成したサブエージェントをスーパーバイザーエージェントに関連付けて、チームとしてタスクを実行。  
  - **手順4**: エージェントの動作をテストし、統合トレースコンソールでワークフローを確認。  

- **専門用語**:  

| **カテゴリー**                | **専門用語**                 | **説明**                                                                                 |
|---------------------------|--------------------------|------------------------------------------------------------------------------------------|
| マルチエージェントシステム        | スーパーバイザーエージェント         | サブエージェントを統括し、タスクを分割・調整する中心的な役割を担うエージェント。                                                |
| マルチエージェントシステム        | サブエージェント                   | 特定のタスクやスキルに特化したエージェント。スーパーバイザーにより調整されて動作する。                                          |
| コラボレーションモード           | スーパーバイザーモード             | スーパーバイザーが入力を分析し、サブエージェントを順次または並列に呼び出してタスクを処理するモード。                          |
| コラボレーションモード           | ルーティングモード                 | 単純なリクエストはサブエージェントに直接ルーティングし、複雑なリクエストはスーパーバイザーモードに切り替えるモード。               |

### Notes
- 後で検証

## [Investigate and remediate operational issues with Amazon Q Developer (in preview)](https://aws.amazon.com/jp/blogs/aws/investigate-and-remediate-operational-issues-with-amazon-q-developer/)

- **機能概要**:  
Amazon Q Developerは、AWSのワークロードにおける運用上の問題を診断し、根本原因を特定して解決するためのジェネレーティブAIを活用した機能を提供します。CloudWatchやAWS Systems Managerと統合されており、リソース間の関係を自動的に発見し、問題を迅速に解決できるようサポートします。

- **アップデート日付**: 2024/12/03

- **特徴**:  
  - **問題の調査と解決の自動化**: Amazon Q Developerは、アプリケーションのトポロジーマップを作成し、アラームの原因となったコンポーネントを特定します。  
  - **仮説生成と提案**: DynamoDB、Lambda、ECSなどのサービスから関連するメトリクスを基に仮説を提示し、その理由も確認可能です。  
  - **リメディエーションの自動化**: AWS Systems Managerのオートメーションランブックを提案し、過去の実行履歴も確認できます。  
  - **統合された操作環境**: CloudWatchやLambdaメトリクスから直接インシデントを調査し、フィードに追加できます。  

- **利用ケース**:  
  - DynamoDBのスロットリングやLambdaのパフォーマンス低下など、クラウドアプリケーションで発生するパフォーマンス問題の診断と解決。  
  - システム管理者が複数のAWSサービスにまたがる複雑な問題を迅速に調査し、影響を最小限に抑える。

- **注意事項**:  
  - 現在プレビュー版であり、US East (N. Virginia) リージョンのみで提供されています。  
  - トラブルシューティングは、AWSサービスの構成やメトリクスの正確な設定に依存します。  

- **対応リージョン**:  
  - **東京リージョン**: 現時点では未対応

- **簡単な利用手順の概要**:  
  - **手順1**: CloudWatchアラームを設定し、メトリクスを監視します。  
  - **手順2**: アラームが発生した際にAmazon Q Developerを使用して新しい調査を開始します。  
  - **手順3**: 提案された仮説と修復アクションを確認し、適用するランブックを選択します。  
  - **手順4**: パラメータを入力し、実行結果を確認してフィードに追加します。

## [Introducing GitLab Duo with Amazon Q](https://aws.amazon.com/jp/blogs/aws/introducing-gitlab-duo-with-amazon-q/)

- **機能概要**:  
GitLab Duo with Amazon Q は、AWS の Amazon Q Developer エージェントを GitLab に統合し、AI を活用した開発支援機能を提供することで、DevSecOps の効率化と開発ワークフローの変革を実現します。これにより、開発者はコードレビューや機能開発、レガシーコードの移行を GitLab 内でシームレスに行えるようになります。

- **アップデート日付**: 2024/12/03  

- **特徴**:  
  - **統合型開発支援**: Amazon Q Developer を GitLab のクイックアクション `/q` コマンドを通じて操作可能。  
  - **コード生成とマージリクエストの自動化**: 既存コードベースの解析に基づき、コードを生成し、自動的にマージリクエストを作成。  
  - **コードレビューの自動化**: セキュリティ脆弱性や品質問題をスキャンし、修正提案を自動生成。  
  - **レガシーコードの移行支援**: Java 8 や 11 から Java 17 へのコード移行を自動化し、移行プランや依存関係の変更を報告。  

- **利用ケース**:  
  - **新機能の開発**: Web アプリケーションの新規サインアップ機能をコード生成機能で素早く実装。  
  - **コードレビューの効率化**: セキュリティやベストプラクティスに従ったコードレビューを AI が補助。  
  - **レガシーコードのアップグレード**: Java 17 への移行タスクを自動化し、移行作業の効率を向上。  

- **注意事項**:  
  - 現在プレビュー版として提供されており、GitLab Ultimate サブスクリプションを持つセルフマネージド環境でのみ利用可能。  
  - 生成されたコードには第三者のオープンソースコードが含まれる場合があり、利用者が責任を持ってレビューする必要がある。  

- **対応リージョン**: 東京リージョンを含むすべての AWS リージョンで対応。  

- **簡単な利用手順の概要**:  
  1. GitLab の Issue、コメント、またはマージリクエスト内で `/q dev` コマンドを使用して Amazon Q Developer を起動。  
  2. 自動生成されたコードをレビューし、コメントを追加。必要に応じて `/q dev` や `/q fix` でコードを修正。  
  3. レガシーコードの移行では、Issue の説明欄に `/q transform` コマンドを記載して移行タスクを自動化。  

### Notes
- GitLab 買収を座して待つ・・

## [Prevent factual errors from LLM hallucinations with mathematically sound Automated Reasoning checks (preview)](https://aws.amazon.com/jp/blogs/aws/prevent-factual-errors-from-llm-hallucinations-with-mathematically-sound-automated-reasoning-checks-preview/)  

- **機能概要**:  
  Amazon Bedrock Guardrailsに「Automated Reasoning Checks」が追加され、生成された回答が幻覚（hallucination）による事実誤認を防ぐための数学的な検証が可能になりました。これにより、回答が既知の事実と整合していることを論理的に証明し、信頼性の高い出力を提供できます。  

- **アップデート日付**: 2024/12/03  

- **特徴**:  
  - **事実誤認防止**: LLMが生成する回答を形式論理に基づいて検証し、誤った情報が含まれていないか確認します。  
  - **唯一の機能**: 他の主要クラウドプロバイダーにはない、生成AIの安全性、プライバシー、正確性を統合的に提供するソリューションです。  
  - **Amazon Bedrock Guardrails対応**: コンテキストベースの検証やPII（個人識別情報）のフィルタリングなど、他のガードレール機能と組み合わせて利用可能です。  
  - **ポリシー管理**: ユーザーのドメイン知識を形式論理で定義し、ポリシーとしてシステムに登録できます。  

- **利用ケース**:  
  - **人事ポリシーの確認**: 従業員からの問い合わせに対する回答が正しいかを検証。  
  - **製品情報の検証**: 製品仕様や販売条件について生成された回答の正確性を保証。  
  - **業務フローの正確性**: オペレーションマニュアルに基づいた回答が適切であるか確認。  

- **注意事項**:  
  - **プレビュー版**: 現在、米国西部（オレゴン）リージョンでのみプレビュー版が提供されています。正式版提供前に、AWSアカウントチームに問い合わせる必要があります。  

- **対応リージョン**:  
  - 東京リージョンは未対応（2024年12月時点）。  

- **簡単な利用手順の概要**:  
  - 手順1: Amazon Bedrockコンソールにアクセスし、ガードレール設定から新しいポリシーを作成。  
  - 手順2: 組織のルールや手順を記載したドキュメントをアップロード。  
  - 手順3: 自動生成されたポリシーをレビューし、精度を確認後、ガードレールに適用。  
  - 手順4: テスト環境でポリシーを使用し、LLMの回答が正しいかを検証。  

### Notes
- 数学的に検証、の部分が気になる、調べる
- 後で検証

## [Build faster, more cost-efficient, highly accurate models with Amazon Bedrock Model Distillation (preview)](https://aws.amazon.com/jp/blogs/aws/build-faster-more-cost-efficient-highly-accurate-models-with-amazon-bedrock-model-distillation-preview/)

- 機能概要: Amazon Bedrock Model Distillationは、特定のユースケースに合わせて高精度で高速かつコスト効率の良いモデルを構築するための機能で、教師モデルから生成された応答を使用して、より小さい学生モデルをファインチューニングすることにより、知識移転を行います。これにより、大規模な基盤モデルを使うよりも最大5倍速く、最大75%コスト削減が可能になりますが、精度の低下はわずか2%未満です。
- アップデート日付: 2024/12/03
- 特徴:
  - 教師モデル（大規模な基盤モデル）から生成された応答を使って、学生モデルをファインチューニングするプロセスを自動化。
  - より小さなモデルでも、教師モデルに近い精度を維持しつつ、最大で5倍の高速化と75%のコスト削減を実現。
  - 対応するサービスはAnthropic、Meta、Amazonのモデル。
  - 合成データ生成技術を活用し、トレーニングデータを強化。
- 利用ケース:
  - 膨大なユーザーインタラクションを扱う生成AIアプリケーションで、モデルの遅延を減らし、コスト効率を高める。
  - Retrieval Augmented Generation（RAG）やその他のタスクで、精度をほとんど損なうことなくモデルのサイズを削減したい場合。
- 注意事項:
  - 教師モデルと学生モデルは同じモデルファミリーから選択する必要がある（例：Meta Llama 3.1 405B Instructを教師モデルとして選択した場合、学生モデルはLlama 3.1 70Bまたは8B Instructに制限される）。
  - 東京リージョンでは利用不可、米国東部（バージニア北部）および米国西部（オレゴン）でプレビュー対応中。
- 対応リージョン: 米国東部（バージニア北部）、米国西部（オレゴン）【東京リージョン未対応】
- 簡単な利用手順の概要
  - 手順1: Amazon Bedrockコンソールにアクセスし、「カスタムモデル」を選択。
  - 手順2: 「ディスティレーションジョブの作成」を選び、教師モデルと学生モデルを選択。
  - 手順3: Amazon S3からデータセットをアップロードし、ディスティレーションジョブを作成。
  - 手順4: 進行状況を「ジョブ」タブで追跡し、モデルを「モデル」タブで確認。

### 専門用語:

| カテゴリー        | 専門用語             | 説明                                                             |
|-------------------|----------------------|------------------------------------------------------------------|
| モデル関連       | 教師モデル（Teacher Model） | より大規模で精度が高い基盤モデル。学生モデルに知識を移転する役割を担う。  |
| モデル関連       | 学生モデル（Student Model） | より小さなモデルで、教師モデルに近い精度を持つようにファインチューニングされる。 |
| データ処理        | 知識移転（Knowledge Transfer） | 教師モデルから学生モデルへ精度を維持しつつ学習内容を移すプロセス。          |
| データ処理        | 合成データ（Synthetic Data） | データセットを強化するために生成されるデータ。教師モデルの応答を基にする。   |

### Notes
- 後で検証
- フィルタしたログを使って微調整可能

## [Introducing Amazon Aurora DSQL](https://aws.amazon.com/jp/blogs/database/introducing-amazon-aurora-dsql/)

- 機能概要: Amazon Aurora DSQLは、常に可用性が求められるアプリケーション向けに設計された最速のサーバーレス分散SQLデータベースです。サーバーレスの設計により、インフラ管理が不要で、データベースのシャーディングやインスタンスのアップグレードなしで、ワークロードに合わせて無制限にスケールします。また、アクティブ-アクティブ分散アーキテクチャを採用しており、単一リージョン構成で99.99%、複数リージョン構成で99.999%の可用性を提供します。PostgreSQL互換で、開発者は既存のツールやフレームワークを活用できます。
- アップデート日付: 2024/12/03
- 特徴:
  - サーバーレス設計でインフラ管理が不要、スケーラビリティと高可用性を提供。
  - 単一リージョン構成で99.99%、複数リージョン構成で99.999%の可用性を実現。
  - PostgreSQL互換で、PostgreSQLドライバー、ツール、フレームワークをサポート。
  - オプティミスティック・コンカレンシー・コントロール（OCC）を使用して、スケーリング時のパフォーマンスを最適化。
- 利用ケース:
  - 高可用性と強いデータ一貫性が求められるアプリケーションの構築。
  - 複数リージョンに跨るデータセンター間の同期と冗長性の確保。
  - クラウド環境でのサーバーレスアーキテクチャを活用したデータベースのスケーリング。
- 注意事項:
  - 現在はプレビュー版として提供されているため、使用には注意が必要。
  - サポートされるリージョンについては公式ガイドを参照する必要がある。
- 対応リージョン: 東京リージョンは現在対応していない可能性があり、公式ガイドで確認が必要。
- 簡単な利用手順の概要:
  - 手順1: Aurora DSQLコンソールまたはAWS CLIを使って、クラスタの作成を開始。
  - 手順2: クラスタを設定し、PostgreSQL互換のデータベースとして利用を開始。
  - 手順3: 必要に応じてマルチリージョン設定やセキュリティ設定を行う。

### 専門用語:

| カテゴリー           | 専門用語                  | 説明                                               |
|--------------------|------------------------|----------------------------------------------------|
| データベース管理   | サーバーレス             | サーバーの管理をユーザーが行わないデータベース運用方式。自動でスケールし、インフラ管理が不要。|
| データベースアーキテクチャ | アクティブ-アクティブ分散アーキテクチャ | データベースノードが複数のアクティブな状態で動作し、障害発生時もダウンタイムなく処理を続ける仕組み。 |
| 同期処理           | オプティミスティック・コンカレンシー・コントロール (OCC) | 競合を検出する前提でトランザクションを処理し、長時間のトランザクションでも他のトランザクションに影響を与えない手法。 |

### Notes
- 今回のトップクラスの What's new
- [Amazon Aurora DSQLのUser Guideに沿ってNode.jsで基本的な操作を試してみた](https://dev.classmethod.jp/articles/trying-aurora-dsql-crud-with-nodejs/)

## [New Amazon S3 Tables: Storage optimized for analytics workloads](https://aws.amazon.com/jp/blogs/aws/new-amazon-s3-tables-storage-optimized-for-analytics-workloads/)

- 機能概要: Amazon S3 Tablesは、日次購入トランザクションやセンサーのストリーミングデータ、広告インプレッションなどのタブラデータを効率よく保存するために最適化されたストレージサービスです。これにより、Amazon AthenaやAmazon EMR、Apache Sparkなどのクエリエンジンで簡単にクエリを実行できます。自己管理型のテーブルストレージと比較して、最大3倍のクエリ性能向上と10倍の取引速度向上が期待でき、完全に管理されたサービスとして運用効率も向上します。
- アップデート日付: 2024/12/03
- 特徴:
  - Apache Icebergフォーマットでのデータ保存とクエリの最適化。
  - クエリパフォーマンスは自己管理型ストレージに対して最大3倍、取引速度は最大10倍向上。
  - Amazon AthenaやAmazon Redshift、EMRなどとの統合が可能。
  - 自動的なデータコンパクション、スナップショット管理、未参照ファイルの削除機能。
- 利用ケース:
  - 日次トランザクションやセンサーデータのクエリに適したストレージ。
  - ビッグデータ解析やストリーミングデータの分析を高速化。
  - データ保管と運用の効率化が求められる大規模なデータセットの管理。
- 注意事項:
  - 現在、AWS Glue Data Catalogとの統合はプレビュー版であり、全てのAWS Analyticsサービスで利用可能というわけではない。
  - 対応するAWS CLIは最新版に更新する必要がある場合がある。
- 対応リージョン: 東京リージョン未対応（US East (Ohio, N. Virginia)およびUS West (Oregon)のみ対応）
- 簡単な利用手順の概要
  - 手順1: AWS CLIを使用してテーブルバケットを作成する。
  - 手順2: Apache Sparkを利用してIcebergテーブルを作成し、データを挿入する。
- 専門用語:

| カテゴリー       | 専門用語          | 説明                                                                                                                                                        |
|----------------|------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------|
| ストレージ      | Iceberg          | Parquetファイルを管理するためのオープンソースフォーマット。大規模データセットのクエリを効率化するために広く使われている。                                          |
| コンパクション  | Compaction       | 小さなテーブルオブジェクトを結合して、クエリ性能を向上させるためのプロセス。対象となるファイルサイズを設定可能で、新しいスナップショットとして書き直される。     |

### Notes
- よさげ

## [Amazon DynamoDB global tables previews multi-Region strong consistency](https://aws.amazon.com/about-aws/whats-new/2024/12/amazon-dynamodb-global-tables-previews-multi-region-strong-consistency)

- 機能概要: Amazon DynamoDB global tablesは、完全に管理されたサーバーレスなマルチリージョン・マルチアクティブデータベースで、今後の強力な一貫性をサポートします。この新機能により、ゼロの回復時点目標(RPO)で、高可用性を備えたマルチリージョンアプリケーションを構築でき、最も高いレベルのレジリエンスを実現します。
- アップデート日付: 2024/12/03
- 特徴:
  - DynamoDB global tablesにおけるマルチリージョン強い一貫性が利用可能になる。
  - アプリケーションは、どのリージョンからでも最新のデータを読み取ることができ、複数のリージョンにわたる一貫性の管理を不要にする。
  - 高い一貫性を必要とするグローバルアプリケーションの構築に最適で、ユーザープロファイル管理、在庫管理、金融取引処理などに利用可能。
  - DynamoDB global tablesの価格設定に基づいて課金される。
- 利用ケース:
  - グローバルアプリケーションの構築で、強い一貫性が必要な場合に使用。
  - ユーザープロファイル管理、在庫追跡、金融取引処理などに最適。
- 注意事項:
  - この機能はプレビュー段階であり、利用可能なリージョンに制限がある。
- 対応リージョン: 東京リージョンには未対応。利用可能なリージョンは、米国東部（バージニア北部）、米国東部（オハイオ）、および米国西部（オレゴン）。
- 簡単な利用手順の概要:
  - 手順1: DynamoDBコンソールにログイン。
  - 手順2: global tables設定を選択。
  - 手順3: マルチリージョン強い一貫性を有効にして、必要なリージョンを選択。

| カテゴリー        | 専門用語             | 説明                                                       |
|-----------------|---------------------|----------------------------------------------------------|
| 一貫性の概念      | 強い一貫性           | システム全体でデータの整合性を保証し、どのリージョンからも最新データを読み取れる状態。  |
| 設定             | RPO（回復時点目標）   | システムの障害発生後に許容されるデータ損失量の指標。                |
| アーキテクチャ     | マルチアクティブ      | 複数のリージョンで同時にアクティブなデータを保持するアーキテクチャ。           |

### Notes
- [[アップデート] Amazon DynamoDB Global Tableがマルチリージョンの強い整合性をサポートしました (プレビュー) #AWSreinvent](https://dev.classmethod.jp/articles/amazon-dynamodb-global-tables-previews-multi-region-strong-consistency/)

## [Introducing queryable object metadata for Amazon S3 buckets (preview)](https://aws.amazon.com/jp/blogs/aws/introducing-queryable-object-metadata-for-amazon-s3-buckets-preview/)

- 機能概要: Amazon S3では、大規模なデータを扱う顧客が多いため、特定の条件に一致するオブジェクトを迅速に検索できる新機能が追加されました。この新しい機能は、S3オブジェクトのメタデータを自動的に生成し、Apache Icebergを使用して完全に管理されたテーブルに格納します。これにより、AthenaやRedshift、QuickSight、Apache Sparkなどのツールを使用して、スケーラブルにメタデータを照会し、オブジェクトを特定することができます。
- アップデート日付: 2024/12/03
- 特徴:
  - S3オブジェクトに関連するメタデータ（オブジェクトのキー、サイズ、作成/変更時間、ストレージクラスなど）を自動生成し、Icebergテーブルとして格納。
  - Apache Iceberg互換のツール（Athena、Redshift、Spark）でメタデータを効率的に照会可能。
  - メタデータは自動的にキャプチャされ、オブジェクトの作成・削除・メタデータの変更が追跡され、数分以内にテーブルに反映される。
  - S3に保存された動画の推論レスポンスには、生成されたコンテンツをAI生成物として識別するためのメタデータが付与される。
- 利用ケース:
  - 大量のS3オブジェクトから特定の条件に一致するデータを迅速に検索する必要がある分析やデータ処理、AIトレーニングのワークロードに活用される。
  - S3のデータ更新履歴を追跡し、オブジェクトごとの履歴データを簡単に管理するユースケースに適用可能。
- 注意事項:
  - メタデータの生成とストレージには追加料金がかかる。
  - 現在は米国東部（オハイオ、ノースバージニア）および米国西部（オレゴン）リージョンでのみ利用可能。
  - AWS Glue Data Catalogとの統合はプレビュー段階。
- 対応リージョン: 東京リージョン（ap-northeast-1）は対応していないが、米国東部（オハイオ、ノースバージニア）および米国西部（オレゴン）で対応。
- 簡単な利用手順の概要:
  - 手順1: S3メタデータテーブルを保存するためのバケットを作成。
  - 手順2: メタデータテーブル設定を作成し、データバケットに関連付け。
  - 手順3: Apache Sparkを使用して、S3テーブルからメタデータをクエリ。
- 専門用語:

| カテゴリー         | 専門用語         | 説明                                                       |
|------------------|----------------|----------------------------------------------------------|
| メタデータ       | S3オブジェクトメタデータ | S3オブジェクトに関連する情報（キー、サイズ、作成日など）を指し、検索や管理に活用される。 |
| クエリツール     | Apache Spark    | 大規模データ処理に使用される分散コンピューティングフレームワーク。                    |
| データ管理      | AWS Glue Data Catalog | データカタログサービス、データセットのメタデータを一元管理する。                    |

### Notes
- 後で検証

## [New Amazon S3 Tables: Storage optimized for analytics workloads](https://aws.amazon.com/jp/blogs/aws/new-amazon-s3-tables-storage-optimized-for-analytics-workloads/)

- 機能概要: Amazon S3 Tablesは、Apache Iceberg形式でのタブラー（表形式）データストレージを提供する新しい機能で、データ分析ワークロード向けに最適化されています。これにより、人気のあるクエリエンジン（Amazon Athena、Amazon EMR、Apache Sparkなど）を使用して効率的にクエリを実行でき、自己管理型のテーブルストレージと比較して高速なパフォーマンスと高い取引処理能力を実現します。
- アップデート日付: 2024/12/03
- 特徴:
  - Amazon S3 Tablesは、タブラー形式のデータを効率的にストレージし、簡単にクエリを実行できるように設計されています。
  - 性能については、自己管理型ストレージと比べて最大3倍のクエリ速度向上と最大10倍の取引処理能力を提供します。
  - 対応するサービスには、Amazon Athena、Amazon EMR、Apache Sparkなどがあり、Iceberg形式でのデータ管理に特化しています。
  - オプション機能として、テーブルのメンテナンス（コンパクション、スナップショット管理、未参照ファイルの削除）が自動的に行われます。
- 利用ケース:
  - 日次の購入トランザクションデータ、ストリーミングセンサーデータ、広告インプレッションデータなどのクエリが必要な分析ワークロードでの利用。
- 注意事項:
  - 本機能は現在、US East（オハイオ、N.バージニア）およびUS West（オレゴン）リージョンで利用可能であり、東京リージョンには対応していません。
  - データ管理や最適化に関連するコストが発生します（ストレージ、リクエスト、コンパクション料金など）。
- 対応リージョン: 現在、US East（オハイオ、N.バージニア）およびUS West（オレゴン）リージョンに対応。東京リージョンは未対応。
- 簡単な利用手順の概要
  - 手順1: AWS CLIを使用してテーブルバケットを作成します。
  - 手順2: 作成したバケット内でテーブルを作成し、Iceberg形式でデータを挿入します。
  - 手順3: AthenaやEMRなどのサービスからテーブルをクエリします。
  
- 専門用語:

| カテゴリー      | 専門用語             | 説明                                                                 |
|-----------------|----------------------|----------------------------------------------------------------------|
| データストレージ | Iceberg              | Apache Icebergは、データレイクでのデータ管理を効率化するオープンソース形式。|
| メンテナンス     | コンパクション       | 複数の小さなファイルを1つの大きなファイルにまとめ、クエリパフォーマンスを向上させる操作。|
| メンテナンス     | スナップショット管理 | テーブルのスナップショット（データの状態）を管理し、不要なものを削除する操作。|
| データストレージ | テーブルバケット     | S3における、Iceberg形式でのデータを管理するための専用バケット。|

### Notes
- 後で検証

## [Amazon EC2 Trn2 Instances and Trn2 UltraServers for AI/ML training and inference are now available](https://aws.amazon.com/jp/blogs/aws/amazon-ec2-trn2-instances-and-trn2-ultraservers-for-aiml-training-and-inference-is-now-available/)

- 機能概要: Amazon EC2 Trn2 インスタンスと Trn2 UltraServers は、機械学習 (ML) のトレーニングと推論に最適化された新しい EC2 コンピューティングオプションです。これらは、AWS Trainium チップの第2世代である Trainium2 を搭載しており、パフォーマンスとコスト効率を大幅に向上させます。
- アップデート日付: 2024/12/03
- 特徴:
  - Trn2 インスタンスは、従来の Trn1 インスタンスに比べて4倍の速度、4倍のメモリ帯域幅、3倍のメモリ容量を提供します。
  - GPUベースの EC2 P5e と P5en インスタンスに対して、30〜40% 優れた価格性能を発揮します。
  - Trn2 UltraServers は、64 個の Trainium2 チップを搭載し、モデル並列処理や推論において優れた性能を提供します。
  - Trn2 インスタンスと UltraServers は、分散トレーニングと推論をサポートするため、EC2 UltraClusters 内でスケールアウト可能です。
  - ソフトウェア側では、AWS Neuron SDK とともに、PyTorch、JAX、Hugging Face、PyTorch Lightning などのフレームワークをサポートします。
- 利用ケース:
  - トレーニング: 大規模な機械学習モデルのトレーニングにおいて、Trn2 インスタンスは高い計算能力と効率を提供します。
  - 推論: UltraServers は、低遅延でリアルタイムの推論をサポートします。
- 注意事項:
  - 現在、US East (Ohio) リージョンで利用可能で、東京リージョンは未対応です。
  - インスタンスの予約は最大64インスタンスまで、最大6ヶ月の期間で行えます。
- 対応リージョン: 現在はUS East (Ohio) リージョンでの提供のみ。東京リージョンには対応していません。
- 簡単な利用手順の概要
  - 手順1: AWS EC2 Capacity Blocks for MLを使用して、最大64インスタンスの予約を行います。
  - 手順2: AWS Deep Learning AMIを使って、PyTorchやJAXなどのフレームワークをセットアップします。
  - 手順3: AWS Neuron SDKを使って、既存のアプリケーションをTrn2 インスタンス向けにコンパイルします。
- 専門用語:

| カテゴリー        | 専門用語        | 説明                                                     |
|-------------------|-----------------|----------------------------------------------------------|
| インスタンス      | Trn2 インスタンス | AWS Trainium2 チップを搭載した新しい EC2 インスタンス。     |
| チップ            | Trainium2 チップ | AWSが開発した、第2世代の機械学習用専用チップ。             |
| メモリ            | HBM (High Bandwidth Memory) | 高速なメモリ技術で、Trn2 チップに搭載された96GiB の高速メモリ。 |
| ネットワーク       | NeuronLink       | 高帯域幅・低遅延のインターコネクト技術。                  |
| ソフトウェア       | Neuron SDK       | AWSの専用開発キットで、PyTorchやJAX向けに最適化されたツール。 |
| トレーニング       | FP8 (Floating Point 8-bit) | 精度が低く、機械学習のトレーニングで効率的な浮動小数点演算。  |

### Notes
- 後で検証

## [Blog: Amazon VPC Lattice: modernize and simplify your enterprise network architectures](https://aws.amazon.com/jp/blogs/networking-and-content-delivery/amazon-vpc-vattice-modernize-and-simplify-your-enterprise-network-architectures/)

## [Amazon VPC Lattice: Modernizing Networking with Application Networking](https://aws.amazon.com/jp/blogs/aws/amazon-vpc-lattice-modernizing-networking-with-application-networking/)

- 機能概要: Amazon VPC Latticeは、サービス間通信を簡素化するためのフルマネージドアプリケーションネットワーキングサービスです。これにより、AWS上のサービスがセキュアで効率的に相互通信できるようになります。また、サービス間認証やアクセス制御が強化され、VPC間やAWS外のリソースとの接続もサポートされます。
- アップデート日付: 2024/12/02
- 特徴:
  - VPC Latticeは、サービス間通信の簡素化とセキュリティ強化を目的としたサービスで、AWSの各種計算サービス（Amazon ECS、AWS Fargate、Amazon EC2、Amazon EKS、AWS Lambda）と統合されている。
  - セキュリティ面では、ゼロトラストアーキテクチャの実現をサポートし、サービス間の認証と認可を強化します。
  - ハイブリッド接続に対応し、オンプレミスとAWS間のシームレスな接続を提供します。
  - 複数のリージョンやVPC、サービスに対する高可用性とレジリエンシーを提供します。
- 利用ケース:
  - 複数のVPCやAWSアカウント間のサービス間通信を簡単に構築したい場合。
  - AWS内外のリソース間でセキュアな接続を行い、ゼロトラストセキュリティモデルを適用したい場合。
  - ハイブリッド環境（AWSとオンプレミス）でのアプリケーション接続を簡素化したい場合。
  - クロスリージョンでのサービス間通信やフェイルオーバーを効率的に管理したい場合。
- 注意事項:
  - 利用するためにはVPC Lattice対応の設定が必要です。
  - 一部の機能は特定のリージョンでのみ利用可能です。
- 対応リージョン: 東京リージョンに対応しています。
- 簡単な利用手順の概要:
  - 手順1: AWSマネジメントコンソールでVPC Latticeサービスを選択。
  - 手順2: 新しいネットワーキング設定を作成し、対象サービスを接続。
  - 手順3: セキュリティポリシーとアクセス制御の設定を行う。
  - 手順4: クロスリージョンやオンプレミスとの接続を設定。

### 専門用語

| カテゴリー         | 専門用語         | 説明                                                  |
|--------------------|------------------|-------------------------------------------------------|
| アーキテクチャ     | ゼロトラスト      | ユーザーやデバイスが内部・外部に関わらず常に検証されるセキュリティモデル。 |

### Notes
- 後で検証
- なにやら VPC Lattice でいい感じでアプリケーション全般をつなげようとしている感じ。EKS Hybrid Node のアップデート含めてアカウントやらネットワークまたいでいろいろなリソースに閉じてアクセスしたり、管理したりがやりやすくなってそう。
- WebSocket で通信する推論専用サーバ (DJL Serving on SageMaker とか)と前段のアプリケーションサーバを VPC Lattice(PrivateLink) 使って WebSocket 通信させるサンプルとかいいかもしれん。

## [AWS Clean Rooms now supports multiple clouds and data sources](https://aws.amazon.com/jp/about-aws/whats-new/2024/12/aws-clean-rooms-multiple-clouds-data-sources/)

- 機能概要: AWS Clean Roomsは、複数のクラウドやデータソースとのデータコラボレーションをサポートするようになりました。これにより、企業とそのパートナーは、SnowflakeやAmazon Athenaに保存されたデータを、データを移動したり共有したりすることなく簡単にコラボレーションできます。これにより、従来のETL処理を排除し、データ移行に伴うコストや複雑さを解消します。
- アップデート日付: 2024/12/01
- 特徴:
  - AWS Clean Roomsは、AWSやSnowflake上のデータセットを活用する企業間で、安全にコラボレーションできるようにします。
  - 従来のデータ移行を避け、データ共有を行わずに、データの整合性を保ちながら共同作業が可能です。
  - 対応するサービス: Snowflake, Amazon Athena, Amazon S3
  - オプション機能: データを移動したりコピーしたりせずに、最新のデータを基にしたコラボレーションが可能。
- 利用ケース:
  - メディアパブリッシャーがAmazon S3に保存されたデータと、広告主がSnowflakeに保存されたデータを共同で分析し、広告支出を評価する。
- 注意事項:
  - 現時点では、利用できるAWSリージョンが限られている可能性があるため、対応リージョンの確認が必要です。
- 対応リージョン: 東京リージョン対応
- 簡単な利用手順の概要
  - 手順1: AWS Clean Roomsにサインインし、必要なデータソースを設定。
  - 手順2: コラボレーションしたい企業を招待し、共同でデータ分析を開始。
- 専門用語:

| カテゴリー      | 専門用語             | 説明                                             |
|-----------------|----------------------|--------------------------------------------------|
| データ管理      | Snowflake             | クラウドデータウェアハウスサービス、分析用のデータを管理するためのツール。 |
| クラウドサービス | Amazon Athena         | サーバーレスのインタラクティブクエリサービス、Amazon S3のデータを分析。 |
| データ移行      | ETL (Extract, Transform, Load) | データの抽出、変換、ロード処理を意味し、データを移動・統合するための手順。 |
| データセキュリティ | データクリーンルーム   | 複数の企業がセキュアな環境でデータを共有し、分析する場所。

### Notes
- [[アップデート] AWS Clean RoomsがデータソースとしてSnowflakeとAmazon Athenaをサポートしました](https://dev.classmethod.jp/articles/aws-clean-rooms-support-multi-clouds-data-sources/)
- Clean Rooms よいじゃん、、

## [Streamline Kubernetes cluster management with new Amazon EKS Auto Mode](https://aws.amazon.com/jp/blogs/aws/streamline-kubernetes-cluster-management-with-new-amazon-eks-auto-mode/)

- 機能概要: Amazon EKS Auto Modeは、Kubernetesクラスタの管理を簡素化する新しい機能で、プロビジョニングから維持管理までを単一のクリックで行えます。これにより、計算、ストレージ、ネットワーキングの管理が自動化され、アプリケーション開発に集中できるようになります。
- アップデート日付: 2024/12/01
- 特徴:
  - クラスタインフラの管理が自動化され、Kubernetesの専門知識がなくても利用可能になります。
  - AWSがインフラ管理を担当し、コスト最適化、スケーラビリティ、セキュリティが強化されます。
  - GPU対応やストレージボリュームサポートなど、Kubernetesの主要機能を自動でサポートします。
  - オプションで、クイック設定またはカスタム設定でクラスタ作成が可能です。
- 利用ケース:
  - Kubernetesインフラ管理の負担を減らし、アプリケーション開発や運用に集中したい場合に利用されます。特に、大規模なクラウド環境でのKubernetesアプリケーション運用を簡素化したいユーザーに最適です。
- 注意事項:
  - 現在、EKS Auto Modeは、Kubernetes 1.29以上を使用するEKSクラスタに対応しており、地域によっては利用できない場合があります。
- 対応リージョン: 東京リージョンを含むすべての商業AWSリージョンに対応（中国リージョンを除く）。
- 簡単な利用手順の概要:
  - 手順1: Amazon EKSコンソールにアクセスし、クラスタ作成を開始。
  - 手順2: クイック設定またはカスタム設定を選択し、必要な情報を入力してクラスタを作成。
- 専門用語:

| カテゴリー     | 専門用語       | 説明                                                           |
|--------------|--------------|----------------------------------------------------------------|
| 自動化         | Auto Mode      | EKSクラスタの計算リソース、ストレージ、ネットワークの管理を自動化する新機能。     |
| インフラ管理   | eksctl        | EKSクラスタの管理ツール。コマンドラインでEKSクラスタの作成や設定が可能。   |

### Notes
- [[アップデート] EKS で Auto Mode が追加されたので試してみた#AWSreInvent](https://dev.classmethod.jp/articles/eks-auto-mode/)
- karpenter との使い分けはどう考えるべきだろうか

## [Use your on-premises infrastructure in Amazon EKS clusters with Amazon EKS Hybrid Nodes](https://aws.amazon.com/jp/blogs/aws/use-your-on-premises-infrastructure-in-amazon-eks-clusters-with-amazon-eks-hybrid-nodes/)

- 機能概要: Amazon EKS Hybrid Nodesは、オンプレミスおよびエッジインフラストラクチャをAWSクラウド内のEKSクラスターに接続し、ハイブリッド環境でのKubernetes管理を統一する新機能です。これにより、EKSがKubernetesの制御プレーンを管理し、オンプレミスのリソースを効率的に利用できます。
- アップデート日付: 2024/12/01
- 特徴:
  - オンプレミスとクラウド両方のインフラストラクチャをEKSクラスターに接続し、Kubernetesの一貫した運用が可能になる。
  - Amazon EKS on AWS OutpostsやEKS Anywhereとの比較で、オンプレミスハードウェア管理が可能。
  - Amazon EKS機能やAWSサービス（CloudWatch, IAM Roles Anywhereなど）と統合。
  - CiliumやCalico CNIドライバがサポートされ、ネットワーク設定の柔軟性が向上。
- 利用ケース:
  - 企業がオンプレミスのインフラをクラウドのEKSクラスターに統合し、ハイブリッド環境で一貫したKubernetes管理を行うケース。
  - 既存のオンプレミスインフラを活用し、クラウドにおけるスケールと可用性を享受する場合。
- 注意事項:
  - 対応するOSは、Amazon Linux 2023、Ubuntu 20.04/22.04/24.04、RHEL 8/9のいずれかが必要。
  - EKS Hybrid Nodesは、AWS GovCloud（US）リージョンおよび中国リージョンには未対応。
- 対応リージョン: 東京リージョン対応
- 簡単な利用手順の概要:
  - 手順1: EKSクラスターを作成し、オンプレミスのノードおよびポッド用のCIDRブロックを指定。
  - 手順2: ハイブリッドノードをEKSクラスターに接続するためのCLIツール「nodeadm」を使用してインストール。
  - 手順3: 必要なCNIドライバ（CiliumやCalico）をインストールして、ノードを準備。
- 専門用語:

| カテゴリー           | 専門用語          | 簡潔な説明                                                       |
|--------------------|-----------------|---------------------------------------------------------------|
| オーケストレーション | CNI (Container Network Interface) | コンテナ間のネットワーク接続を管理するための標準インターフェース。               |

### Notes
- [EKSハイブリッドノードの一般提供開始。EKSのコントロールプレーンのみをAWSに移譲できるようになりました！](https://dev.classmethod.jp/articles/eks-hybrid-node/)
- 待ってたやつ
- 後で試す(VPC Endpoint も併用したい)

## [New APIs in Amazon Bedrock to enhance RAG applications, now available](https://aws.amazon.com/jp/blogs/aws/new-apis-in-amazon-bedrock-to-enhance-rag-applications-now-available/)

## [New APIs in Amazon Bedrock to enhance RAG applications, now available](https://aws.amazon.com/blogs/aws/new-apis-in-amazon-bedrock-to-enhance-rag-applications-now-available/)

- 機能概要: Amazon Bedrock Knowledge Basesでは、カスタムコネクタやストリーミングデータの取り込みをサポートし、Retrieval Augmented Generation（RAG）を活用することで、より精度の高い生成AIアプリケーションの開発を支援します。これにより、データの更新や削除が効率的に行え、リアルタイムでのデータアクセスが可能になります。また、Rerank APIを使用することで、RAGアプリケーションのレスポンス精度を向上させます。
- アップデート日付: 2024/12/01
- 特徴:
  - カスタムコネクタとストリーミングデータの取り込みにより、データの同期が効率化され、データの更新や削除が迅速に行えるようになりました。
  - レスポンスの精度を向上させるために、Rerank APIを使用して、最も関連性の高いコンテンツを優先的に提供することが可能です。
  - Amazon Rerank 1.0とCohere Rerank 3.5モデルに対応。
  - 独立して使用可能なRerank APIも提供されており、Amazon Bedrock Knowledge Basesを利用していなくても活用できます。
- 利用ケース:
  - チャットボットやエンタープライズ検索などのRAGベースの生成AIアプリケーションで、データのリアルタイム更新や精度の高い回答が求められる場合に役立ちます。
  - 複雑なクエリに対応するため、Rerank APIを活用して関連性の高い文書を優先的に提供するシナリオにも最適です。
- 注意事項:
  - Rerank APIの利用には、特定のリージョン（US West, Canada, Europe, Asia Pacific）での対応が必要です。東京リージョンでも利用可能です。
  - データ同期には時間がかかる場合があり、初期設定時には数分から数時間を要することがあります。
- 対応リージョン: 東京リージョン対応有

- 簡単な利用手順の概要
  - 手順1: Amazon BedrockコンソールまたはAWS SDKを使用してカスタムコネクタとストリーミングデータの取り込みを設定。
  - 手順2: 必要なモデルへのアクセスをリクエストし、データソースを同期。
  - 手順3: Rerank APIを使用して、取得したデータをクエリに基づいて優先順位付け。

- 専門用語:

| カテゴリー        | 専門用語               | 説明                                                                 |
|------------------|-----------------------|----------------------------------------------------------------------|
| サービス        | Rerank API             | 取得したデータの優先順位を再調整するAPIで、クエリに基づいて最も関連性の高いコンテンツを優先的に提供。   |
| データ操作      | カスタムコネクタ       | 特定のデータソースから直接データを取り込むための設定。                                                      |

### Notes
- [[アップデート] Amazon BedrockにRerankモデルが追加されました #AWSreInvent](https://dev.classmethod.jp/articles/update-amazon-bedrock-rerank-model-added-aws-reinvent/)
- 後で検証
- 調査: リランキングはどう実装しているのだろうか、ラベルデータはどう集めている、トレーニング

## [Securely share AWS resources across VPC and account boundaries with PrivateLink, VPC Lattice, EventBridge, and Step Functions](https://aws.amazon.com/jp/blogs/aws/securely-share-aws-resources-across-vpc-and-account-boundaries-with-privatelink-vpc-lattice-eventbridge-and-step-functions/)

- 機能概要: AWSリソース（EC2インスタンスやECS、EKSコンテナサービス、HTTPSサービスなど）をVPCやAWSアカウントを超えて安全に共有し、EventBridgeやStep Functionsを利用してイベント駆動型アプリケーションを構築したり、ワークフローをオーケストレーションする機能が提供されます。これにより、既存のレガシーシステムをモダンなクラウドネイティブアプリケーションと統合することが可能になります。
- アップデート日付: 2024/12/01
- 特徴:
  - VPC LatticeやAWS PrivateLinkを基盤に、複数のAWSアカウント間でリソースの共有を実現。
  - EventBridgeやStep Functionsと連携することで、プライベートHTTPSサービスのイベント駆動型アーキテクチャの構築が容易に。
  - 新たに、プライベートリソースにアクセスするための簡素化された手段が提供され、既存のLambdaやSQSの統合を除去可能。
  - オプション機能として、複数のサービスを一括で管理できるリソース共有の作成が可能。
- 利用ケース:
  - 複数のAWSアカウント間でEC2インスタンスやECS、EKSなどのリソースを共有して、イベント駆動型アプリケーションを構築。
  - プライベートリソースへのアクセスをEventBridgeやStep Functionsで行い、ワークフローの自動化や統合を実現。
- 注意事項:
  - リソースの共有にはAWS Resource Access Manager (RAM) を使用。
  - データ転送に対するGB単位の課金が適用される。
  - この機能は一部のAWSリージョンで利用可能。
- 対応リージョン: 東京リージョンは対応しており、21のAWSリージョンで利用可能。
- 簡単な利用手順の概要
  - 手順1: VPCコンソールからリソースゲートウェイを作成。
  - 手順2: リソース設定を行い、リソースの共有を設定。
  - 手順3: 他のアカウントでリソースをアクセスし、EventBridgeやStep Functionsを利用してサービスを接続。

### 専門用語:

| カテゴリー          | 専門用語           | 説明                                                       |
|---------------------|--------------------|------------------------------------------------------------|
| VPCリソース管理    | Resource Owner VPC  | リソースを共有するVPC。リソースゲートウェイを作成し、共有を管理。         |
| VPCリソース管理    | Resource Gateway    | リソースにアクセスするためのエントリーポイント。複数のリソースを提供。     |
| リソース共有      | Resource Configuration | リソースゲートウェイを通じてアクセス可能なリソースを定義。                   |
| リソース管理      | Resource Consumer    | リソースを消費する側のユーザー。例えば、サービスにアクセスして利用。      |
| ネットワーク      | Service Network      | リソースの共有を行うために使用するネットワーク。                           |
| イベント駆動型アーキテクチャ | EventBridge           | イベントの配信および管理サービス。プライベートリソースへのアクセスも可能。   |
| ワークフロー管理  | Step Functions       | 複数のAWSサービスを連携させ、ワークフローをオーケストレーションするサービス。 |

### Notes
- [【アップデート】AWS PrivateLinkでNLBを介さずにVPC内リソースにアクセスできるようになりました！ #AWSreInvent](https://dev.classmethod.jp/articles/privatelink-vpc-endpoint-direct-access-update/)

## [New RAG evaluation and LLM-as-a-judge capabilities in Amazon Bedrock](https://aws.amazon.com/jp/blogs/aws/new-rag-evaluation-and-llm-as-a-judge-capabilities-in-amazon-bedrock/)

- 機能概要: Amazon Bedrockでは、RAG評価とLLM-as-a-judgeという2つの新機能が追加され、生成AIアプリケーションのテストと改善が効率化されます。これにより、生成AIアプリケーションの評価が自動化され、フィードバックループが短縮され、開発のスピードが向上します。
- アップデート日付: 2024/12/01
- 特徴:
  - RAG評価（Retrieval Augmented Generation）を使用して、知識ベースに基づくアプリケーションの評価を自動で行い、設定の最適化を支援します。
  - LLM-as-a-judgeは、ヒューマンエバリュエーションを模倣する評価を、コストと時間を削減しながら提供します。
  - これらの機能は、正確さ、役立ち度、責任あるAIの基準（拒否応答や有害性など）を含む複数の評価基準を提供します。
  - 評価結果は自然言語での説明と0から1のスコア形式で提供され、誰でも理解できるようになっています。
- 利用ケース:
  - AI-poweredアプリケーションのテストと評価を効率化し、生成AIシステムの性能改善を支援。
  - モデルの選定や設定調整を短期間で行い、品質改善を進める。
- 注意事項:
  - 現在、英語コンテンツが最適化されており、他の言語もサポートされているが、対応言語には制限がある。
  - 対応リージョンでのみ利用可能（東京リージョンも対応）。
- 対応リージョン: 東京リージョン（Asia Pacific (Tokyo)）
- 簡単な利用手順の概要
  - 手順1: Amazon Bedrockコンソールで「Evaluations」セクションにアクセスし、評価を作成。
  - 手順2: 評価の設定を選択（RAG評価またはLLM-as-a-judgeを選択）し、使用する評価モデルを選定。
  - 手順3: 評価データセット（JSONLファイル）をS3にアップロードし、結果の保存先を設定。
  - 手順4: 評価結果を確認し、生成されたスコアとフィードバックを分析。

| カテゴリー       | 専門用語                        | 説明                                                                                           |
|----------------|--------------------------------|------------------------------------------------------------------------------------------------|
| モデル評価     | LLM-as-a-judge                 | 人間の評価者のように動作し、AIモデルの応答を評価する機能。                                           |
| 評価基準       | 正確さ（Correctness）            | モデルの応答が正しいかどうかを評価する基準。                                                           |
| 評価基準       | 役立ち度（Helpfulness）          | 応答がどれだけ有用であるかを評価する基準。                                                           |
| 評価基準       | 有害性（Harmfulness）            | 応答が有害でないかを評価する基準。                                                                 |

### Notes
- 後で検証
- [[新機能] Amazon Bedrock Knowledge BasesにRAG評価機能が追加されました(プレビュー版) #AWSreInvent](https://dev.classmethod.jp/articles/amazon-bedrock-knowledge-bases-rag-evaluation-preview/)

## [Amazon Bedrock Knowledge Bases now provides auto-generated query filters for improved retrieval](https://aws.amazon.com/jp/about-aws/whats-new/2024/12/amazon-bedrock-knowledge-bases-auto-generated-query-filters-improved-retrieval/)

- 機能概要: Amazon Bedrock Knowledge Basesは、Retrieval-Augmented Generation (RAG) ワークフローを活用して、ユーザーが独自のGenAIアプリケーションにコンテキスト情報を取り込む際に、精度が高く低遅延で安全な検索結果を提供します。今回のアップデートにより、自動生成されたクエリフィルターにより、検索結果の精度が向上し、手動で複雑なフィルター式を作成することなく関連するドキュメントを取得できます。
- アップデート日付: 2024/12/01
- 特徴:
  - 自動生成されたクエリフィルターにより、検索結果の精度が向上し、ドキュメントのメタデータを基にしたフィルタリングが可能になります。
  - 以前の手動メタデータフィルタリング機能を拡張した形で、特定の属性やコンテンツを持つドキュメントの取得を簡素化します。
  - RAGアプリケーションが複数のドキュメントからユーザーの問い合わせに対して検索を行い、特定の属性に基づいたフィルターを自動で適用します。
  - 特に「ワシントン州でクレームを提出する方法」などの具体的なクエリに対して、州名を自動的にフィルターとして適用します。
- 利用ケース:
  - 特定の地域や条件に関連する情報を素早くフィルタリングして取得したい場合に有用です。例えば、ユーザーが「ワシントン州に関するクレーム」の情報を求めているときに、地域を自動的に適用して、関連するドキュメントのみを表示することができます。
- 注意事項:
  - 現時点では、US East (N. Virginia), US West (Oregon), Asia Pacific (Tokyo), Asia Pacific (Seoul), Europe (Frankfurt), Europe (Zurich), AWS GovCloud (US-West) の各リージョンでサポートされています。
- 対応リージョン: 東京リージョン (Asia Pacific (Tokyo)) に対応
- 簡単な利用手順の概要:
  - 手順1: Amazon Bedrock Knowledge Basesの設定画面に移動し、クエリフィルター自動生成機能を有効化します。
  - 手順2: 必要に応じて、対象のドキュメントやデータソースを指定し、検索を実行します。
  - 手順3: クエリに対して自動で適用されるフィルター結果を確認し、最適なドキュメントを取得します。

| カテゴリー   | 専門用語                       | 説明                                                                                                                                     |
|-------------|--------------------------------|------------------------------------------------------------------------------------------------------------------------------------------|
| 機能        | クエリフィルター自動生成        | ユーザーのクエリに関連するドキュメントを自動的にフィルタリングし、より精度高く情報を提供する機能。                                                   |
| サービス    | Amazon Bedrock Knowledge Bases  | Amazonのクラウドサービスで、GenAIアプリケーションに必要なデータからコンテキスト情報を抽出し、高精度で生成・取得できるプラットフォーム。                  |

### Notes
- 後で検証
- [Knowledge Bases for Amazon Bedrock がメタデータフィルタリングをサポートし検索精度向上](https://aws.amazon.com/jp/blogs/news/knowledge-bases-for-amazon-bedrock-now-supports-metadata-filtering-to-improve-retrieval-accuracy/)
- The implicitFilterConfiguration is specified in the vectorSearchConfiguration of the Retrieve request body. Include the following fields: これか？

## [Amazon Bedrock Knowledge Bases now supports streaming responses](https://aws.amazon.com/jp/about-aws/whats-new/2024/12/amazon-bedrock-knowledge-bases-streaming-retrieveandgeneratestream-api/)

- 機能概要: Amazon Bedrock Knowledge Basesは、企業のデータソースからのコンテキスト情報を組み込んだ高精度で低遅延なカスタムGenAIアプリケーションを作成するための完全管理型エンドツーエンドのRetrieval-Augmented Generation（RAG）ワークフローを提供します。この度、RetrieveAndGenerateStream APIが新たにサポートされ、Bedrock Knowledge Baseの顧客は、Large Language Model（LLM）によって生成される応答を、完全な応答を待つことなく、生成中にストリーミングで受け取ることができるようになりました。
- アップデート日付: 2024/12/01
- 特徴:
  - RetrieveAndGenerateStream APIにより、LLMからの応答をストリーミングで受け取れるようになり、応答開始までの遅延が減少します。
  - 従来、RAGワークフローの最終段階では、応答生成に数秒の遅延が発生していましたが、この新機能により、最初の応答が迅速に提供され、ユーザー体験が向上します。
  - 現在、すべてのAmazon Bedrock Knowledge Baseリージョンでサポートされています。
  - ドキュメントで詳細を確認可能。
- 利用ケース:
  - ラテンシーが重要なアプリケーションにおいて、生成中に応答を受け取りながらインタラクティブに処理を進める場合に役立ちます。
- 注意事項:
  - まだすべてのユースケースに最適化されていない可能性があり、特に非常に大規模なデータセットを扱う場合に遅延や精度の問題が発生することがあります。
- 対応リージョン: 東京リージョンにも対応しています。
- 簡単な利用手順の概要:
  - 手順1: RetrieveAndGenerateStream APIを使用するために、Amazon Bedrock Knowledge Baseでの設定を行います。
  - 手順2: APIを呼び出し、生成される応答をストリーミングで受け取ります。
  - 手順3: 必要に応じて、リアルタイムで応答を処理し、アプリケーションに反映させます。

| カテゴリー      | 専門用語       | 簡潔な説明                                             |
|----------------|----------------|--------------------------------------------------------|
| API            | RetrieveAndGenerateStream | ストリーミングで応答を受け取るための新しいAPI。                         |

- support of RetrieveAndGenerateStream API in Bedrock Knowledge Bases

### Notes
- [[アップデート] Amazon Bedrock Knowledge bases がストリーミングレスポンスをサポートしました #AWSreInvent](https://dev.classmethod.jp/articles/amazon-bedrock-knowledge-bases-streaming-retrieveandgeneratestream-api/)
- The feature currently only works with Anthropic Claude 3.5 Sonnet.


## [AWS Marketplace now offers EC2 Image Builder components from independent software vendors](https://aws.amazon.com/jp/about-aws/whats-new/2024/12/aws-marketplace-ec2-image-builder-components-software-vendors/)

- 機能概要: AWS Marketplaceで独立系ソフトウェアベンダー（ISV）のEC2 Image Builderコンポーネントが提供され、Amazon Machine Image（AMI）のビルドプロセスを簡素化できるようになりました。これにより、ISVのコンポーネントを簡単に検索、購読、そしてゴールデンイメージに組み込むことができ、最新のバージョンのコンポーネントがAWS Marketplaceでリリースされると自動的に更新されます。
- アップデート日付: 2024/12/01
- 特徴:
  - AWS MarketplaceでISVのEC2 Image Builderコンポーネントを見つけて購読し、ゴールデンイメージに組み込むことが可能に。
  - カスタムコードを記述することなく、システムを最新の状態に保つ自動更新機能。
  - Datadog、Dynatrace、Fortinet、OpenVPN、Ciscoなど、人気のあるプロバイダーのソフトウェアがサポートされている。
  - ゴールデンイメージを組織内で共有するために、購読したコンポーネントの権限をAWSアカウント間で配布することが可能。
- 利用ケース:
  - 企業がAMIsのビルドプロセスを効率化したい場合、特にセキュリティ、ガバナンス、コンプライアンスの要件を満たすためのソフトウェアを組み込みたい場合。
  - 自動更新機能を活用して、システムを常に最新の状態に保ち、手動での更新作業を削減する場合。
- 注意事項:
  - ゴールデンイメージをAWSアカウント間で共有する際は、購読したコンポーネントのライセンスが必要。
- 対応リージョン: 東京リージョン対応あり
- 簡単な利用手順の概要
  - 手順1: AWS MarketplaceでEC2 Image Builderコンポーネントを検索して購読
  - 手順2: Image Builderコンソールでコンポーネントをゴールデンイメージに組み込む
  - 手順3: 自動更新設定を行い、システムを最新状態に保つ
- 専門用語:

| カテゴリ | 専門用語 | 説明 |
| --- | --- | --- |
| サービス | EC2 Image Builder | Amazon EC2インスタンスのイメージ（AMI）を自動で作成するサービス |
| サービス | AWS Marketplace | ソフトウェア、サービス、AMIを販売・購入できるAWSのオンラインマーケットプレイス |
| コンポーネント | ISV (Independent Software Vendor) | 独立系ソフトウェアベンダー、AWS Marketplaceで提供されるサードパーティのソフトウェアの提供者 |
| ガバナンス | ゴールデンイメージ | 設定、セキュリティ、ポリシーが規定された標準的なイメージ、全組織で一貫して使用されることを目的に作成 |

### Notes
- [AWS Marketplace で EC2 Image Builder で利用できるコンポーネントが提供されるようになりました #AWSreInvent](https://dev.classmethod.jp/articles/marketplace-ec2-image-builder-component/)

## [New Amazon EC2 P5en instances with NVIDIA H200 Tensor Core GPUs and EFAv3 networking](https://aws.amazon.com/jp/blogs/aws/new-amazon-ec2-p5en-instances-with-nvidia-h200-tensor-core-gpus-and-efav3-networking/)

- 機能概要: Amazon EC2 P5enインスタンスは、NVIDIA H200 Tensor Core GPUsと4世代Intel Xeon Scalableプロセッサを搭載し、機械学習（ML）トレーニングや推論のパフォーマンスを向上させます。これにより、MLや深層学習、生成AI、リアルタイムデータ処理、HPCアプリケーションにおける効率が大幅に向上します。
- アップデート日付: 2024/12/02
- 特徴:
  - NVIDIA H200 Tensor Core GPUsと4世代Intel Xeon Scalableプロセッサによる強力な性能。
  - PCIe Gen5を使用し、CPUとGPU間の帯域幅が最大4倍に向上。P5インスタンスに比べて最大35％のレイテンシ改善を実現。
  - 最大3200 GbpsのElastic Fabric Adapter（EFAv3）によるネットワーク性能向上。
  - 高いスループットを提供し、深層学習、生成AI、HPCなどの大規模なワークロードに最適化された性能を提供。
  - AWS Management ConsoleやAWS CLIを使用して、EC2 Capacity Blocksを予約してインスタンスを実行可能。
- 利用ケース:
  - 大規模なMLトレーニングや推論、特に生成AIや深層学習モデルにおける効率的なデータ処理と低レイテンシの実現。
  - HPCワークロード（天気予測や金融モデリング、製薬研究など）でのシミュレーションや解析。
- 注意事項:
  - EC2 Capacity Blocksは最大8週間前に予約可能で、事前購入したブロックでのみインスタンスを実行できます。
  - 現在利用可能なリージョンはUS East (Ohio), US West (Oregon), Asia Pacific (Tokyo)など。
- 対応リージョン: 東京リージョンを含むUS East (Ohio), US West (Oregon), Asia Pacific (Tokyo), US East (Atlanta) Local Zone (us-east-1-atl-2a)。
- 簡単な利用手順の概要
  - 手順1: EC2 Capacity Blocksを選択して、必要な容量と期間を指定。
  - 手順2: EC2 Capacity Blockの購入後、AWS Management Console、CLI、またはSDKを使用してインスタンスを起動。
  - 手順3: AWS CLIでのサンプルコマンドを使用して、EFAv3の利点を最大化したインスタンスの起動。
  
- 専門用語:

| カテゴリー | 専門用語            | 簡潔な説明                                            |
|------------|---------------------|-----------------------------------------------------|
| ハードウェア | NVIDIA H200 Tensor Core GPUs | 高速な計算処理を提供するNVIDIAの次世代GPU。深層学習に最適。   |
| ハードウェア | Intel Xeon Scalableプロセッサ | 高性能なサーバー用プロセッサ。大規模計算処理向け。             |
| ネットワーク | PCIe Gen5            | CPUとGPU間の通信帯域幅を大幅に向上させるインターフェース。        |
| ネットワーク | Elastic Fabric Adapter (EFA) | AWSでの低レイテンシ、スケーラブルなネットワーク通信を提供。         |
| ストレージ   | Amazon FSx for Lustre | 高速ストレージシステムで、大規模なデータ解析に使用。             |

### Notes
- [NVIDIA H200 Tensor Core GPU を搭載した EC2 インスタンスが東京リージョンに初登場 P5en インスタンス一般提供開始されました](https://dev.classmethod.jp/articles/nvidia-h200-tensor-core-gpu-ec2-p5en/)
- [AWS Nitro System](https://docs.aws.amazon.com/ja_jp/ec2/latest/instancetypes/ec2-nitro-instances.html)
- EFAv3 using Nitro v5
- ローカルストレージのパフォーマンスが上がってるので推論処理も効果あるよ

# 20241125(JST) 編集

## [Amazon SageMaker Inference now supports G6e instances](https://aws.amazon.com/blogs/machine-learning/amazon-sagemaker-inference-now-supports-g6e-instances/)

- 機能概要: Amazon SageMakerが、NVIDIAのL40S Tensor Core GPUを搭載したG6eインスタンスのサポートを開始しました。これにより、開発者はコスト効率の良い高性能なGPUインスタンスを利用して、さまざまな生成AIモデルを推論することが可能になりました。
- アップデート日付: 2024/11/22
- 特徴:
  - G6eインスタンスは、最大384GBのGPUメモリを搭載し、大規模言語モデルを単一GPUノードで処理する能力を提供します。
  - G5インスタンスと比較して、G6eインスタンスは高いパフォーマンスを提供し、コスト効率が向上しています。
  - サポートするモデルには、Llama 3.2 11B VisionやLlama 2 13B、Qwen 2.5 14Bなどが含まれます。
  - 最大8GPUを持つインスタンスで、モデルサイズが最大90Bパラメータまで対応可能です。
- 利用ケース:
  - チャットボットや会話型AI
  - テキスト生成や要約
  - 画像生成や視覚モデル
- 注意事項:
  - G6eインスタンスは、コスト効率とパフォーマンスを重視する高スループットな推論ワークロード向けに最適化されていますが、大規模なモデルを扱う場合はインスタンスの選定に注意が必要です。
- 対応リージョン: 東京リージョンに対応
- 簡単な利用手順の概要:
  - 手順1: AWSアカウントとIAMロールを準備し、Amazon SageMaker StudioでG6eインスタンスを使ってモデルをデプロイします。

| カテゴリー           | 専門用語                | 説明                                                             |
|--------------------|------------------------|------------------------------------------------------------------|
| GPUインスタンス      | G6eインスタンス         | NVIDIA L40S Tensor Core GPUを搭載した、生成AI向けの高性能インスタンス。 |
| モデル               | Llama 3.2 11B Vision   | 高性能な生成AIモデルの一つで、画像生成や視覚認識に使用される。          |
| パラメータ数         | 90Bパラメータモデル     | G6eインスタンスは最大90Bのパラメータを持つ大規模なモデルをサポート。       |

### Notes
- G6e instances powered by NVIDIA’s L40S Tensor Core GPUs
- each GPU providing 48 GB of high bandwidth memory
- Up to 400 Gbps of networking throughput
- G6e instances are ideal for fine-tuning and deploying open large language models