---
title: "Amazon SageMaker HyperPod -- Checkpointless Training"
emoji: "ğŸŒµ"
type: "tech"
topics: ["AWS", "SageMaker", "HyperPod", "åˆ†æ•£å­¦ç¿’", "Checkpointless"]
published: false
---

## ã¯ã˜ã‚ã«

å¤§è¦æ¨¡åˆ†æ•£å­¦ç¿’ã«ãŠã„ã¦ã€ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢éšœå®³ã¯é¿ã‘ã‚‰ã‚Œãªã„èª²é¡Œã§ã™ã€‚1000 GPU ã§å­¦ç¿’ä¸­ã€ãŸã£ãŸ 1 å°ã®éšœå®³ã§å…¨ä½“ãŒåœæ­¢ã—ã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ã®å¾©æ—§ã«æ•°ååˆ†ã‚’è¦ã—ã€ãã®é–“ã€æ®‹ã‚Š 999 å°ã® GPU ã¯ã‚¢ã‚¤ãƒ‰ãƒ«çŠ¶æ…‹ã¨ãªã‚Šã¾ã™ã€‚

Amazon SageMaker HyperPod ã® **Checkpointless Training** ã¯ã€ã“ã®å•é¡Œã‚’è§£æ±ºã—ã¾ã™ã€‚ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã«æ›¸ãè¾¼ã‚€ã“ã¨ãªãã€GPU ãƒ¡ãƒ¢ãƒªå†…ã®å†—é•·ãƒ¬ãƒ—ãƒªã‚«ã‹ã‚‰æ•°åˆ†ä»¥å†…ã«è‡ªå‹•å¾©æ—§ã—ã€95% ä»¥ä¸Šã® goodputï¼ˆéšœå®³ã‚„ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã§å¤±ã‚ã‚Œãªã„å®ŸåŠ¹å­¦ç¿’æ™‚é–“ã®å‰²åˆï¼‰ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

æœ¬è¨˜äº‹ã§ã¯ã€ã“ã®é©æ–°çš„ãªæŠ€è¡“ã®ä»•çµ„ã¿ã€3 ã¤ã®æœ€é©åŒ–ãƒˆãƒ©ãƒƒã‚¯ã€å°å…¥æ–¹æ³•ã«ã¤ã„ã¦ã€2026 å¹´ 2 æœˆæ™‚ç‚¹ã®æƒ…å ±ã‚’ã‚‚ã¨ã«è©³ã—ãè§£èª¬ã—ã¾ã™ã€‚

:::message
**å®Ÿéš›ã«è©¦ã™éš›ã®æ¨å¥¨ãƒªãƒã‚¸ãƒˆãƒª**: Checkpointless Training ã‚’è©¦ã™éš›ã¯ã€AWS ã® GenAI Frameworks team ãŒç®¡ç†ã™ã‚‹ [`awsome-distributed-training`](https://github.com/aws-samples/awsome-distributed-training) ãƒªãƒã‚¸ãƒˆãƒªãŠã‚ˆã³å…¬å¼ã® [`sagemaker-hyperpod-checkpointless-training`](https://github.com/aws/sagemaker-hyperpod-checkpointless-training) ãƒªãƒã‚¸ãƒˆãƒªã®åˆ©ç”¨ã‚’å¼·ãæ¨å¥¨ã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã«ã¯å®Ÿç¸¾ã®ã‚ã‚‹ã‚³ãƒ¼ãƒ‰ã¨ã‚µãƒ³ãƒ—ãƒ«ãŒå«ã¾ã‚Œã¦ãŠã‚Šã€ç’°å¢ƒæ§‹ç¯‰æœŸé–“ã‚’å¤§å¹…ã«çŸ­ç¸®ã§ãã¾ã™ã€‚ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã¨ã—ã¦ã¯ [AI on SageMaker HyperPod](https://awslabs.github.io/ai-on-sagemaker-hyperpod/) ã‚‚ãŠã™ã™ã‚ã§ã™ã€‚
:::

:::message alert
æœ¬è¨˜äº‹ã¯ 2026 å¹´ 2 æœˆæ™‚ç‚¹ã®å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã€ãªã©ã«åŸºã¥ãèª¿æŸ»è¨˜äº‹ã§ã™ã€‚é–“é•ã£ã¦ã„ã‚‹å¯èƒ½æ€§ã‚‚ã‚ã‚‹ãŸã‚å¿…ãšæœ€æ–°ã®å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ­£ã¨ã—ã¦ç¢ºèªã—ã¦ãã ã•ã„ã€‚
:::

![](https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2025/11/18/2025-sageamker-hyperpod-checkpointless-training.gif)
*Checkpointless Training ã®å‹•ä½œã‚¤ãƒ¡ãƒ¼ã‚¸ï¼ˆå‡ºå…¸: AWS å…¬å¼ãƒ–ãƒ­ã‚°ï¼‰*

## æ¦‚è¦

Checkpointless Training ã¯ã€GPU ãƒ¡ãƒ¢ãƒªå†…ã«ä¿æŒã•ã‚ŒãŸå†—é•·ãƒ¬ãƒ—ãƒªã‚«ã‚’ä½¿ã£ã¦éšœå®³ã‹ã‚‰å¾©æ—§ã™ã‚‹æŠ€è¡“ã§ã™ã€‚å¾“æ¥ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ™ãƒ¼ã‚¹ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¨ã¯ç•°ãªã‚Šã€ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã¸ã®æ›¸ãè¾¼ã¿ã‚’ã»ã¼ä¸è¦ã«ã™ã‚‹ã“ã¨ã§ã€å­¦ç¿’ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’è§£æ¶ˆã—ã¾ã™ã€‚

### ä¸»è¦ãªç‰¹å¾´

- **goodputï¼ˆå®ŸåŠ¹å­¦ç¿’æ™‚é–“ã®å‰²åˆï¼‰**: æ•°åƒå°è¦æ¨¡ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã§ 95% ä»¥ä¸Šã‚’é”æˆ[^1]
- **å¾©æ—§æ™‚é–“**: éšœå®³ã‹ã‚‰æ•°åˆ†ä»¥å†…ã«è‡ªå‹•å¾©æ—§[^1]
- **å¯¾å¿œç’°å¢ƒ**: [HyperPod EKS ç’°å¢ƒ](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-eks-checkpointless.html)
- **ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯è¦ä»¶**: NVIDIA NeMo Framework ãŒå¿…é ˆï¼ˆè©³ç´°ã¯å¾Œè¿°ã® details ãƒ–ãƒ­ãƒƒã‚¯å‚ç…§ï¼‰

Llamaã€Qwenã€DeepSeek ã®ã‚ˆã†ãªæ¨™æº–çš„ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹å ´åˆã€ã™ã§ã«ç”¨æ„ã•ã‚Œã¦ã„ã‚‹ãƒ¬ã‚·ãƒ”ã‚’ä½¿ãˆã°ã‚³ãƒ¼ãƒ‰å¤‰æ›´ã‚¼ãƒ­ã§å§‹ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚å¿…è¦ãªã®ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚’æŒã¡è¾¼ã¿ã€æœ€å°ãƒãƒ¼ãƒ‰æ•°ã¨æœ€å¤§ãƒãƒ¼ãƒ‰æ•°ã‚’è¨­å®šã™ã‚‹ã ã‘ãªã®ã§ã€å¤šãã®ã‚±ãƒ¼ã‚¹ã§ã¯ç‰¹ã«è£å´ã®å®Ÿè£…ã‚’æ°—ã«ã™ã‚‹ã“ã¨ã¯ãªã„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ä»¥ä¸‹ã« Llama3 ã®ã‚µãƒ³ãƒ—ãƒ«ãŒã‚ã‚Šã¾ã™ã€‚

https://github.com/aws/sagemaker-hyperpod-checkpointless-training/tree/main/examples/llama3/launch

re: Invent 2024 ã§æœ¬æ©Ÿèƒ½ã«é–¢ã™ã‚‹ã‚»ãƒƒã‚·ãƒ§ãƒ³ãŒã‚ã‚Šã€å…·ä½“çš„ãª Salesforce ã‹ã‚‰ã®åˆ©ç”¨äº‹ä¾‹ãŒç´¹ä»‹ã•ã‚Œã¦ã„ã¾ã—ãŸã€‚

https://youtu.be/r9J10L2K0F4

::::details ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯è¦ä»¶

2026 å¹´ 2 æœˆæ™‚ç‚¹ã§ã¯ã€[**NVIDIA NeMo Framework ãŒå¿…é ˆ**](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-eks-checkpointless.html)ã§ã™ã€‚ã“ã‚Œã¯ä»¥ä¸‹ã®æŠ€è¡“çš„éšå±¤ã«ã‚ˆã‚‹åˆ¶ç´„ã§ã™ã€‚

```text
[AWS å®Ÿè£…] Checkpointless Training
    â†“ (NeMo API ã«ä¾å­˜)
[ãƒ©ãƒƒãƒ‘ãƒ¼å±¤] NeMo Framework
    â†“ (Megatron-Core ã‚’ãƒ©ãƒƒãƒ—)
[ã‚³ã‚¢æŠ€è¡“] Megatron-Core ã® num_distributed_optimizer_instances
    â†“ (å®Ÿç¾æ‰‹æ®µ)
[åŸç†] GPU ãƒ¡ãƒ¢ãƒªå†…å†—é•·ãƒ¬ãƒ—ãƒªã‚«
```

1. **Megatron-Core ã®å†—é•·ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹æ©Ÿèƒ½ãŒã‚³ã‚¢æŠ€è¡“**: Checkpointless Training ã¯ Megatron-Core ã® `num_distributed_optimizer_instances >= 2` ã¨ã„ã†æ©Ÿèƒ½ã«ä¾å­˜ã—ã¾ã™ã€‚ã“ã®æ©Ÿèƒ½ã«ã‚ˆã‚Šã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶çŠ¶æ…‹ã®å†—é•·ãƒ¬ãƒ—ãƒªã‚«ã‚’ GPU ãƒ¡ãƒ¢ãƒªå†…ã«ä¿æŒã§ãã¾ã™ã€‚
2. **AWS å®Ÿè£…ã® NeMo ä¾å­˜**: AWS ã®å®Ÿè£…ã¯ NeMo ã® PyTorch Lightning çµ±åˆã€PEFT (LoRA) ã‚µãƒãƒ¼ãƒˆã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç®¡ç† API ã«ä¾å­˜ã—ã¦ã„ã‚‹ãŸã‚ã€ç¾æ™‚ç‚¹ã§ã¯ NeMo ãŒå¿…é ˆã§ã™ã€‚
3. **FSDP/DeepSpeed ZeRO ãŒä½¿ãˆãªã„ç†ç”±**: ã“ã‚Œã‚‰ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«ã¯ã€Megatron-Core ã® `num_distributed_optimizer_instances` ã«ç›¸å½“ã™ã‚‹å†—é•·ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹æ©Ÿèƒ½ãŒç¾æ™‚ç‚¹ã§å®Ÿè£…ã•ã‚Œã¦ã„ãªã„ãŸã‚ã€GPU ãƒ¡ãƒ¢ãƒªå†…ã§ã®å†—é•·ãƒ¬ãƒ—ãƒªã‚«ä¿æŒãŒã§ãã¾ã›ã‚“ã€‚å„ãƒ©ãƒ³ã‚¯ãŒä¸€æ„ã®ã‚·ãƒ£ãƒ¼ãƒ‰ã‚’ä¿æŒã™ã‚‹è¨­è¨ˆã®ãŸã‚ã€1 ãƒ©ãƒ³ã‚¯ãŒå¤±ã‚ã‚Œã‚‹ã¨ãã®ã‚·ãƒ£ãƒ¼ãƒ‰ã®çŠ¶æ…‹ã¯å¾©æ—§ä¸å¯èƒ½ã§ã™ã€‚

> Checkpointless training on SageMaker HyperPod is built on top of the NVIDIA NeMo Framework User Guide. You can run checkpointless training with pre-created SageMaker HyperPod recipes. If you're familiar with NeMo, the process of using the checkpointless training recipes is similar. With minor changes, you can start training a model using checkpointless training features that enable you to recover quickly from training faults.

ï¼ˆå‡ºå…¸: [AWS å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-eks-checkpointless.html)ï¼‰
::::

AWS å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯ Checkpointless Training ã‚’ [3 ã¤ã®æœ€é©åŒ–ãƒˆãƒ©ãƒƒã‚¯](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-eks-checkpointless-features.html) ã§èª¬æ˜ã—ã¦ã„ã¾ã™ã€‚

![](https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/hyperpod/hyperpod-checkpointless-optimization-tracks.png)

## (1) Optimized Collective Communication(CC) Initialization

https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-eks-checkpointless-features-communication.html

![](https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/hyperpod/hyperpod-checkpointless-training-libraries.png)


å¾“æ¥ã® NCCL/Gloo ã§ã¯ã™ã¹ã¦ã®ãƒ—ãƒ­ã‚»ã‚¹ãŒ TCPStore ã¨ã„ã†é›†ä¸­ã‚¹ãƒˆã‚¢ã«æ¥ç¶šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã€éšœå®³å¾©æ—§ã®ãŸã³ã«å®Œå…¨ãªåˆæœŸåŒ–ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ç¹°ã‚Šè¿”ã™å¿…è¦ãŒã‚ã‚Šã¾ã—ãŸã€‚ãã—ã¦ãƒ«ãƒ¼ãƒˆãƒ—ãƒ­ã‚»ã‚¹ãŒ SPOF ã§ã—ãŸã€‚1 ã®æœ€é©åŒ–ã§ CC ã®åˆæœŸåŒ–ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’è§£æ¶ˆã—ã€Rootless ã‹ã¤ TCPStoreless ãªåˆæœŸåŒ–æ–¹å¼ã‚’å®Ÿç¾ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šéšœå®³å¾©æ—§æ™‚ã®é€šä¿¡ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒå¤§å¹…ã«å‰Šæ¸›ã•ã‚Œã¾ã™ã€‚

::::details è£œè¶³: TCPStore ã¨ã¯ä½•ã‹

### TCPStore ã®å½¹å‰²

**TCPStore** ã¯ PyTorch Distributed ã«ãŠã‘ã‚‹**åˆ†æ•£ãƒ—ãƒ­ã‚»ã‚¹é–“ã®åˆæœŸåŒ–ãƒ»èª¿æ•´ã®ãŸã‚ã®ä¸­å¤®é›†ç´„å‹ã‚­ãƒ¼ãƒãƒªãƒ¥ãƒ¼ã‚¹ãƒˆã‚¢**ã§ã™ã€‚åˆ†æ•£å­¦ç¿’ã®åˆæœŸåŒ–ãƒ•ã‚§ãƒ¼ã‚ºã§ã€å„ãƒ©ãƒ³ã‚¯ï¼ˆãƒ—ãƒ­ã‚»ã‚¹ï¼‰ãŒäº’ã„ã‚’èªè­˜ã—ã€å¿…è¦ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’å…±æœ‰ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚8,192 ãƒ©ãƒ³ã‚¯è¦æ¨¡ã®å¤§è¦æ¨¡å­¦ç¿’ã§ã¯ã€Master ã¸ã®æ¥ç¶šæ•°ãŒ 8,191 ã«é”ã—ã€åˆæœŸåŒ–ã«æ•°ååˆ†ã‚’è¦ã™ã‚‹ã‚ˆã†ã§ã™ã€‚

**åˆæœŸåŒ–ã‚³ãƒ¼ãƒ‰ä¾‹**

```python
import torch.distributed as dist

dist.init_process_group(
    backend="nccl",
    init_method="tcp://master:29500",  # â† TCPStore ã®ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
    rank=0,
    world_size=8
)
```

ã“ã®ã‚³ãƒ¼ãƒ‰ã¯å†…éƒ¨ã§ TCPStore ã‚’ä½œæˆã—ã€ä»¥ä¸‹ã®ã‚ˆã†ãªä¸­å¤®é›†ç´„å‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§å‹•ä½œã—ã¾ã™ã€‚

```mermaid
graph TB
    subgraph "TCPStore ä½¿ç”¨"
        Master["TCPStore Master<br/>(rank 0)<br/>Port: 29500"]
        R0["rank 0<br/>(Client)"]
        R1["rank 1<br/>(Client)"]
        R2["rank 2<br/>(Client)"]
        RN["rank N<br/>(Client)"]

        R0 -->|TCP æ¥ç¶š| Master
        R1 -->|TCP æ¥ç¶š| Master
        R2 -->|TCP æ¥ç¶š| Master
        RN -->|TCP æ¥ç¶š| Master
    end

    style Master fill: #ffcccc
    style R0 fill: #ccffcc
    style R1 fill: #ccffcc
    style R2 fill: #ccffcc
    style RN fill: #ccffcc
```

### Rootless Configuration ã«ã‚ˆã‚‹æ”¹å–„

Rootless Configuration ã¯ã€ãƒ«ãƒ¼ãƒˆãƒ—ãƒ­ã‚»ã‚¹ã¸ã®ä¾å­˜ã‚’æ’é™¤ã—ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚°ãƒ«ãƒ¼ãƒ—ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã‚’é€šã˜ãŸå¯¾ç§°ã‚¢ãƒ‰ãƒ¬ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚ˆã‚‹åˆ†æ•£åˆæœŸåŒ–æ–¹å¼ã‚’å°å…¥ã—ã¾ã™ã€‚TCPStoreless æœ€é©åŒ–ã¨åˆã‚ã›ã¦ã€é›†ä¸­å‹ã®èª¿æ•´ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’æ’é™¤ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã£ã¦ã€SPOF ã®æ’é™¤ã€ä¸¦åˆ—åˆæœŸåŒ–ã«ã‚ˆã‚‹é«˜é€ŸåŒ–ã€å±€æ‰€çš„ãªéšœå®³ã®å½±éŸ¿ã‚’æœ€å°åŒ–ã€ã—ã¾ã™ã€‚ãŸã ã— TCP ãƒãƒ¼ãƒˆæ¶ˆè²»å¢—åŠ ãªã©ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ãŒã‚ã‚Šã¾ã™ã€‚

```mermaid
graph LR
    subgraph "Rootless æ–¹å¼"
        R0["rank 0"]
        R1["rank 1"]
        RN["rank N"]

        R0 <-->|åˆ†æ•£åˆæœŸåŒ–| R1
        R1 <-.->|åˆ†æ•£åˆæœŸåŒ–| RN
    end

    style R0 fill: #ccffcc
    style R1 fill: #ccffcc
    style RN fill: #ccffcc
```

æ³¨: å…·ä½“çš„ãªé€šä¿¡ãƒˆãƒãƒ­ã‚¸ãƒ¼ã®è©³ç´°ã¯å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«æ˜ç¤ºã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã“ã®å›³ã¯æ¦‚å¿µçš„ãªè¡¨ç¾ã§ã™ã€‚

**å‚è€ƒ**
- [PyTorch Distributed TCPStore ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://pytorch.org/docs/stable/distributed.html#torch.distributed.TCPStore)
- [PyTorch ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰: rendezvous.py](https://github.com/pytorch/pytorch/blob/main/torch/distributed/rendezvous.py) - `_create_c10d_store()` é–¢æ•°
- [PyTorch ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰: distributed_c10d.py](https://github.com/pytorch/pytorch/blob/main/torch/distributed/distributed_c10d.py) - `_store_based_barrier()` é–¢æ•°

::::

### Rootless Configuration ã«ã‚ˆã‚‹è§£æ±ºç­–

HyperPod ã¯ **Rootless ã‹ã¤ TCPStoreless** ãªåˆæœŸåŒ–æ–¹å¼ã‚’å°å…¥ã™ã‚‹ã“ã¨ã§ã€ã“ã‚Œã‚‰ã®å•é¡Œã‚’è§£æ±ºã—ã¦ã„ã¾ã™ã€‚

**æœ‰åŠ¹åŒ–æ–¹æ³•**

```bash
export HPCT_USE_ROOTLESS=1 && \
sysctl -w net.ipv4.ip_local_port_range="20000 65535"
```

**è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**
- `HPCT_USE_ROOTLESS`: 1 ã§æœ‰åŠ¹åŒ–ã€0 ã§ç„¡åŠ¹åŒ–
- `net.ipv4.ip_local_port_range`: åˆ†æ•£é€šä¿¡ç”¨ã«ã‚·ã‚¹ãƒ†ãƒ ãƒãƒ¼ãƒˆç¯„å›²ã‚’æ‹¡å¼µ

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å¤‰æ›´

| å±¤ | ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ | ä¿®æ­£å†…å®¹ |
|---|---|---|
| **PyTorch Distributed API å±¤**<br/>(Python) | `torch.distributed.new_group()`<br/>`torch.distributed.init_process_group()` | TCPStore ä½œæˆã‚’ãƒã‚¤ãƒ‘ã‚¹<br/>ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚°ãƒ«ãƒ¼ãƒ—ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã‚’é€šã˜ã¦å¯¾ç§°ã‚¢ãƒ‰ãƒ¬ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç¶­æŒ |
| **PyTorch ãƒ—ãƒ­ã‚»ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—å±¤**<br/>(Python/C++) | `ProcessGroupNCCL`<br/>`ProcessGroupGloo` | ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šã«åŸºã¥ã„ã¦æœ€é©åŒ–ã‚³ãƒ¼ãƒ‰ãƒ‘ã‚¹ã‚’æ¡ä»¶ä»˜ãã§å‘¼ã³å‡ºã—<br/>In-Process Recovery ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ã‚µãƒãƒ¼ãƒˆ |
| **Third Party ãƒ©ã‚¤ãƒ–ãƒ©ãƒªå±¤**<br/>(C++) | NCCL<br/>Gloo | ã‚µãƒ¼ãƒ‰ãƒ‘ãƒ¼ãƒ†ã‚£ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã® API ã‚’æ‹¡å¼µã—ã€Rootless ãŠã‚ˆã³ Storeless æœ€é©åŒ–ã‚’å®Ÿç¾<br/>ã‚ªãƒªã‚¸ãƒŠãƒ«ã¨æœ€é©åŒ–ãƒ‘ã‚¹ã®æŸ”è»Ÿãªåˆ‡ã‚Šæ›¿ãˆã‚’ã‚µãƒãƒ¼ãƒˆã—å¾Œæ–¹äº’æ›æ€§ã‚’ç¶­æŒ |

### å®Ÿè£…ä¾‹

https://github.com/aws/sagemaker-hyperpod-checkpointless-training/blob/24520b41db0df3495b3946a7b74edaa70fbc339d/examples/llama3/launch/pretrain_llama3_70b_checkpointless_p5.yaml#L111-L113

## (2) Memory-Mapped Data Loading (MMAP)

![](https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/hyperpod/hyperpod-checkpointless-mmap-dataloader.png)

å¾“æ¥ã® DataLoader ã§ã¯ã€ãƒ—ãƒ­ã‚»ã‚¹å†èµ·å‹•æ™‚ã«ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã‚’å†æ§‹ç¯‰ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã€æœ€åˆã®ãƒãƒƒãƒç”Ÿæˆã«æ•°åˆ†ã‹ã‹ã£ã¦ã„ã¾ã—ãŸã€‚ã¾ãŸã€å„ GPU ãƒ©ãƒ³ã‚¯ãŒãƒ¡ãƒ¢ãƒªå†…ã«ç‹¬ç«‹ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚³ãƒ”ãƒ¼ã‚’ä¿æŒã™ã‚‹ãŸã‚ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒéåŠ¹ç‡ã§ã—ãŸã€‚MMAP ã¯å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’æ°¸ç¶šçš„ãªãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—ãƒ‰ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿æŒã™ã‚‹ã“ã¨ã§ã€ãƒ—ãƒ­ã‚»ã‚¹å†èµ·å‹•æ™‚ã®æœ€åˆã®ãƒãƒƒãƒç”Ÿæˆæ™‚é–“ã‚’ã‚¼ãƒ­ã«ã—ã€GPU 8 å°ã® p5 ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§ã¯ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ”ãƒ¼ã‚’ 8 å€‹ã‹ã‚‰ 1 å€‹ã«å‰Šæ¸›ã—ã¾ã™ã€‚

**Memory-Mapped Data Loading ã®æŠ€è¡“è©³ç´°**

https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-eks-checkpointless-features-mmap.html

å¾“æ¥ã® DataLoader ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ã¯ã€ä»¥ä¸‹ã® 3 ã¤ã®å•é¡ŒãŒã‚ã‚Šã¾ã—ãŸã€‚1. **ãƒ—ãƒ­ã‚»ã‚¹å†èµ·å‹•æ™‚ã®ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å†æ§‹ç¯‰ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰** - éšœå®³å¾©æ—§ã‚„ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ™‚ã«ã€ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“ã‚’å†åˆæœŸåŒ–ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã€æœ€åˆã®ãƒãƒƒãƒç”Ÿæˆã«æ•°åˆ†ã‚’è¦ã—ã¾ã™ã€‚2. **ãƒ¡ãƒ¢ãƒªä½¿ç”¨ã®éåŠ¹ç‡æ€§** - å„ GPU ãƒ©ãƒ³ã‚¯ãŒç‹¬ç«‹ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚³ãƒ”ãƒ¼ã‚’ãƒ¡ãƒ¢ãƒªã«ä¿æŒã™ã‚‹ãŸã‚ã€ãƒãƒ¼ãƒ‰ã‚ãŸã‚Šã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒ GPU æ•°ã«æ¯”ä¾‹ã—ã¦å¢—åŠ ã—ã¾ã™ã€‚3. **ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®å¾…æ©Ÿæ™‚é–“** - å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ãŒãƒ‡ãƒ¼ã‚¿åˆ°ç€ã‚’å¾…ã¤ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ãŒç™ºç”Ÿã—ã€GPU åˆ©ç”¨ç‡ãŒä½ä¸‹ã—ã¾ã™ã€‚

:::message
HyperPod ã¯ **ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—ãƒ‰ I/Oï¼ˆmmapï¼‰ã¨æ°¸ç¶šã‚­ãƒ£ãƒƒã‚·ãƒ¥** ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ã“ã‚Œã‚‰ã®å•é¡Œã‚’è§£æ±ºã—ã¦ã„ã¾ã™ã€‚
:::

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ­ãƒ¼

Memory-Mapped DataLoader ã¯ã€ä»¥ä¸‹ã® 3 å±¤æ§‹é€ ã§å‹•ä½œã—ã¾ã™ã€‚

```mermaid
graph TB
    subgraph "ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸å±¤"
        S3[å­¦ç¿’ãƒ‡ãƒ¼ã‚¿<br/>S3 / FSx]
    end

    subgraph "æ°¸ç¶šã‚­ãƒ£ãƒƒã‚·ãƒ¥å±¤"
        Cache[ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—ãƒ‰<br/>ã‚­ãƒ£ãƒƒã‚·ãƒ¥<br/>ãƒ—ãƒ­ã‚»ã‚¹å†èµ·å‹•å¾Œã‚‚æ®‹å­˜]
        PF[ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒ<br/>ãƒãƒƒãƒ]
        LB[éå»ä½¿ç”¨æ¸ˆã¿<br/>ãƒãƒƒãƒ]
    end

    subgraph "å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹å±¤"
        DL[DataLoader]
        Rank0[TP Rank 0<br/>ãƒ‡ãƒ¼ã‚¿å–å¾—æ‹…å½“]
        Rank1[TP Rank 1-N<br/>ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿å–ã‚Š]
        Train[å­¦ç¿’ãƒ«ãƒ¼ãƒ—]
    end

    S3 -->|åˆå›ãƒ­ãƒ¼ãƒ‰| DL
    DL -->|ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒ| PF
    PF -->|mmap å…±æœ‰| Rank0
    PF -->|mmap å…±æœ‰| Rank1
    Rank0 --> Train
    Rank1 --> Train
    Train -->|æ¶ˆè²»æ¸ˆã¿| LB

    style Cache fill: #d4edda
    style PF fill: #cce5ff
    style LB fill: #fff3cd
```

1. **åˆå›ãƒ­ãƒ¼ãƒ‰æ™‚** DataLoader ãŒ S3/FSx ã‹ã‚‰å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€æ°¸ç¶šã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«æ›¸ãè¾¼ã¿
2. **ãƒ—ãƒªãƒ•ã‚§ãƒƒãƒ** å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã‚ˆã‚Šå…ˆè¡Œã—ã¦ã€æ¬¡ã® `prefetch_length` å€‹ãƒãƒƒãƒã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«æº–å‚™
3. **ãƒ¡ãƒ¢ãƒªãƒãƒƒãƒ—ãƒ‰å…±æœ‰** å„ TP Rank 0 ã®ã¿ãŒãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã€åŒã˜ãƒ¬ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—å†…ã®ä»–ã®ãƒ©ãƒ³ã‚¯ã¯ mmap ã§å…±æœ‰ã‚¢ã‚¯ã‚»ã‚¹
4. **éå»ãƒãƒƒãƒã®ä¿æŒ** æ¶ˆè²»æ¸ˆã¿ãƒãƒƒãƒã‚‚ `lookback_length` æ•°ã ã‘ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿æŒã—ã€ãƒ—ãƒ­ã‚»ã‚¹å†èµ·å‹•æ™‚ã®å³åº§ã®å†é–‹ã‚’å¯èƒ½ã«

### å®Ÿè£…ä¾‹

https://github.com/aws/sagemaker-hyperpod-checkpointless-training/blob/24520b41db0df3495b3946a7b74edaa70fbc339d/examples/llama3/llama3_70b_pretrain_checkpointless.py#L101-L109

MMAP ã®æœ‰åŠ¹åŒ–ã¯ã€æ—¢å­˜ã® PyTorch Lightning DataModule ã‚’ `MMAPDataModule` ã§ãƒ©ãƒƒãƒ—ã™ã‚‹ã ã‘ã§å®Œäº†ã—ã¾ã™ã€‚

## (3) Program Restart Overhead Reduction

https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-eks-checkpointless-in-process-recovery.html

![](https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/hyperpod/hyperpod-checkpointless-training-flow.png)

AWS å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯ã€ŒProgram restart overhead reductionã€ã¨ã—ã¦èª¬æ˜ã•ã‚Œã¦ã„ã‚‹æœ€é©åŒ–ãƒˆãƒ©ãƒƒã‚¯ã§ã™ã€‚ã“ã‚Œã¯ **In-Process Recovery** ã¨ **Checkpointless Recovery** ã® 2 ã¤ã®æŠ€è¡“ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§å®Ÿç¾ã•ã‚Œã¾ã™ã€‚

å¾“æ¥ã®éšœå®³å¾©æ—§ã§ã¯ã€å…¨ãƒ—ãƒ­ã‚»ã‚¹ã‚’åœæ­¢ã—ã€ãƒ‡ã‚£ã‚¹ã‚¯ã‹ã‚‰ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦å†èµ·å‹•ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã€å¾©æ—§ã«æ•°åˆ†ã€œæ•°ååˆ†ã‚’è¦ã—ã¦ã„ã¾ã—ãŸã€‚ã¾ãŸã€å‰å›ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä»¥é™ã®å­¦ç¿’é€²è¡ŒãŒå¤±ã‚ã‚Œã‚‹ãŸã‚ã€è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ãŒç„¡é§„ã«ãªã£ã¦ã„ã¾ã—ãŸã€‚

HyperPod ã¯ä»¥ä¸‹ã® 2 ã¤ã®æŠ€è¡“ã§ã“ã‚Œã‚’è§£æ±ºã—ã¾ã™ã€‚

| æŠ€è¡“ | è²¬å‹™ | è§£æ±ºã™ã‚‹å•é¡Œ |
|------|------|-------------|
| **In-Process Recovery (IPR)** | **å¾©æ—§ãƒ¡ã‚«ãƒ‹ã‚ºãƒ **<br/>ãƒ—ãƒ­ã‚»ã‚¹ã‚’çµ‚äº†ã›ãšã«éšœå®³ã‹ã‚‰å¾©æ—§ã™ã‚‹ä»•çµ„ã¿ | ãƒ—ãƒ­ã‚»ã‚¹å†èµ·å‹•ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã€å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ã®æå¤± |
| **Checkpointless Recovery** | **çŠ¶æ…‹ç®¡ç†ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**<br/>GPU ãƒ¡ãƒ¢ãƒªå†…ã«ãƒ¢ãƒ‡ãƒ«ã®å†—é•·ãƒ¬ãƒ—ãƒªã‚«ã‚’ä¿æŒã™ã‚‹è¨­è¨ˆ | ãƒ‡ã‚£ã‚¹ã‚¯ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã® I/O ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã€ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚³ã‚¹ãƒˆ |

1. **In-Process Recovery** ã€Œã©ã†ã‚„ã£ã¦å¾©æ—§ã™ã‚‹ã‹ã€
   - éšœå®³ç™ºç”Ÿæ™‚ã«ã€ãƒ—ãƒ­ã‚»ã‚¹ã‚’çµ‚äº†ã›ãšã« RCB (Re-Executable Code Block: éšœå®³å¾©æ—§æ™‚ã«å†å®Ÿè¡Œå¯èƒ½ãªã‚³ãƒ¼ãƒ‰ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ) ã‚’å†å®Ÿè¡Œã™ã‚‹ã“ã¨ã§ã€ãƒ—ãƒ­ã‚»ã‚¹å†èµ·å‹•ã‚„ãƒ‡ã‚£ã‚¹ã‚¯ I/O ã‚’ä¼´ã†å¾“æ¥æ–¹å¼ã¨æ¯”è¼ƒã—ã¦å¤§å¹…ã«é«˜é€Ÿãªå¾©æ—§ã‚’å®Ÿç¾
   - å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã®å®Œäº†ã—ãŸçŠ¶æ…‹ã‹ã‚‰å†é–‹ã™ã‚‹ãŸã‚ã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé–“ã®å­¦ç¿’é€²è¡ŒãŒå¤±ã‚ã‚Œã‚‹å¾“æ¥æ–¹å¼ã¨æ¯”è¼ƒã—ã¦ã€å­¦ç¿’é€²è¡Œã®æå¤±ã‚’æœ€å°åŒ–

2. **Checkpointless Recovery** ã€Œã©ã“ã‹ã‚‰å¾©æ—§ã™ã‚‹ã‹ã€
   - è¤‡æ•°ã®ãƒãƒ¼ãƒ‰ã‚°ãƒ«ãƒ¼ãƒ—é–“ã§ãƒ¢ãƒ‡ãƒ«ãƒ»ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶çŠ¶æ…‹ã‚’å†—é•·åŒ–ã—ã€GPU ãƒ¡ãƒ¢ãƒªå†…ã«ä¿æŒ
   - ãƒ‡ã‚£ã‚¹ã‚¯ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ã‚ãšã«ã€å¥å…¨ãªãƒ¬ãƒ—ãƒªã‚«ã‹ã‚‰çŠ¶æ…‹ã‚’å–å¾—

**2 ã¤ã®æŠ€è¡“ã®é€£æº** In-Process Recovery ãŒã€Œãƒ—ãƒ­ã‚»ã‚¹ã‚’ç¶™ç¶šã—ãªãŒã‚‰å¾©æ—§ã™ã‚‹ä»•çµ„ã¿ã€ã‚’æä¾›ã—ã€Checkpointless Recovery ãŒã€Œå¾©æ—§ã«å¿…è¦ãªçŠ¶æ…‹ã‚’ GPU ãƒ¡ãƒ¢ãƒªå†…ã«ä¿æŒã™ã‚‹è¨­è¨ˆã€ã‚’æä¾›ã™ã‚‹ã“ã¨ã§ã€ãƒ‡ã‚£ã‚¹ã‚¯ I/O ã‚’å®Œå…¨ã«æ’é™¤ã—ãŸé«˜é€Ÿéšœå®³å¾©æ—§ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

### In-Process Recovery

![](https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/hyperpod/hyperpod-checkpointless-fault-controller-module.png)

å¾“æ¥ã®éšœå®³å¾©æ—§ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ã¯ã€ä»¥ä¸‹ã® 4 ã¤ã®å•é¡ŒãŒã‚ã‚Šã¾ã—ãŸã€‚1. **ãƒ—ãƒ­ã‚»ã‚¹å†èµ·å‹•ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰** - å…¨ãƒ©ãƒ³ã‚¯ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚’åœæ­¢ã—ã€æ–°è¦ãƒ—ãƒ­ã‚»ã‚¹ã‚’èµ·å‹•ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã€NCCL é›†å›£é€šä¿¡ã®å†åˆæœŸåŒ–ã«æ•°åˆ†ã‚’è¦ã™ã‚‹ã€2. **ãƒ‡ã‚£ã‚¹ã‚¯ I/O ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯** - ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆæ•°å GBã€œæ•°ç™¾ GBï¼‰ã‚’ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ï¼ˆS3 ã‚„ FSxï¼‰ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã€å¾©æ—§ã«æ•°åˆ†ã€œæ•°ååˆ†ã‹ã‹ã‚‹ã€3. **å­¦ç¿’é€²è¡Œã®æå¤±** - å‰å›ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä»¥é™ã®å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ãŒç„¡é§„ã«ãªã‚Šã€å†è¨ˆç®—ãŒå¿…è¦ï¼ˆä¾‹: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé–“éš”ãŒ 100 ã‚¹ãƒ†ãƒƒãƒ—ã®å ´åˆã€æœ€å¤§ 100 ã‚¹ãƒ†ãƒƒãƒ—åˆ†ã®è¨ˆç®—ãŒå¤±ã‚ã‚Œã‚‹ï¼‰ã€4. **ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å†æ§‹ç¯‰ã®é…å»¶** - DataLoader ã‚’å†åˆæœŸåŒ–ã—ã€æœ€åˆã®ãƒãƒƒãƒç”Ÿæˆã‚’å¾…ã¤å¿…è¦ãŒã‚ã‚‹

:::message
HyperPod ã¯ **GPU ãƒ¡ãƒ¢ãƒªå†…ã®ãƒ¢ãƒ‡ãƒ«å†—é•·åŒ–ã¨ãƒ—ãƒ­ã‚»ã‚¹å†…å¾©æ—§** ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ã“ã‚Œã‚‰ã®å•é¡Œã‚’è§£æ±ºã—ã¦ã„ã¾ã™ã€‚
:::

| è¦³ç‚¹ | In-Process Recovery | æ—¢å­˜ã‚¢ãƒ—ãƒ­ãƒ¼ãƒæ–¹å¼ |
|------|---------------------|----------------|
| **çŠ¶æ…‹ä¿å­˜å…ˆ** | GPU ãƒ¡ãƒ¢ãƒªï¼ˆãƒ—ãƒ­ã‚»ã‚¹å†…ï¼‰ | ãƒ‡ã‚£ã‚¹ã‚¯ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ |
| **å¾©æ—§é€Ÿåº¦** | é«˜é€Ÿï¼ˆãƒ—ãƒ­ã‚»ã‚¹å†…å¾©æ—§ï¼‰ | æ•°ç§’ã€œæ•°åˆ†ï¼ˆãƒ‡ã‚£ã‚¹ã‚¯ I/Oï¼‰ |
| **å­¦ç¿’é€²è¡Œã®æå¤±** | æœ€å°åŒ–ï¼ˆå‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã‹ã‚‰å†é–‹ï¼‰ | å‰å›ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹ |
| **ãƒ—ãƒ­ã‚»ã‚¹ç¶™ç¶šæ€§** | ãƒ—ãƒ­ã‚»ã‚¹ã¯ç¶™ç¶š | ãƒ—ãƒ­ã‚»ã‚¹å†èµ·å‹•ãŒå¿…è¦ãªå ´åˆãŒã‚ã‚‹ |
| **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£** | è¤‡æ•°ãƒãƒ¼ãƒ‰ã‚°ãƒ«ãƒ¼ãƒ—é–“ã®ãƒ¢ãƒ‡ãƒ«å†—é•·åŒ– | ãƒ¬ãƒ—ãƒªã‚«ã”ã¨ã®å˜ä¸€ã‚³ãƒ”ãƒ¼ |

#### éšœå®³ã‚¿ã‚¤ãƒ—ã¨å¾©æ—§ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 

| éšœå®³ã‚¿ã‚¤ãƒ— | åŸå›  | å¾©æ—§ã‚¿ã‚¤ãƒ— | ãƒ¡ã‚«ãƒ‹ã‚ºãƒ  |
|-----------|------|-----------|-----------|
| **In-Process éšœå®³** | ã‚³ãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã‚¨ãƒ©ãƒ¼ã€ä¾‹å¤– | In-Process Recovery (IPR) | æ—¢å­˜ãƒ—ãƒ­ã‚»ã‚¹å†…ã§ RCB ã‚’å†å®Ÿè¡Œ |
| **Process Restart éšœå®³** | CUDA ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç ´æã€ãƒ—ãƒ­ã‚»ã‚¹çµ‚äº† | Process Level Restart (PLR) | SageMaker HyperPod ãŒãƒ—ãƒ­ã‚»ã‚¹ã‚’å†èµ·å‹•ã€‚K8s Pod å†èµ·å‹•ã¯ã‚¹ã‚­ãƒƒãƒ— |
| **Node Replacement éšœå®³** | æ’ä¹…çš„ãªãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢éšœå®³ | Job Level Restart (JLR) | æ•…éšœãƒãƒ¼ãƒ‰ã‚’äº¤æ›ã€‚ã‚¸ãƒ§ãƒ–å…¨ä½“ã‚’å†èµ·å‹• |

#### Atomic Lock ã®å½¹å‰²

åˆ†æ•£å­¦ç¿’ã§ã¯ã€1 å›ã®å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ã¯ **Forward Passã€Backward Passã€Optimizer Step** ã® 3 ã¤ã®ãƒ•ã‚§ãƒ¼ã‚ºã§æ§‹æˆã•ã‚Œã¾ã™ã€‚å®Ÿè¡Œæ™‚é–“ã®è¦³ç‚¹ã§ã¯ Forward/Backward Pass ãŒå¤§åŠã‚’å ã‚ã‚‹ã¨æ€ã‚ã‚Œã¾ã™ãŒã€**Optimizer Step ã¯ä¸€åº¦å®Œäº†ã™ã‚‹ã¨ç ´æ£„ã§ããªã„é‡è¦ãªæ“ä½œ**ã§ã™ã€‚

Optimizer Step ãŒå®Œäº†ã™ã‚‹ã¨ã€æ–°ã—ã„ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ï¼ˆé‡ã¿ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶çŠ¶æ…‹ã€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã®çŠ¶æ…‹ãªã©ï¼‰ãŒç¢ºå®šã—ã¾ã™ã€‚ã“ã®ç¢ºå®šã—ãŸçŠ¶æ…‹ã¯æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«å¿…é ˆã§ã‚ã‚Šã€ç ´æ£„ã™ã‚‹ã¨å†è¨ˆç®—ãŒå¿…è¦ã«ãªã‚Šã¾ã™ã€‚å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§ã¯æ•°ç™¾ GB ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶çŠ¶æ…‹ã¸ã®ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ã€ã•ã‚‰ã«åˆ†æ•£ç’°å¢ƒã§ã®é€šä¿¡ã‚³ã‚¹ãƒˆï¼ˆå‹¾é…ã® all-reduceã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚·ãƒ£ãƒ¼ãƒ‰ã®åŒæœŸãªã©ï¼‰ãŒç™ºç”Ÿã™ã‚‹ãŸã‚ã€å†è¨ˆç®—ã®ã‚³ã‚¹ãƒˆã¯ç„¡è¦–ã§ãã¾ã›ã‚“ã€‚

å¾“æ¥ã®éšœå®³å¾©æ—§ã§ã¯ã€ã©ã®ãƒ•ã‚§ãƒ¼ã‚ºã§éšœå®³ãŒç™ºç”Ÿã—ã¦ã‚‚ã€å¸¸ã«å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã®é–‹å§‹åœ°ç‚¹ã¾ã§ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¦ã„ã¾ã—ãŸã€‚ã“ã‚Œã¯ã€Optimizer Step ã®é€”ä¸­ã§éšœå®³ãŒç™ºç”Ÿã—ãŸå ´åˆã€å¥å…¨ãªãƒ¬ãƒ—ãƒªã‚«ãŒæ—¢ã«å®Œäº†ã—ãŸè¨ˆç®—çµæœã‚’ç ´æ£„ã™ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚ä¾‹ãˆã°ã€8 ãƒ¬ãƒ—ãƒªã‚«ã®ã†ã¡ 1 ã¤ãŒ Optimizer Step ã® 90% å®Œäº†æ™‚ç‚¹ã§éšœå®³ã‚’èµ·ã“ã™ã¨ã€æ®‹ã‚Š 7 ã¤ã®å¥å…¨ãªãƒ¬ãƒ—ãƒªã‚«ãŒå®Œäº†ã—ãŸ Optimizer Step ã‚’ã™ã¹ã¦æ¨ã¦ã¦ã€å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã‹ã‚‰å†è¨ˆç®—ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

Atomic Lock ã¯ã€ã“ã®ç„¡é§„ã‚’é˜²ããŸã‚ã«å°å…¥ã•ã‚ŒãŸä»•çµ„ã¿ã§ã™ã€‚**Optimizer Step ã‚’ã‚¢ãƒˆãƒŸãƒƒã‚¯ãªæ“ä½œã¨ã—ã¦ä¿è­·**ã™ã‚‹ã“ã¨ã§ã€å¥å…¨ãªãƒ¬ãƒ—ãƒªã‚«ãŒå®Œäº†ã—ãŸè¨ˆç®—çµæœã‚’å¤±ã‚ãšã«æ¸ˆã‚€ã‚ˆã†ã«ã—ã¾ã™ã€‚

#### å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ã® 3 ã¤ã®ãƒ•ã‚§ãƒ¼ã‚ºã¨éšœå®³æ™‚ã®å‹•ä½œ

![](https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/hyperpod/hyperpod-checkpointless-optimizer.png)

**Case 1: Forward/Backward Pass ä¸­ã®éšœå®³ï¼ˆOptimizer Step å‰ï¼‰**

Forward Pass ã¾ãŸã¯ Backward Pass ã®å®Ÿè¡Œä¸­ã«éšœå®³ãŒç™ºç”Ÿã—ãŸå ´åˆã€ã¾ã  Atomic Lock ã¯å–å¾—ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã“ã®æ™‚ç‚¹ã§ã¯ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã¯ã¾ã æ›´æ–°ã•ã‚Œã¦ãŠã‚‰ãšã€è¨ˆç®—é€”ä¸­ã®ä¸­é–“çµæœï¼ˆå‹¾é…ãªã©ï¼‰ã®ã¿ãŒå­˜åœ¨ã—ã¾ã™ã€‚

éšœå®³ãŒæ¤œå‡ºã•ã‚Œã‚‹ã¨ã€ã‚·ã‚¹ãƒ†ãƒ ã¯**ç¾åœ¨ã®å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ã®å…ˆé ­ã¾ã§ãƒ­ãƒ¼ãƒ«ãƒãƒƒã‚¯**ã—ã¾ã™ã€‚Checkpointless Recovery ã¨é€£æºã—ã€å¥å…¨ãªãƒ¬ãƒ—ãƒªã‚«ã¯å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã§å®Œäº†ã—ãŸãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ï¼ˆæ—¢ã« Optimizer Step ã§æ›´æ–°æ¸ˆã¿ã®é‡ã¿ï¼‰ã‚’ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆã—ã€ã™ã¹ã¦ã®ãƒ¬ãƒ—ãƒªã‚«ãŒãã®çŠ¶æ…‹ã‹ã‚‰æ–°ãŸã« Forward Pass ã‚’å†å®Ÿè¡Œã—ã¾ã™ã€‚ã“ã®å ´åˆã€å¤±ã‚ã‚Œã‚‹ã®ã¯ç¾åœ¨ã®ã‚¹ãƒ†ãƒƒãƒ—ã®è¨ˆç®—é€”ä¸­ã®ä¸­é–“çµæœã ã‘ãªã®ã§ã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé–“éš”ã«ä¾å­˜ã™ã‚‹å¾“æ¥æ–¹å¼ã¨æ¯”è¼ƒã—ã¦æå¤±ã¯æœ€å°é™ã§ã™ã€‚

**Case 2: Optimizer Step ä¸­ã®éšœå®³ï¼ˆAtomic Lock ä¿æŒä¸­ï¼‰**

å„ãƒ¬ãƒ—ãƒªã‚«ã¯ **Optimizer Step ã®ç›´å‰**ï¼ˆå‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°å¾Œï¼‰ã« **Atomic Lock (`ParameterUpdateLock`)** ã‚’å–å¾—ã—ã€ãƒãƒƒãƒå®Œäº†æ™‚ï¼ˆ`on_train_batch_end`ï¼‰ã«è§£æ”¾ã—ã¾ã™ã€‚ã¤ã¾ã‚Šã€Forward Pass ã¨ Backward Pass ã¯ãƒ­ãƒƒã‚¯ã®ä¿è­·å¯¾è±¡å¤–ã§ã‚ã‚Šã€ã“ã‚Œã‚‰ã®ãƒ•ã‚§ãƒ¼ã‚ºä¸­ã®éšœå®³ã¯ Case 1 ã¨ã—ã¦æ‰±ã‚ã‚Œã¾ã™ã€‚ãƒ­ãƒƒã‚¯ãŒä¿æŒã•ã‚Œã¦ã„ã‚‹é–“ã€Checkpointless Recovery ã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¾ã™ã€‚

:::message
ä»¥ä¸‹ã®èª¬æ˜ã¯ã€Optimizer Step ã®ã¿ã«ç„¦ç‚¹ã‚’å½“ã¦ãŸæ¦‚å¿µçš„ãªèª¬æ˜ã§ã™ã€‚Forward/Backward Pass ä¸­ã®éšœå®³ã¯å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã‹ã‚‰ã®å†å®Ÿè¡Œã§å¯¾å‡¦ã§ãã‚‹ãŸã‚ã€Optimizer Step ã®ä¿è­·ãŒæœ€ã‚‚é‡è¦ã«ãªã‚Šã¾ã™ã€‚
:::

Optimizer Step ã®å®Ÿè¡Œä¸­ã«ä¸€éƒ¨ã®ãƒ¬ãƒ—ãƒªã‚«ã§éšœå®³ãŒç™ºç”Ÿã—ã¦ã‚‚ã€å¥å…¨ãªãƒ¬ãƒ—ãƒªã‚«ã¯**ãƒ­ãƒƒã‚¯ã«ä¿è­·ã•ã‚ŒãŸã¾ã¾ Optimizer Step ã‚’ç¶™ç¶šã—ã€æœ€å¾Œã¾ã§å®Œäº†**ã—ã¾ã™ã€‚éšœå®³ãŒç™ºç”Ÿã—ãŸãƒ¬ãƒ—ãƒªã‚«ã¯è¨ˆç®—ã‚’ä¸­æ–­ã—ã¾ã™ãŒã€ãƒ—ãƒ­ã‚»ã‚¹è‡ªä½“ã¯çµ‚äº†ã—ã¾ã›ã‚“ã€‚

å¥å…¨ãªãƒ¬ãƒ—ãƒªã‚«ãŒ Optimizer Step ã‚’å®Œäº†ã™ã‚‹ã¨ã€**æ›´æ–°ã•ã‚ŒãŸæ–°ã—ã„ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹**ï¼ˆé‡ã¿ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶çŠ¶æ…‹ã€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã®çŠ¶æ…‹ãªã©ï¼‰ã‚’éšœå®³ã‹ã‚‰å¾©æ—§ã—ãŸãƒ¬ãƒ—ãƒªã‚«ã«ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆã—ã¾ã™ã€‚å¾©æ—§ã—ãŸãƒ¬ãƒ—ãƒªã‚«ã¯ã€ã“ã®æ›´æ–°æ¸ˆã¿ã®çŠ¶æ…‹ã‚’å—ã‘å–ã‚Šã€**æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã‹ã‚‰å†é–‹**ã—ã¾ã™ã€‚

ã“ã®ä»•çµ„ã¿ã«ã‚ˆã‚Šã€å¥å…¨ãªãƒ¬ãƒ—ãƒªã‚«ãŒæ—¢ã«å®Œäº†ã—ãŸ Optimizer Step ã®è¨ˆç®—çµæœã‚’ç ´æ£„ã™ã‚‹ã“ã¨ãªãã€ãã®ã¾ã¾æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€ã“ã¨ãŒã§ãã¾ã™ã€‚

**2 ã¤ã®ã‚±ãƒ¼ã‚¹ã®æ¯”è¼ƒ**

```mermaid
sequenceDiagram
    autonumber
    participant R1 as å¥å…¨ãªãƒ¬ãƒ—ãƒªã‚«
    participant R2 as éšœå®³ãƒ¬ãƒ—ãƒªã‚«
    participant Lock as Atomic Lock

    rect rgb(255, 230, 230)
        Note over R1, Lock: Case 1: Forward/Backward ä¸­ã®éšœå®³
        R1->>R1: Forward Pass å®Ÿè¡Œä¸­
        R2->>R2: Forward Pass å®Ÿè¡Œä¸­
        Note over R2: [éšœå®³ç™ºç”Ÿ]
        R1->>R1: å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã®çŠ¶æ…‹ã‚’ä¿æŒ
        R1->>R2: å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ã‚’ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ
        R2->>R2: çŠ¶æ…‹ã‚’å¾©å…ƒ
        R1->>R1: Forward Pass ã‚’å†å®Ÿè¡Œ
        R2->>R2: Forward Pass ã‚’å†å®Ÿè¡Œ
        Note over R1, R2: çµæœ: å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã‹ã‚‰å†å®Ÿè¡Œ<br/>ï¼ˆForward/Backward ã®è¨ˆç®—ã ã‘ãŒå¤±ã‚ã‚Œã‚‹ï¼‰
    end

    rect rgb(230, 255, 230)
        Note over R1, Lock: Case 2: Optimizer Step ä¸­ã®éšœå®³
        R1->>Lock: Atomic Lock å–å¾—
        R2->>Lock: Atomic Lock å–å¾—
        R1->>R1: Optimizer Step å®Ÿè¡Œä¸­
        R2->>R2: Optimizer Step å®Ÿè¡Œä¸­ï¼ˆ80% å®Œäº†ï¼‰
        Note over R2: [éšœå®³ç™ºç”Ÿ]
        R1->>R1: Optimizer Step ã‚’ç¶™ç¶š
        R1->>R1: Optimizer Step å®Œäº†
        R1->>Lock: Atomic Lock è§£æ”¾
        R1->>R2: æ›´æ–°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ã‚’ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ
        R2->>R2: æ–°ã—ã„çŠ¶æ…‹ã‚’å—ä¿¡
        R1->>R1: æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆForward Passï¼‰é–‹å§‹
        R2->>R2: æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆForward Passï¼‰é–‹å§‹
        Note over R1, R2: çµæœ: æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã‹ã‚‰å†é–‹<br/>ï¼ˆOptimizer Step ã®è¨ˆç®—çµæœã¯ä¿æŒï¼‰
    end
```

### In-Process Recovery ã®å®Ÿè£…

In-Process Recovery ã¯ `@HPWrapper` ã®è¨­å®šã«ã‚ˆã£ã¦å®Ÿç¾ã•ã‚Œã¾ã™ã€‚ä»¥ä¸‹ãŒå®Ÿè£…ã®éµã¨ãªã‚‹è¨­å®šã§ã™ï¼š

```python
from hyperpod_checkpointless_training.inprocess import HPWrapper

@HPWrapper(
    health_check=CudaHealthCheck(),           # CUDA ãƒ‡ãƒã‚¤ã‚¹ã®å¥å…¨æ€§ç›£è¦–
    hp_api_factory=HPAgentK8sAPIFactory(),   # HyperPod API ã¨ã®çµ±åˆ
    abort_timeout=60.0,                       # éšœå®³æ™‚ã®ä¸­æ–­ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆç§’ï¼‰
    checkpoint_manager=PEFTCheckpointManager(enable_offload=False),  # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç®¡ç†
    abort=CheckpointlessAbortManager.get_default_checkpointless_abort(),  # ä¸­æ–­å‡¦ç†
    finalize=CheckpointlessFinalizeCleanup(), # çµ‚äº†æ™‚ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
)
def training_function():
    """
    ã“ã®é–¢æ•°å…¨ä½“ãŒ RCB (Re-Executable Code Block) ã§ã™ã€‚
    éšœå®³å¾©æ—§æ™‚ã«ã€ãƒ¡ãƒ¢ãƒªå†…ã®ãƒ¢ãƒ‡ãƒ«ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶çŠ¶æ…‹ã‚’
    ä¿æŒã—ãŸã¾ã¾ã€ã“ã®é–¢æ•°ãŒå†å®Ÿè¡Œã•ã‚Œã¾ã™ã€‚
    """
    trainer.fit(model, datamodule)
```

**å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å½¹å‰²**

| ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å½¹å‰² |
|-----------|------|
| `health_check` | CUDA ãƒ‡ãƒã‚¤ã‚¹ã®å¥å…¨æ€§ã‚’ç›£è¦–ã—ã€éšœå®³ã‚’æ¤œå‡º |
| `hp_api_factory` | HyperPod ã® Health Monitoring Agent ã¨é€£æº |
| `abort_timeout` | éšœå®³ç™ºç”Ÿæ™‚ã®ä¸­æ–­å‡¦ç†ã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚é–“ |
| `checkpoint_manager` | Checkpointless Recovery ã¨é€£æºã—ãŸãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç®¡ç† |
| `abort` | éšœå®³æ™‚ã®ä¸­æ–­å‡¦ç†ã¨ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— |
| `finalize` | éšœå®³å¾©æ—§æ™‚ã®ãƒ©ãƒ³ã‚¯ãƒ­ãƒ¼ã‚«ãƒ«ãªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å‡¦ç† |

å‚è€ƒå®Ÿè£…: [llama3_70b_peft_checkpointless.py](https://github.com/aws/sagemaker-hyperpod-checkpointless-training/blob/24520b41db0df3495b3946a7b74edaa70fbc339d/examples/llama3/llama3_70b_peft_checkpointless.py#L184-L193)

**RCB (Re-Executable Code Block) ã¨ã¯**

`@HPWrapper` ã§ãƒ©ãƒƒãƒ—ã•ã‚ŒãŸé–¢æ•°å…¨ä½“ãŒ RCB ã¨ã—ã¦ç™»éŒ²ã•ã‚Œã¾ã™ã€‚å¾“æ¥ã®å¾©æ—§æ–¹å¼ã§ã¯ãƒ—ãƒ­ã‚»ã‚¹å…¨ä½“ã‚’å†èµ·å‹•ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã—ãŸãŒã€RCB ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€GPU ãƒ¡ãƒ¢ãƒªå†…ã®ãƒ¢ãƒ‡ãƒ«ãƒ»ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶çŠ¶æ…‹ã‚’ä¿æŒã—ãŸã¾ã¾ã€éšœå®³æ™‚ã«ã“ã®é–¢æ•°ã ã‘ã‚’å†å®Ÿè¡Œã§ãã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ—ãƒ­ã‚»ã‚¹å†èµ·å‹•ã‚„ãƒ‡ã‚£ã‚¹ã‚¯ I/O ã‚’ä¼´ã‚ãªã„é«˜é€Ÿãªå¾©æ—§ãŒå®Ÿç¾ã•ã‚Œã¾ã™ã€‚

**RCB ã®å‹•ä½œãƒ•ãƒ­ãƒ¼**

```mermaid
sequenceDiagram
    autonumber
    participant User as ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ¼ãƒ‰
    participant HPW as HPWrapper
    participant FC as Fault Controller
    participant GPU as GPU ãƒ¡ãƒ¢ãƒª
    participant Health as Health Monitor

    rect rgb(230, 245, 255)
        Note over User, Health: æ­£å¸¸æ™‚ã®å­¦ç¿’
        User->>HPW: training_function() å‘¼ã³å‡ºã—
        HPW->>FC: RCB ã‚’ç™»éŒ²
        HPW->>GPU: ãƒ¢ãƒ‡ãƒ«ãƒ»ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶çŠ¶æ…‹ã‚’ä¿æŒ
        HPW->>User: trainer.fit() å®Ÿè¡Œä¸­
    end

    rect rgb(255, 230, 230)
        Note over Health: éšœå®³æ¤œå‡º
        Health->>FC: ã‚¤ãƒ³ãƒ•ãƒ©éšœå®³é€šçŸ¥
        FC->>HPW: RCB ä¸­æ–­ã‚·ã‚°ãƒŠãƒ«
        HPW->>GPU: GPU ãƒ¡ãƒ¢ãƒªçŠ¶æ…‹ã‚’ä¿æŒ
        Note over GPU: ãƒ¢ãƒ‡ãƒ«ãƒ»ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶çŠ¶æ…‹ã¯<br/>ãƒ¡ãƒ¢ãƒªã«æ®‹å­˜
    end

    rect rgb(230, 255, 230)
        Note over FC, GPU: In-Process Recovery
        FC->>FC: ãƒªã‚½ãƒ¼ã‚¹ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        FC->>HPW: RCB å†å®Ÿè¡Œãƒˆãƒªã‚¬ãƒ¼
        HPW->>GPU: ãƒ¡ãƒ¢ãƒªå†…ã®çŠ¶æ…‹ã‚’ä½¿ç”¨
        HPW->>User: training_function() å†å®Ÿè¡Œ
        User->>HPW: trainer.fit() ç¶™ç¶š
        Note over User: å‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã‹ã‚‰å†é–‹<br/>ï¼ˆãƒ‡ã‚£ã‚¹ã‚¯ I/O ãªã—ï¼‰
    end
```

**Fault Controller ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®å½¹å‰²**

| ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« | å½¹å‰² | å®Ÿè¡Œå†…å®¹ |
|-----------|------|---------|
| éšœå®³æ¤œå‡º | ã‚¤ãƒ³ãƒ•ãƒ©éšœå®³ã®ç›£è¦– | Health Monitoring Agent ã‹ã‚‰éšœå®³é€šçŸ¥ã‚’å—ä¿¡ |
| RCB å®šç¾© API | ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚³ãƒ¼ãƒ‰ã®ç™»éŒ² | `@HPWrapper` ã§è£…é£¾ã•ã‚ŒãŸé–¢æ•°ã‚’ RCB ã¨ã—ã¦ç™»éŒ² |
| å†èµ·å‹•ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« | In-Process Recovery ã®å®Ÿè¡Œ | RCB ã‚’ä¸­æ–­ã€ãƒªã‚½ãƒ¼ã‚¹ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã€GPU ãƒ¡ãƒ¢ãƒªã‚’ä¿æŒã—ãŸã¾ã¾ RCB ã‚’å†å®Ÿè¡Œ |

:::message
In-Process Recovery ã¯ã€Checkpointless Recovery ã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§æœ€å¤§ã®åŠ¹æœã‚’ç™ºæ®ã—ã¾ã™ã€‚ãƒ¢ãƒ‡ãƒ«å†—é•·åŒ–ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚„å¿…é ˆè¨­å®šï¼ˆ`num_distributed_optimizer_instances â‰¥ 2`ï¼‰ã«ã¤ã„ã¦ã¯ã€å¾Œè¿°ã®ã€ŒCheckpointless Recoveryã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§è©³ã—ãè§£èª¬ã—ã¾ã™ã€‚
:::

:::message alert
#### In-Process Recovery ã¾ã¨ã‚

æœ€å¾Œã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹ã™ã‚‹å¾“æ¥ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ–¹å¼ã¨ã¯ç•°ãªã‚Šã€In-Process Recovery ã¯æœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«ãƒ»ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶çŠ¶æ…‹ã‚’ GPU ãƒ¡ãƒ¢ãƒªã«ä¿æŒã™ã‚‹ãŸã‚ã€éšœå®³ãŒç™ºç”Ÿã—ãŸæ™‚ç‚¹ã‹ã‚‰ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ã«ãƒªã‚«ãƒãƒªãƒ¼ã§ãã¾ã™ã€‚
:::

### Checkpointless Recovery

In-Process Recovery ã«åŠ ãˆã¦ã€HyperPod ã¯ **Checkpointless Recovery** ã‚’æä¾›ã—ã¾ã™ã€‚ã“ã‚Œã¯ã€ãƒ¢ãƒ‡ãƒ«ãƒ¬ãƒ—ãƒªã‚«ã‚’è¤‡æ•°ã®ãƒãƒ¼ãƒ‰ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†æ•£é…ç½®ã™ã‚‹ã“ã¨ã§ã€ãƒ‡ã‚£ã‚¹ã‚¯ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’å®Œå…¨ã«æ’é™¤ã—ãŸéšœå®³å¾©æ—§ã‚’å®Ÿç¾ã™ã‚‹æŠ€è¡“ã§ã™ã€‚

#### è§£æ±ºã™ã‚‹èª²é¡Œ

å¾“æ¥ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ™ãƒ¼ã‚¹ã®éšœå®³å¾©æ—§ã«ã¯ã€ä»¥ä¸‹ã® 3 ã¤ã®å•é¡ŒãŒã‚ã‚Šã¾ã—ãŸã€‚

1. **ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ I/O ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰** - å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆï¼ˆæ•°å GBã€œæ•°ç™¾ GBï¼‰ã‚’ãƒ‡ã‚£ã‚¹ã‚¯ã«å®šæœŸçš„ã«ä¿å­˜ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã€å­¦ç¿’ãŒãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã‚‹ï¼ˆæ•°ç§’ã€œæ•°åˆ†ï¼‰
2. **ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚³ã‚¹ãƒˆã®å¢—å¤§** - ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ S3 ã‚„ FSx ã«ä¿å­˜ã™ã‚‹ãŸã‚ã€ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚³ã‚¹ãƒˆãŒå¢—åŠ ã—ã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé »åº¦ã¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚³ã‚¹ãƒˆã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ãŒç™ºç”Ÿã™ã‚‹
3. **å˜ä¸€ãƒãƒ¼ãƒ‰éšœå®³æ™‚ã®ã‚·ãƒ£ãƒ¼ãƒ‰æå¤±** - FSDP ã®ã‚ˆã†ãªã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ–¹å¼ã§ã¯ã€å„ãƒãƒ¼ãƒ‰ãŒä¸€æ„ã®ãƒ¢ãƒ‡ãƒ«ã‚·ãƒ£ãƒ¼ãƒ‰ã‚’ä¿æŒã™ã‚‹ãŸã‚ã€1 ãƒãƒ¼ãƒ‰ãŒæ•…éšœã™ã‚‹ã¨ãã®ã‚·ãƒ£ãƒ¼ãƒ‰ãŒå¤±ã‚ã‚Œã€ãƒ‡ã‚£ã‚¹ã‚¯ã‹ã‚‰ã®å¾©æ—§ãŒå¿…é ˆã¨ãªã‚‹

:::message
HyperPod ã¯ **GPU ãƒ¡ãƒ¢ãƒªå†…ã®ãƒ¢ãƒ‡ãƒ«å†—é•·åŒ–ã¨ In-Process Recovery** ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ãƒ‡ã‚£ã‚¹ã‚¯ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’å®Œå…¨ã«æ’é™¤ã—ã¾ã™ã€‚
:::

**åŸºæœ¬åŸç†**

Checkpointless Recovery ã¯ã€ä»¥ä¸‹ã® 2 ã¤ã®æŠ€è¡“ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§å®Ÿç¾ã•ã‚Œã¾ã™ã€‚

1. **In-Process Recovery**: ãƒ—ãƒ­ã‚»ã‚¹å†…ã§ã®é«˜é€Ÿå¾©æ—§ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 
2. **Model Redundancy**: è¤‡æ•°ãƒãƒ¼ãƒ‰ã‚°ãƒ«ãƒ¼ãƒ—é–“ã§ã®ãƒ¢ãƒ‡ãƒ«ãƒ»ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶çŠ¶æ…‹ã®å®Œå…¨è¤‡è£½

ã“ã®çµ„ã¿åˆã‚ã›ã«ã‚ˆã‚Šã€ãƒ‡ã‚£ã‚¹ã‚¯ãƒ™ãƒ¼ã‚¹ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ãƒ»å¾©å…ƒã‚’å®Œå…¨ã«ã‚¹ã‚­ãƒƒãƒ—ã—ã€ãƒ¡ãƒ¢ãƒªã‹ã‚‰ãƒ¡ãƒ¢ãƒªã¸ã®çŠ¶æ…‹è»¢é€ã®ã¿ã§å¾©æ—§ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

#### ãƒ¬ãƒ—ãƒªã‚«é…ç½®æˆ¦ç•¥

HyperPod Checkpointless Training ã¯ã€ãƒ¢ãƒ‡ãƒ«ãƒ¬ãƒ—ãƒªã‚«ã‚’ç‰©ç†çš„ã«ç•°ãªã‚‹ãƒãƒ¼ãƒ‰ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†æ•£é…ç½®ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å˜ä¸€ãƒãƒ¼ãƒ‰ã¾ãŸã¯å˜ä¸€ãƒãƒ¼ãƒ‰ã‚°ãƒ«ãƒ¼ãƒ—ã®éšœå®³ãŒç™ºç”Ÿã—ã¦ã‚‚ã€ä»–ã®ãƒãƒ¼ãƒ‰ã‚°ãƒ«ãƒ¼ãƒ—ä¸Šã®å¥å…¨ãªãƒ¬ãƒ—ãƒªã‚«ã‹ã‚‰çŠ¶æ…‹ã‚’å¾©æ—§ã§ãã¾ã™ã€‚

**ãƒ¬ãƒ—ãƒªã‚«é…ç½®ã®ä¾‹ï¼ˆp5.48xlarge ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®å ´åˆï¼‰**

```mermaid
graph TB
    subgraph "Node Group 1"
        direction LR
        N1_R1[Replica 1<br/>Node 1<br/>GPU 0-7]
    end

    subgraph "Node Group 2"
        direction LR
        N2_R2[Replica 2<br/>Node 2<br/>GPU 0-7]
    end

    subgraph "Node Group 3"
        direction LR
        N3_R1[Replica 1<br/>Node 3<br/>GPU 0-7]
    end

    subgraph "Node Group 4"
        direction LR
        N4_R2[Replica 2<br/>Node 4<br/>GPU 0-7]
    end

    N1_R1 -.åŒæœŸ.-> N3_R1
    N2_R2 -.åŒæœŸ.-> N4_R2
    N1_R1 <-.å¾©æ—§æ™‚ã®çŠ¶æ…‹è»¢é€.-> N2_R2
    N3_R1 <-.å¾©æ—§æ™‚ã®çŠ¶æ…‹è»¢é€.-> N4_R2

    style N1_R1 fill: #d4edda
    style N3_R1 fill: #d4edda
    style N2_R2 fill: #cce5ff
    style N4_R2 fill: #cce5ff
```

**é…ç½®ã®ç‰¹å¾´**

ã“ã®ãƒ¬ãƒ—ãƒªã‚«é…ç½®ã§ã¯ã€å„ãƒ¬ãƒ—ãƒªã‚«ã‚’ç‰©ç†çš„ã«ç•°ãªã‚‹ãƒãƒ¼ãƒ‰ã«é…ç½®ã™ã‚‹ã“ã¨ã§ã€å˜ä¸€ãƒãƒ¼ãƒ‰éšœå®³ã«å¯¾ã™ã‚‹è€æ€§ã‚’ç¢ºä¿ã—ã¦ã„ã¾ã™ã€‚åŒä¸€ãƒ¬ãƒ—ãƒªã‚«å†…ï¼ˆReplica 1 ã‚„ Replica 2ï¼‰ã§ã¯ã€å„ãƒ©ãƒ³ã‚¯ãŒ reduce-scatter ã¨ all-gather ã‚’é€šã˜ã¦ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶çŠ¶æ…‹ã®ã‚·ãƒ£ãƒ¼ãƒ‰ã‚’å…±æœ‰ãƒ»åŒæœŸã—ã¾ã™ã€‚ã•ã‚‰ã«ã€ãƒ¬ãƒ—ãƒªã‚«é–“ï¼ˆReplica 1 â†” Replica 2ï¼‰ã§ã‚‚é€šå¸¸æ™‚ã‹ã‚‰ all-reduce ã§åŒæœŸãŒè¡Œã‚ã‚Œã¦ãŠã‚Šã€ã“ã®ç¶™ç¶šçš„ãªåŒæœŸãŒå†—é•·æ€§ã‚’ç¶­æŒã—ã€éšœå®³æ™‚ã®è¿…é€Ÿãªå¾©æ—§ã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚ãã®çµæœã€å˜ä¸€ãƒãƒ¼ãƒ‰ã§éšœå®³ãŒç™ºç”Ÿã—ã¦ã‚‚ã€ä»–ã®ãƒãƒ¼ãƒ‰ã‚°ãƒ«ãƒ¼ãƒ—ä¸Šã«å­˜åœ¨ã™ã‚‹å¥å…¨ãªãƒ¬ãƒ—ãƒªã‚«ã‹ã‚‰çŠ¶æ…‹ã‚’å¾©å…ƒã§ãã¾ã™ã€‚

#### ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ¬ã‚¹å¾©æ—§ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

**æ­£å¸¸æ™‚ã®å‹•ä½œ**

æ­£å¸¸æ™‚ã€ã‚·ã‚¹ãƒ†ãƒ ã¯å„å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—ã‚’å®Ÿè¡Œã—ã€å„ãƒ¬ãƒ—ãƒªã‚«ã‚°ãƒ«ãƒ¼ãƒ—å†…ã§é‡ã¿ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶çŠ¶æ…‹ã‚’åŒæœŸã—ã¾ã™ã€‚ã“ã®åŒæœŸã¯ãƒ¡ãƒ¢ãƒªä¸Šã§å®Œçµã™ã‚‹ãŸã‚ã€ãƒ‡ã‚£ã‚¹ã‚¯ã¸ã®ä¿å­˜ã¯ä¸€åˆ‡è¡Œã‚ãšã€å³åº§ã«æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã¸é€²è¡Œã—ã¾ã™ã€‚

**éšœå®³ç™ºç”Ÿæ™‚ã®å‹•ä½œ**

éšœå®³ãŒç™ºç”Ÿã™ã‚‹ã¨ï¼ˆä¾‹: Node 1 ã® GPU æ•…éšœï¼‰ã€ã¾ãšã‚·ã‚¹ãƒ†ãƒ ã¯å¾©æ—§å®Ÿè¡Œå¯èƒ½æ€§ã‚’è©•ä¾¡ã—ã¾ã™ã€‚`CheckpointManager.checkpointless_recovery_feasible()` ãƒ¡ã‚½ãƒƒãƒ‰ãŒã€å¥å…¨ãªãƒ¬ãƒ—ãƒªã‚«ï¼ˆNode 2, 3, 4ï¼‰ã®å­˜åœ¨ã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¹ãƒ†ãƒƒãƒ—ã®ä¸€è²«æ€§ã€ãŠã‚ˆã³ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ã®ãƒã‚§ãƒƒã‚¯ã‚µãƒ ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰ã‚’æ¤œè¨¼ã—ã¾ã™ã€‚æ¤œè¨¼ãŒæˆåŠŸã™ã‚Œã°ã€Node 2 ã¾ãŸã¯ Node 4 ã®å¥å…¨ãªãƒ¬ãƒ—ãƒªã‚«ã‹ã‚‰æœ€æ–°ã®ãƒ¢ãƒ‡ãƒ«ãƒ»ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶çŠ¶æ…‹ã‚’å–å¾—ã—ã€Node 1 ã®ãƒ—ãƒ­ã‚»ã‚¹å†…ã§çŠ¶æ…‹ã‚’å¾©å…ƒã—ã¾ã™ã€‚å¾©å…ƒãŒå®Œäº†ã™ã‚‹ã¨ã€ãƒ‡ã‚£ã‚¹ã‚¯ã‹ã‚‰ã®ãƒ­ãƒ¼ãƒ‰ã‚’çµŒã‚‹ã“ã¨ãªãã€åŒã˜ã‚¹ãƒ†ãƒƒãƒ—ã‹ã‚‰å­¦ç¿’ã‚’å†é–‹ã§ãã¾ã™ã€‚

##### ãƒ‡ã‚£ã‚¹ã‚¯ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã¨ã®æ¯”è¼ƒ

| è¦³ç‚¹ | Checkpointless Recovery | å¾“æ¥ã®ãƒ‡ã‚£ã‚¹ã‚¯ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ |
|------|------------------------|---------------------------|
| **çŠ¶æ…‹ä¿å­˜å…ˆ** | GPU ãƒ¡ãƒ¢ãƒªï¼ˆè¤‡æ•°ãƒãƒ¼ãƒ‰ã‚°ãƒ«ãƒ¼ãƒ—ï¼‰ | ãƒ‡ã‚£ã‚¹ã‚¯ï¼ˆFSx Lustre ãªã©ï¼‰ |
| **ä¿å­˜é »åº¦** | æ¯ã‚¹ãƒ†ãƒƒãƒ—ï¼ˆãƒ¡ãƒ¢ãƒªåŒæœŸï¼‰ | å®šæœŸçš„ï¼ˆä¾‹: 100 ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ï¼‰ |
| **å¾©æ—§é€Ÿåº¦** | é«˜é€Ÿï¼ˆãƒ¡ãƒ¢ãƒªè»¢é€ï¼‰ | æ•°ç§’ã€œæ•°åˆ†ï¼ˆãƒ‡ã‚£ã‚¹ã‚¯ I/Oï¼‰ |
| **å­¦ç¿’é€²è¡Œã®æå¤±** | æœ€å°åŒ–ï¼ˆå‰ã®ã‚¹ãƒ†ãƒƒãƒ—ã‹ã‚‰å†é–‹ï¼‰ | ã‚ã‚Šï¼ˆæœ€å¾Œã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹ï¼‰ |
| **ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸è¦ä»¶** | ä¸è¦ï¼ˆãƒ¡ãƒ¢ãƒªã®ã¿ï¼‰ | å¿…è¦ï¼ˆå¤§å®¹é‡ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ï¼‰ |
| **I/O ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰** | ãªã— | ã‚ã‚Šï¼ˆä¿å­˜ãƒ»èª­ã¿è¾¼ã¿æ™‚ï¼‰ |

##### å¿…é ˆè¨­å®š: Distributed Optimizer Instances

Checkpointless Recovery ã‚’æœ‰åŠ¹åŒ–ã™ã‚‹ã«ã¯ã€`num_distributed_optimizer_instances` ã‚’ **2 ä»¥ä¸Š** ã«è¨­å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

```python
trainer = Trainer(
    strategy=CheckpointlessMegatronStrategy(
        num_distributed_optimizer_instances=2  # æœ€å°å€¤ã¯ 2
    ),
)
```

ã“ã®è¨­å®šã«ã‚ˆã‚Šã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãŒå°‘ãªãã¨ã‚‚ 2 ã¤ã®ãƒãƒ¼ãƒ‰ã‚°ãƒ«ãƒ¼ãƒ—é–“ã§è¤‡è£½ã•ã‚Œã€å˜ä¸€ãƒãƒ¼ãƒ‰ã‚°ãƒ«ãƒ¼ãƒ—ã®éšœå®³ã«å¯¾ã™ã‚‹ãƒ•ã‚©ãƒ¼ãƒ«ãƒˆãƒˆãƒ¬ãƒ©ãƒ³ã‚¹ãŒç¢ºä¿ã•ã‚Œã¾ã™ã€‚å½“ç„¶ã§ã™ãŒãƒ¢ãƒ‡ãƒ«ãƒ¬ãƒ—ãƒªã‚«ã‚’è¿½åŠ ã™ã‚‹ã¨ã€ãƒ‡ãƒã‚¤ã‚¹ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¢—åŠ ã—ã¾ã™ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã¯ã‚ã‚‹ãŸã‚å®Ÿéš›ã«æ¤œè¨¼ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚

##### ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æˆ¦ç•¥

Checkpointless Recovery ãŒå®Ÿè¡Œä¸å¯èƒ½ãªå ´åˆã€è‡ªå‹•çš„ã«ãƒ‡ã‚£ã‚¹ã‚¯ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚**ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ãŒç™ºç”Ÿã™ã‚‹å ´åˆ**ã€ã™ã¹ã¦ã®ãƒ¬ãƒ—ãƒªã‚«ã‚°ãƒ«ãƒ¼ãƒ—ãŒåŒæ™‚ã«éšœå®³ã‚’èµ·ã“ã—ãŸå ´åˆã€ãƒ¢ãƒ‡ãƒ«çŠ¶æ…‹ã®ãƒã‚§ãƒƒã‚¯ã‚µãƒ ãŒä¸ä¸€è‡´ã®å ´åˆã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¹ãƒ†ãƒƒãƒ—ã®ä¸ä¸€è‡´ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã€ãªã©ã§ã™ã€‚`checkpoint_frequency` ã‚’è¨­å®šã—ã€å®šæœŸçš„ã«ãƒ‡ã‚£ã‚¹ã‚¯ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã—ã¦ã€æ·±åˆ»ãªéšœå®³ã«å¯¾ã™ã‚‹æœ€çµ‚çš„ãªå¾©æ—§æ‰‹æ®µã¨ã—ã¦ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

## 3 ã¤ã®æœ€é©åŒ–ãƒˆãƒ©ãƒƒã‚¯ã®é€£æº

ã“ã‚Œã‚‰ 3 ã¤ã®æœ€é©åŒ–ãƒˆãƒ©ãƒƒã‚¯ã¯ç‹¬ç«‹ã—ã¦å‹•ä½œã™ã‚‹ã®ã§ã¯ãªãã€ç›¸äº’ã«é€£æºã™ã‚‹ã“ã¨ã§æœ€å¤§ã®åŠ¹æœã‚’ç™ºæ®ã—ã¾ã™ã€‚ã“ã®çµ±åˆçš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ã‚ˆã‚Šã€HyperPod Checkpointless Training ã¯å¾“æ¥ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ™ãƒ¼ã‚¹ã®å¾©æ—§ã¨æ¯”è¼ƒã—ã¦ã€**å¤§å¹…ãªé«˜é€ŸåŒ–ã¨å­¦ç¿’é€²è¡Œã®æå¤±æœ€å°åŒ–**ã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚

## å¯¾å¿œç’°å¢ƒã¨åˆ¶ç´„

Checkpointless Training ã¯ä»¥ä¸‹ã®ç’°å¢ƒåˆ¶ç´„ãŒã‚ã‚Šã¾ã™ã€‚

| é …ç›® | è©³ç´° |
|------|------|
| ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ | **EKS ã®ã¿**ï¼ˆSlurm ç’°å¢ƒã¯å…¬å¼ã‚µãƒãƒ¼ãƒˆå¤–ï¼‰ï¼ˆ[å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-eks-operate-training-checkpointless.html)å‚ç…§ï¼‰ |
| ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ | **NVIDIA NeMo Toolkit 2.6.0rc0 ãŒå¿…é ˆ**ï¼ˆ[ã‚½ãƒ¼ã‚¹](https://github.com/aws/sagemaker-hyperpod-recipes)ï¼‰ã€‚PyTorch â‰¥ 2.6.0ã€PyTorch Lightningã€Megatron-Core ã«ä¾å­˜ï¼ˆå…·ä½“çš„ãªãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¯ãƒªãƒã‚¸ãƒˆãƒªã® pyproject.toml ã‚’å‚ç…§ï¼‰ |
| éå¯¾å¿œãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ | HuggingFace Transformers Trainerã€DeepSpeedã€ç´”ç²‹ãª PyTorchï¼ˆå…¬å¼ã‚µãƒãƒ¼ãƒˆå¤–ã€‚[å…¬å¼ãƒ¬ã‚·ãƒ”](https://github.com/aws/sagemaker-hyperpod-recipes)ã§ NeMo ãƒ™ãƒ¼ã‚¹ã®ã‚µãƒ³ãƒ—ãƒ«ã®ã¿æä¾›ï¼‰ |
| ã‚µãƒãƒ¼ãƒˆãƒ¢ãƒ‡ãƒ« | **Checkpointless Training ç”¨å…¬å¼ãƒ¬ã‚·ãƒ”**: GPT OSSã€Llama 3ã€‚**HyperPod Recipes å…¨ä½“**: Amazon Novaã€DeepSeek R1ã€Qwen ç­‰ã‚‚å«ã‚€ï¼ˆ[å…¬å¼ãƒ¬ã‚·ãƒ”ãƒªãƒã‚¸ãƒˆãƒª](https://github.com/aws/sagemaker-hyperpod-recipes)å‚ç…§ï¼‰ |
| ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ— | **ml.p5.48xlarge / ml.p5e.48xlarge æ¨å¥¨**ï¼ˆH100 GPUï¼‰ã€‚ml.p4d.24xlargeï¼ˆA100ï¼‰ã¯æœªæ¤œè¨¼ï¼ˆ[å…¬å¼ãƒ¬ã‚·ãƒ”](https://github.com/aws/sagemaker-hyperpod-recipes/blob/main/recipes_collection/recipes/fine-tuning/gpt_oss/checkpointless_gpt_oss_120b_full_fine_tuning.yaml)å‚ç…§ï¼‰ |
| GPU è¦ä»¶ | CUDA 12.5+ã€è¤‡æ•° GPU å¿…é ˆï¼ˆ`num_distributed_optimizer_instances >= 2`ï¼‰ï¼ˆ[å…¬å¼ãƒ¬ã‚·ãƒ”](https://github.com/aws/sagemaker-hyperpod-recipes)å‚ç…§ï¼‰ |
| Kubernetes CRD | `HyperPodPyTorchJob` ã‚’ä½¿ç”¨ï¼ˆEKS Training Operator â‰¥ 1.2.0ï¼‰ï¼ˆ[HyperPod CLI](https://github.com/aws/sagemaker-hyperpod-cli)å‚ç…§ï¼‰ |

## ã¾ã¨ã‚

Amazon SageMaker HyperPod ã® Checkpointless Training ã¯ã€å¤§è¦æ¨¡åˆ†æ•£å­¦ç¿’ã«ãŠã‘ã‚‹è€éšœå®³æ€§ã®æ–°ã—ã„ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ã‚’æä¾›ã—ã¾ã™ã€‚æœ¬è¨˜äº‹ã§è§£èª¬ã—ãŸ 3 ã¤ã®æœ€é©åŒ–ãƒˆãƒ©ãƒƒã‚¯ï¼ˆOptimized CC Initializationã€MMAPã€Program Restart Overhead Reductionã€‚3 ã¤ç›®ã¯ In-Process Recovery ã¨ Checkpointless Recovery ã§æ§‹æˆï¼‰ãŒç›¸äº’ã«é€£æºã™ã‚‹ã“ã¨ã§ã€å¾“æ¥ã¯æ•°ååˆ†ã‹ã‹ã£ã¦ã„ãŸéšœå®³å¾©æ—§ã‚’æ•°åˆ†ä»¥å†…ã«çŸ­ç¸®ã—ã€95% ä»¥ä¸Šã® goodput ã‚’å®Ÿç¾ã—ã¾ã™ã€‚äºˆç®—çš„ã«é™ã‚‰ã‚ŒãŸãƒªã‚½ãƒ¼ã‚¹ã‚’ã©ã‚Œã ã‘ã†ã¾ãåŠ¹ç‡çš„ã«ç„¡é§„ãªãä½¿ã†ã®ã‹ã€ã¨ã„ã†ã“ã¨ãŒå¤§è¦æ¨¡å­¦ç¿’ã§ã¯é‡è¦ã§ã™ã­ï¼

## å‚è€ƒè³‡æ–™

- [Amazon SageMaker HyperPod Features](https://aws.amazon.com/sagemaker/hyperpod/features/) -- å…¬å¼æ©Ÿèƒ½ãƒšãƒ¼ã‚¸ï¼ˆgoodput 95%ä»¥ä¸Šç­‰ã®æ•°å€¤ã®å‡ºå…¸ï¼‰
- [AWS SageMaker HyperPod å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod.html) -- 2026 å¹´ 2 æœˆæ™‚ç‚¹
- [AWS Blog: Introducing Checkpointless and Elastic Training on Amazon SageMaker HyperPod](https://aws.amazon.com/blogs/aws/introducing-checkpointless-and-elastic-training-on-amazon-sagemaker-hyperpod/) -- re:Invent 2024 å…¬å¼ç™ºè¡¨ãƒ–ãƒ­ã‚°ï¼ˆ[æ—¥æœ¬èªç‰ˆ](https://aws.amazon.com/jp/blogs/news/introducing-checkpointless-and-elastic-training-on-amazon-sagemaker-hyperpod/)ï¼‰
- [Checkpointless Training ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-checkpointless.html)
- [Checkpointless Training æ©Ÿèƒ½è©³ç´°](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-eks-checkpointless-features.html)
- [aws/sagemaker-hyperpod-checkpointless-training (GitHub)](https://github.com/aws/sagemaker-hyperpod-checkpointless-training) -- å…¬å¼ã‚µãƒ³ãƒ—ãƒ«ã¨ãƒ¬ã‚·ãƒ”
- [aws-samples/awsome-distributed-training (GitHub)](https://github.com/aws-samples/awsome-distributed-training) -- å®Ÿè·µçš„ãªåˆ†æ•£å­¦ç¿’ã‚³ãƒ¼ãƒ‰é›†
- [NVIDIA/NeMo (GitHub)](https://github.com/NVIDIA/NeMo) -- NeMo Framework
- [NVIDIA/Megatron-LM (GitHub)](https://github.com/NVIDIA/Megatron-LM) -- Megatron-Core

[^1]: [Amazon SageMaker HyperPod Features](https://aws.amazon.com/sagemaker/hyperpod/features/) -- å…¬å¼æ©Ÿèƒ½ãƒšãƒ¼ã‚¸ã«ã€Œenables over 95% training goodput on clusters with thousands of AI acceleratorsã€ã€Œautomatic recovery from infrastructure faults in minutesã€ã¨è¨˜è¼‰ã•ã‚Œã¦ã„ã¾ã™ã€‚
[^2]: [AWS Blog: Introducing Checkpointless and Elastic Training on Amazon SageMaker HyperPod](https://aws.amazon.com/blogs/aws/introducing-checkpointless-and-elastic-training-on-amazon-sagemaker-hyperpod/) -- re:Invent 2024 ã§ç™ºè¡¨ã•ã‚ŒãŸå…¬å¼ãƒ–ãƒ­ã‚°è¨˜äº‹ã§ã™ã€‚
[^3]: å¾“æ¥æ–¹å¼ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ•°å€¤ï¼ˆå¾©æ—§æ™‚é–“ã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ I/O ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ç­‰ï¼‰ã¯ã€å¤§è¦æ¨¡åˆ†æ•£å­¦ç¿’ã«ãŠã‘ã‚‹ä¸€èˆ¬çš„ãªçŸ¥è¦‹ã«åŸºã¥ãæ¨å®šå€¤ã§ã‚ã‚Šã€AWS ã®å…¬å¼ç™ºè¡¨ã«ã‚ˆã‚‹æ•°å€¤ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚å®Ÿéš›ã®å€¤ã¯ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ§‹æˆã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé »åº¦ç­‰ã«ã‚ˆã‚Šå¤§ããç•°ãªã‚Šã¾ã™ã€‚
[^4]: Checkpointless Training ã¯ EKS ç’°å¢ƒã§ã®ã¿åˆ©ç”¨å¯èƒ½ã§ã™ã€‚[å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-eks-operate-training-checkpointless.html)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚