---
title: "Amazon SageMaker HyperPod -- Checkpointless Training"
emoji: "🌵"
type: "tech"
topics: ["AWS", "SageMaker", "HyperPod", "分散学習", "Checkpointless"]
published: true
---

## はじめに

大規模分散学習において、ハードウェア障害は避けられない課題です。1000 ノード規模のクラスターで学習中、1 ノードの障害で全体が停止し、チェックポイントからの復旧に数十分を要し[^3]、その間、残りの数千 GPU はアイドル状態となります。

Amazon SageMaker HyperPod の **Checkpointless Training** は、この問題を解決します。チェックポイントをストレージに書き込むことなく、GPU メモリ内の冗長レプリカから数分以内に自動復旧し、95% 以上の goodput（障害やオーバーヘッドで失われない実効学習時間の割合）を実現します。

本記事では、この革新的な技術の仕組み、3 つの最適化トラック、導入方法について、2026 年 2 月時点の情報をもとに詳しく解説します。

:::message
**実際に試す際の推奨リポジトリ**: Checkpointless Training を試す際は、AWS の GenAI Frameworks team が管理する [`awsome-distributed-training`](https://github.com/aws-samples/awsome-distributed-training) リポジトリおよび公式の [`sagemaker-hyperpod-checkpointless-training`](https://github.com/aws/sagemaker-hyperpod-checkpointless-training) リポジトリの利用を強く推奨します。これらには実績のあるコードとサンプルが含まれており、環境構築期間を大幅に短縮できます。チュートリアルとしては [AI on SageMaker HyperPod](https://awslabs.github.io/ai-on-sagemaker-hyperpod/) もおすすめです。
:::

:::message alert
本記事は 2026 年 2 月時点の公式ドキュメント、オープンソースコード、などに基づく調査記事です。間違っている可能性もあるため必ず最新の公式ドキュメントを正として確認してください。間違いがあればコメントください。
:::

![](https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2025/11/18/2025-sageamker-hyperpod-checkpointless-training.gif)
*Checkpointless Training の動作イメージ（出典: AWS 公式ブログ）*

## 概要

Checkpointless Training は、GPU メモリ内に保持された冗長レプリカを使って障害から復旧する技術です。従来のチェックポイントベースのアプローチとは異なり、ストレージへの書き込みをほぼ不要にすることで、学習のボトルネックを解消します。

### 主要な特徴

- **goodput（実効学習時間の割合）**: 数千台規模のクラスターで 95% 以上を達成[^1]
- **復旧時間**: In-Process Recovery では数分以内に自動復旧（障害タイプにより異なる）[^1]
- **対応環境**: [HyperPod EKS 環境](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-eks-checkpointless.html)
- **フレームワーク要件**: NVIDIA NeMo Framework が必須（詳細は後述の details ブロック参照）

Llama、Qwen、DeepSeek のような標準的なアーキテクチャを使用している場合、すでに用意されているレシピを使えばコード変更ゼロで始めることができます。必要なのは、データを持ち込み、最小ノード数と最大ノード数を設定するだけなので、多くのケースでは特に裏側の実装を気にすることはないかもしれません。以下に Llama 3 のサンプルがあります。

https://github.com/aws/sagemaker-hyperpod-checkpointless-training/tree/main/examples/llama3/launch

re: Invent 2025 で本機能に関するセッションがあり、具体的な Salesforce からの利用事例が紹介されていました[^2]。

https://youtu.be/r9J10L2K0F4

::::details フレームワーク要件

2026 年 2 月時点では、[**NVIDIA NeMo Framework が必須**](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-eks-checkpointless.html)です。これは以下の技術的階層による制約です。

```text
[AWS 実装] Checkpointless Training
    ↓ (NeMo API に依存)
[ラッパー層] NeMo Framework
    ↓ (Megatron-Core をラップ)
[コア技術] Megatron-Core の num_distributed_optimizer_instances
    ↓ (実現手段)
[原理] GPU メモリ内冗長レプリカ
```

1. **Megatron-Core の冗長インスタンス機能がコア技術**: Checkpointless Training は Megatron-Core の `num_distributed_optimizer_instances >= 2` という機能に依存します。この機能により、オプティマイザ状態の冗長レプリカを GPU メモリ内に保持できます。
2. **AWS 実装の NeMo 依存**: AWS の実装は NeMo の PyTorch Lightning 統合、PEFT (LoRA) サポート、チェックポイント管理 API に依存しているため、現時点では NeMo が必須です。
3. **FSDP/DeepSpeed ZeRO が使えない理由**: これらのフレームワークには、Megatron-Core の `num_distributed_optimizer_instances` に相当する冗長インスタンス機能が現時点で実装されていないため、GPU メモリ内での冗長レプリカ保持ができません。各ランクが一意のシャードを保持する設計のため、1 ランクが失われるとそのシャードの状態は復旧不可能です。

> Checkpointless training on SageMaker HyperPod is built on top of the NVIDIA NeMo Framework User Guide. You can run checkpointless training with pre-created SageMaker HyperPod recipes. If you're familiar with NeMo, the process of using the checkpointless training recipes is similar. With minor changes, you can start training a model using checkpointless training features that enable you to recover quickly from training faults.

（出典: [AWS 公式ドキュメント](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-eks-checkpointless.html)）
::::

AWS 公式ドキュメントでは Checkpointless Training を [3 つの最適化トラック](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-eks-checkpointless-features.html) で説明しています。

![](https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/hyperpod/hyperpod-checkpointless-optimization-tracks.png)

## (1) Optimized Collective Communication (CC) Initialization

https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-eks-checkpointless-features-communication.html

![](https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/hyperpod/hyperpod-checkpointless-training-libraries.png)


従来の NCCL/Gloo ではすべてのプロセスが TCPStore という集中ストアに接続する必要があり、障害復旧のたびに完全な初期化シーケンスを繰り返す必要がありました。そしてルートプロセスが SPOF でした。1 の最適化で CC の初期化ボトルネックを解消し、Rootless かつ TCPStoreless な初期化方式を実現します。これにより障害復旧時の通信オーバーヘッドが大幅に削減されます。

::::details 補足: TCPStore とは何か

### TCPStore の役割

**TCPStore** は PyTorch Distributed における**分散プロセス間の初期化・調整のための中央集約型キーバリューストア**です。分散学習の初期化フェーズで、各ランク（プロセス）が互いを認識し、必要なメタデータを共有するために使用されます。8,192 ランク規模の大規模学習では、Master への接続数が 8,191 に達し、初期化に数十分を要するようです。

**初期化コード例**

```python
import torch.distributed as dist

dist.init_process_group(
    backend="nccl",
    init_method="tcp://master:29500",  # ← TCPStore のエンドポイント
    rank=0,
    world_size=8
)
```

このコードは内部で TCPStore を作成し、以下のような中央集約型アーキテクチャで動作します。

```mermaid
graph TB
    subgraph "TCPStore 使用"
        Master["TCPStore Master<br/>(rank 0)<br/>Port: 29500"]
        R0["rank 0<br/>(Client)"]
        R1["rank 1<br/>(Client)"]
        R2["rank 2<br/>(Client)"]
        RN["rank N<br/>(Client)"]

        R0 -->|TCP 接続| Master
        R1 -->|TCP 接続| Master
        R2 -->|TCP 接続| Master
        RN -->|TCP 接続| Master
    end

    style Master fill: #ffcccc
    style R0 fill: #ccffcc
    style R1 fill: #ccffcc
    style R2 fill: #ccffcc
    style RN fill: #ccffcc
```

### Rootless Configuration による改善

Rootless Configuration は、ルートプロセスへの依存を排除し、グローバルグループカウンターを通じた対称アドレスパターンによる分散初期化方式を導入します。TCPStoreless 最適化と合わせて、集中型の調整メカニズムを排除します。これによって、SPOF の排除、並列初期化による高速化、局所的な障害の影響を最小化、します。ただし TCP ポート消費増加などのトレードオフがあります。

```mermaid
graph LR
    subgraph "Rootless 方式"
        R0["rank 0"]
        R1["rank 1"]
        RN["rank N"]

        R0 <-->|分散初期化| R1
        R1 <-.->|分散初期化| RN
    end

    style R0 fill: #ccffcc
    style R1 fill: #ccffcc
    style RN fill: #ccffcc
```

注: 具体的な通信トポロジーの詳細は公式ドキュメントに明示されていません。この図は概念的な表現です。

**参考**
- [PyTorch Distributed TCPStore ドキュメント](https://pytorch.org/docs/stable/distributed.html#torch.distributed.TCPStore)
- [PyTorch ソースコード: rendezvous.py](https://github.com/pytorch/pytorch/blob/main/torch/distributed/rendezvous.py) - `_create_c10d_store()` 関数
- [PyTorch ソースコード: distributed_c10d.py](https://github.com/pytorch/pytorch/blob/main/torch/distributed/distributed_c10d.py) - `_store_based_barrier()` 関数

::::

### Rootless Configuration による解決策

HyperPod は **Rootless かつ TCPStoreless** な初期化方式を導入することで、これらの問題を解決しています。

**有効化方法**

```bash
export HPCT_USE_ROOTLESS=1 && \
sysctl -w net.ipv4.ip_local_port_range="20000 65535"
```

**設定パラメータ**
- `HPCT_USE_ROOTLESS`: 1 で有効化、0 で無効化
- `net.ipv4.ip_local_port_range`: 分散通信用にシステムポート範囲を拡張

### アーキテクチャの変更

| 層 | 主要コンポーネント | 修正内容 |
|---|---|---|
| **PyTorch Distributed API 層**<br/>(Python) | `torch.distributed.new_group()`<br/>`torch.distributed.init_process_group()` | TCPStore 作成をバイパス<br/>グローバルグループカウンターを通じて対称アドレスパターンを維持 |
| **PyTorch プロセスグループ層**<br/>(Python/C++) | `ProcessGroupNCCL`<br/>`ProcessGroupGloo` | ユーザー設定に基づいて最適化コードパスを条件付きで呼び出し<br/>In-Process Recovery メカニズムをサポート |
| **Third Party ライブラリ層**<br/>(C++) | NCCL<br/>Gloo | サードパーティライブラリの API を拡張し、Rootless および Storeless 最適化を実現<br/>オリジナルと最適化パスの柔軟な切り替えをサポートし後方互換性を維持 |

### 実装例

https://github.com/aws/sagemaker-hyperpod-checkpointless-training/blob/24520b41db0df3495b3946a7b74edaa70fbc339d/examples/llama3/launch/pretrain_llama3_70b_checkpointless_p5.yaml#L111-L113

## (2) Memory-Mapped Data Loading (MMAP)

![](https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/hyperpod/hyperpod-checkpointless-mmap-dataloader.png)

従来の DataLoader では、プロセス再起動時にデータパイプライン全体を再構築する必要があり、最初のバッチ生成に数分かかっていました。また、各 GPU ランクがメモリ内に独立したデータコピーを保持するため、メモリ使用量が非効率でした。MMAP は学習データを永続的なメモリマップドキャッシュに保持することで、プロセス再起動時の最初のバッチ生成時間をゼロにし、GPU 8 台の p5 インスタンスではデータコピーを 8 個から 1 個に削減します。

**Memory-Mapped Data Loading の技術詳細**

https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-eks-checkpointless-features-mmap.html

従来の DataLoader アプローチには、以下の 3 つの問題がありました。

1. **プロセス再起動時のデータパイプライン再構築オーバーヘッド** - 障害復旧やスケーリング時に、データ前処理パイプライン全体を再初期化する必要があり、最初のバッチ生成に数分を要します。
2. **メモリ使用の非効率性** - 各 GPU ランクが独立したデータコピーをメモリに保持するため、ノードあたりのメモリ使用量が GPU 数に比例して増加します。
3. **データローディングの待機時間** - 学習ステップがデータ到着を待つボトルネックが発生し、GPU 利用率が低下します。

:::message
HyperPod は **メモリマップド I/O（mmap）と永続キャッシュ** を組み合わせることで、これらの問題を解決しています。
:::

### アーキテクチャとデータフロー

Memory-Mapped DataLoader は、以下の 3 層構造で動作します。

```mermaid
graph TB
    subgraph "ストレージ層"
        S3[学習データ<br/>S3 / FSx]
    end

    subgraph "永続キャッシュ層"
        Cache[メモリマップド<br/>キャッシュ<br/>プロセス再起動後も残存]
        PF[プリフェッチ<br/>バッチ]
        LB[過去使用済み<br/>バッチ]
    end

    subgraph "学習プロセス層"
        DL[DataLoader]
        Rank0[TP Rank 0<br/>データ取得担当]
        Rank1[TP Rank 1-N<br/>キャッシュ読み取り]
        Train[学習ループ]
    end

    S3 -->|初回ロード| DL
    DL -->|プリフェッチ| PF
    PF -->|mmap 共有| Rank0
    PF -->|mmap 共有| Rank1
    Rank0 --> Train
    Rank1 --> Train
    Train -->|消費済み| LB

    style Cache fill: #d4edda
    style PF fill: #cce5ff
    style LB fill: #fff3cd
```

1. **初回ロード時** DataLoader が S3/FSx から学習データを取得し、永続キャッシュに書き込み
2. **プリフェッチ** 学習ループより先行して、次の `prefetch_length` 個バッチをキャッシュに準備
3. **メモリマップド共有** 各 TP Rank 0 のみがデータを取得し、同じレプリケーショングループ内の他のランクは mmap で共有アクセス
4. **過去バッチの保持** 消費済みバッチも `lookback_length` 数だけキャッシュに保持し、プロセス再起動時の即座の再開を可能に

### 実装例

https://github.com/aws/sagemaker-hyperpod-checkpointless-training/blob/24520b41db0df3495b3946a7b74edaa70fbc339d/examples/llama3/llama3_70b_pretrain_checkpointless.py#L101-L109

MMAP の有効化は、既存の PyTorch Lightning DataModule を `MMAPDataModule` でラップするだけで完了します。

## (3) Program Restart Overhead Reduction

https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-eks-checkpointless-in-process-recovery.html

![](https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/hyperpod/hyperpod-checkpointless-training-flow.png)

AWS 公式ドキュメントでは「Program restart overhead reduction」として説明されている最適化トラックです。これは **In-Process Recovery** と **Checkpointless Recovery** の 2 つの技術を組み合わせることで実現されます。

従来の障害復旧では、全プロセスを停止し、ディスクからチェックポイントをロードして再起動する必要があり、復旧に数分〜数十分を要していました。また、前回のチェックポイント以降の学習進行が失われるため、計算リソースが無駄になっていました。

HyperPod は以下の 2 つの技術でこれを解決します。

| 技術 | 責務 | 解決する問題 |
|------|------|-------------|
| **In-Process Recovery (IPR)** | **復旧メカニズム**<br/>プロセスを終了せずに障害から復旧する仕組み | プロセス再起動のオーバーヘッド、学習ステップの損失 |
| **Checkpointless Recovery** | **状態管理アーキテクチャ**<br/>GPU メモリ内にモデルの冗長レプリカを保持する設計 | ディスクチェックポイントの I/O ボトルネック、ストレージコスト |

1. **In-Process Recovery** 「どうやって復旧するか」
   - 障害発生時に、プロセスを終了せずに RCB (Re-Executable Code Block: 障害復旧時に再実行可能なコードセグメント) を再実行することで、プロセス再起動やディスク I/O を伴う従来方式と比較して大幅に高速な復旧を実現
   - 前のステップの完了した状態から再開するため、チェックポイント間の学習進行が失われる従来方式と比較して、学習進行の損失を最小化

2. **Checkpointless Recovery** 「どこから復旧するか」
   - 複数のノードグループ間でモデル・オプティマイザ状態を冗長化し、GPU メモリ内に保持
   - ディスクチェックポイントを使わずに、健全なレプリカから状態を取得

**2 つの技術の連携** In-Process Recovery が「プロセスを継続しながら復旧する仕組み」を提供し、Checkpointless Recovery が「復旧に必要な状態を GPU メモリ内に保持する設計」を提供することで、ディスク I/O を完全に排除した高速障害復旧を実現します。

### In-Process Recovery

![](https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/hyperpod/hyperpod-checkpointless-fault-controller-module.png)

従来の障害復旧アプローチには、以下の 4 つの問題がありました。

1. **プロセス再起動のオーバーヘッド** - 全ランクのプロセスを停止し、新規プロセスを起動する必要があり、NCCL 集団通信の再初期化に数分を要する
2. **ディスク I/O のボトルネック** - チェックポイントファイル（数十 GB〜数百 GB）をストレージ（S3 や FSx）からロードする必要があり、復旧に数分〜数十分かかる[^3]
3. **学習進行の損失** - 前回のチェックポイント以降の学習ステップが無駄になり、再計算が必要（例: チェックポイント間隔が 100 ステップの場合、最大 100 ステップ分の計算が失われる）
4. **データパイプライン再構築の遅延** - DataLoader を再初期化し、最初のバッチ生成を待つ必要がある

:::message
HyperPod は **GPU メモリ内のモデル冗長化とプロセス内復旧** を組み合わせることで、これらの問題を解決しています。
:::

| 観点 | In-Process Recovery | 既存アプローチ方式 |
|------|---------------------|----------------|
| **状態保存先** | GPU メモリ（プロセス内） | ディスクチェックポイント |
| **復旧速度** | 高速（プロセス内復旧） | 数秒〜数分（ディスク I/O） |
| **学習進行の損失** | 最小化（前のステップから再開） | 前回チェックポイントから再開 |
| **プロセス継続性** | プロセスは継続 | プロセス再起動が必要な場合がある |
| **アーキテクチャ** | 複数ノードグループ間のモデル冗長化 | レプリカごとの単一コピー |

#### 障害タイプと復旧メカニズム

| 障害タイプ | 原因 | 復旧タイプ | メカニズム |
|-----------|------|-----------|-----------|
| **In-Process 障害** | コードレベルエラー、例外 | In-Process Recovery (IPR) | 既存プロセス内で RCB を再実行 |
| **Process Restart 障害** | CUDA コンテキスト破損、プロセス終了 | Process Level Restart (PLR) | SageMaker HyperPod がプロセスを再起動。K8s Pod 再起動はスキップ |
| **Node Replacement 障害** | 恒久的なハードウェア障害 | Job Level Restart (JLR) | 故障ノードを交換。ジョブ全体を再起動 |

#### Atomic Lock の役割

分散学習では、1 回の学習ステップは **Forward Pass、Backward Pass、Optimizer Step** の 3 つのフェーズで構成されます。実行時間の観点では Forward/Backward Pass が大半を占めると思われますが、**Optimizer Step は一度完了すると破棄できない重要な操作**です。

Optimizer Step が完了すると、新しいモデル状態（重み、オプティマイザ状態、学習率スケジューラの状態など）が確定します。この確定した状態は次のステップに必須であり、破棄すると再計算が必要になります。大規模モデルでは数百 GB のパラメータとオプティマイザ状態へのメモリアクセス、さらに分散環境での通信コスト（勾配の all-reduce、パラメータシャードの同期など）が発生するため、再計算のコストは無視できません。

従来の障害復旧では、どのフェーズで障害が発生しても、常に前のステップの開始地点までロールバックしていました。これは、Optimizer Step の途中で障害が発生した場合、健全なレプリカが既に完了した計算結果を破棄することを意味します。例えば、8 レプリカのうち 1 つが Optimizer Step の 90% 完了時点で障害を起こすと、残り 7 つの健全なレプリカが完了した Optimizer Step をすべて捨てて、前のステップから再計算する必要があります。

Atomic Lock は、この無駄を防ぐために導入された仕組みです。**Optimizer Step をアトミックな操作として保護**することで、健全なレプリカが完了した計算結果を失わずに済むようにします。

#### 学習ステップの 3 つのフェーズと障害時の動作

![](https://docs.aws.amazon.com/images/sagemaker/latest/dg/images/hyperpod/hyperpod-checkpointless-optimizer.png)

**Case 1: Forward/Backward Pass 中の障害（Optimizer Step 前）**

Forward Pass または Backward Pass の実行中に障害が発生した場合、まだ Atomic Lock は取得されていません。この時点ではモデルの重みはまだ更新されておらず、計算途中の中間結果（勾配など）のみが存在します。

障害が検出されると、システムは**現在の学習ステップの先頭までロールバック**します。Checkpointless Recovery と連携し、健全なレプリカは前のステップで完了したモデル状態（既に Optimizer Step で更新済みの重み）をブロードキャストし、すべてのレプリカがその状態から新たに Forward Pass を再実行します。この場合、失われるのは現在のステップの計算途中の中間結果だけなので、チェックポイント間隔に依存する従来方式と比較して損失は最小限です。

**Case 2: Optimizer Step 中の障害（Atomic Lock 保持中）**

各レプリカは **Optimizer Step の直前**（勾配クリッピング後）に **Atomic Lock (`ParameterUpdateLock`)** を取得し、バッチ完了時（`on_train_batch_end`）に解放します。つまり、Forward Pass と Backward Pass はロックの保護対象外であり、これらのフェーズ中の障害は Case 1 として扱われます。ロックが保持されている間、Checkpointless Recovery は無効化されます。

:::message
以下の説明は、Optimizer Step のみに焦点を当てた概念的な説明です。Forward/Backward Pass 中の障害は前のステップからの再実行で対処できるため、Optimizer Step の保護が最も重要になります。
:::

Optimizer Step の実行中に一部のレプリカで障害が発生しても、健全なレプリカは**ロックに保護されたまま Optimizer Step を継続し、最後まで完了**します。障害が発生したレプリカは計算を中断しますが、プロセス自体は終了しません。

健全なレプリカが Optimizer Step を完了すると、**更新された新しいモデル状態**（重み、オプティマイザ状態、学習率スケジューラの状態など）を障害から復旧したレプリカにブロードキャストします。復旧したレプリカは、この更新済みの状態を受け取り、**次のステップから再開**します。

この仕組みにより、健全なレプリカが既に完了した Optimizer Step の計算結果を破棄することなく、そのまま次のステップに進むことができます。

**2 つのケースの比較**

```mermaid
sequenceDiagram
    autonumber
    participant R1 as 健全なレプリカ
    participant R2 as 障害レプリカ
    participant Lock as Atomic Lock

    rect rgb(255, 230, 230)
        Note over R1, Lock: Case 1: Forward/Backward 中の障害
        R1->>R1: Forward Pass 実行中
        R2->>R2: Forward Pass 実行中
        Note over R2: [障害発生]
        R1->>R1: 前のステップの状態を保持
        R1->>R2: 前のステップのモデル状態をブロードキャスト
        R2->>R2: 状態を復元
        R1->>R1: Forward Pass を再実行
        R2->>R2: Forward Pass を再実行
        Note over R1, R2: 結果: 前のステップから再実行<br/>（Forward/Backward の計算だけが失われる）
    end

    rect rgb(230, 255, 230)
        Note over R1, Lock: Case 2: Optimizer Step 中の障害
        R1->>Lock: Atomic Lock 取得
        R2->>Lock: Atomic Lock 取得
        R1->>R1: Optimizer Step 実行中
        R2->>R2: Optimizer Step 実行中（80% 完了）
        Note over R2: [障害発生]
        R1->>R1: Optimizer Step を継続
        R1->>R1: Optimizer Step 完了
        R1->>Lock: Atomic Lock 解放
        R1->>R2: 更新済みモデル状態をブロードキャスト
        R2->>R2: 新しい状態を受信
        R1->>R1: 次のステップ（Forward Pass）開始
        R2->>R2: 次のステップ（Forward Pass）開始
        Note over R1, R2: 結果: 次のステップから再開<br/>（Optimizer Step の計算結果は保持）
    end
```

### In-Process Recovery の実装

In-Process Recovery は `@HPWrapper` の設定によって実現されます。以下が実装の鍵となる設定です：

```python
from hyperpod_checkpointless_training.inprocess import HPWrapper

@HPWrapper(
    health_check=CudaHealthCheck(),           # CUDA デバイスの健全性監視
    hp_api_factory=HPAgentK8sAPIFactory(),   # HyperPod API との統合
    abort_timeout=60.0,                       # 障害時の中断タイムアウト（秒）
    checkpoint_manager=PEFTCheckpointManager(enable_offload=False),  # チェックポイント管理
    abort=CheckpointlessAbortManager.get_default_checkpointless_abort(),  # 中断処理
    finalize=CheckpointlessFinalizeCleanup(), # 障害復旧時のクリーンアップ
)
def training_function():
    """
    この関数全体が RCB (Re-Executable Code Block) です。
    障害復旧時に、メモリ内のモデルとオプティマイザ状態を
    保持したまま、この関数が再実行されます。
    """
    trainer.fit(model, datamodule)
```

**各パラメータの役割**

| パラメータ | 役割 |
|-----------|------|
| `health_check` | CUDA デバイスの健全性を監視し、障害を検出 |
| `hp_api_factory` | HyperPod の Health Monitoring Agent と連携 |
| `abort_timeout` | 障害発生時の中断処理のタイムアウト時間 |
| `checkpoint_manager` | Checkpointless Recovery と連携したチェックポイント管理 |
| `abort` | 障害時の中断処理とリソースクリーンアップ |
| `finalize` | 障害復旧時のランクローカルなクリーンアップ処理 |

参考実装: [llama3_70b_peft_checkpointless.py](https://github.com/aws/sagemaker-hyperpod-checkpointless-training/blob/24520b41db0df3495b3946a7b74edaa70fbc339d/examples/llama3/llama3_70b_peft_checkpointless.py#L184-L193)

**RCB (Re-Executable Code Block) とは**

`@HPWrapper` でラップされた関数全体が RCB として登録されます。従来の復旧方式ではプロセス全体を再起動する必要がありましたが、RCB を使用すると、GPU メモリ内のモデル・オプティマイザ状態を保持したまま、障害時にこの関数だけを再実行できます。これにより、プロセス再起動やディスク I/O を伴わない高速な復旧が実現されます。

**RCB の動作フロー**

```mermaid
sequenceDiagram
    autonumber
    participant User as ユーザーコード
    participant HPW as HPWrapper
    participant FC as Fault Controller
    participant GPU as GPU メモリ
    participant Health as Health Monitor

    rect rgb(230, 245, 255)
        Note over User, Health: 正常時の学習
        User->>HPW: training_function() 呼び出し
        HPW->>FC: RCB を登録
        HPW->>GPU: モデル・オプティマイザ状態を保持
        HPW->>User: trainer.fit() 実行中
    end

    rect rgb(255, 230, 230)
        Note over Health: 障害検出
        Health->>FC: インフラ障害通知
        FC->>HPW: RCB 中断シグナル
        HPW->>GPU: GPU メモリ状態を保持
        Note over GPU: モデル・オプティマイザ状態は<br/>メモリに残存
    end

    rect rgb(230, 255, 230)
        Note over FC, GPU: In-Process Recovery
        FC->>FC: リソースクリーンアップ
        FC->>HPW: RCB 再実行トリガー
        HPW->>GPU: メモリ内の状態を使用
        HPW->>User: training_function() 再実行
        User->>HPW: trainer.fit() 継続
        Note over User: 前のステップから再開<br/>（ディスク I/O なし）
    end
```

**Fault Controller モジュールの役割**

| モジュール | 役割 | 実行内容 |
|-----------|------|---------|
| 障害検出 | インフラ障害の監視 | Health Monitoring Agent から障害通知を受信 |
| RCB 定義 API | ユーザーコードの登録 | `@HPWrapper` で装飾された関数を RCB として登録 |
| 再起動モジュール | In-Process Recovery の実行 | RCB を中断、リソースをクリーンアップ、GPU メモリを保持したまま RCB を再実行 |

:::message
In-Process Recovery は、Checkpointless Recovery と組み合わせることで最大の効果を発揮します。モデル冗長化のトレードオフや必須設定（`num_distributed_optimizer_instances ≥ 2`）については、後述の「Checkpointless Recovery」セクションで詳しく解説します。
:::

:::message alert
#### In-Process Recovery まとめ

最後のチェックポイントから再開する従来のチェックポイント方式とは異なり、In-Process Recovery は最新のモデル・オプティマイザ状態を GPU メモリに保持するため、障害が発生した時点からシームレスにリカバリーできます。
:::

### Checkpointless Recovery

In-Process Recovery に加えて、HyperPod は **Checkpointless Recovery** を提供します。これは、モデルレプリカを複数のノードグループに分散配置することで、ディスクチェックポイントを完全に排除した障害復旧を実現する技術です。

#### 解決する課題

従来のチェックポイントベースの障害復旧には、以下の 3 つの問題がありました。

1. **チェックポイント I/O のオーバーヘッド** - 大規模モデルのチェックポイント（数十 GB〜数百 GB）をディスクに定期的に保存する必要があり、学習がブロックされる（数秒〜数分）
2. **ストレージコストの増大** - チェックポイントを S3 や FSx に保存するため、ストレージコストが増加し、チェックポイント頻度とストレージコストのトレードオフが発生する
3. **単一ノード障害時のシャード損失** - FSDP のようなシャーディング方式では、各ノードが一意のモデルシャードを保持するため、1 ノードが故障するとそのシャードが失われ、ディスクからの復旧が必須となる

:::message
HyperPod は **GPU メモリ内のモデル冗長化と In-Process Recovery** を組み合わせることで、ディスクチェックポイントを完全に排除します。
:::

**基本原理**

Checkpointless Recovery は、以下の 2 つの技術を組み合わせることで実現されます。

1. **In-Process Recovery**: プロセス内での高速復旧メカニズム
2. **Model Redundancy**: 複数ノードグループ間でのモデル・オプティマイザ状態の完全複製

この組み合わせにより、ディスクベースのチェックポイント保存・復元を完全にスキップし、メモリからメモリへの状態転送のみで復旧を実現します。

#### レプリカ配置戦略

HyperPod Checkpointless Training は、モデルレプリカを物理的に異なるノードグループに分散配置します。これにより、単一ノードまたは単一ノードグループの障害が発生しても、他のノードグループ上の健全なレプリカから状態を復旧できます。

**レプリカ配置の例（p5.48xlarge インスタンスの場合）**

以下の図は `num_distributed_optimizer_instances=2` の設定に対応しています。Replica 1 と Replica 2 がそれぞれ 1 つの冗長インスタンスに該当し、同一のモデル・オプティマイザ状態を異なるノードグループに保持します。

```mermaid
graph TB
    subgraph "Node Group 1"
        direction LR
        N1_R1[Replica 1<br/>Node 1<br/>GPU 0-7]
    end

    subgraph "Node Group 2"
        direction LR
        N2_R2[Replica 2<br/>Node 2<br/>GPU 0-7]
    end

    subgraph "Node Group 3"
        direction LR
        N3_R1[Replica 1<br/>Node 3<br/>GPU 0-7]
    end

    subgraph "Node Group 4"
        direction LR
        N4_R2[Replica 2<br/>Node 4<br/>GPU 0-7]
    end

    N1_R1 -.同期.-> N3_R1
    N2_R2 -.同期.-> N4_R2
    N1_R1 <-.復旧時の状態転送.-> N2_R2
    N3_R1 <-.復旧時の状態転送.-> N4_R2

    style N1_R1 fill: #d4edda
    style N3_R1 fill: #d4edda
    style N2_R2 fill: #cce5ff
    style N4_R2 fill: #cce5ff
```

**配置の特徴**

このレプリカ配置では、各レプリカを物理的に異なるノードに配置することで、単一ノード障害に対する耐性を確保しています。同一レプリカ内（Replica 1 や Replica 2）では、各ランクが reduce-scatter と all-gather を通じてオプティマイザ状態のシャードを共有・同期します。さらに、レプリカ間（Replica 1 ↔ Replica 2）でも通常時から all-reduce で同期が行われており、この継続的な同期が冗長性を維持し、障害時の迅速な復旧を可能にします。その結果、単一ノードで障害が発生しても、他のノードグループ上に存在する健全なレプリカから状態を復元できます。

#### チェックポイントレス復旧のワークフロー

**正常時の動作**

正常時、システムは各学習ステップを実行し、各レプリカグループ内で重みとオプティマイザ状態を同期します。この同期はメモリ上で完結するため、ディスクへの保存は一切行わず、即座に次のステップへ進行します。

**障害発生時の動作**

障害が発生すると（例: Node 1 の GPU 故障）、まずシステムは復旧実行可能性を評価します。`CheckpointManager.checkpointless_recovery_feasible()` メソッドが、健全なレプリカ（Node 2, 3, 4）の存在、グローバルステップの一貫性、およびモデル状態のチェックサム（オプション）を検証します。検証が成功すれば、Node 2 または Node 4 の健全なレプリカから最新のモデル・オプティマイザ状態を取得し、Node 1 のプロセス内で状態を復元します。復元が完了すると、ディスクからのロードを経ることなく、同じステップから学習を再開できます。

##### ディスクチェックポイントとの比較

| 観点 | Checkpointless Recovery | 従来のディスクチェックポイント |
|------|------------------------|---------------------------|
| **状態保存先** | GPU メモリ（複数ノードグループ） | ディスク（FSx Lustre など） |
| **保存頻度** | 毎ステップ（メモリ同期） | 定期的（例: 100 ステップごと） |
| **復旧速度** | 高速（メモリ転送） | 数秒〜数分（ディスク I/O） |
| **学習進行の損失** | 最小化（前のステップから再開） | あり（最後のチェックポイントから再開） |
| **ストレージ要件** | 不要（メモリのみ） | 必要（大容量ストレージ） |
| **I/O オーバーヘッド** | なし | あり（保存・読み込み時） |

##### 必須設定: Distributed Optimizer Instances

Checkpointless Recovery を有効化するには、`num_distributed_optimizer_instances` を **2 以上** に設定する必要があります。

```python
trainer = Trainer(
    strategy=CheckpointlessMegatronStrategy(
        num_distributed_optimizer_instances=2  # 最小値は 2
    ),
)
```

この設定により、オプティマイザが少なくとも 2 つのノードグループ間で複製され、単一ノードグループの障害に対するフォールトトレランスが確保されます。当然ですがモデルレプリカを追加すると、デバイスメモリ使用量が増加します。トレードオフはあるため実際に検証することをお勧めします。

##### フォールバック戦略

Checkpointless Recovery が実行不可能な場合、自動的にディスクチェックポイントにフォールバックします。**フォールバックが発生する場合**、すべてのレプリカグループが同時に障害を起こした場合、モデル状態のチェックサムが不一致の場合、グローバルステップの不一致が検出された場合、などです。`checkpoint_frequency` を設定し、定期的にディスクチェックポイントを保存して、深刻な障害に対する最終的な復旧手段として使用してください。

## 3 つの最適化トラックの連携

これら 3 つの最適化トラックは独立して動作するのではなく、相互に連携することで最大の効果を発揮します。この統合的なアプローチにより、HyperPod Checkpointless Training は従来のチェックポイントベースの復旧と比較して、**大幅な高速化と学習進行の損失最小化**を実現しています。

## 対応環境と制約

Checkpointless Training は以下の環境制約があります。

| 項目 | 詳細 |
|------|------|
| オーケストレーター | **EKS のみ**（Slurm 環境は公式サポート外）（[公式ドキュメント](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-eks-operate-training-checkpointless.html)参照） |
| フレームワーク | **NVIDIA NeMo Toolkit が実質必須**（[公式ドキュメント](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-eks-checkpointless.html)）。PyTorch、PyTorch Lightning、Megatron-Core に依存（具体的なバージョンは [HyperPod Recipes リポジトリ](https://github.com/aws/sagemaker-hyperpod-recipes)を参照） |
| 非対応フレームワーク | HuggingFace Transformers Trainer、DeepSpeed、純粋な PyTorch（公式サポート外。[公式レシピ](https://github.com/aws/sagemaker-hyperpod-recipes)で NeMo ベースのサンプルのみ提供） |
| インスタンスタイプ | **ml.p5.48xlarge / ml.p5e.48xlarge 推奨**（H100 GPU）。 推奨以外は検証が必須と思われる。 |
| GPU 要件 | CUDA 12.5+、複数 GPU 必須（`num_distributed_optimizer_instances >= 2`）（[公式レシピ](https://github.com/aws/sagemaker-hyperpod-recipes)参照） |
| Kubernetes CRD | `HyperPodPyTorchJob` を使用（EKS Training Operator ≥ 1.2.0）（[HyperPod CLI](https://github.com/aws/sagemaker-hyperpod-cli)参照） |

## まとめ

Amazon SageMaker HyperPod の Checkpointless Training は、大規模分散学習における耐障害性の新しいパラダイムを提供します。本記事で解説した 3 つの最適化トラック（Optimized CC Initialization、MMAP、Program Restart Overhead Reduction。3 つ目は In-Process Recovery と Checkpointless Recovery で構成）が相互に連携することで、従来は数十分かかっていた障害復旧を数分以内に短縮し、95% 以上の goodput を実現します。予算的に限られたリソースをどれだけうまく効率的に無駄なく使うのか、ということが大規模学習では重要ですね！

## 参考資料

- [Amazon SageMaker HyperPod Features](https://aws.amazon.com/sagemaker/hyperpod/features/) -- 公式機能ページ（goodput 95%以上等の数値の出典）
- [AWS SageMaker HyperPod 公式ドキュメント](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod.html) -- 2026 年 2 月時点
- [AWS Blog: Introducing Checkpointless and Elastic Training on Amazon SageMaker HyperPod](https://aws.amazon.com/blogs/aws/introducing-checkpointless-and-elastic-training-on-amazon-sagemaker-hyperpod/) -- re:Invent 2024 公式発表ブログ（[日本語版](https://aws.amazon.com/jp/blogs/news/introducing-checkpointless-and-elastic-training-on-amazon-sagemaker-hyperpod/)）
- [Checkpointless Training ドキュメント](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-checkpointless.html)
- [Checkpointless Training 機能詳細](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-eks-checkpointless-features.html)
- [aws/sagemaker-hyperpod-checkpointless-training (GitHub)](https://github.com/aws/sagemaker-hyperpod-checkpointless-training) -- 公式サンプルとレシピ
- [aws-samples/awsome-distributed-training (GitHub)](https://github.com/aws-samples/awsome-distributed-training) -- 実践的な分散学習コード集
- [NVIDIA/NeMo (GitHub)](https://github.com/NVIDIA/NeMo) -- NeMo Framework
- [NVIDIA/Megatron-LM (GitHub)](https://github.com/NVIDIA/Megatron-LM) -- Megatron-Core

[^1]: [Amazon SageMaker HyperPod Features](https://aws.amazon.com/sagemaker/hyperpod/features/) -- 公式機能ページに「enables over 95% training goodput on clusters with thousands of AI accelerators」「automatic recovery from infrastructure faults in minutes」と記載されています。
[^2]: [AWS Blog: Introducing Checkpointless and Elastic Training on Amazon SageMaker HyperPod](https://aws.amazon.com/blogs/aws/introducing-checkpointless-and-elastic-training-on-amazon-sagemaker-hyperpod/) -- re:Invent 2024 で発表された公式ブログ記事です。
[^3]: 従来方式のパフォーマンス数値（復旧時間、チェックポイント I/O オーバーヘッド等）は、大規模分散学習における一般的な知見に基づく推定値であり、AWS の公式発表による数値ではありません。実際の値はクラスター構成、モデルサイズ、チェックポイント頻度等により大きく異なります。
[^4]: Checkpointless Training は EKS 環境でのみ利用可能です。[公式ドキュメント](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-eks-operate-training-checkpointless.html)を参照してください。