---
title: "SageMaker HyperPod の Resiliency"
emoji: "🌵"
type: "idea"
topics: ["AWS", "SageMaker", "HyperPod", "分散学習", "Resilient"]
published: false
---

## はじめに

Amazon SageMaker HyperPod は、機械学習ワークロードを実行し、大規模言語モデル、拡散モデル、基盤モデルなどの最先端のモデルを開発するための回復力のあるクラスターをプロビジョニングできるマネージドサービスです。今回は比較的最近発表された以下のアップデートについて整理します。

| 機能 | 概要 | 対応環境 |
|------|------|---------|
| **Checkpointless Training** | チェックポイントを書き込まずに障害から復旧する革新的な耐障害性メカニズム | EKS のみ |
| **Elastic Training** | クラスター容量に応じてノード数を動的に増減する弾力的訓練 | EKS のみ |
| **Managed Tiered Checkpointing** | CPU メモリと S3 の 2 階層を活用した高速チェックポイント | EKS / Slurm |
| **HyperPod Health Monitoring Agent** | GPU/Trainium/EFA の常時監視と自動障害復旧 | EKS / Slurm |

:::message
**実際に試す際の推奨リポジトリ**: これらの機能を実環境で試す際は、AWS Frameworks Team が管理する [`awsome-distributed-training`](https://github.com/aws-samples/awsome-distributed-training) リポジトリの利用を強く推奨します。このリポジトリには、HyperPod の各機能に対応した構成済みの環境設定、Docker イメージ、サンプルコードが含まれており、環境構築を大幅にスムーズにできます。特に Checkpointless Training や Elastic Training は複雑な設定が必要なため、公式の実証済み構成をベースにすることで試行錯誤の時間を大幅に短縮できます。
:::

これらの機能は、大規模訓練における 3 つの課題を解決します。

| 課題 | 従来のアプローチ | HyperPod のソリューション |
|------|----------------|-------------------------------|
| ハードウェア障害による訓練中断 | 定期的なチェックポイント保存 + 手動復旧 | Checkpointless Training + Health Monitoring Agent |
| チェックポイントの I/O ボトルネック | S3 への同期書き込み | Managed Tiered Checkpointing |
| リソースの非効率な利用 | 固定ノード数での運用 | Elastic Training |

本記事では、各機能の実装メカニズムをソースコードレベルで分析し、実践的なコード例とともに解説します。

:::message
本記事は 2026 年 2 月時点の情報に基づいています。AWS サービスは頻繁にアップデートされるため、最新の仕様は公式ドキュメントを参照してください。
:::

:::message alert
本記事は公式ドキュメント、オープンソースコード、AWS の発表資料に基づく調査記事です。各機能の実装の詳細や実環境での挙動検証については、今後別の記事で深掘りする予定です。実際の導入にあたっては、必ず公式ドキュメントと最新の仕様を確認してください。
:::

---

## 1. Checkpointless Training

![](https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2025/11/18/2025-sageamker-hyperpod-checkpointless-training.gif)

### 1.1 概要

Checkpointless Training は、**チェックポイントをストレージに書き込むことなく、障害から復旧する**機能です。従来の分散訓練では、障害復旧のために定期的にモデルの状態をストレージ（S3 や FSx）に書き込む必要があり、これが大きなオーバーヘッドとなっていました。

AWS の公式発表[^1]によれば、数千台の AI アクセラレータを持つクラスターにおいて **95% 以上の訓練 goodput**（実効訓練時間の割合）を実現し、障害からの自動復旧を**数分以内**に完了するとされています。

現時点では **HyperPod EKS 環境限定**の機能です（[Checkpointless Training ドキュメント](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-eks-operate-training-checkpointless.html)参照）。コアライブラリ（`hyperpod_checkpointless_training`）は PyTorch のみに依存しますが、公式サンプルでは NeMo Framework との統合プラグインを通じて利用されます（2026 年 2 月時点）。公式サンプルで動作確認されているモデルは GPT OSS および Llama 3 です。

### 1.2 4 つのイノベーション

Checkpointless Training は、以下の 4 つの技術コンポーネントで構成されます。

```mermaid
graph TB
    subgraph "Checkpointless Training の 4 つのコンポーネント"
        A["1. Optimized Collective<br/>Communication Initialization<br/>（最適化された集団通信初期化）"]
        B["2. Memory-Mapped<br/>Data Loading<br/>（メモリマップドデータ読込）"]
        C["3. In-Process Recovery<br/>（プロセス内復旧）"]
        D["4. Checkpointless Recovery<br/>（チェックポイントレス復旧）"]
    end

    A --> E["高速な訓練再開"]
    B --> E
    C --> E
    D --> E

    E --> F["従来: 数十分の復旧時間"]
    E --> G["改善後: 数分以内の復旧"]
```

#### (1) Optimized Collective Communication Initialization

分散訓練の起動時に行われる NCCL 集団通信の初期化処理を最適化します。従来はノード数に比例して初期化時間が増大していましたが、効率的なトポロジ検出と通信パス設定により、大規模クラスターでも高速に初期化が完了します。

#### (2) Memory-Mapped Data Loading

訓練データの読み込みにメモリマップド I/O（mmap）を使用します。従来のファイルシステム経由のデータ読み込みと比較して、障害復旧後のデータパイプライン再構築が高速化されます。データセットのインデックスと状態をメモリマップとして管理することで、中断したバッチの正確な位置から訓練を再開できます。

#### (3) In-Process Recovery

プロセスを再起動せずに、プロセス内部で訓練状態を復旧する機構です。従来の障害復旧では以下のステップが必要でした。

```text
[従来の復旧フロー]
1. 全プロセスの停止
2. 新規プロセスの起動
3. NCCL 集団通信の再初期化（数分）
4. チェックポイントのロード（数分〜数十分）
5. データパイプラインの再構築
6. 訓練再開

[In-Process Recovery]
1. 障害検出
2. 通信グループの再構成（プロセス内で完了）
3. メモリ内の状態から訓練再開
```

#### (4) Checkpointless Recovery

GPU メモリ内にモデル状態のレプリカを保持し、ノード障害時に他のノードのレプリカから復旧します。ストレージへの書き込みが完全に不要になるため、訓練のスループットが向上します。

```text
[レプリカ配置の概念図]

Node 0: [Model Shard 0] [Replica of Shard 1]
Node 1: [Model Shard 1] [Replica of Shard 0]
Node 2: [Model Shard 2] [Replica of Shard 3]
Node 3: [Model Shard 3] [Replica of Shard 2]

Node 1 が障害 → Node 0 が Shard 1 のレプリカから復旧
```

### 1.3 対応環境と制約

Checkpointless Training は以下の環境制約があります。

| 項目 | 詳細 |
|------|------|
| オーケストレーター | **EKS のみ**（Slurm 環境は公式サポート外）（[公式ドキュメント](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-eks-operate-training-checkpointless.html)参照） |
| フレームワーク | **NVIDIA NeMo Toolkit 2.0.0 が必須**（[ソース](https://github.com/aws/sagemaker-hyperpod-recipes)）。PyTorch ≥ 2.3.0、PyTorch Lightning 2.5.5、Megatron-Core 0.13.1 に依存 |
| 非対応フレームワーク | HuggingFace Transformers Trainer、DeepSpeed、純粋な PyTorch（公式サポート外。[公式レシピ](https://github.com/aws/sagemaker-hyperpod-recipes)で NeMo ベースのサンプルのみ提供） |
| サポートモデル | GPT OSS、Llama 3（[公式サンプル](https://github.com/aws/sagemaker-hyperpod-recipes)で確認済み、NeMo 対応モデルのみ） |
| インスタンスタイプ | **ml.p5.48xlarge / ml.p5e.48xlarge 推奨**（H100 GPU）。ml.p4d.24xlarge（A100）は未検証（[公式レシピ](https://github.com/aws/sagemaker-hyperpod-recipes/blob/main/recipes_collection/recipes/fine-tuning/gpt_oss/checkpointless_gpt_oss_120b_full_fine_tuning.yaml)参照） |
| GPU 要件 | CUDA 12.5+、複数 GPU 必須（`num_distributed_optimizer_instances >= 2`）（[公式レシピ](https://github.com/aws/sagemaker-hyperpod-recipes)参照） |
| Kubernetes CRD | `HyperPodPyTorchJob` を使用（EKS Training Operator ≥ 1.2.0）（[HyperPod CLI](https://github.com/aws/sagemaker-hyperpod-cli)参照） |

:::message alert
**重要な制約**: Checkpointless Training は **NVIDIA NeMo Framework が事実上必須**です。`requirements.txt` に `nemo-toolkit==2.0.0` が必須依存として含まれており、統合コンポーネント（`CheckpointlessCallback`、`CheckpointlessMegatronStrategy`）が NeMo 固有 API に強く依存しています。

**使えないケース**:
- HuggingFace Transformers Trainer を使用する訓練
- Microsoft DeepSpeed フレームワークを使用する訓練
- NeMo を使わない純粋な PyTorch 訓練ループ
- AWS Trainium インスタンス（ml.trn1.*）での訓練
- Slurm オーケストレーター環境

2026 年 2 月時点では、公式レシピリポジトリ（`sagemaker-hyperpod-recipes`）の全サンプルが NeMo ベースです。NeMo 以外のフレームワークを使用する場合は、独自実装が必要ですが公式サポート外となります。

注意: PyPI に `hyperpod-checkpointless-training`（v100.0.0）という同名のプレースホルダーパッケージが存在しますが、これは AWS 公式パッケージではありません。公式の `hyperpod_checkpointless_training` は `sagemaker-hyperpod-recipes` リポジトリ経由で利用してください。
:::

### 1.4 導入方法

Checkpointless Training の有効化は、訓練設定ファイルと `HyperPodPyTorchJob` の構成を組み合わせて行います。

#### 訓練設定での有効化

```yaml
# 訓練設定ファイル (例: gpt_config.yaml)
model:
  # 分散オプティマイザの冗長インスタンス数
  # 各 Data Parallel グループ内でモデル状態の冗長コピーを保持
  num_distributed_optimizer_instances: 2
```

`num_distributed_optimizer_instances: 2` は、各 Data Parallel グループ内でオプティマイザ状態の冗長コピーを 2 つ保持する設定です。これにより、1 つのノードが障害を起こしても、もう 1 つのコピーから状態を復元できます。この設定は NeMo Framework 固有のパラメータで、DP グループサイズ以下の値を指定する必要があります。GPU メモリ使用量が増加する点に注意が必要です（冗長コピー数とモデルサイズに依存します）。

#### HyperPodPyTorchJob の起動コマンド

```yaml
# HyperPodPyTorchJob spec 内の containers.command 例
command:
  - "python"
  - "-u"
  - "training_script.py"
args:
  - "--config-path=/workspace/configs"
  - "--config-name=gpt_config"
```

#### コアライブラリの HPWrapper による統合

```python
from hyperpod_checkpointless_training.inprocess import HPWrapper

# HPWrapper はフレームワーク非依存のコア API である。
# 訓練関数をラップすることで、In-Process Recovery を有効化する。
# 2026 年 2 月時点では、NeMo を使用する場合は nemo_plugins/ の Callback を通じて統合される。
```

:::message
2026 年 2 月時点では、公式サンプルで NeMo 統合プラグイン（`CheckpointlessCallback` 等）を通じた利用例が提供されています（`sagemaker-hyperpod-recipes` の `ModelType: hyperpod_checkpointless_nemo`）。
:::

:::message
Checkpointless Training は HyperPod の Managed Tiered Checkpointing と併用することが推奨されます。Checkpointless Recovery は高速な復旧を実現しますが、全ノードが同時に障害となるカタストロフィックな障害には対応できないため、定期的な永続チェックポイントも別途保存するべきです。
:::

:::message
上記のコード例は AWS 公式ドキュメントおよび公開リポジトリの情報に基づいています。Checkpointless Training の内部実装は AWS のプロプライエタリコンポーネントであり、実際の SDK インターフェースは予告なく変更される可能性があります。最新の仕様は公式ドキュメントを参照してください。
:::

### 1.5 パフォーマンス

AWS の公式発表[^1]および公式ブログ[^2]に基づくパフォーマンス数値:

| メトリクス | 従来方式 | Checkpointless Training |
|-----------|---------|----------------------|
| 障害復旧時間 | 数十分（推定値[^3]） | 数分以内[^1] |
| goodput（数千アクセラレータ規模） | -- | 95% 以上[^1] |
| チェックポイント I/O | 訓練時間の数%〜十数%（推定値[^3]） | ほぼゼロ |
| メモリオーバーヘッド | なし | レプリカ分（GPU メモリの一部） |

---

## 2. Elastic Training

![](https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2025/11/18/2025-sageamker-hyperpod-elastic-training.gif)

### 2.1 概要

:::message
本機能は EKS 環境でのみ利用可能です。Slurm 環境では利用できません（[公式ドキュメント](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-eks-operate-training-elastic.html)参照）。
:::

Elastic Training は、クラスターの容量変化に応じて訓練ジョブのノード数を**動的に増減**する機能です。ハードウェア障害でノードが減少した場合は自動的に縮小運転を継続し、新しいノードが利用可能になれば自動的にスケールアップします。

### 2.2 PyTorch Elastic との関係

HyperPod Elastic Training は、PyTorch Elastic（`torch.distributed.elastic`）を基盤技術として採用しています。以下のレイヤー構成で動作します。

```text
Layer 4: ユーザーインターフェース
  hyp CLI / Python SDK (HyperPodPytorchJob クラス)
     |
Layer 3: Kubernetes リソース管理
  HyperPod Training Operator (aws-hyperpod namespace)
  HyperPodPyTorchJob CRD (sagemaker.amazonaws.com/v1)
     |
Layer 2: Pod 内部の分散訓練制御
  torchrun / torch.distributed.elastic.agent
  SimpleElasticAgent._invoke_run() ループ
     |
Layer 1: Rendezvous メカニズム
  DynamicRendezvousHandler
  C10dRendezvousBackend (TCPStore ベース)
```

HyperPod は、Kubeflow の `PyTorchJob`（`kubeflow.org/v1`）ではなく、独自の CRD を使用する点に注意が必要です。

| 項目 | Kubeflow PyTorchJob | HyperPod PyTorchJob |
|------|---|---|
| apiVersion | `kubeflow.org/v1` | `sagemaker.amazonaws.com/v1` |
| kind | `PyTorchJob` | `HyperPodPyTorchJob` |
| ElasticPolicy | `rdzvBackend` 等を手動指定 | Operator が自動管理 |
| レプリカ種別 | Master/Worker 分離 | 単一 ReplicaSpec（"pod"） |
| スケーリング | ユーザー管理 | Operator による自動管理 |

### 2.3 HyperPodPyTorchJob CRD の仕様

HyperPod Elastic Training のジョブは、以下の CRD で定義されます。

```yaml
apiVersion: sagemaker.amazonaws.com/v1
kind: HyperPodPyTorchJob
metadata:
  name: my-elastic-job
  namespace: my-namespace
spec:
  nprocPerNode: "8"
  replicaSpecs:
    - name: "pod"        # HyperPod は単一 ReplicaSpec を採用（Rendezvous 管理を Operator に委任するため Master/Worker の区別が不要）
      replicas: 4          # 初期ノード数
      maxReplicas: 8        # 最大ノード数
      template:
        spec:
          containers:
            - name: "pytorch-job-container"
              image: "my-registry/training: latest"
              resources:
                requests:
                  nvidia.com/gpu: 8
                limits:
                  nvidia.com/gpu: 8
          nodeSelector:
            node.kubernetes.io/instance-type: "ml.p5.48xlarge"
  elasticPolicy:
    minReplicas: 4
    maxReplicas: 8
    replicaIncrementStep: 2     # 2 ノード単位でスケーリング
    scalingTimeoutInSeconds: 300
    gracefulShutdownTimeoutInSeconds: 120
  runPolicy:
    cleanPodPolicy: "None"
    jobMaxRetryCount: 3
    restartPolicy:
      scaleUpSnoozeTimeInSeconds: 600  # 再起動後のスケールアップ抑止期間
```

#### ElasticPolicy の主要フィールド

| フィールド | 型 | 説明 |
|---|---|---|
| `minReplicas` | int | 最小レプリカ数（`node_count` と同値） |
| `maxReplicas` | int | 最大レプリカ数（`max_node_count` と同値） |
| `replicaIncrementStep` | int | ステップサイズ（例: 2 = 2 ノードずつ増減） |
| `replicaDiscreteValues` | list[int] | 離散的なレプリカ数（`replicaIncrementStep` と**相互排他**） |
| `scalingTimeoutInSeconds` | int | スケーリング操作のタイムアウト |
| `gracefulShutdownTimeoutInSeconds` | int | グレースフルシャットダウンのタイムアウト |
| `faultyScaleDownTimeoutInSeconds` | int | 障害 Pod のスケールダウンまでの待機時間 |

:::message alert
`replicaIncrementStep` と `replicaDiscreteValues` は同時に指定できません。[HyperPod CLI のソースコード](https://github.com/aws/sagemaker-hyperpod-cli)（`validate_elastic_replica_config()`）で明示的に排他チェックが行われます。
:::

### 2.4 Rendezvous メカニズム

Elastic Training の中核となる Rendezvous は、分散訓練に参加するワーカーの**合流・調整**メカニズムです。PyTorch Elastic の [`DynamicRendezvousHandler`](https://github.com/pytorch/pytorch/blob/main/torch/distributed/elastic/rendezvous/dynamic_rendezvous.py) が以下のフローで動作します。

```mermaid
sequenceDiagram
    autonumber
    participant W1 as Worker A
    participant W2 as Worker B
    participant W3 as Worker C (新規)
    participant RH as RendezvousHandler
    participant BE as Backend (C10d Store)

    Note over W1, BE: 初期状態: min_nodes=2, max_nodes=4

    rect rgb(230, 245, 255)
        Note over W1, W2: Phase 1: 初回 Rendezvous
        W1 ->> RH: next_rendezvous()
        RH ->> BE: ADD_TO_PARTICIPANTS(A)
        W2 ->> RH: next_rendezvous()
        RH ->> BE: ADD_TO_PARTICIPANTS(B)
        Note over RH: min_nodes=2 到達 → last_call 30 秒
        Note over RH: deadline 到達 → MARK_COMPLETE
        RH -->> W1: rank=0, world_size=2
        RH -->> W2: rank=1, world_size=2
        Note over W1, W2: 訓練開始
    end

    rect rgb(255, 245, 220)
        Note over W3, RH: Phase 2: ワーカー追加
        W3 ->> RH: next_rendezvous()
        RH ->> BE: ADD_TO_WAIT_LIST(C)
        Note over RH: complete=True なので wait_list へ
    end

    rect rgb(255, 230, 230)
        Note over W1, BE: Phase 3: Re-rendezvous
        Note over W1: num_nodes_waiting() > 0 を検出
        W1 ->> W1: チェックポイント保存
        W2 ->> W2: チェックポイント保存
        W1 ->> RH: next_rendezvous() (再参加)
        W2 ->> RH: next_rendezvous() (再参加)
        W3 ->> RH: (wait_list から参加)
        Note over RH: 新ラウンド: 3 ノードで MARK_COMPLETE
        RH -->> W1: rank=0, world_size=3
        RH -->> W2: rank=1, world_size=3
        RH -->> W3: rank=2, world_size=3
        Note over W1, W3: 訓練再開 (world_size: 2→3)
    end
```

#### Rendezvous の状態管理

[`_RendezvousState`](https://github.com/pytorch/pytorch/blob/main/torch/distributed/elastic/rendezvous/dynamic_rendezvous.py) が管理する主要なフィールド（PyTorch ソースコードで確認済み）:

```python
from typing import Optional

class _RendezvousState:
    round: int                             # ラウンド番号
    complete: bool                         # Rendezvous 完了フラグ
    deadline: Optional[datetime]           # last_call のデッドライン
    closed: bool                           # Rendezvous が閉じられたか
    participants: dict[_NodeDesc, int]      # 参加者 → rank マッピング
    wait_list: set[_NodeDesc]              # 次ラウンド待機リスト
    redundancy_list: set[_NodeDesc]        # max_nodes 超過時の冗長リスト
    last_heartbeats: dict[_NodeDesc, datetime]  # ハートビート時刻
```

状態管理は [`C10dRendezvousBackend`](https://github.com/pytorch/pytorch/blob/main/torch/distributed/elastic/rendezvous/c10d_rendezvous_backend.py)（Store ベース）のバックエンドで行われ、Store の `compare_set()` 操作（CAS: Compare-And-Swap パターン）による楽観的ロックで一貫性が保証されます。

#### ハートビートと死亡検出

| パラメータ | デフォルト値 | 説明 |
|---|---|---|
| `keep_alive_interval` | 5 秒 | ハートビート送信間隔（[ソースコード](https://github.com/pytorch/pytorch/blob/main/torch/distributed/elastic/rendezvous/dynamic_rendezvous.py)で確認） |
| `keep_alive_max_attempt` | 3 回 | 最大失敗回数（同上） |
| **死亡検出までの最大遅延** | **15 秒** | 5 秒 x 3 回（`_sanitize()` メソッドで実装） |

### 2.5 Agent のメインループ

[`SimpleElasticAgent._invoke_run()`](https://github.com/pytorch/pytorch/blob/main/torch/distributed/elastic/agent/server/api.py) が Elastic Training の中核ループとして動作します（PyTorch ソースコードで確認済み）。

```python
# PyTorch Elastic: agent/server/api.py (簡略化)
def _invoke_run(self, role):
    self._initialize_workers(self._worker_group)  # 初回 Rendezvous + 起動

    while True:
        time.sleep(monitor_interval)
        run_result = self._monitor_workers(self._worker_group)
        state = run_result.state

        if state == WorkerState.SUCCEEDED:
            self._exit_barrier()
            return run_result

        elif state in {WorkerState.UNHEALTHY, WorkerState.FAILED}:
            if self._remaining_restarts > 0:
                self._remaining_restarts -= 1
                self._restart_workers(self._worker_group)  # Re-rendezvous
            else:
                self._stop_workers(self._worker_group)
                return run_result

        elif state == WorkerState.HEALTHY:
            # [重要] wait_list に新ノードがいるか確認
            # 注: membership changes (ノード追加/削除) は remaining_restarts にカウントされない
            num_nodes_waiting = rdzv_handler.num_nodes_waiting()
            if num_nodes_waiting > 0:
                self._restart_workers(self._worker_group)
```

このループにより、ワーカーの障害検出と新規ワーカーの参加検出の両方が自動的に処理されます。

### 2.6 CLI によるジョブ作成

HyperPod CLI（[`hyp`](https://github.com/aws/sagemaker-hyperpod-cli)）を使用して Elastic Training ジョブを作成します。

```bash
hyp create hyp-pytorch-job \
  --job-name my-elastic-job \
  --image my-registry/training: latest \
  --node-count 4 \
  --max-node-count 8 \
  --instance-type ml.p5.48xlarge \
  --tasks-per-node 8 \
  --elastic-replica-increment-step 2 \
  --elastic-scaling-timeout-in-seconds 300 \
  --elastic-graceful-shutdown-timeout-in-seconds 120 \
  --elastic-scale-up-snooze-time-in-seconds 600
```

CLI は内部で以下の変換を行います。

```text
CLI パラメータ                     CRD フィールド
--node-count 4              →   replicaSpecs[0].replicas = 4
                                elasticPolicy.minReplicas = 4
--max-node-count 8          →   replicaSpecs[0].maxReplicas = 8
                                elasticPolicy.maxReplicas = 8
--elastic-replica-increment-step 2  →  elasticPolicy.replicaIncrementStep = 2
--instance-type ml.p5.48xlarge  →  nodeSelector + resources 自動計算
                                   (gpu:8, cpu:192, memory:2048Gi, efa:32)
```

::::details CLI の内部処理フロー（詳細）

CLI は以下の処理チェーンで CRD を生成します。

```text
1. Click コマンドパーサー (training.py)
   @generate_click_command デコレータが schema.json (v1.1) からオプションを動的生成

2. Pydantic バリデーション (v1_1/model.py)
   - validate_elastic_replica_config(): increment_step と discrete_values の排他チェック
   - validate_tasks_per_node(): "auto"/"cpu"/"gpu"/整数のみ許可

3. to_domain() 変換
   - node_count → replicas + minReplicas
   - max_node_count → maxReplicas (ReplicaSpec + ElasticPolicy 両方)
   - elastic_* パラメータ → ElasticPolicy オブジェクト

4. HyperPodPytorchJob.create()
   - allocate_quotas_if_applicable(): instance_type からリソース自動計算
   - CustomObjectsApi().create_namespaced_custom_object() で K8s API に送信
```

::::

### 2.7 スケーリング動作

#### スケールアップ（ノード追加）

```text
1. Operator が ElasticPolicy に基づき Pod を追加（step=2 なら 2 Pod ずつ）
2. 新 Pod で torchrun が起動 → next_rendezvous() を呼出
3. complete=True なので wait_list に追加
4. 既存 Agent が num_nodes_waiting() > 0 を検出
5. 全ワーカーが _restart_workers() → Re-rendezvous
6. 新しい world_size で訓練再開
```

#### スケールダウン（ノード離脱）

```text
1. ワーカー Pod が異常終了（ハードウェア障害等）
2. ハートビート送信が途絶
3. _sanitize() が 15 秒後に死亡ノードを検出
4. 残存ワーカーが NCCL 通信エラーを検出
5. チェックポイント保存
6. Re-rendezvous → 縮小した world_size で訓練再開
```

:::message alert
Re-rendezvous 時には**全ワーカープロセスが一時停止**します。これは数秒から数十秒の中断を伴うため、頻繁なスケーリングは訓練効率を低下させます。`scaleUpSnoozeTimeInSeconds` を適切に設定して、再起動直後のスケールアップを抑止することが重要です。
:::

### 2.8 GRPO マルチフェーズ学習への適用可能性

GRPO（Group Relative Policy Optimization）のようなマルチフェーズ学習では、Generation Phase（応答生成）と Training Phase（ポリシー更新）でリソース要件が大きく異なります。フェーズ比率はモデルサイズ、生成長、バッチサイズ等に依存しますが、一般的なベンチマークでは Generation Phase が全体の約 80%、Training Phase が約 20% を占めます。理論上、Elastic Training でフェーズに応じてノード数を動的に変更できれば、リソース利用率を大幅に改善できます。

しかし、**訓練クラスタのノード数をフェーズ間で動的に変更することは、現時点では技術的に非常に困難**です。主な理由は以下の 3 点です。

1. **FSDP/DeepSpeed の再シャーディングの繰り返しコスト**: Actor モデルが FSDP で分散されている場合、ノード数変更時にパラメータ・勾配・オプティマイザ状態の再シャーディングが必要となり、大規模モデルでは数百 GB のデータ移動が発生します。再シャーディング自体は Elastic Training でも発生しますが、GRPO では**毎イテレーション**（1000 回以上）実行されるため、積算コストが膨大になります
2. **Re-rendezvous の繰り返しコスト**: フェーズ遷移は各イテレーションで発生するため、毎回 Re-rendezvous（全プロセス一時停止、数秒〜数十秒）+ チェックポイント I/O（数分）が発生すると訓練効率が大幅に低下します
3. **TP/PP の固定構成**: Tensor Parallel / Pipeline Parallel はノード数に依存した固定構成が前提であり、動的な変更は事実上モデルの再構築を意味します

#### Elastic Training の有効なユースケース

**頻度が低い場合は Elastic Training が有効**です：

| ユースケース | 変更頻度 | 再シャーディング回数 | 判定 |
|-------------|---------|---------------------|------|
| **障害復旧** | 数日に 1 回 | 1〜数回 | ✅ 実用的 |
| **実験単位の変更**（事前学習→ファインチューニング等） | 実験の区切りで 1 回 | 1 回 | ✅ 実用的 |
| **GRPO フェーズ連動型**（毎イテレーション） | 1000 回以上 | 1000 回以上 | ❌ 非実用的 |

例えば、事前学習（128 ノード）からファインチューニング（16 ノード）への移行は、実験の区切りで 1 回のみノード数を変更するため、再シャーディングのコストは十分に許容できます。

#### Slurm と Elastic Training（EKS）の比較

実験単位のノード数変更は Slurm でも可能ですが、実現方法が異なります：

| 機能 | Slurm | Elastic Training（EKS） |
|------|-------|----------------------|
| 実験単位のノード数変更 | ✅ 可能（別ジョブ投入） | ✅ 可能（同一ジョブ内） |
| 障害時の自動縮小運転 | ❌ 不可 | ✅ 可能 |
| 同一ジョブ内での動的変更 | ❌ 不可 | ✅ 可能 |

**Slurm 環境**:
```bash
# 事前学習: 128 ノード
sbatch --nodes=128 --job-name=pretrain pretrain.sh

# ファインチューニング: 16 ノード（別ジョブとして投入）
sbatch --nodes=16 --job-name=finetune finetune.sh
```

**Elastic Training（EKS 環境）**:
```bash
# 同一ジョブ内でレプリカ数を変更
kubectl scale hyperpodjob/my-job --replicas=16
```

Slurm では異なるノード数で別々のジョブを投入しますが、Elastic Training では同一ジョブ内でノード数を動的に変更できます。

#### 推奨アプローチ

現実的なアプローチは、**訓練クラスタは固定ノード数で運用し、推論プール（vLLM サーバー）のみを弾力的にスケーリング**する方式です。vLLM サーバーはステートレスに近く、Kubernetes HPA や KubeRay Autoscaler で制御できます。Generation Phase で推論ノードをスケールアップし、Training Phase で最小構成にスケールダウンすることで、10-14% 程度のコスト削減が見込めます[^grpo_cost]。

[^grpo_cost]: Generation Phase 80%、Training Phase 20% の比率を前提とし、推論プールを Training Phase で最小構成（1 ノード）にスケールダウンした場合の推定値。実際の削減率は、推論ノード数、インスタンスタイプ、スポット割引率等に依存します。

一方、Elastic Training は**障害復旧目的と実験単位のノード数変更**での活用に適しています。Health Monitoring Agent と組み合わせることで、ノード障害時の自動縮小運転と復旧後のスケールアップを実現し、訓練の継続性を確保できます。

#### trn2 を推論フェーズに使用する構成

GRPO の Generation Phase に AWS Trainium2（trn2）を推論アクセラレータとして使用し、p5（H100 GPU）と混在させる構成も検討に値します。

**技術的な実現方法**:

HyperPodPyTorchJob の Elastic Training は単一インスタンスタイプのみサポートするため、GPU + trn2 の混在には使用できません（[HyperPod CLI のソースコード](https://github.com/aws/sagemaker-hyperpod-cli)で `--instance-type` パラメータが単一値のみを受け付けることを確認済み）。代わりに、以下の分離アーキテクチャが現実的です:

- **訓練クラスタ**: p5.48xlarge（固定ノード数）で HyperPodPyTorchJob を使用
- **推論プール**: trn2.48xlarge 上の vLLM Server を弾力的に管理

vLLM on Neuron（NxD Inference ライブラリ）は Neuron SDK 2.27.1 時点で trn2 をサポートしており、Llama 3.x 等の主要モデルでの推論が可能です。推論サーバーはステートレスであるため、以下のいずれかの方法で弾力的にスケーリングできます：

**オプション A: Kubernetes Deployment + HPA**

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-trn2-inference
spec:
  replicas: 2
  # ... (Pod template with trn2 nodeSelector)
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: vllm-trn2-hpa
spec:
  scaleTargetRef:
    kind: Deployment
    name: vllm-trn2-inference
  minReplicas: 2
  maxReplicas: 16
```

**オプション B: VERL + Ray Autoscaler**

VERL（Versatile RLHF Library）を使用する場合、vLLM を Ray Actor として管理し、Ray Autoscaler による弾力的スケーリングが可能です。ただし、VERL の `ResourcePoolManager` は現状では静的なリソース割り当てのため、動的スケーリングにはカスタマイズが必要です（研究課題レベル）。

**利点**:
- trn2 の推論コスト効率（GPU 比で価格性能優位性が期待される）
- 推論プールの独立したスケーリングにより、Generation Phase でのリソース効率を向上
- 訓練クラスタの安定性を維持しつつ、推論リソースを弾力的に管理

**課題**:
- GPU で訓練したモデルの trn2 推論サーバーへの配布と Neuron コンパイル
- Neuron コンパイルキャッシュの管理（初回コンパイルに 15 分〜数時間）
- 異種アクセラレータ間の数値精度の微小な差異

:::message
GRPO のフェーズ連動型 Elastic Training は、PyTorch の将来バージョンでの Elastic FSDP サポートに依存する研究課題です。一方、VERL（Versatile RLHF Library）の Colocate Placement（3D-HybridEngine による効率的なフェーズ切り替えを実装）は、同一 GPU セット上で Generation と Training を切り替えることで、フェーズ間のリソース非効率性を設計レベルで解消する別のアプローチを提供します。VERL を使用しても訓練ノード数の動的変更は依然として困難ですが、そもそも Elastic Training の必要性自体が低下します。推論プールの弾力的スケーリングと VERL の Colocate Placement を組み合わせるアプローチが現時点では最も実用的です。
:::

---

## 3. Managed Tiered Checkpointing

### 3.1 概要

:::message
本機能は EKS および Slurm の両環境で利用可能です。

**FSx for Lustre との併用について**: Managed Tiered Checkpointing と FSx for Lustre + DRA は、どちらも最終的にチェックポイントを S3 に永続化しますが、技術的には併用可能です。ただし、両方とも「チェックポイントを S3 に保存する」という同じ目的を持つため、通常は併用する必要はありません。FSx が既に構築済みの場合、データセット保存用に FSx を継続使用し、チェックポイント保存には Managed Tiered Checkpointing を使用する構成が推奨されます。
:::

Managed Tiered Checkpointing は、チェックポイントの保存先を**階層化**することで、保存の高速化とコストの最適化を両立する機能です。

#### 従来のアプローチ: FSx for Lustre + DRA

Managed Tiered Checkpointing が導入される以前、HyperPod では **FSx for Lustre + DRA（Data Repository Association）** を使用したチェックポイント保存が標準的でした。

**アーキテクチャ**:

```text
訓練ノード → FSx for Lustre（共有ストレージ）→ DRA → S3
```

1. 訓練スクリプトが `torch.save()` で FSx に書き込み（5-30 秒、訓練ブロック）
2. DRA が変更を検知し、S3 へのエクスポートをスケジュール
3. バックグラウンドで S3 に永続化（数分）

**主な課題**:

- **訓練のブロック時間**: FSx への書き込み完了まで訓練が停止（5-30 秒、モデルサイズ依存）
- **高コスト**: FSx for Lustre のコスト（月額 $140-$500/TB）
- **エクスポート遅延**: S3 への永続化に数分の遅延が発生
- **容量管理**: FSx の容量不足時に訓練が停止するリスク

#### Managed Tiered Checkpointing への移行メリット

Managed Tiered Checkpointing は、これらの課題を解決するために設計されました:

| 比較項目 | FSx + DRA（従来） | Managed Tiered Checkpointing |
|---------|------------------|----------------------------|
| 訓練のブロック時間 | 5-30 秒（FSx 書き込み） | 1-2 秒（メモリコピー） |
| 月額コスト（1.2 TB 想定） | $180（FSx）+ $32（S3） | $32（S3 のみ） |
| ノード障害時の復旧 | 数分（FSx または S3 から復元） | 数秒（メモリレプリカから復旧） |
| スケーラビリティ | 数百ノード（FSx のクライアント数制限） | 数千ノード（各ノードのローカルメモリを使用） |
| S3 保存形式 | 単一の `.pt` ファイル | PyTorch DCP の sharded checkpoint（`.distcp`） |
| 推奨ケース | データセット共有が必須な既存構成 | 新規プロジェクトまたはコスト最適化 |

以降では、この新しい Managed Tiered Checkpointing の仕組みと実装方法を解説します。

### 3.2 階層化戦略

Managed Tiered Checkpointing は **2 つの階層**でチェックポイントを管理します。主要層としてクラスターノードの **CPU メモリ（RAM）**を使用し、副次層として **Amazon S3** に永続化します。

```mermaid
graph TB
    subgraph "Tier 1: CPU メモリ (高速・主要層)"
        M1["クラスターノードの<br/>CPU メモリ (RAM)"]
        M2["隣接ノードへの<br/>自動レプリケーション"]
    end

    subgraph "Tier 2: Amazon S3 (耐久性・副次層)"
        S1["S3 バケット<br/>(永続保存先)"]
    end

    M1 -->|"自動レプリケート"| M2
    M1 -->|"定期的に<br/>非同期アップロード"| S1

    style M1 fill: #e8f5e9
    style M2 fill: #e8f5e9
    style S1 fill: #e3f2fd
```

#### 各階層の特性

| 階層 | 保存先 | 速度 | 耐久性 | 用途 |
|------|--------|------|--------|------|
| Tier 1 | CPU メモリ（RAM） | 高速（GB/s） | ノード間レプリケーションで保護 | 高頻度保存・高速復旧 |
| Tier 2 | Amazon S3 | 低速（数百 MB/s） | 高耐久（99.999999999%） | 低頻度保存（永続バックアップ） |

#### レプリケーション戦略

Tier 1（CPU メモリ）に保存されたチェックポイントは、**隣接する計算ノード間で自動的にレプリケート**されます。これにより、単一または複数のノード障害からデータを保護しながら、復旧操作のための高速アクセスを提供します。メモリ管理デーモン（EKS 環境では Kubernetes DaemonSet として、Slurm 環境ではシステムデーモンとしてデプロイ）がチェックポイント用の分散メモリを管理します。

:::message
`InstanceMemoryAllocationPercentage` パラメータで、チェックポイントストレージに割り当てるクラスターメモリの割合を設定できます（20-100% の範囲）。訓練に必要なメモリとのバランスを考慮して設定してください。

設定例（`--tiered-storage-config` 内に含める）:
```json
{
  "Mode": "Enable",
  "InstanceMemoryAllocationPercentage": 50
}
```
:::

### 3.3 非同期パイプライン

最大の特徴は、**訓練をブロックせずにチェックポイントを保存**できることです。PyTorch DCP（Distributed Checkpoint）の `async_save()` を使用して非同期保存を実現します。

```text
[従来方式: 同期チェックポイント]
訓練 ████████░░░░░░░░░░████████░░░░░░░░░░████████
チェック          ████████                ████████
                 (ブロック)              (ブロック)

[Tiered Checkpointing: 非同期パイプライン]
訓練 ████████████████████████████████████████████████
Tier1(RAM)  ▼(メモリコピー)  ▼(メモリコピー)
Tier2(S3)     ░░░░░░░░(S3 アップロード)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
           訓練のブロック時間は大幅に削減（ステージング時の短時間コピーのみ）
```

### 3.4 実装と API

#### クラスターの構成

Managed Tiered Checkpointing を利用するには、クラスター作成時に `--tiered-storage-config` を有効化する必要があります（[セットアップドキュメント](https://docs.aws.amazon.com/sagemaker/latest/dg/managed-tier-checkpointing-setup.html)参照、AWS 公式ドキュメントで確認済み）。

**EKS 環境の構成例**:

```bash
aws sagemaker create-cluster \
    --cluster-name my-training-cluster \
    --orchestrator "Eks={ClusterArn=arn: aws: eks: us-west-2:123456789012: cluster/my-eks}" \
    --instance-groups '[{
        "InstanceGroupName": "training-group",
        "InstanceType": "ml.p5.48xlarge",
        "InstanceCount": 4,
        "LifeCycleConfig": {
            "SourceS3Uri": "s3://my-bucket/lifecycle-scripts",
            "OnCreate": "on_create.sh"
        },
        "ExecutionRole": "arn: aws: iam::123456789012: role/MyRole",
        "InstanceStorageConfigs": [
            { "EbsVolumeConfig": {"VolumeSizeInGB": 500} }
        ]
    }]' \
    --tiered-storage-config '{"Mode": "Enable"}'
```

**Slurm 環境の構成例**:

```bash
aws sagemaker create-cluster \
    --cluster-name my-training-cluster \
    --instance-groups '[
        {
            "InstanceGroupName": "controller-group",
            "InstanceType": "ml.m5.xlarge",
            "InstanceCount": 1,
            "ExecutionRole": "arn: aws: iam::123456789012: role/MyRole",
            "ThreadsPerCore": 1
        },
        {
            "InstanceGroupName": "worker-group",
            "InstanceType": "ml.p5.48xlarge",
            "InstanceCount": 4,
            "ExecutionRole": "arn: aws: iam::123456789012: role/MyRole",
            "ThreadsPerCore": 1
        }
    ]' \
    --tiered-storage-config '{"Mode": "Enable"}'
```

無効化する場合:

```bash
aws sagemaker update-cluster \
    --cluster-name my-training-cluster \
    --tiered-storage-config '{"Mode": "Disable"}'
    # 注: 実際の update-cluster API では --instance-groups パラメータも必須
```

#### Python ライブラリ

専用の [`amzn-sagemaker-checkpointing`](https://pypi.org/project/amzn-sagemaker-checkpointing/) ライブラリ（v1.1.2、Apache License 2.0、PyPI で確認済み）を使用します。`sagemaker` SDK とは別パッケージである点に注意してください。

```bash
pip install amzn-sagemaker-checkpointing s3torchconnector tenacity torch boto3
```

#### チェックポイント設定

```python
import os
import time
import torch.distributed as dist
from amzn_sagemaker_checkpointing.checkpointing.filesystem.filesystem import (
    SageMakerTieredStorageWriter,
    SageMakerTieredStorageReader,
)
from amzn_sagemaker_checkpointing import SageMakerCheckpointConfig

# チェックポイント設定
checkpoint_config = SageMakerCheckpointConfig(
    namespace=os.environ.get("TRAINING_JOB_NAME", f"job-{int(time.time())}"),
    world_size=dist.get_world_size(),
    s3_tier_base_path="s3://my-bucket/checkpoints",
)
```

`SageMakerCheckpointConfig` の主要パラメータ:

| パラメータ | 型 | 説明 |
|---|---|---|
| `namespace` | str | 訓練ジョブの一意な識別子（英数字・ハイフン・アンダースコアのみ） |
| `world_size` | int | 分散プロセス数（`dist.get_world_size()` から取得） |
| `s3_tier_base_path` | str | S3 保存先パス |
| `save_to_s3` | bool | S3 への保存を有効化（保存ごとに動的に切替可能） |

#### 訓練スクリプトへの統合

PyTorch DCP（Distributed Checkpoint）の `async_save()` / `load()` と組み合わせて使用します。

```python
from torch.distributed.checkpoint import async_save, load

future = None
in_memory_ckpt_freq = 10   # 10 ステップごとにメモリ保存
s3_ckpt_freq = 50           # 50 ステップごとに S3 永続化

for step, batch in enumerate(dataloader):
    # 通常の訓練ステップ
    loss = model(batch)
    loss.backward()
    optimizer.step()

    # Tiered Checkpointing: 非同期で保存
    if step % in_memory_ckpt_freq == 0 or step % s3_ckpt_freq == 0:
        state_dict = {
            "model": model.state_dict(),  # FSDP 使用時は SHARDED_STATE_DICT と組み合わせるのが標準
            "optimizer": optimizer.state_dict(),
            "step": step,
        }

        # S3 への保存はより低い頻度で実行
        checkpoint_config.save_to_s3 = (step % s3_ckpt_freq == 0)

        storage_writer = SageMakerTieredStorageWriter(
            checkpoint_config=checkpoint_config,
            step=step,
        )

        # 前回の非同期保存の完了を確認
        if future is not None:
            try:
                future.result()  # 完了を待機
            except Exception as exc:
                print(f"Checkpoint save failed: {str(exc)}")

        future = async_save(state_dict=state_dict, storage_writer=storage_writer)
```

#### チェックポイントの読み込み

`SageMakerTieredStorageReader` は、メモリ層からの読み込みに失敗した場合、自動的に S3 層にフォールバックします。

```python
state_dict = {
    "model": model.state_dict(),
    "optimizer": optimizer.state_dict(),
    "step": 0,  # 初期値。load() により実際の保存値で上書きされる
}

# 最新のチェックポイントを自動検出して読み込み（step 省略時）
storage_reader = SageMakerTieredStorageReader(
    checkpoint_config=checkpoint_config,
)
load(state_dict, storage_reader=storage_reader)

# 特定のステップを指定して読み込み
storage_reader = SageMakerTieredStorageReader(
    checkpoint_config=checkpoint_config,
    step=500,
)
load(state_dict, storage_reader=storage_reader)
```

### 3.5 パフォーマンス比較

| メトリクス | 同期 S3 チェックポイント | Managed Tiered Checkpointing |
|-----------|----------------------|----------------------------|
| チェックポイント保存時間 | 数分〜数十分（モデルサイズ依存、推定値[^3]） | 数秒（Tier 1 メモリコピー） |
| 訓練ブロック時間 | 保存時間と同等 | ほぼゼロ（非同期） |
| 復旧元の選択 | S3 のみ | メモリ → S3 の順にフォールバック |
| ストレージコスト | S3 のみ | メモリ + S3（段階的） |
| 訓練スループット低下 | 数%〜十数%（推定値[^3]） | 大幅に削減[^4] |

:::message
Managed Tiered Checkpointing と Checkpointless Training は補完関係にあります。Checkpointless Training が高速な in-memory 復旧を提供し、Tiered Checkpointing がカタストロフィックな障害に対する永続バックアップを提供します。両者を併用することで、あらゆる障害シナリオに対応できます。
:::

### 3.6 使い分けガイドライン

#### Managed Tiered Checkpointing を選ぶべき場合

- 新規プロジェクトで FSx for Lustre がまだ構築されていない
- コスト最適化を重視する（FSx の月額コストが不要）
- 訓練のブロック時間を最小化したい（1-2 秒のステージングのみ）
- 数千ノード規模のスケーラビリティが必要
- PyTorch DCP を使用した分散チェックポイントを標準としたい

#### FSx + DRA を継続すべき場合

- FSx for Lustre が既に構築済みで、他のワークロードでも使用している
- 大規模データセット（数十 TB）を複数ノードで共有する必要がある
- POSIX ファイルシステムの互換性が必須（レガシーコードの移植）
- Managed Tiered Checkpointing が利用不可能な環境（リージョン、インスタンスタイプ制約。詳細は[公式セットアップガイド](https://docs.aws.amazon.com/sagemaker/latest/dg/managed-tier-checkpointing-setup.html)の前提条件を参照）

#### ハイブリッド構成（FSx + Managed Tiered）

データセット保存用に FSx を継続使用し、チェックポイント保存には Managed Tiered Checkpointing を使用する構成も可能です。ただし、データセットも S3 から直接読み込む（`s3torchconnector` など）ことでコストを削減できる場合が多いため、FSx が必須かどうかを検討することを推奨します。

---

## 4. HyperPod Health Monitoring Agent

### 4.1 アーキテクチャ

Health Monitoring Agent（HMA）は、各ノード上で動作する常駐エージェントです。GPU/Trainium/EFA の健全性を継続的に監視し、障害を検出した場合は自動的にノードの交換を行います。

**Slurm 環境の構成**:

```mermaid
graph TB
    subgraph "SageMaker HyperPod 管理プレーン"
        NRS[Node Recovery Service]
        UCA[UpdateCluster API]
        CWL[CloudWatch Logs]
    end

    subgraph "Slurm クラスター"
        subgraph "Controller Node"
            CTRL[slurmctld]
            HMA_CTRL[HMA]
        end

        subgraph "Compute Node 1"
            SLURMD1[slurmd]
            HMA1[HMA]
            GPU1[GPU/Trainium]
            EFA1[EFA]
        end

        subgraph "Compute Node 2"
            SLURMD2[slurmd]
            HMA2[HMA]
            GPU2[GPU/Trainium]
            EFA2[EFA]
        end

        subgraph "Compute Node N"
            SLURMD_N[slurmd]
            HMA_N[HMA]
            GPU_N[GPU/Trainium]
            EFA_N[EFA]
        end
    end

    NRS -->|障害通知| CTRL
    UCA -->|ソフトウェア更新| CTRL
    HMA_CTRL -->|ログ送信| CWL
    HMA1 -->|ログ送信| CWL
    HMA2 -->|ログ送信| CWL
    HMA_N -->|ログ送信| CWL

    HMA1 -.->|監視| GPU1
    HMA1 -.->|監視| EFA1
    HMA2 -.->|監視| GPU2
    HMA2 -.->|監視| EFA2
    HMA_N -.->|監視| GPU_N
    HMA_N -.->|監視| EFA_N
```

**EKS 環境の構成**:

```mermaid
graph TB
    subgraph "SageMaker HyperPod 管理プレーン"
        NRS[Node Recovery Service]
        UCA[UpdateCluster API]
        CWL[CloudWatch Logs]
    end

    subgraph "EKS クラスター"
        CP[Control Plane<br/>AWS Managed]

        subgraph "Worker Node 1"
            K8S1[kubelet]
            HMA_DS1[HMA DaemonSet]
            GPU1[GPU/Trainium]
            EFA1[EFA]
        end

        subgraph "Worker Node 2"
            K8S2[kubelet]
            HMA_DS2[HMA DaemonSet]
            GPU2[GPU/Trainium]
            EFA2[EFA]
        end

        subgraph "Worker Node N"
            K8S_N[kubelet]
            HMA_DS_N[HMA DaemonSet]
            GPU_N[GPU/Trainium]
            EFA_N[EFA]
        end
    end

    NRS -->|障害通知| CP
    UCA -->|ソフトウェア更新| CP
    HMA_DS1 -->|ログ送信| CWL
    HMA_DS2 -->|ログ送信| CWL
    HMA_DS_N -->|ログ送信| CWL

    HMA_DS1 -.->|監視| GPU1
    HMA_DS1 -.->|監視| EFA1
    HMA_DS2 -.->|監視| GPU2
    HMA_DS2 -.->|監視| EFA2
    HMA_DS_N -.->|監視| GPU_N
    HMA_DS_N -.->|監視| EFA_N
```

- **Slurm 環境**: HyperPod 管理プレーンがシステムデーモンとして各ノードにインストール
- **EKS 環境**: DaemonSet として Kubernetes クラスター内にデプロイ

### 4.2 ヘルスチェックの分類

HMA は**パッシブチェック（常時監視）**と**アクティブチェック（Deep Health Checks）**の 2 種類の監視を行います（[Health Monitoring ドキュメント](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-operate-health-monitor.html)参照）。

#### パッシブチェック（HMA による常時監視）

各ノード上で継続的に実行される軽量な監視です。

**GPU (NVIDIA) チェック: **

| チェック項目 | 説明 |
|---|---|
| DCGM ポリシー違反通知 | NVIDIA DCGM からのポリシー違反イベントを監視 |
| `nvidia-smi` 出力エラー | GPU の健全性を判定 |
| EC2 プラットフォームログ | プラットフォームレベルのエラーを検出 |
| GPU 数の検証 | 期待 GPU 数との差異を検出（例: ml.p5.48xlarge = 8 GPU） |

**Trainium (AWS Neuron) チェック: **

| チェック項目 | 説明 |
|---|---|
| Neuron Monitor 出力 | AWS Neuron Monitor からのエラーを監視 |
| Neuron NPD 出力 | Node Problem Detector のエラーを検出 |
| EC2 プラットフォームログ | プラットフォームレベルのエラーを検出 |
| Neuron デバイス数の検証 | `neuron-ls` の出力と期待デバイス数を比較 |

**ネットワーク (EFA) チェック: **

| チェック項目 | 説明 |
|---|---|
| EFA 接続テスト | Elastic Fabric Adapter の接続性を検証 |

#### アクティブチェック（Deep Health Checks）

クラスターの**作成時**と**更新時**に自動実行される包括的なハードウェア診断です。

**インスタンスレベル: **

| テスト | 対象 | 説明 |
|---|---|---|
| DCGM Diagnostics Level 4 | GPU | メモリテストを含む包括的な GPU 診断 |
| GPU/NVLink Count | GPU | GPU 数および NVLink 接続数の検証 |
| Neuron sysfs | Trainium | Neuron ドライバーが伝播する sysfs カウンターの読み取り |
| Neuron Hardware Check | Trainium | 訓練ワークロードを実行して検証 |
| NCCOM Local Test | Trainium | 単一 Trainium ノード上の集約通信性能を評価 |
| EFA Test | GPU/Trainium | EFA の遅延・帯域幅ベンチマーク |
| stress-ng | 全タイプ | CPU/メモリ/ディスクのストレステスト |

**クラスターレベル: **

| テスト | 対象 | 説明 |
|---|---|---|
| NCCL Test | GPU | `all_reduce_perf` による複数 GPU 間の集約通信性能検証 |
| NCCOM Cluster Test | Trainium | 複数 Trainium ノード間の集約通信性能検証 |

### 4.3 障害検出と対応フロー

HMA が障害を検出した場合、以下の 4 ステップで対応します。

```mermaid
graph TD
    A["障害検出<br/>(HMA パッシブチェック)"] --> B["Step 1: ノードラベル付与<br/>node-health-status<br/>fault-types / fault-reasons"]
    B --> C["Step 2: ノードのスケジューリング除外<br/>EKS: NoSchedule テイント<br/>Slurm: drain 状態"]
    C --> D["Step 3: アノテーション更新<br/>fault-details (最大 20 件)"]
    D --> E["Step 4: ノード条件更新"]
    E --> F{"自動復旧設定?"}
    F -->|Automatic| G["ジョブ完了待機"]
    F -->|None| H["手動対応が必要"]
    G --> I["ノード交換<br/>(同じホスト名で復旧)"]
    I --> J{"Auto-Resume?"}
    J -->|有効| K["チェックポイントから<br/>自動再開"]
    J -->|無効| L["手動での<br/>ジョブ再投入"]
```

:::message
上記フロー図の Step 1（ラベル付与）、Step 3（アノテーション更新）、Step 4（ノード条件更新）は Kubernetes（EKS 環境）固有の用語です。Slurm 環境では、これらの情報は Slurm のノード状態管理およびログに記録されます。
:::

#### NCCL テストの閾値判定例

```json
{
  "NcclMaxAlgoBw": 1.190000,
  "NcclAvgAlgoBw": 0.488398,
  "NcclThresholdAlgoBw": 1.180000,
  "NcclOutOfBoundError": "OK",
  "NcclOperations": "all_reduce_perf",
  "NcclTotalDevices": 2,
  "NcclNodes": 2
}
```

**判定条件**: `NcclMaxAlgoBw >= NcclThresholdAlgoBw` かつ `NcclOutOfBoundError == "OK"` で合格。

#### 誤検知の防止策

HMA は以下の手法で誤検知を防止します。

- **段階的な対応**: リブートで解決可能な一時的障害（GPU 数不一致など）と、ノード交換が必要な永続的障害を区別します
- **複合的な診断**: DCGM、`nvidia-smi`、プラットフォームログなど複数のソースを統合して判定します（単一指標のみでは障害と判定しません）
- **閾値ベースの判定**: NCCL 帯域幅や EFA 遅延に対して事前定義された閾値を使用し、一時的な変動は無視します

### 4.4 Slurm との統合

HMA は Slurm の `HealthCheckProgram` とは独立して動作し、HyperPod 管理プレーンが独自にノードの健全性を管理します。

#### 手動操作（Slurm コマンド）

```bash
# リブート
scontrol update node=<ip-address> state=fail reason="Action: Reboot"

# 交換
scontrol update node=<ip-address> state=fail reason="Action: Replace"

# 強制交換（最終手段: sudo 権限が必要、実行中のジョブを強制終了する）
scontrol update node=<ip-address> state=down reason="Action: Replace"
```

#### API ベースの操作（推奨）

```bash
# リブート
aws sagemaker batch-reboot-cluster-nodes \
    --cluster-name arn: aws: sagemaker: us-west-2:123456789012: cluster/my-cluster \
    --node-ids i-0123456789abcdef0

# 交換
aws sagemaker batch-replace-cluster-nodes \
    --cluster-name arn: aws: sagemaker: us-west-2:123456789012: cluster/my-cluster \
    --node-ids i-0123456789abcdef0
```

### 4.5 Auto-Resume の実装

`srun --auto-resume` は HyperPod 独自のカスタムフラグで、障害復旧後にジョブを自動的に再開します。

```bash
#!/bin/bash
#SBATCH --nodes 2
#SBATCH --ntasks-per-node=1
#SBATCH --exclusive

srun --auto-resume=1 train_auto_resume.sh
```

::::details Auto-Resume 対応の訓練スクリプト例

```bash
#!/bin/bash
# train_auto_resume.sh

# [重要] $SLURM_JOB_NODELIST は使用しない（Auto-Resume 後に古くなる）
# 動的にノードリストを取得する
NODE_LIST=$(scontrol show jobid=$SLURM_JOBID | \
            awk -F= '/NodeList=/{print $2}' | \
            grep -v Exc)

MASTER_NODE=$(scontrol show hostname $NODE_LIST | head -n 1)

MASTER_ADDR=$(scontrol show node=$MASTER_NODE | \
              awk -F= '/NodeAddr=/{print $2}' | \
              awk '{print $1}')

torchrun --nnodes=$SLURM_NNODES \
         --nproc_per_node=8 \
         --node_rank=$SLURM_NODEID \
         --master_addr=$MASTER_ADDR \
         --master_port=1234 \
         your_training_script.py
```

::::

:::message alert
`--auto-resume=1` を使用する場合、必ず `--exclusive` フラグも指定する必要があります。また、`$SLURM_JOB_NODELIST` は Auto-Resume 後に古い値が残るため、必ず `scontrol` コマンドで動的にノードリストを取得してください。
:::

:::message
Slurm の GRES（GPU など）が有効な場合、Auto-Resume によるノードの割り当て変更が制約されます。この場合、ジョブは停止・キューへの再配置を経て最初から再開されるため、チェックポイントからの途中復旧ではなくジョブ全体の再実行となる点に注意が必要です。
:::

:::message
Slurm Auto-Resume と Checkpointless Training（セクション 1 参照）は、どちらも障害からの自動復旧を実現しますが、復旧メカニズムが根本的に異なります。Auto-Resume はチェックポイントからの復旧を「自動化」するもので、定期的なチェックポイント保存が前提です。一方、Checkpointless Training は GPU メモリ内のレプリカから復旧するため、チェックポイント I/O 自体が不要です。Slurm 環境では Auto-Resume + Tiered Checkpointing、EKS 環境では Checkpointless Training が推奨されます。
:::

### 4.6 ログとモニタリング

#### CloudWatch ログ

HMA のログは CloudWatch に送信されます。

```text
ロググループ: /aws/sagemaker/Clusters/<cluster_name>/<cluster_id>
ログストリーム: SagemakerHealthMonitoringAgent (ノードごとに 1 つ)
```

CloudWatch Insights でのクエリ例:

```text
fields @timestamp, @message
| filter @message like /HealthMonitoringAgentDetectionEvent/
| sort @timestamp desc
| limit 50
```

#### ノードローカルログ

Deep Health Check の結果は各ノード上のファイルにも保存されます。

```text
/var/log/aws/clusters/sagemaker-deep-health-check.log
```

---

## 5. 4 つの機能の関係性と使い分け

### 5.1 機能間の補完関係

4 つの機能は独立して動作しますが、組み合わせることで訓練の耐障害性とリソース効率を最大化できます。

```mermaid
graph LR
    subgraph "障害検出"
        HMA["Health Monitoring<br/>Agent"]
    end

    subgraph "障害復旧"
        CL["Checkpointless<br/>Training"]
        TC["Tiered<br/>Checkpointing"]
    end

    subgraph "リソース最適化"
        ET["Elastic<br/>Training"]
    end

    HMA -->|"障害を検出"| CL
    HMA -->|"障害を検出"| TC
    HMA -->|"ノード変動を通知"| ET

    CL -->|"高速復旧<br/>(メモリ内)"| Recovery["訓練再開"]
    TC -->|"永続バックアップ<br/>(S3)"| Recovery
    ET -->|"ノード数を調整"| Recovery

    CL <-->|"補完関係"| TC
```

:::message
上記の 4 機能は独立して動作しますが、組み合わせることであらゆる障害シナリオに対応する耐障害性を実現できます。Checkpointless Training と Tiered Checkpointing は並列に動作し、Elastic Training も独立してノード数を管理します。
:::

#### Elastic Training と Checkpointless Training の組み合わせ

Elastic Training と Checkpointless Training の組み合わせは、特に重要な相乗効果を生み出します。

**Elastic Training 単独の場合**:

Re-rendezvous（ノード数変更時の再同期）後、訓練を再開するには**チェックポイントからの復旧が必要**です。これは通常、以下の責務をユーザーコード（torchrun で起動するスクリプト）に課します:

```python
# Elastic Training でのチェックポイント復旧（通常の実装）
def train():
    if dist.get_rank() == 0:
        # Re-rendezvous 後、最新チェックポイントをロード
        checkpoint = torch.load("checkpoint.pt")
        model.load_state_dict(checkpoint["model"])
        optimizer.load_state_dict(checkpoint["optimizer"])

    # 訓練ループ
    for epoch in range(start_epoch, num_epochs):
        for batch in dataloader:
            # ...
            if should_checkpoint():
                torch.save({...}, "checkpoint.pt")  # チェックポイント保存
```

Re-rendezvous のたびに S3 や FSx からの checkpoint I/O が発生し、数分〜数十分のオーバーヘッドが生じます。

**Checkpointless Training との組み合わせ**:

Checkpointless Training を併用すると、**GPU メモリ内のレプリカから自動復旧**するため、チェックポイント I/O が不要になります:

```python
# Checkpointless Training との組み合わせ（概念的なコード例）
from hyperpod_checkpointless_training.inprocess import HPWrapper

# Checkpointless Training を有効化（実際は NeMo Framework のプラグインを通じて統合）
# HPWrapper は訓練関数全体をラップし、In-Process Recovery を有効化します
# 2026 年 2 月時点では、公式レシピで NeMo の CheckpointlessCallback を使用

def train():
    # チェックポイントの明示的なロード/セーブ不要
    # Re-rendezvous 後、メモリ内レプリカから自動復旧

    for epoch in range(num_epochs):
        for batch in dataloader:
            # 通常の訓練ループ
            loss.backward()
            optimizer.step()
```

:::message
上記は概念的なコード例です。実際の Checkpointless Training の統合は、NeMo Framework の `CheckpointlessCallback` を通じて行われます（2026 年 2 月時点）。詳細はセクション 1.4 および公式ドキュメントを参照してください。
:::

**トレードオフ**:

| 項目 | Elastic Training のみ | + Checkpointless Training |
|------|---------------------|--------------------------|
| Re-rendezvous コスト | 数秒〜数十秒 | 数秒〜数十秒（同じ） |
| 復旧後の checkpoint I/O | 数分〜数十分 | **不要** |
| GPU メモリオーバーヘッド | なし | レプリカ分（冗長コピー数とモデルサイズに依存） |
| ユーザーコードの責務 | checkpoint 実装必要 | **自動化** |
| 総復旧時間 | Re-rendezvous + I/O | Re-rendezvous のみ |

Re-rendezvous のオーバーヘッド（全プロセス一時停止）は残りますが、checkpoint I/O が不要になるため、**トータルの復旧時間が大幅に短縮**されます。

:::message
Elastic Training によるノード数変更は、頻繁に行うと Re-rendezvous のコストで訓練効率が低下します。しかし Checkpointless Training と組み合わせることで、checkpoint I/O のボトルネックが解消され、より積極的なスケーリングが実用的になります。ただし、GPU メモリオーバーヘッド（レプリカ保存用）は避けられないため、メモリが制約となる大規模モデルでは注意が必要です。
:::

### 5.2 機能対比表

| 観点 | Checkpointless Training | Elastic Training | Managed Tiered Checkpointing | Health Monitoring Agent |
|------|----------------------|-----------------|---------------------------|----------------------|
| 主な目的 | 高速障害復旧 | リソース効率化 | チェックポイント高速化 | 障害検出 |
| 動作レベル | GPU メモリ内 | 分散プロセスグループ（Rendezvous） | CPU メモリ→S3 | ノードハードウェア |
| 復旧時間 | 数分以内 | Re-rendezvous に依存 | Tier に依存 | 検出のみ（15 秒以内） |
| 対応する障害 | 単一/少数ノード障害 | ノード増減 | あらゆる障害 | GPU/Trainium/EFA 障害 |
| カタストロフィック障害 | 対応不可 | 対応不可 | S3 バックアップで対応 | 検出は可能 |
| オーケストレーター | EKS のみ[^5] | EKS のみ[^6] | EKS / Slurm[^7] | EKS / Slurm[^8] |

### 5.3 適用シナリオ

#### シナリオ 1: 大規模 LLM 事前訓練（数千 GPU 規模・EKS 環境）

推奨構成: **全 4 機能をフル活用**（本シナリオは EKS 環境を前提とします）

```text
[Health Monitoring Agent]
  各ノードで GPU/EFA を常時監視
      |
      v 障害検出
[Checkpointless Training]
  メモリ内レプリカから数分で復旧
      |
      v 永続バックアップ
[Managed Tiered Checkpointing]
  非同期で S3 に永続化（ブロックなし）
      |
      v リソース調整
[Elastic Training]
  障害ノード離脱 → 縮小運転 → 復旧後スケールアップ
```

#### シナリオ 2: 中規模ファインチューニング（数十 GPU）

推奨構成: **Health Monitoring + Tiered Checkpointing**（EKS / Slurm いずれの環境でも適用可能です）

Checkpointless Training のメモリオーバーヘッドが相対的に大きくなるため、Tiered Checkpointing による非同期チェックポイントで十分な耐障害性を確保できます。

#### シナリオ 3: Slurm クラスターでの訓練

推奨構成: **Health Monitoring + Tiered Checkpointing + Auto-Resume**

Elastic Training と Checkpointless Training は EKS オーケストレーターでのみ利用可能なため、Slurm 環境では `srun --auto-resume` と Tiered Checkpointing を組み合わせて耐障害性を確保します。

### 5.4 ベストプラクティス

1. **Health Monitoring Agent は常に有効化** -- 追加コストなしでハードウェア障害を早期検出できます
2. **Checkpointless Training + Tiered Checkpointing を併用** -- 高速復旧と永続バックアップの両方を確保できます
3. **Elastic Training は EKS 環境で大規模クラスター向け** -- 小規模クラスターではオーバーヘッドが相対的に大きくなります
4. **`replicaIncrementStep` は適切に設定** -- 小さすぎると頻繁な Re-rendezvous でオーバーヘッドが増大し、大きすぎるとリソースの粒度が粗くなります
5. **CloudWatch ログを定期的に監視** -- 障害パターンの分析と予防保守に活用できます

---

## まとめ

SageMaker HyperPod の re: Invent 2024 を含む一連のアップデートで追加された 4 つの機能は、大規模分散訓練の 3 大課題（ハードウェア障害、チェックポイント I/O、リソース効率）に対する包括的なソリューションを提供します。

| 機能 | 解決する課題 | 技術的アプローチ |
|------|------------|----------------|
| Checkpointless Training | 障害復旧の高速化 | GPU メモリ内レプリカ + In-Process Recovery |
| Elastic Training | リソースの動的調整 | PyTorch Elastic + HyperPodPyTorchJob CRD |
| Managed Tiered Checkpointing | チェックポイント I/O 最適化 | CPU メモリ→S3 の非同期パイプライン + ノード間レプリケーション |
| Health Monitoring Agent | 障害の早期検出と自動復旧 | パッシブ/アクティブチェック + 自動ノード交換 |

AWS の公式発表[^1]によれば、これらの機能を適切に組み合わせることで、数千台の AI アクセラレータ規模の大規模クラスターにおいても 95% 以上の goodput を実現できるとされています。特に Checkpointless Training と Managed Tiered Checkpointing の併用は、従来の「定期的なチェックポイント保存 + 手動復旧」というパラダイムを根本的に変革するものであり、大規模 LLM 訓練のコスト効率を大幅に改善します。

:::message
本記事の各主張の検証状況は以下の通りです。

**原典で確認済みの主張: **
- PyTorch Elastic の `_RendezvousState` フィールド構成、`DynamicRendezvousHandler`、`SimpleElasticAgent._invoke_run()` のフロー -- [PyTorch GitHub リポジトリ](https://github.com/pytorch/pytorch/tree/main/torch/distributed/elastic)のソースコードで確認しています
- `C10dRendezvousBackend` の存在と `compare_set()` による楽観的ロック -- [c10d_rendezvous_backend.py](https://github.com/pytorch/pytorch/blob/main/torch/distributed/elastic/rendezvous/c10d_rendezvous_backend.py) で確認しています
- `keep_alive_interval` = 5 秒、`keep_alive_max_attempt` = 3 のデフォルト値 -- [dynamic_rendezvous.py](https://github.com/pytorch/pytorch/blob/main/torch/distributed/elastic/rendezvous/dynamic_rendezvous.py) の `create_handler()` で確認しています
- `amzn-sagemaker-checkpointing` v1.1.2（Apache License 2.0）の API -- [PyPI](https://pypi.org/project/amzn-sagemaker-checkpointing/) で確認しています
- `SageMakerCheckpointConfig`、`SageMakerTieredStorageWriter`、`SageMakerTieredStorageReader` -- PyPI ドキュメントおよび [AWS 公式セットアップガイド](https://docs.aws.amazon.com/sagemaker/latest/dg/managed-tier-checkpointing-setup.html) で確認しています
- HyperPod CLI の Elastic Training パラメータ -- [aws/sagemaker-hyperpod-cli GitHub](https://github.com/aws/sagemaker-hyperpod-cli) README で確認しています
- `--tiered-storage-config` の API -- AWS 公式ドキュメントで確認しています

**AWS 発表資料に基づく主張（実環境での独立検証は未実施）: **
- goodput 95% 以上、障害復旧時間数分以内 -- [HyperPod 機能ページ](https://aws.amazon.com/sagemaker/hyperpod/features/)（「enables over 95% training goodput on clusters with thousands of AI accelerators」「automatic recovery from infrastructure faults in minutes」）および [AWS 公式ブログ](https://aws.amazon.com/blogs/aws/introducing-checkpointless-and-elastic-training-on-amazon-sagemaker-hyperpod/) に基づきます。なお「従来方式の復旧時間」の具体的な数値は公式ソースに記載がなく、大規模訓練における一般的な知見に基づく推定値です
- Checkpointless Training の 4 つのイノベーション名と概要 -- [Checkpointless Training ドキュメント](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-eks-operate-training-checkpointless.html) に基づきます
- HMA のパッシブチェック/Deep Health Checks の詳細項目 -- [Health Monitoring ドキュメント](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-operate-health-monitor.html) に基づきます
- GPU メモリ使用量の増加 -- 冗長コピー数とモデルサイズに依存します。具体的な増加率の公式数値は確認できていません
- 従来方式のチェックポイント保存時間・スループット低下率 -- 大規模分散訓練における一般的な知見に基づく推定値であり、AWS 公式の数値ではありません

**推測を含む記述: **
- HyperPod Training Operator の内部動作 -- ソースコードは非公開です。公開されている CLI コード（[aws/sagemaker-hyperpod-cli](https://github.com/aws/sagemaker-hyperpod-cli)）と PyTorch Elastic のソースコード分析に基づく推測を含みます

本記事のコード例は公式ドキュメントおよび公開リポジトリの情報に基づくものであり、実際の SDK インターフェースは予告なく変更される可能性があります。最新かつ正確な仕様は AWS 公式ドキュメントを参照してください。
:::

## 参考資料

- [Amazon SageMaker HyperPod Features](https://aws.amazon.com/sagemaker/hyperpod/features/) -- 公式機能ページ（goodput 95%以上等の数値の出典）
- [AWS SageMaker HyperPod 公式ドキュメント](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod.html) -- 2025 年 12 月時点
- [AWS Blog: Introducing Checkpointless and Elastic Training on Amazon SageMaker HyperPod](https://aws.amazon.com/jp/blogs/news/introducing-checkpointless-and-elastic-training-on-amazon-sagemaker-hyperpod/) -- 公式発表ブログ
- [Checkpointless Training ドキュメント](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-checkpointless.html)
- [Managed Tiered Checkpointing ドキュメント](https://docs.aws.amazon.com/sagemaker/latest/dg/managed-tier-checkpointing.html)
- [aws/sagemaker-hyperpod-cli (GitHub)](https://github.com/aws/sagemaker-hyperpod-cli) -- v3.5.0 / sagemaker-hyperpod v3.6.0
- [aws-samples/awsome-distributed-training (GitHub)](https://github.com/aws-samples/awsome-distributed-training)
- [amzn-sagemaker-checkpointing (PyPI)](https://pypi.org/project/amzn-sagemaker-checkpointing/) -- v1.1.2
- [PyTorch Elastic (torch.distributed.elastic)](https://pytorch.org/docs/stable/distributed.elastic.html) -- PyTorch 2.10.0
- [PyTorch Elastic ソースコード: dynamic_rendezvous.py](https://github.com/pytorch/pytorch/blob/main/torch/distributed/elastic/rendezvous/dynamic_rendezvous.py) -- `_RendezvousState`、`DynamicRendezvousHandler` の実装
- [PyTorch Elastic ソースコード: c10d_rendezvous_backend.py](https://github.com/pytorch/pytorch/blob/main/torch/distributed/elastic/rendezvous/c10d_rendezvous_backend.py) -- `C10dRendezvousBackend` の実装
- [PyTorch Elastic ソースコード: agent/server/api.py](https://github.com/pytorch/pytorch/blob/main/torch/distributed/elastic/agent/server/api.py) -- `SimpleElasticAgent._invoke_run()` の実装
- [Managed Tiered Checkpointing セットアップガイド](https://docs.aws.amazon.com/sagemaker/latest/dg/managed-tier-checkpointing-setup.html)

[^1]: [Amazon SageMaker HyperPod Features](https://aws.amazon.com/sagemaker/hyperpod/features/) -- 公式機能ページに「enables over 95% training goodput on clusters with thousands of AI accelerators」「automatic recovery from infrastructure faults in minutes」と記載されています。
[^2]: [AWS Blog: Introducing Checkpointless and Elastic Training on Amazon SageMaker HyperPod](https://aws.amazon.com/jp/blogs/news/introducing-checkpointless-and-elastic-training-on-amazon-sagemaker-hyperpod/) -- re:Invent 2024 で発表された公式ブログ記事です。
[^3]: 従来方式のパフォーマンス数値（復旧時間、チェックポイント I/O オーバーヘッド等）は、大規模分散訓練における一般的な知見に基づく推定値であり、AWS の公式発表による数値ではありません。実際の値はクラスター構成、モデルサイズ、チェックポイント頻度等により大きく異なります。
[^4]: Managed Tiered Checkpointing の公式ドキュメントでは「Reduced checkpoint overhead」「Improved training throughput」と定性的に記載されていますが、具体的な削減率は公開されていません。
[^5]: Checkpointless Training は EKS 環境でのみ利用可能です。[公式ドキュメント](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-eks-operate-training-checkpointless.html)を参照してください。
[^6]: Elastic Training は EKS 環境でのみ利用可能です。[公式ドキュメント](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-eks-operate-training-elastic.html)を参照してください。
[^7]: Managed Tiered Checkpointing は EKS と Slurm の両環境で利用可能です。[公式セットアップガイド](https://docs.aws.amazon.com/sagemaker/latest/dg/managed-tier-checkpointing-setup.html)を参照してください。
[^8]: Health Monitoring Agent は EKS と Slurm の両環境で利用可能です。[公式ドキュメント](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-operate-health-monitor.html)を参照してください。
