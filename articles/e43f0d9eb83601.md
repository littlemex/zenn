---
title: "vLLM MultiLoRA Inferenceã®ä»•çµ„ã¿ã¨S-LoRAã¨ã®é•ã„"
emoji: "ğŸš€"
type: "tech"
topics: ["vllm", "lora", "llm", "æ©Ÿæ¢°å­¦ç¿’", "æ¨è«–"]
published: true
---

# ã¯ã˜ã‚ã«

LLMç­‰ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã¨ã—ã¦åºƒãä½¿ã‚ã‚Œã¦ã„ã‚‹LoRAï¼ˆLow-Rank Adaptationï¼‰ã§ã™ãŒã€æœ¬ç•ªç’°å¢ƒã§è¤‡æ•°ã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’åŠ¹ç‡çš„ã«æ¨è«–ã™ã‚‹ã«ã¯å·¥å¤«ãŒå¿…è¦ã§ã™ã€‚æœ¬è¨˜äº‹ã§ã¯ã€vLLMã®[MultiLoRA Inference](https://docs.vllm.ai/en/stable/examples/offline_inference/multilora_inference/?query=multilora)æ©Ÿèƒ½ã«ã¤ã„ã¦ç¢ºèªã—ã€å­¦è¡“çš„ãªæœ€é©åŒ–æ‰‹æ³•ã§ã‚ã‚‹S-LoRAã¨ã®å®Ÿè£…ä¸Šã®é•ã„ã«ã¤ã„ã¦æ•´ç†ã—ã¾ã™ã€‚

https://arxiv.org/html/2311.03285v3

# vLLM MultiLoRA Inferenceã¨ã¯

vLLMã¯æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã¨ã—ã¦ãƒ¡ã‚¸ãƒ£ãƒ¼ã§ã™ãŒã€MultiLoRA Inferenceã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å˜ä¸€ã®ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦è¤‡æ•°ã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’å‹•çš„ã«åˆ‡ã‚Šæ›¿ãˆãªãŒã‚‰æ¨è«–ã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚vLLMè‡ªä½“ã®æ§‹é€ ã«ã¤ã„ã¦ã¯ä»¥ä¸‹ãŒå‚è€ƒã«ãªã‚Šã¾ã™ã€‚

https://zenn.dev/tosshi/articles/f64ba0b86e330b

## ä¸»ãªç‰¹å¾´

### 1. è¤‡æ•°ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®åŒæ™‚ç®¡ç†

vLLMã®Multi-LoRAæ©Ÿèƒ½ã¯ã€æŸ”è»Ÿãªæ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚åŒä¸€ãƒãƒƒãƒå†…ã§ç•°ãªã‚‹LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ä½¿ç”¨ã—ãŸãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å‡¦ç†ã§ãã‚‹ãŸã‚ã€ä¾‹ãˆã°SQLç”Ÿæˆã‚¿ã‚¹ã‚¯ã¨æ•°å­¦å•é¡Œè§£ç­”ã‚¿ã‚¹ã‚¯ã‚’åŒã˜ãƒãƒƒãƒã§åŠ¹ç‡çš„ã«å®Ÿè¡Œã§ãã¾ã™ã€‚ã•ã‚‰ã«ã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ã¿ã‚’ä½¿ç”¨ã™ã‚‹ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¨LoRAã‚’ä½¿ç”¨ã™ã‚‹ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’æ··åœ¨ã•ã›ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã€ã‚¿ã‚¹ã‚¯ã«å¿œã˜ãŸæœ€é©ãªãƒ¢ãƒ‡ãƒ«é¸æŠãŒå®Ÿç¾ã§ãã¾ã™ã€‚ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®å‹•çš„ãªãƒ­ãƒ¼ãƒ‰ãƒ»ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½ã«ã‚ˆã‚Šã€ãƒ¡ãƒ¢ãƒªåˆ¶ç´„å†…ã§å¤šæ•°ã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ç®¡ç†ã§ãã‚‹ç‚¹ã‚‚é‡è¦ãªç‰¹å¾´ã§ã™ã€‚

```mermaid
graph LR
    A[ãƒãƒƒãƒãƒªã‚¯ã‚¨ã‚¹ãƒˆ] --> B{ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°}
    B --> C[ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ã¿]
    B --> D[SQL LoRA]
    B --> E[Math LoRA]
    C --> F[å‡ºåŠ›]
    D --> F
    E --> F

    style B fill:#e1f5ff
    style F fill:#ffe1e1
```

### 2. è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿

Multi-LoRA Inferenceã‚’æœ‰åŠ¹ã«ã™ã‚‹ã«ã¯ã€`enable_lora=True`ã«åŠ ãˆã¦ã€ã„ãã¤ã‹ã®é‡è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã“ã‚Œã‚‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¨ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’åˆ¶å¾¡ã—ã¾ã™ã€‚

```python
from vllm import LLM, SamplingParams

llm = LLM(
    model="meta-llama/Llama-2-7b-hf",
    enable_lora=True,        # LoRAæ©Ÿèƒ½ã‚’æœ‰åŠ¹åŒ–
    max_loras=4,             # åŒæ™‚ã«ãƒ­ãƒ¼ãƒ‰å¯èƒ½ãªLoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼æ•°
    max_lora_rank=64,        # ã‚µãƒãƒ¼ãƒˆã™ã‚‹æœ€å¤§rankå€¤
    max_cpu_loras=8          # CPUå´ã®LoRAã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚º
)
```

`max_loras`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€ãƒãƒƒãƒå†…ã§åŒæ™‚ã«ä½¿ç”¨ã§ãã‚‹LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼æ•°ã‚’æŒ‡å®šã—ã¾ã™ã€‚ã“ã®å€¤ã‚’å¤§ããã™ã‚‹ã¨ã€ã‚ˆã‚Šå¤šãã®LoRAã‚’ä¸¦åˆ—å‡¦ç†ã§ãã¾ã™ãŒã€GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¢—åŠ ã—ã¾ã™ã€‚`max_lora_rank`ã¯ã€å…¨ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã§ä½¿ç”¨å¯èƒ½ãªæœ€å¤§rankå€¤ã‚’è¨­å®šã—ã€ãƒ¡ãƒ¢ãƒªå‰²ã‚Šå½“ã¦ã«ç›´æ¥å½±éŸ¿ã—ã¾ã™ã€‚`max_cpu_loras`ã¯ã€CPUå´ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼æ•°ã‚’åˆ¶å¾¡ã—ã€é »ç¹ã«åˆ‡ã‚Šæ›¿ã‚ã‚‹ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰æ™‚é–“ã‚’å‰Šæ¸›ã™ã‚‹å½¹å‰²ã‚’æœãŸã—ã¾ã™ã€‚

### 3. APIä½¿ç”¨ä¾‹

LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’æŒ‡å®šã—ã¦ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ä¿¡ã™ã‚‹ã«ã¯ã€`LoRARequest`ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

```pythMultion
from vllm import SamplingParams
from vllm.lora.request import LoRARequest

# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¨­å®š
sampling_params = SamplingParams(temperature=0.0, max_tokens=256)

# LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’æŒ‡å®šã—ãŸãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆv0.14.0ä»¥é™ï¼‰
lora_request = LoRARequest(
    lora_name="sql_adapter",
    lora_int_id=1,
    lora_path="/path/to/sql-lora"  # ã¾ãŸã¯HuggingFace ãƒªãƒã‚¸ãƒˆãƒªID
)

prompts = ["ã‚¿ã‚¹ã‚¯1ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ", "ã‚¿ã‚¹ã‚¯2ç”¨ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"]
outputs = llm.generate(
    prompts=prompts,
    sampling_params=sampling_params,
    lora_request=lora_request
)
```

**æ³¨æ„ï¼š** v0.14.0ä»¥é™ã€APIãŒå¤‰æ›´ã•ã‚Œã¾ã—ãŸã€‚è©³ç´°ã¯ã€Œå®Ÿè£…ã‚¬ã‚¤ãƒ‰ã¨çŸ¥è¦‹ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

### 4. ãƒãƒƒãƒå‡¦ç†ã®å‹•ä½œ

`max_loras=1`ã®å ´åˆã€vLLMã¯ä»¥ä¸‹ã®ã‚ˆã†ã«å‹•ä½œã—ã¾ã™ï¼š

> "requests with the second LoRA adapter will be ran after all requests with the first adapter have finished"

ã¤ã¾ã‚Šã€ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼1ã‚’ä½¿ç”¨ã™ã‚‹ã™ã¹ã¦ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒå®Œäº†ã—ã¦ã‹ã‚‰ã€ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼2ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆãŒå‡¦ç†ã•ã‚Œã¾ã™ã€‚`max_loras`ã‚’å¢—ã‚„ã™ã“ã¨ã§ã€ã‚ˆã‚Šå¤šãã®ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’åŒæ™‚ã«å‡¦ç†ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

::::details Multi-LoRAãƒãƒƒãƒå‡¦ç†ã®ä¾‹ï¼ˆv0.14.0å¯¾å¿œï¼‰

```python
from vllm import LLM, SamplingParams
from vllm.lora.request import LoRARequest

# ã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ–
llm = LLM(
    model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    enable_lora=True,
    max_loras=3,           # åŒæ™‚ã«3ã¤ã®LoRAã‚’ã‚µãƒãƒ¼ãƒˆ
    max_lora_rank=64,
    gpu_memory_utilization=0.85
)

# ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
sampling_params = SamplingParams(temperature=0.0, max_tokens=200)

# å„ã‚¿ã‚¹ã‚¯ã®LoRAãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å®šç¾©
lora_requests = {
    "sql": LoRARequest("sql", 1, "sid321axn/tiny-llama-text2sql"),
    "math": LoRARequest("math", 2, "philimon/TinyLlama-gsm8k-lora"),
    "function": LoRARequest("function", 3, "unclecode/tinyllama-function-call-lora-adapter-250424")
}

# SQLã‚¿ã‚¹ã‚¯
sql_prompts = ["Create a SELECT query", "Write an INSERT statement"]
sql_outputs = llm.generate(
    prompts=sql_prompts,
    sampling_params=sampling_params,
    lora_request=lora_requests["sql"]
)

# æ•°å­¦ã‚¿ã‚¹ã‚¯
math_prompts = ["What is 15 * 23?", "Solve: x + 5 = 12"]
math_outputs = llm.generate(
    prompts=math_prompts,
    sampling_params=sampling_params,
    lora_request=lora_requests["math"]
)
```
::::

ã“ã®æ–¹æ³•ã«ã‚ˆã‚Šã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’1ã¤ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã ã‘ã§ã€è¤‡æ•°ã®ã‚¿ã‚¹ã‚¯ç‰¹åŒ–å‹LoRAã‚’åŠ¹ç‡çš„ã«ä½¿ã„åˆ†ã‘ã‚‰ã‚Œã¾ã™ã€‚

# S-LoRAã¨ã¯

S-LoRAï¼ˆScalable LoRAï¼‰ã¯ã€æ•°åƒã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’å˜ä¸€ã¾ãŸã¯è¤‡æ•°ã®GPUä¸Šã§åŠ¹ç‡çš„ã«æä¾›ã™ã‚‹ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚2023å¹´11æœˆã«[Sheng et al.ã«ã‚ˆã£ã¦ç™ºè¡¨ã•ã‚ŒãŸç ”ç©¶ï¼ˆarXiv:2311.03285ï¼‰](https://arxiv.org/abs/2311.03285)ã§ææ¡ˆã•ã‚Œã¾ã—ãŸã€‚ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã¯ã€vLLMã®åŸºæœ¬çš„ãªMultiLoRAå®Ÿè£…ã‚’ã•ã‚‰ã«ç™ºå±•ã•ã›ã€ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã®å¤§è¦æ¨¡LoRA servingã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã®æŠ€è¡“ã‚’æä¾›ã—ã¾ã™ã€‚

## S-LoRAã®ä¸»è¦ãªæŠ€è¡“é©æ–°

### 1. çµ±ä¸€ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«ï¼ˆUnified Pagingï¼‰

S-LoRAã®æœ€ã‚‚é‡è¦ãªæŠ€è¡“ã¯ã€KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨LoRAã‚¦ã‚§ã‚¤ãƒˆã‚’çµ±ä¸€çš„ã«ç®¡ç†ã™ã‚‹ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«ã§ã™ã€‚ã“ã®è¨­è¨ˆã¯ã€[PagedAttentionæ©Ÿæ§‹](https://arxiv.org/abs/2309.06180)ã®åŸç†ã‚’æ‹¡å¼µã—ãŸã‚‚ã®ã§ã€KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨LoRAã‚¦ã‚§ã‚¤ãƒˆã®ä¸¡æ–¹ã‚’åŒä¸€ã®ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«å†…ã§ç®¡ç†ã—ã¾ã™ã€‚å„ãƒšãƒ¼ã‚¸ã¯ã‚µã‚¤ã‚ºHï¼ˆéš ã‚Œæ¬¡å…ƒï¼‰ã®ãƒ™ã‚¯ãƒˆãƒ«ã«å¯¾å¿œã—ã€éé€£ç¶šã‹ã¤ã‚¤ãƒ³ã‚¿ãƒ¼ãƒªãƒ¼ãƒ–ã•ã‚ŒãŸæ ¼ç´ã«ã‚ˆã‚Šã€å¾“æ¥ã®é€£ç¶šãƒ¡ãƒ¢ãƒªå‰²ã‚Šå½“ã¦ã§å•é¡Œã¨ãªã£ã¦ã„ãŸãƒ¡ãƒ¢ãƒªã®æ–­ç‰‡åŒ–ã‚’åŠ‡çš„ã«å‰Šæ¸›ã—ã¾ã™ã€‚

```mermaid
graph TB
    subgraph "çµ±ä¸€ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«"
        P1[Page 1<br/>KV Cache]
        P2[Page 2<br/>LoRA Weight]
        P3[Page 3<br/>KV Cache]
        P4[Page 4<br/>LoRA Weight]
        P5[Page 5<br/>KV Cache]
        P6[...]
    end

    Request1[Request 1] --> P1
    Request1 --> P3
    Request1 --> P4
    Request2[Request 2] --> P2
    Request2 --> P5

    style P1 fill:#e1f5ff
    style P3 fill:#e1f5ff
    style P5 fill:#e1f5ff
    style P2 fill:#ffe1e1
    style P4 fill:#ffe1e1
```

ã“ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãŒé«˜ã„ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’å®Ÿç¾ã§ãã‚‹ç†ç”±ã¯ã€KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨LoRAã‚¦ã‚§ã‚¤ãƒˆãŒå…±ã«éš ã‚Œæ¬¡å…ƒHã‚’åŸºæœ¬å˜ä½ã¨ã—ã¦ã„ã‚‹ç‚¹ã«ã‚ã‚Šã¾ã™ã€‚åŒä¸€ã®ãƒšãƒ¼ã‚¸ã‚µã‚¤ã‚ºã§ä¸¡è€…ã‚’ç®¡ç†ã™ã‚‹ã“ã¨ã§ã€å‹•çš„ãªã‚µã‚¤ã‚ºå¤‰åŒ–ã«æŸ”è»Ÿã«å¯¾å¿œã—ãªãŒã‚‰ã€ãƒ¡ãƒ¢ãƒªã®æ–­ç‰‡åŒ–ã‚’æœ€å°é™ã«æŠ‘ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

### 2. ã‚«ã‚¹ã‚¿ãƒ CUDAã‚«ãƒ¼ãƒãƒ«

S-LoRAã¯ã€ç•°ãªã‚‹rankã‚’æŒã¤LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’åŠ¹ç‡çš„ã«ãƒãƒƒãƒå‡¦ç†ã™ã‚‹ãŸã‚ã€2ã¤ã®å°‚ç”¨CUDAã‚«ãƒ¼ãƒãƒ«ã‚’å®Ÿè£…ã—ã¦ã„ã¾ã™ï¼š

#### MBGMM (Multi-size Batched Gather Matrix-Matrix Multiplication)

ã“ã®ã‚«ãƒ¼ãƒãƒ«ã¯Prefillã‚¹ãƒ†ãƒ¼ã‚¸ï¼ˆè¤‡æ•°ãƒˆãƒ¼ã‚¯ãƒ³ã®åŒæ™‚å‡¦ç†ï¼‰ã§ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚ç•°ãªã‚‹rankã‚’æŒã¤ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚¦ã‚§ã‚¤ãƒˆã‚’éé€£ç¶šãƒ¡ãƒ¢ãƒªé ˜åŸŸã‹ã‚‰åŠ¹ç‡çš„ã«åé›†ã—ã€ãƒãƒƒãƒè¡Œåˆ—ä¹—ç®—ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚å®Ÿè£…ã¯[Triton](https://github.com/openai/triton)ã‚’ç”¨ã„ã¦é–‹ç™ºã•ã‚Œã¦ãŠã‚Šã€[Punica](https://arxiv.org/abs/2310.18547)ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å®Ÿè£…ã‚’æ”¹è‰¯ã—ãŸã‚‚ã®ã§ã™ã€‚

#### MBGMV (Multi-size Batched Gather Matrix-Vector Multiplication)

Decodeã‚¹ãƒ†ãƒ¼ã‚¸ï¼ˆå˜ä¸€ãƒˆãƒ¼ã‚¯ãƒ³ã®é€æ¬¡çš„ç”Ÿæˆï¼‰ã§ä½¿ç”¨ã•ã‚Œã‚‹ã“ã®ã‚«ãƒ¼ãƒãƒ«ã¯ã€å˜ä¸€ãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾ã™ã‚‹è¡Œåˆ—ãƒ™ã‚¯ãƒˆãƒ«ä¹—ç®—ã‚’ã€ç•°ãªã‚‹rankã®ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼é–“ã§ãƒãƒƒãƒå‡¦ç†ã—ã¾ã™ã€‚MBGMMåŒæ§˜ã€Triton/Punicaãƒ™ãƒ¼ã‚¹ã®å®Ÿè£…ã«ã‚ˆã‚Šã€æ¨™æº–çš„ãªCUDAã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨æ¯”è¼ƒã—ã¦å¤§å¹…ãªé«˜é€ŸåŒ–ã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚

### 3. åˆ†é›¢è¨ˆç®—ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

S-LoRAã®é‡è¦ãªè¨­è¨ˆä¸Šã®é¸æŠã¨ã—ã¦ã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨LoRAã®è¨ˆç®—ã‚’åˆ†é›¢ã™ã‚‹ç‚¹ãŒæŒ™ã’ã‚‰ã‚Œã¾ã™ã€‚å¾“æ¥ã®æ–¹æ³•ã§ã¯ã€LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ã‚¦ã‚§ã‚¤ãƒˆã«ãƒãƒ¼ã‚¸ã—ã¦`(W + Î”W) Ã— x`ã¨ã„ã†å½¢ã§è¨ˆç®—ã—ã¦ã„ã¾ã—ãŸã€‚ã“ã‚Œã«å¯¾ã—S-LoRAã¯ã€`W Ã— x`ã¨`(A Ã— B) Ã— x`ã‚’åˆ¥ã€…ã«è¨ˆç®—ã—ã€å¾Œã§åŠ ç®—ã™ã‚‹æ–¹å¼ã‚’æ¡ç”¨ã—ã¦ã„ã¾ã™ã€‚

```mermaid
graph TB
    Input[å…¥åŠ› x] --> Split{è¨ˆç®—åˆ†é›¢}

    Split --> Base[ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«è¨ˆç®—<br/>W Ã— x]
    Split --> LoRA[LoRAè¨ˆç®—<br/>A Ã— B Ã— x]

    Base --> |æ¨™æº–GEMM<br/>å…¨ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§<br/>ãƒãƒƒãƒå‡¦ç†| Add
    LoRA --> |ã‚«ã‚¹ã‚¿ãƒ ã‚«ãƒ¼ãƒãƒ«<br/>MBGMM/MBGMV| Add

    Add[+] --> Output[å‡ºåŠ›]

    style Base fill:#e1f5ff
    style LoRA fill:#ffe1e1
    style Add fill:#e1ffe1
```

ã“ã®åˆ†é›¢è¨ˆç®—ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã«ã‚ˆã‚Šã€è¨ˆç®—ã‚³ã‚¹ãƒˆã®é«˜ã„ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«è¨ˆç®—ï¼ˆWÃ—xï¼‰ã‚’å…¨ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã§ãƒãƒƒãƒå‡¦ç†ã§ãã‚‹ãŸã‚ã€ã‚¦ã‚§ã‚¤ãƒˆã®é‡è¤‡ãƒ­ãƒ¼ãƒ‰ã‚’å®Œå…¨ã«å›é¿ã§ãã¾ã™ã€‚çµæœã¨ã—ã¦ã€ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãŒå¤§å¹…ã«å‘ä¸Šã—ã€æ•°åƒã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’æ‰±ã†éš›ã®å®Ÿè¡Œå¯èƒ½æ€§ãŒå®Ÿç¾ã•ã‚Œã¦ã„ã¾ã™ã€‚

### 4. ãƒ¡ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªã‹ã‚‰ã®å‹•çš„ãƒ•ã‚§ãƒƒãƒ

S-LoRAã¯ã€å…¨ã¦ã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ãƒ¡ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒªï¼ˆCPU RAMï¼‰ã«ä¿æŒã—ã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆã«å¿œã˜ã¦å¿…è¦ãªã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ã¿ã‚’GPUã«ãƒ•ã‚§ãƒƒãƒã™ã‚‹æˆ¦ç•¥ã‚’æ¡ç”¨ã—ã¦ã„ã¾ã™ã€‚ã“ã®è¨­è¨ˆã«ã‚ˆã‚Šã€æ•°åƒã®ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’æä¾›ã—ãªãŒã‚‰GPUãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æœ€å°é™ã«æŠ‘ãˆã‚‹ã“ã¨ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

```mermaid
graph LR
    subgraph CPU["ãƒ¡ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒª (CPU RAM)"]
        L1[LoRA-1]
        L2[LoRA-2]
        L3[LoRA-3]
        L4[...]
        L1000[LoRA-1000]
    end

    subgraph GPU["GPU VRAM"]
        Base[ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«<br/>20GB]
        Active[ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªLoRA<br/>æ•°å€‹ã®ã¿]
    end

    L1 -.å‹•çš„ãƒ•ã‚§ãƒƒãƒ.-> Active
    L3 -.å‹•çš„ãƒ•ã‚§ãƒƒãƒ.-> Active
    Active -.è§£æ”¾.-> L2

    style Base fill:#e1f5ff
    style Active fill:#ffe1e1
```

ã“ã®å‹•çš„ãªãƒ­ãƒ¼ãƒ‰ãƒ»ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰æ©Ÿæ§‹ã«ã‚ˆã‚Šã€å®Ÿéš›ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹LoRAã®ã¿ã‚’GPUã«é…ç½®ã§ãã‚‹ãŸã‚ã€ç†è«–ä¸Šã¯ç„¡åˆ¶é™ã®æ•°ã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ã‚µãƒãƒ¼ãƒˆã§ãã¾ã™ã€‚

## S-LoRAã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

[S-LoRAã®ç ”ç©¶è«–æ–‡](https://arxiv.org/abs/2311.03285)ã«ã‚ˆã‚‹ã¨ã€S-LoRAã¯æ—¢å­˜ã®å®Ÿè£…ã¨æ¯”è¼ƒã—ã¦é¡•è‘—ãªæ€§èƒ½å‘ä¸Šã‚’é”æˆã—ã¦ã„ã¾ã™ã€‚ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆé¢ã§ã¯ã€HuggingFace PEFTã‚„vLLMã®åˆæœŸLoRAå®Ÿè£…ã¨æ¯”è¼ƒã—ã¦æœ€å¤§4å€ã®å‘ä¸Šã‚’å®Ÿç¾ã—ã¦ãŠã‚Šã€å¾“æ¥ã®æ‰‹æ³•ï¼ˆvLLM-packedï¼‰ãŒ5å€‹æœªæº€ã®ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã—ã‹å®Ÿç”¨çš„ã«æä¾›ã§ããªã‹ã£ãŸã®ã«å¯¾ã—ã€S-LoRAã¯æ•°åƒã®ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’åŒæ™‚ã‚µãƒãƒ¼ãƒˆã§ãã¾ã™ã€‚ã•ã‚‰ã«é‡è¦ãªç‚¹ã¨ã—ã¦ã€æ•°åƒã®ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’æä¾›ã™ã‚‹å ´åˆã§ã‚‚æœ€å°é™ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã§å‹•ä½œã™ã‚‹ãŸã‚ã€GPUåŠ¹ç‡ãŒæ¥µã‚ã¦é«˜ã„æ°´æº–ã‚’ç¶­æŒã—ã¦ã„ã¾ã™ã€‚

# vLLM Multi-LoRAã¨S-LoRAã®å®Ÿè£…ä¸Šã®é•ã„

## 1. ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®é•ã„

| å´é¢ | vLLM Multi-LoRA | S-LoRA |
|------|-----------------|--------|
| ãƒ¡ãƒ¢ãƒªç®¡ç† | åŸºæœ¬çš„ãªãƒ­ãƒ¼ãƒ‰/ã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ | çµ±ä¸€ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«ï¼ˆUnified Pagingï¼‰ |
| è¨ˆç®—æ–¹å¼ | æ¨™æº–çš„ãªGEMM | ã‚«ã‚¹ã‚¿ãƒ CUDAã‚«ãƒ¼ãƒãƒ«ï¼ˆMBGMM/MBGMVï¼‰ |
| ãƒãƒƒãƒå‡¦ç† | ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚° | ç•°ç¨®LoRAã®é«˜åº¦ãªãƒãƒƒãƒå‡¦ç† |
| ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼æ ¼ç´ | GPU/CPUã‚­ãƒ£ãƒƒã‚·ãƒ¥ | ãƒ¡ã‚¤ãƒ³ãƒ¡ãƒ¢ãƒª + å‹•çš„GPUãƒ•ã‚§ãƒƒãƒ |

## 2. ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£

ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã®è¦³ç‚¹ã§ã¯ã€ä¸¡ã‚·ã‚¹ãƒ†ãƒ ã¯æ˜ç¢ºã«ç•°ãªã‚‹è¨­è¨ˆç›®æ¨™ã‚’æŒã£ã¦ã„ã¾ã™ã€‚vLLM Multi-LoRAã¯ã€å®Ÿç”¨çš„ã«ã¯æ•°å€‹ã‹ã‚‰åæ•°å€‹ã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’æ‰±ã†ã“ã¨ã‚’æƒ³å®šã—ã¦ãŠã‚Šã€`max_loras`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ã‚ˆã£ã¦GPUãƒ¡ãƒ¢ãƒªã¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’èª¿æ•´ã—ã¾ã™ã€‚ã“ã‚Œã¯ä¸­è¦æ¨¡ã®ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«é©ã—ãŸè¨­è¨ˆã§ã™ã€‚

ä¸€æ–¹ã€S-LoRAã¯æ•°ç™¾ã‹ã‚‰æ•°åƒã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’åŒæ™‚æä¾›ã™ã‚‹ã“ã¨ã‚’ç›®æ¨™ã«è¨­è¨ˆã•ã‚Œã¦ãŠã‚Šã€å¤§è¦æ¨¡ãªãƒãƒ«ãƒãƒ†ãƒŠãƒ³ãƒˆç’°å¢ƒã‚„ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºãƒ¬ãƒ™ãƒ«ã®LoRAã‚µãƒ¼ãƒ“ãƒ³ã‚°ã«æœ€é©åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã®é•ã„ã¯ã€ã‚·ã‚¹ãƒ†ãƒ ã®è¤‡é›‘ã•ã¨ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç‰¹æ€§ã«ç›´æ¥çš„ãªå½±éŸ¿ã‚’ä¸ãˆã¦ã„ã¾ã™ã€‚

## 3. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡

ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®é¢ã§ã‚‚ã€ä¸¡ã‚·ã‚¹ãƒ†ãƒ ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯å¤§ããç•°ãªã‚Šã¾ã™ã€‚vLLM Multi-LoRAã§ã¯ã€GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ã‚µã‚¤ã‚ºã«åŠ ãˆã¦`max_loras Ã— LoRAã‚µã‚¤ã‚º`ã«æ¯”ä¾‹ã—ã¦å¢—åŠ ã—ã¾ã™ã€‚ã“ã‚Œã¯ç›´æ„Ÿçš„ã§ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ¢ãƒ‡ãƒ«ã§ã™ãŒã€å¤šæ•°ã®LoRAã‚’æ‰±ã†å ´åˆã«ã¯ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãŒèª²é¡Œã¨ãªã‚Šã¾ã™ã€‚

ã“ã‚Œã«å¯¾ã—ã€S-LoRAã¯ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã«åŠ ãˆã¦çµ±ä¸€ãƒãƒƒãƒ•ã‚¡ã®ã¿ã‚’ç¢ºä¿ã—ã€ãã®ä¸­ã§ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªLoRAã¨KVã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’å‹•çš„ã«ç®¡ç†ã—ã¾ã™ã€‚çµ±ä¸€ãƒšãƒ¼ã‚¸ãƒ³ã‚°æ©Ÿæ§‹ã«ã‚ˆã‚Šã€ãƒ¡ãƒ¢ãƒªã®æ–­ç‰‡åŒ–ãŒå¤§å¹…ã«å‰Šæ¸›ã•ã‚Œã€ã‚ˆã‚ŠåŠ¹ç‡çš„ãªãƒ¡ãƒ¢ãƒªä½¿ç”¨ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚ã“ã®è¨­è¨ˆã«ã‚ˆã‚Šã€S-LoRAã¯ç†è«–ä¸Šã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ã‚µã‚¤ã‚ºã«ã‚ãšã‹ãªã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’åŠ ãˆãŸç¨‹åº¦ã®GPUãƒ¡ãƒ¢ãƒªã§ã€æ•°åƒã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’æ‰±ãˆã¾ã™ã€‚

## 4. å®Ÿè£…ã®è¤‡é›‘ã•

å®Ÿè£…ã®è¤‡é›‘ã•ã«ãŠã„ã¦ã‚‚ã€ä¸¡ã‚·ã‚¹ãƒ†ãƒ ã¯å¯¾ç…§çš„ã§ã™ã€‚vLLM Multi-LoRAã¯æ¯”è¼ƒçš„ã‚·ãƒ³ãƒ—ãƒ«ãªå®Ÿè£…ã‚’æ¡ç”¨ã—ã¦ãŠã‚Šã€æ¨™æº–çš„ãªPyTorch/CUDAã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãŸã‚ã€ãƒ‡ãƒãƒƒã‚°ã¨ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ãŒå®¹æ˜“ã§ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€é–‹ç™ºãƒãƒ¼ãƒ ã¯è¿…é€Ÿã«æ©Ÿèƒ½ã‚’ç†è§£ã—ã€å¿…è¦ã«å¿œã˜ã¦ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã§ãã¾ã™ã€‚

ä¸€æ–¹ã€S-LoRAã¯ã‚«ã‚¹ã‚¿ãƒ CUDAã‚«ãƒ¼ãƒãƒ«ã‚’å«ã‚€é«˜åº¦ãªå®Ÿè£…ã‚’å¿…è¦ã¨ã—ã¾ã™ã€‚Tritonã§ã®å°‚ç”¨ã‚«ãƒ¼ãƒãƒ«é–‹ç™ºãŒå¿…è¦ã§ã‚ã‚Šã€æœ€é«˜ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å¼•ãå‡ºã™ã«ã¯GPUã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ·±ã„ç†è§£ãŒæ±‚ã‚ã‚‰ã‚Œã¾ã™ã€‚ã“ã®è¤‡é›‘ã•ã¯ã€æœ€å¤§é™ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã¨è¨€ãˆã¾ã™ã€‚

## 5. ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹

ä¸¡ã‚·ã‚¹ãƒ†ãƒ ã®é¸æŠã¯ã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®å…·ä½“çš„ãªè¦ä»¶ã«ã‚ˆã£ã¦æ±ºã¾ã‚Šã¾ã™ã€‚vLLM Multi-LoRAã¯ã€å°‘æ•°ã®ç‰¹åŒ–ã‚¿ã‚¹ã‚¯ç”¨LoRAã‚’é‹ç”¨ã™ã‚‹å ´åˆã‚„ã€é–‹ç™ºãƒ»å®Ÿé¨“ãƒ•ã‚§ãƒ¼ã‚ºã€ä¸­è¦æ¨¡ã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ãŠã„ã¦ã€ã‚·ãƒ³ãƒ—ãƒ«ã•ã¨ä¿å®ˆæ€§ã‚’é‡è¦–ã™ã‚‹å ´åˆã«é©ã—ã¦ã„ã¾ã™ã€‚

S-LoRAã¯ã€å¤§è¦æ¨¡ãƒãƒ«ãƒãƒ†ãƒŠãƒ³ãƒˆã‚µãƒ¼ãƒ“ã‚¹ï¼ˆå„ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å°‚ç”¨ã®LoRAã‚’æä¾›ã™ã‚‹å ´åˆãªã©ï¼‰ã‚„ã€æ•°ç™¾ã‹ã‚‰æ•°åƒã®ã‚¿ã‚¹ã‚¯ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ã‚’æä¾›ã™ã‚‹å¿…è¦ãŒã‚ã‚‹å ´åˆã€æœ€é«˜ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã¨ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãŒæ±‚ã‚ã‚‰ã‚Œã‚‹å ´åˆã€ãã—ã¦ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºã‚°ãƒ¬ãƒ¼ãƒ‰ã®LLMã‚µãƒ¼ãƒ“ãƒ³ã‚°ã‚’æ§‹ç¯‰ã™ã‚‹å ´åˆã«æœ€é©ãªé¸æŠã¨ãªã‚Šã¾ã™ã€‚

https://aws.amazon.com/jp/blogs/machine-learning/efficient-and-cost-effective-multi-tenant-lora-serving-with-amazon-sagemaker/

AWSã®Amazon SageMakerä¸Šã§LMIã¨ã„ã†ã‚³ãƒ³ãƒ†ãƒŠãŒæä¾›ã•ã‚Œã¦ãŠã‚Šã€ãã®LMIã‹ã‚‰S-LoRAãŒåˆ©ç”¨å¯èƒ½ã§ã‚ã‚‹æ—¨ãŒãƒ–ãƒ­ã‚°ã¨ã—ã¦å…¬é–‹ã•ã‚Œã¦ãŠã‚Šã€LMIã®ä¸­ã§ã¯DJL Servingã¨ã„ã†ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãŒå®Ÿè³ªçš„ã«S-LoRAã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚ãŸã ã—ã€ä»Šå›ã®è¨˜äº‹ã§ã¯ã“ã¡ã‚‰ã®å‹•ä½œç¢ºèªã¯å®Ÿæ–½ã—ã¦ãŠã‚Šã¾ã›ã‚“ã€‚

https://docs.djl.ai/v0.29.0/docs/demos/aws/sagemaker/large-model-inference/sample-llm/multi_lora_adapter_inference_advanced.html

# çŸ¥è¦‹

æœ¬ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€å®Ÿéš›ã«vLLMã®Multi-LoRAæ©Ÿèƒ½ã‚’å‹•ã‹ã—ã¦å¾—ã‚‰ã‚ŒãŸçŸ¥è¦‹ã‚’å…±æœ‰ã—ã¾ã™ã€‚ç¶²ç¾…çš„ã«å®Ÿè£…ã‚„å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã‚“ã ã‚ã‘ã§ã¯ãªã„ãŸã‚å‚è€ƒç¨‹åº¦ã®æƒ…å ±ã¨ã—ã¦æ´»ç”¨ã—ã¦ãã ã•ã„ã€‚

## vLLM v0.14.0ã§ã®APIå¤‰æ›´

vLLM v0.14.0ä»¥é™ã€`generate()`ãƒ¡ã‚½ãƒƒãƒ‰ã®APIãŒå¤‰æ›´ã•ã‚Œã¾ã—ãŸã€‚

::::details æ—§APIï¼ˆv0.13ä»¥å‰ï¼‰

```python
# ã‚¿ãƒ—ãƒ«å½¢å¼ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€LoRAãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’æŒ‡å®š
prompts = [
    ("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ1", sampling_params, lora_request_1),
    ("ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ2", sampling_params, lora_request_2),
]
outputs = llm.generate(prompts)
```
::::

::::details æ–°APIï¼ˆv0.14.0ä»¥é™ï¼‰

```python
# ãƒªã‚¹ãƒˆã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å¼•æ•°ã«åˆ†é›¢
prompts = ["ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ1", "ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ2"]
outputs = llm.generate(
    prompts=prompts,
    sampling_params=sampling_params,
    lora_request=lora_request
)
```
::::

æ—§å½¢å¼ã‚’ä½¿ç”¨ã™ã‚‹ã¨`AttributeError: 'tuple' object has no attribute 'get'`ã¨ã„ã†ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã™ã€‚v0.14.0ä»¥é™ã§ã¯å¿…ãšæ–°APIå½¢å¼ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

## LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

vLLMã¯`LoRARequest`ã®`lora_path`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«HuggingFaceãƒªãƒã‚¸ãƒˆãƒªIDã‚’æŒ‡å®šã™ã‚‹ã¨ã€è‡ªå‹•çš„ã«ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

```python
from vllm.lora.request import LoRARequest

# HuggingFaceã‹ã‚‰è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
lora_request = LoRARequest(
    lora_name="sql_adapter",
    lora_int_id=1,
    lora_path="sid321axn/tiny-llama-text2sql"  # ãƒªãƒã‚¸ãƒˆãƒªID
)
```

**å†…éƒ¨å®Ÿè£…ã®è©³ç´°ï¼š**
vLLMã®`get_adapter_absolute_path()`é–¢æ•°ã¯ã€ãƒ‘ã‚¹ãŒãƒ­ãƒ¼ã‚«ãƒ«ã«å­˜åœ¨ã—ãªã„å ´åˆã€`huggingface_hub.snapshot_download()`ã‚’è‡ªå‹•çš„ã«å‘¼ã³å‡ºã—ã¾ã™ã€‚ãã®ãŸã‚ã€äº‹å‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¯ä¸è¦ã§ã™ã€‚

https://github.com/vllm-project/vllm/blob/a8eb1182f172c616af99c1bfc5ba70b792793114/vllm/lora/utils.py#L210-L225

## ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©ç”¨ï¼ˆåˆå­¦è€…å‘ã‘ï¼‰

ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ï¼ˆTinyLlamaã€Llama-3ãªã©ï¼‰ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€æ­£ã—ã„ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é©ç”¨ã—ãªã„ã¨å‡ºåŠ›ãŒåŠ£åŒ–ã—ã¾ã™ã€‚

### å•é¡Œã®ç—‡çŠ¶

ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é©ç”¨ã›ãšã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é€ä¿¡ã™ã‚‹ã¨ã€ä»¥ä¸‹ã®ã‚ˆã†ãªç¹°ã‚Šè¿”ã—å‡ºåŠ›ãŒç™ºç”Ÿã—ã¾ã™ï¼š

```
The The The The The The The The...
And And And And And And And And...
```

### è§£æ±ºç­–ï¼šTinyLlamaã®ä¾‹

TinyLlamaã¯ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’æœŸå¾…ã—ã¦ã„ã¾ã™ï¼š

::::details TinyLlamaã®ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ

```python
def format_chat_prompt(user_message: str, system_message: str = None) -> str:
    """TinyLlamaã®ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é©ç”¨"""
    if system_message:
        return f"<|system|>\n{system_message}</s>\n<|user|>\n{user_message}</s>\n<|assistant|>\n"
    else:
        return f"<|user|>\n{user_message}</s>\n<|assistant|>\n"

# ä½¿ç”¨ä¾‹
prompt = format_chat_prompt(
    user_message="Create a SQL query to select all users",
    system_message="You are a helpful SQL expert."
)

outputs = llm.generate(
    prompts=[prompt],
    sampling_params=sampling_params,
    lora_request=lora_request
)
```
::::

:::message
**é‡è¦ï¼š** å„ãƒ¢ãƒ‡ãƒ«ã«ã¯ç‹¬è‡ªã®ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãŒã‚ã‚Šã¾ã™ã€‚HuggingFaceã®ãƒ¢ãƒ‡ãƒ«ã‚«ãƒ¼ãƒ‰ã§å¿…ãšç¢ºèªã—ã¦ãã ã•ã„ã€‚
:::

## åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã¨LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼

å®Ÿè£…æ™‚ã«ã¯è»½é‡ã§å…¬é–‹ã•ã‚Œã¦ã„ã‚‹LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãŒè±Šå¯Œãªãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠã™ã‚‹ã®ãŒåŠ¹ç‡çš„ã§ã™ã€‚

### æ¨å¥¨ãƒ¢ãƒ‡ãƒ«

| ãƒ¢ãƒ‡ãƒ« | ã‚µã‚¤ã‚º | ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼æ•° | ç‰¹å¾´ |
|--------|--------|-------------|------|
| TinyLlama/TinyLlama-1.1B-Chat-v1.0 | 1.1B | 36+ | æœ€è»½é‡ã€ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã«æœ€é© |
| microsoft/phi-2 | 2.7B | 948+ | ãƒãƒ©ãƒ³ã‚¹å‹ã€è±Šå¯Œãªã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ |
| Qwen/Qwen2.5-7B-Instruct | 7B | 859+ | é«˜æ€§èƒ½ã€ä¸­å›½èªã«å¼·ã„ |

### TinyLlamaç”¨ã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ä¾‹

::::details å®Ÿç”¨çš„ãªTinyLlama LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼

```python
LORA_ADAPTERS = {
    "text2sql": {
        "path": "sid321axn/tiny-llama-text2sql",
        "base_model": "TinyLlama/TinyLlama-1.1B-Chat-v0.3",  # æ³¨æ„: v0.3ãƒ™ãƒ¼ã‚¹
        "ç”¨é€”": "SQLç”Ÿæˆ",
        "system_message": "You are a SQL expert."
    },
    "math": {
        "path": "philimon/TinyLlama-gsm8k-lora",
        "base_model": "TinyLlama/TinyLlama-1.1B-Chat-v0.3",  # æ³¨æ„: v0.3ãƒ™ãƒ¼ã‚¹
        "ç”¨é€”": "æ•°å­¦å•é¡Œã®è§£ç­”",
        "system_message": "You are a math expert."
    },
    "function": {
        "path": "unclecode/tinyllama-function-call-lora-adapter-250424",
        "base_model": "TinyLlama/TinyLlama-1.1B-Chat-v1.0",  # v1.0ãƒ™ãƒ¼ã‚¹
        "ç”¨é€”": "Function calling",
        "system_message": "You are a helpful assistant with access to functions."
    }
}
```
::::

:::message
**é‡è¦ãªæ³¨æ„**: ã“ã‚Œã‚‰ã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã¯ç•°ãªã‚‹ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¦ã„ã¾ã™ã€‚`text2sql`ã¨`math`ã¯v0.3ãƒ™ãƒ¼ã‚¹ã€`function`ã¯v1.0ãƒ™ãƒ¼ã‚¹ã§ã™ã€‚LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚ã«ä½¿ç”¨ã—ãŸãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨åŒã˜ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒæ¨å¥¨ã•ã‚Œã¾ã™ã€‚ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒç•°ãªã‚‹ã¨ã€å‡ºåŠ›å“è³ªãŒä½ä¸‹ã—ãŸã‚Šã€äºˆæœŸã—ãªã„å‹•ä½œãŒç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚å®Ÿç”¨ç’°å¢ƒã§ã¯ã€åŒä¸€ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸLoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’é¸æŠã—ã¦ãã ã•ã„ã€‚

HuggingFaceã§`base_model:adapter:TinyLlama/TinyLlama-1.1B-Chat-v1.0`ã§æ¤œç´¢ã™ã‚‹ã¨ã€v1.0ãƒ™ãƒ¼ã‚¹ã®åˆ©ç”¨å¯èƒ½ãªã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã™ã€‚
:::

## Function Callingã®å®Ÿè£…

Function callingç”¨ã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã€ç‰¹å®šã®å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆãŒæœŸå¾…ã•ã‚Œã¾ã™ã€‚

### æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

glaive-function-calling-llama3ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å½¢å¼ï¼š

```
<functioncall>
{"name": "get_weather", "arguments": '{"location": "Tokyo"}'}
<|endoftext|>
```

### ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®æ§‹æˆ

::::details Function callingç”¨ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸

```python
import json

def format_functions_for_system(functions: List[Dict]) -> str:
    """é–¢æ•°å®šç¾©ã‚’ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç”¨ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ"""
    functions_json = json.dumps(functions, ensure_ascii=False)
    return f"You are a helpful assistant with access to the following functions. Use them if required -\n{functions_json}"

# é–¢æ•°å®šç¾©ã®ä¾‹
functions = [
    {
        "name": "get_weather",
        "description": "Get the current weather information",
        "parameters": {
            "type": "object",
            "properties": {
                "location": {
                    "type": "string",
                    "description": "City name"
                }
            },
            "required": ["location"]
        }
    }
]

system_message = format_functions_for_system(functions)
```
::::

### å‡ºåŠ›ã®ãƒ‘ãƒ¼ã‚¹

::::details Function callingå‡ºåŠ›ã®ãƒ‘ãƒ¼ã‚¹å®Ÿè£…

```python
import json

def parse_function_call(output: str) -> Dict:
    """ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã‹ã‚‰é–¢æ•°å‘¼ã³å‡ºã—ã‚’ãƒ‘ãƒ¼ã‚¹"""
    if "<functioncall>" not in output:
        return {"success": False, "error": "No <functioncall> tag found"}

    try:
        start = output.index("<functioncall>") + len("<functioncall>")

        # çµ‚äº†ã‚¿ã‚°ã‚’æ¢ã™
        end_tags = ["<|endoftext|>", "<|eot_id|>", "</s>"]
        end = len(output)
        for tag in end_tags:
            if tag in output[start:]:
                end = start + output[start:].index(tag)
                break

        # JSONéƒ¨åˆ†ã‚’æŠ½å‡ºã—ã¦ãƒ‘ãƒ¼ã‚¹
        json_str = output[start:end].strip()
        function_call = json.loads(json_str)

        return {
            "success": True,
            "function_name": function_call.get("name"),
            "arguments": json.loads(function_call.get("arguments", "{}"))
        }
    except Exception as e:
        return {"success": False, "error": str(e)}
```
::::

## ãƒ¡ãƒ¢ãƒªæ¸¬å®šã®æ³¨æ„ç‚¹

vLLMã¯ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ï¼ˆEngineCoreï¼‰ã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã€PyTorchã®`torch.cuda.memory_allocated()`ã§ã¯ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã®ãƒ¡ãƒ¢ãƒªã—ã‹æ¸¬å®šã§ãã¾ã›ã‚“ã€‚

### å•é¡Œ

```python
# âŒ ã“ã‚Œã¯0ã‚’è¿”ã™
memory_mb = torch.cuda.memory_allocated() / 1024 / 1024
```

vLLMã®ãƒ­ã‚°ã«ã¯`Model loading took 2.05 GiB memory`ã¨è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹ã®ã«ã€æ¸¬å®šçµæœã¯0 MBã«ãªã‚Šã¾ã™ã€‚

### è§£æ±ºç­–ï¼šnvidia-smiã®ä½¿ç”¨

::::details nvidia-smiã‚’ä½¿ã£ãŸãƒ¡ãƒ¢ãƒªæ¸¬å®š

```python
import subprocess

def get_gpu_memory_mb() -> float:
    """nvidia-smiã‚’ä½¿ã£ã¦GPUå…¨ä½“ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å–å¾—"""
    try:
        result = subprocess.run(
            ['nvidia-smi', '--query-gpu=memory.used', '--format=csv,noheader,nounits'],
            capture_output=True,
            text=True,
            check=True
        )
        # æœ€åˆã®GPUã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆMBï¼‰
        memory_mb = float(result.stdout.strip().split('\n')[0])
        return memory_mb
    except Exception:
        return 0.0

# æ¸¬å®šä¾‹
memory_before = get_gpu_memory_mb()
llm = LLM(model="TinyLlama/TinyLlama-1.1B-Chat-v1.0")
memory_after = get_gpu_memory_mb()
consumed = memory_after - memory_before
print(f"æ¶ˆè²»ãƒ¡ãƒ¢ãƒª: {consumed:.2f} MB")
```
::::

ã“ã®æ–¹æ³•ã«ã‚ˆã‚Šã€ã‚µãƒ–ãƒ—ãƒ­ã‚»ã‚¹ã‚’å«ã‚€GPUå…¨ä½“ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æ¸¬å®šã§ãã¾ã™ã€‚

## Multi-LoRAã®ãƒ¡ãƒ¢ãƒªå‰Šæ¸›åŠ¹æœã®å®Ÿæ¸¬

Multi-LoRA servingã®æœ€å¤§ã®åˆ©ç‚¹ã¯ã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’å…±æœ‰ã™ã‚‹ã“ã¨ã«ã‚ˆã‚‹ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã§ã™ã€‚å®Ÿéš›ã«æ¸¬å®šã—ãŸçµæœã‚’ç¤ºã—ã¾ã™ã€‚

### æ¸¬å®šç’°å¢ƒ

- **GPU**: NVIDIA A10G (22.5 GB)
- **ãƒ¢ãƒ‡ãƒ«**: TinyLlama/TinyLlama-1.1B-Chat-v1.0
- **LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼**: text2sqlã€mathã€function callingï¼ˆå„rank=64ï¼‰
- **æ¸¬å®šæ–¹æ³•**: nvidia-smiã«ã‚ˆã‚‹GPUå…¨ä½“ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡

### å®Ÿæ¸¬çµæœ

| æ§‹æˆ | ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ | è©³ç´° |
|------|------------|------|
| **ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ã¿** | 19,919 MB | LoRAãªã—ã®çŠ¶æ…‹ |
| **å€‹åˆ¥LoRA Ã— 1** | 20,391 MB | 1ã¤ã®LoRAã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ |
| **å€‹åˆ¥LoRA Ã— 3ï¼ˆåˆè¨ˆï¼‰** | **61,173 MB** | 3ã¤ã®ç‹¬ç«‹ã—ãŸã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ |
| **Multi-LoRA serving** | **20,431 MB** | 3ã¤ã®LoRAã‚’åŒæ™‚ç®¡ç† |

### ãƒ¡ãƒ¢ãƒªå‰Šæ¸›åŠ¹æœ

å®Ÿæ¸¬çµæœã‹ã‚‰ã€Multi-LoRA servingã«ã‚ˆã‚‹é¡•è‘—ãªãƒ¡ãƒ¢ãƒªå‰Šæ¸›åŠ¹æœãŒç¢ºèªã•ã‚Œã¾ã—ãŸã€‚å€‹åˆ¥ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®åˆè¨ˆãŒ61,173 MBã§ã‚ã£ãŸã®ã«å¯¾ã—ã€Multi-LoRA servingã§ã¯20,431 MBã«æŠ‘ãˆã‚‰ã‚Œã€40,742 MBï¼ˆ66.6%ï¼‰ã®å‰Šæ¸›ã‚’é”æˆã—ã¦ã„ã¾ã™ã€‚

```mermaid
graph TB
    subgraph Individual["å€‹åˆ¥ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹æ–¹å¼<br/>61,173 MB"]
        I1[Instance 1<br/>Base: 20 GB<br/>+ LoRA]
        I2[Instance 2<br/>Base: 20 GB<br/>+ LoRA]
        I3[Instance 3<br/>Base: 20 GB<br/>+ LoRA]
    end

    subgraph MultiLoRA["Multi-LoRAæ–¹å¼<br/>20,431 MB"]
        M1[Base Model<br/>20 GB<br/>å…±æœ‰]
        M2[LoRA 1]
        M3[LoRA 2]
        M4[LoRA 3]
        M1 --- M2
        M1 --- M3
        M1 --- M4
    end

    Individual -->|66.6%å‰Šæ¸›| MultiLoRA

    style Individual fill:#ffe1e1
    style MultiLoRA fill:#e1ffe1
    style M1 fill:#e1f5ff
```

ã“ã®çµæœã‹ã‚‰ã€ä»¥ä¸‹ã®é‡è¦ãªçŸ¥è¦‹ãŒå¾—ã‚‰ã‚Œã¾ã—ãŸã€‚å€‹åˆ¥ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§ã¯ã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆç´„20 GBï¼‰ã‚’3å›ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ç´„60 GBã®ãƒ¡ãƒ¢ãƒªã‚’æ¶ˆè²»ã—ã¾ã™ã€‚ã“ã‚Œã«å¯¾ã—ã€Multi-LoRAã®åŠ¹ç‡æ€§ã«ã‚ˆã‚Šã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’1åº¦ã ã‘ãƒ­ãƒ¼ãƒ‰ã—ã€LoRAã‚¦ã‚§ã‚¤ãƒˆã®ã¿ã‚’è¿½åŠ ã™ã‚‹ãŸã‚ç´„20 GBã§æ¸ˆã¿ã¾ã™ã€‚66.6%ã®ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ç‡ã¯ç†è«–å€¤ã¨ã»ã¼ä¸€è‡´ã—ã¦ãŠã‚Šã€Multi-LoRA servingã®å®Ÿç”¨æ€§ã‚’å®Ÿè¨¼ã—ã¦ã„ã¾ã™ã€‚

### ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã¸ã®ç¤ºå”†

ã“ã®å®Ÿæ¸¬çµæœã‹ã‚‰ã€LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®æ•°ãŒå¢—ãˆã‚‹ã»ã©ã€Multi-LoRA servingã®åˆ©ç‚¹ãŒé£›èºçš„ã«å¤§ãããªã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚3å€‹ã®LoRAã§66.6%ã®å‰Šæ¸›ç‡ã‚’é”æˆã—ã¦ã„ã¾ã™ãŒã€10å€‹ã«å¢—ã‚„ã™ã¨87.5-90%ã€50å€‹ã§ã¯97%ã‚‚ã®å‰Šæ¸›ãŒæœŸå¾…ã§ãã¾ã™ã€‚ã“ã‚Œã¯ã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ãŒå˜ä¸€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§å…±æœ‰ã•ã‚Œã‚‹ãŸã‚ã€LoRAæ•°ã«æ¯”ä¾‹ã—ã¦ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãŒå‘ä¸Šã™ã‚‹æ€§è³ªã«ã‚ˆã‚‹ã‚‚ã®ã§ã™ã€‚

| LoRAæ•° | å€‹åˆ¥ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ | Multi-LoRA | å‰Šæ¸›ç‡ |
|--------|----------------|------------|--------|
| 3å€‹ | 60 GB | 20 GB | 66.6% |
| 10å€‹ | 200 GB | 20-25 GB | 87.5-90% |
| 50å€‹ | 1,000 GB | 25-30 GB | 97% |

ãŸã ã—å®Ÿéš›ã«ã¯ã€`max_loras`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚„GPUãƒ¡ãƒ¢ãƒªã®åˆ¶ç´„ã«ã‚ˆã‚Šã€åŒæ™‚ã«ãƒ­ãƒ¼ãƒ‰ã§ãã‚‹LoRAæ•°ã«ã¯ä¸Šé™ãŒã‚ã‚‹ã“ã¨ã«æ³¨æ„ãŒå¿…è¦ã§ã™ã€‚ã“ã®åˆ¶ç´„ã‚’ç†è§£ã—ã€é©åˆ‡ãªè¨­å®šã‚’è¡Œã†ã“ã¨ãŒã€åŠ¹ç‡çš„ãªMulti-LoRA servingã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ã«ã¯ä¸å¯æ¬ ã§ã™ã€‚

### CPUå´ãƒ¡ãƒ¢ãƒªï¼ˆRAMï¼‰ã®åˆ¶ç´„

vLLMã®`max_cpu_loras`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€CPUå´ã®RAMã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼æ•°ã‚’åˆ¶å¾¡ã—ã¾ã™ã€‚ã“ã®è¨­å®šã«ã¯é‡è¦ãªåˆ¶ç´„ãŒã‚ã‚Šã¾ã™ã€‚

#### max_cpu_lorasã®å‹•ä½œ

```python
llm = LLM(
    model="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
    enable_lora=True,
    max_loras=3,           # GPUä¸Šã§åŒæ™‚ã«ä½¿ç”¨å¯èƒ½ãªLoRAæ•°
    max_cpu_loras=10,      # CPU RAMã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚‹LoRAæ•°
)
```

ã“ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹•ä½œã‚’ç†è§£ã™ã‚‹ã«ã¯ã€LoRAã®ãƒ¡ãƒ¢ãƒªéšå±¤ã‚’æŠŠæ¡ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚`max_cpu_loras`ã‚’æŒ‡å®šã—ãªã„å ´åˆã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§`max_loras`ã¨åŒã˜å€¤ã«ãªã‚Šã¾ã™ã€‚ã¾ãŸã€`max_cpu_loras` â‰¥ `max_loras`ã¨ã„ã†åˆ¶ç´„ãŒã‚ã‚Šã¾ã™ã€‚CPU RAMã«LoRAã‚¦ã‚§ã‚¤ãƒˆã‚’ä¿æŒã™ã‚‹ã“ã¨ã§ã€GPU-CPUé–“ã®è»¢é€ã‚’é«˜é€ŸåŒ–ã—ã€é »ç¹ã«åˆ‡ã‚Šæ›¿ã‚ã‚‹LoRAã®ãƒ­ãƒ¼ãƒ‰æ™‚é–“ã‚’å‰Šæ¸›ã§ãã¾ã™ã€‚

```mermaid
graph TB
    subgraph Disk["ãƒ‡ã‚£ã‚¹ã‚¯: HF ã‚­ãƒ£ãƒƒã‚·ãƒ¥"]
        D1[LoRA-1<br/>6MB]
        D2[LoRA-2<br/>6MB]
        D3[...]
        D100[LoRA-100<br/>98MB]
    end

    subgraph CPU["CPU RAM: max_cpu_loraså€‹"]
        C1[LoRA-1]
        C2[LoRA-2]
        C3[LoRA-3]
    end

    subgraph GPU["GPU VRAM: max_loraså€‹"]
        G1[Active LoRA-1]
        G2[Active LoRA-2]
    end

    D1 -.åˆå›ãƒ­ãƒ¼ãƒ‰.-> C1
    D2 -.åˆå›ãƒ­ãƒ¼ãƒ‰.-> C2
    C1 -->|è»¢é€| G1
    C2 -->|è»¢é€| G2
    G1 -.è§£æ”¾.-> C1

    style GPU fill:#e1f5ff
    style CPU fill:#fff9e1
    style Disk fill:#f0f0f0
```

#### ãƒ‡ã‚£ã‚¹ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®è€ƒæ…®

LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã¯HuggingFaceã‹ã‚‰è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã€`~/.cache/huggingface/`ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã¾ã™ã€‚ã“ã®ãƒ‡ã‚£ã‚¹ã‚¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å®¹é‡ç®¡ç†ã‚‚é‡è¦ãªè€ƒæ…®äº‹é …ã§ã™ã€‚ç®¡ç†ã™ã‚‹LoRAæ•°ã«å¿œã˜ã¦å¿…è¦ãªãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ã‚’ç¢ºä¿ã™ã‚‹ã“ã¨ãŒæœ›ã¾ã—ã„ã§ã—ã‚‡ã†ã€‚

```bash
# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºã®ç¢ºèª
du -sh ~/.cache/huggingface/

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ã‚¯ãƒªã‚¢ï¼ˆå¿…è¦ãªå ´åˆï¼‰
rm -rf ~/.cache/huggingface/hub/models--*
```

#### ãƒ¡ãƒ¢ãƒªä¸è¶³æ™‚ã®æŒ™å‹•

`max_cpu_loras`ã‚’è¶…ãˆã¦LoRAã‚’ãƒ­ãƒ¼ãƒ‰ã—ã‚ˆã†ã¨ã™ã‚‹ã¨ã€å¤ã„LoRAãŒã‚¢ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã€å¿…è¦ã«å¿œã˜ã¦å†ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã™ã€‚ã“ã®å‹•ä½œã«ã¯åˆ©ç‚¹ã¨æ¬ ç‚¹ãŒã‚ã‚Šã¾ã™ã€‚åˆ©ç‚¹ã¨ã—ã¦ã€ãƒ¡ãƒ¢ãƒªåˆ¶ç´„å†…ã§å¤šæ•°ã®LoRAã‚’ã‚µãƒãƒ¼ãƒˆã§ãã‚‹ç‚¹ãŒæŒ™ã’ã‚‰ã‚Œã¾ã™ãŒã€æ¬ ç‚¹ã¨ã—ã¦é »ç¹ãªå†ãƒ­ãƒ¼ãƒ‰ã«ã‚ˆã‚Šãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒå¢—åŠ ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

## LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’å®Ÿéš›ã«å‹•ã‹ã™

Multi-LoRA servingã‚’å®Ÿè£…ã™ã‚‹éš›ã€å„LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®å“è³ªã¨ç‰¹åŒ–åº¦ã‚’ç¢ºèªã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¤‡æ•°ã®LoRAã§å®Ÿè¡Œã—ã¦è©•ä¾¡æŒ‡æ¨™ã‚’ç”¨ã„ã¦è©•ä¾¡ã™ã‚‹ã®ãŒè‰¯ã„ã§ã—ã‚‡ã†ã€‚æœ¬è¨˜äº‹ã§ã¯è©•ä¾¡ã¾ã§ã¯è¸ã¿è¾¼ã¿ã¾ã›ã‚“ã€‚

### ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ

```
John runs 3 miles per day for 5 days. How many miles does he run in total?
```

### å„LoRAã®å‡ºåŠ›æ¯”è¼ƒ

::::details ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆLoRAãªã—ï¼‰

```
John runs 3 miles per day for 5 days, which is 15 miles in total.
```

**çµæœ**: âœ… æ­£ç¢ºãªå›ç­”ï¼ˆ22ãƒˆãƒ¼ã‚¯ãƒ³ã€ç°¡æ½”ï¼‰
::::

::::details SQLç”ŸæˆLoRA (text2sql)

```
John runs 3 miles per day for 5 days. Therefore, his total number of miles is 3 x 5 = 15.

7 miles
5 days = 15
15

The number of miles does not exceed 15.
[ä»¥ä¸‹ã€å†—é•·ãªç¹°ã‚Šè¿”ã—ãŒç¶šã...]
```

**çµæœ**: âš ï¸ æ­£è§£ã‚’å«ã‚€ãŒå†—é•·ã€‚SQLã‚¿ã‚¹ã‚¯å‘ã‘ã«ç‰¹åŒ–ã—ã¦ã„ã‚‹ãŸã‚ã€é€šå¸¸ã®ç®—æ•°å•é¡Œã§ã¯æœ€é©åŒ–ã•ã‚Œã¦ã„ãªã„ã€‚
::::

::::details æ•°å­¦LoRA (math)

```
John runs 5 miles per day for 5 days for 5 total days. How many miles does he run in total for 3 days...
[ç¹°ã‚Šè¿”ã—ã¨èª¤ã£ãŸè¨ˆç®—ãŒç¶šã]
```

**çµæœ**: âŒ å‡ºåŠ›ãŒå´©å£Šã€‚GSM8Kï¼ˆå°å­¦æ ¡ãƒ¬ãƒ™ãƒ«ã®æ•°å­¦ï¼‰ç”¨ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¦ã„ã‚‹ãŒã€TinyLlamaãƒ™ãƒ¼ã‚¹ã§ã¯å“è³ªãŒä¸å®‰å®šã€‚
::::

::::details é–¢æ•°å‘¼ã³å‡ºã—LoRA (function)

```
The number of 33333333333333333333333333...
[æ•°å­—ã®ç¹°ã‚Šè¿”ã—]
```

**çµæœ**: âŒ å®Œå…¨ã«å¤±æ•—ã€‚é–¢æ•°å®šç¾©ãŒãªã„å ´åˆã®æŒ™å‹•ãŒä¸é©åˆ‡ã€‚
::::

# ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰

::::details ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰

```bash
# å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰
uv run compare_lora_adapters.py --prompt-file math_problems.txt
```

```:math_problems.txt
# æ•°å­¦å•é¡Œãƒ†ã‚¹ãƒˆç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
# GSM8K Math LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ç”¨

If I have 5 apples and buy 3 more, how many apples do I have?
A shirt costs $20 and is on sale for 25% off. What is the sale price?
There are 24 students in a class. If 1/3 are girls, how many boys are there?
John runs 3 miles per day for 5 days. How many miles does he run in total?
A pizza is cut into 8 slices. If you eat 3 slices, what fraction is left?
```

```python:compare_lora_adapters.py
#!/usr/bin/env python3
"""
åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¤‡æ•°ã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã§æ¯”è¼ƒã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

Multi-LoRA servingã®çœŸä¾¡ï¼š
- 1ã¤ã®ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§è¤‡æ•°ã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ä½¿ç”¨
- åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã™ã‚‹å„ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®å‡ºåŠ›ã‚’ä¸¦ã¹ã¦æ¯”è¼ƒ

ä½¿ç”¨ä¾‹:
  python compare_lora_adapters.py --prompt-file test_prompts/sql_generation.txt
  python compare_lora_adapters.py --prompt "What is 2+2?" --output results.txt
"""

import argparse
from pathlib import Path
from typing import List, Dict
from vllm import LLM, SamplingParams
from vllm.lora.request import LoRARequest


def load_prompts(file_path: Path) -> List[str]:
    """txtãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’èª­ã¿è¾¼ã‚€"""
    prompts = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line and not line.startswith('#'):
                prompts.append(line)
    return prompts


def format_chat_prompt(user_message: str, system_message: str) -> str:
    """TinyLlamaã®ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é©ç”¨"""
    return f"<|system|>\n{system_message}</s>\n<|user|>\n{user_message}</s>\n<|assistant|>\n"


# LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®å®šç¾©
LORA_CONFIGS = {
    "base": {
        "name": "ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«",
        "request": None,
        "system_message": "You are a helpful assistant.",
    },
    "text2sql": {
        "name": "SQLç”ŸæˆLoRA",
        "request": LoRARequest(
            lora_name="text2sql",
            lora_int_id=1,
            lora_path="sid321axn/tiny-llama-text2sql",
        ),
        "system_message": "You are a SQL expert.",
    },
    "math": {
        "name": "æ•°å­¦LoRA",
        "request": LoRARequest(
            lora_name="math",
            lora_int_id=2,
            lora_path="philimon/TinyLlama-gsm8k-lora",
        ),
        "system_message": "You are a math tutor.",
    },
    "function": {
        "name": "é–¢æ•°å‘¼ã³å‡ºã—LoRA",
        "request": LoRARequest(
            lora_name="function",
            lora_int_id=3,
            lora_path="unclecode/tinyllama-function-call-lora-adapter-250424",
        ),
        "system_message": "You are a function calling assistant.",
    },
}


def compare_lora_outputs(
    llm: LLM,
    prompt: str,
    lora_keys: List[str],
    sampling_params: SamplingParams,
) -> Dict[str, Dict]:
    """
    åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¤‡æ•°ã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã§å®Ÿè¡Œã—ã¦çµæœã‚’æ¯”è¼ƒ

    Returns:
        {
            "lora_key": {
                "name": "...",
                "output": "...",
                "tokens": int
            }
        }
    """
    results = {}

    for lora_key in lora_keys:
        config = LORA_CONFIGS[lora_key]

        # ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’é©ç”¨
        formatted_prompt = format_chat_prompt(prompt, config["system_message"])

        # æ¨è«–å®Ÿè¡Œ
        outputs = llm.generate(
            prompts=[formatted_prompt],
            sampling_params=sampling_params,
            lora_request=config["request"],
        )

        generated_text = outputs[0].outputs[0].text.strip()
        num_tokens = len(outputs[0].outputs[0].token_ids)

        results[lora_key] = {
            "name": config["name"],
            "output": generated_text,
            "tokens": num_tokens,
        }

    return results


def print_comparison(prompt: str, results: Dict[str, Dict]):
    """æ¯”è¼ƒçµæœã‚’è¦‹ã‚„ã™ãè¡¨ç¤º"""
    print("=" * 100)
    print(f"ğŸ“ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {prompt}")
    print("=" * 100)

    for lora_key, result in results.items():
        print(f"\nğŸ¯ {result['name']} ({lora_key})")
        print("-" * 100)
        print(result['output'])
        print(f"\nğŸ“Š ãƒˆãƒ¼ã‚¯ãƒ³æ•°: {result['tokens']}")
        print("-" * 100)

    print("\n" + "=" * 100 + "\n")


def main():
    parser = argparse.ArgumentParser(
        description="åŒã˜ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¤‡æ•°ã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã§æ¯”è¼ƒ"
    )

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæŒ‡å®š
    prompt_group = parser.add_mutually_exclusive_group(required=True)
    prompt_group.add_argument(
        "--prompt",
        type=str,
        help="å˜ä¸€ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ–‡å­—åˆ—",
    )
    prompt_group.add_argument(
        "--prompt-file",
        type=Path,
        help="ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¨˜è¼‰ã•ã‚ŒãŸtxtãƒ•ã‚¡ã‚¤ãƒ«",
    )

    # LoRAé¸æŠ
    parser.add_argument(
        "--loras",
        type=str,
        nargs="+",
        choices=list(LORA_CONFIGS.keys()),
        default=["base", "text2sql", "math", "function"],
        help="æ¯”è¼ƒã™ã‚‹LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ï¼ˆè¤‡æ•°æŒ‡å®šå¯ï¼‰",
    )

    # ãƒ¢ãƒ‡ãƒ«è¨­å®š
    parser.add_argument(
        "--model",
        type=str,
        default="TinyLlama/TinyLlama-1.1B-Chat-v1.0",
        help="ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«",
    )
    parser.add_argument(
        "--max-tokens",
        type=int,
        default=150,
        help="æœ€å¤§ç”Ÿæˆãƒˆãƒ¼ã‚¯ãƒ³æ•°",
    )
    parser.add_argument(
        "--temperature",
        type=float,
        default=0.7,
        help="ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¸©åº¦",
    )

    # å‡ºåŠ›è¨­å®š
    parser.add_argument(
        "--output",
        type=Path,
        help="çµæœã‚’ä¿å­˜ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰",
    )

    args = parser.parse_args()

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—
    if args.prompt:
        prompts = [args.prompt]
        print(f"ğŸ“„ å˜ä¸€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ¢ãƒ¼ãƒ‰")
    else:
        prompts = load_prompts(args.prompt_file)
        print(f"ğŸ“„ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«: {args.prompt_file}")
        print(f"âœ… {len(prompts)}å€‹ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")

    # LLMåˆæœŸåŒ–ï¼ˆMulti-LoRAæœ‰åŠ¹ï¼‰
    print(f"\nğŸš€ ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–: {args.model}")
    print(f"ğŸ¯ ä½¿ç”¨ã™ã‚‹LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼: {', '.join(args.loras)}")

    llm = LLM(
        model=args.model,
        enable_lora=True,
        max_loras=len([k for k in args.loras if k != "base"]),
        max_lora_rank=64,
        gpu_memory_utilization=0.85,
    )

    # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    sampling_params = SamplingParams(
        temperature=args.temperature,
        top_p=0.95,
        max_tokens=args.max_tokens,
    )

    print(f"\nâš™ï¸  Temperature: {args.temperature}, Max tokens: {args.max_tokens}")
    print("\n" + "=" * 100)
    print("ğŸ”„ Multi-LoRAæ¯”è¼ƒé–‹å§‹")
    print("=" * 100 + "\n")

    # å„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§æ¯”è¼ƒ
    all_results = []

    for i, prompt in enumerate(prompts, 1):
        print(f"\nğŸ“Œ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ {i}/{len(prompts)}")

        # å„LoRAã§æ¨è«–
        results = compare_lora_outputs(
            llm=llm,
            prompt=prompt,
            lora_keys=args.loras,
            sampling_params=sampling_params,
        )

        # çµæœã‚’è¡¨ç¤º
        print_comparison(prompt, results)

        # ä¿å­˜ç”¨ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
        comparison = {
            "prompt": prompt,
            "results": results,
        }
        all_results.append(comparison)

    # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    if args.output:
        args.output.parent.mkdir(parents=True, exist_ok=True)
        with open(args.output, 'w', encoding='utf-8') as f:
            f.write(f"Multi-LoRAæ¯”è¼ƒçµæœ\n")
            f.write(f"ãƒ¢ãƒ‡ãƒ«: {args.model}\n")
            f.write(f"LoRA: {', '.join(args.loras)}\n")
            f.write(f"Temperature: {args.temperature}\n")
            f.write(f"Max tokens: {args.max_tokens}\n")
            f.write("=" * 100 + "\n\n")

            for comparison in all_results:
                f.write(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: {comparison['prompt']}\n")
                f.write("=" * 100 + "\n\n")

                for lora_key, result in comparison['results'].items():
                    f.write(f"{result['name']} ({lora_key})\n")
                    f.write("-" * 100 + "\n")
                    f.write(result['output'] + "\n")
                    f.write(f"\nãƒˆãƒ¼ã‚¯ãƒ³æ•°: {result['tokens']}\n")
                    f.write("-" * 100 + "\n\n")

                f.write("\n\n")

        print(f"ğŸ’¾ çµæœã‚’ä¿å­˜: {args.output}")

    print("\nâœ… Multi-LoRAæ¯”è¼ƒå®Œäº†ï¼")
    print("\nğŸ’¡ é‡è¦ãªè¦³å¯Ÿãƒã‚¤ãƒ³ãƒˆ:")
    print("  - SQLãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«Math LoRAã‚’ä½¿ã†ã¨ã©ã†ãªã‚‹ã‹ï¼Ÿ")
    print("  - å„LoRAãŒå°‚é–€å¤–ã®ã‚¿ã‚¹ã‚¯ã§ã©ã†æŒ¯ã‚‹èˆã†ã‹ï¼Ÿ")
    print("  - ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¦LoRAã®åŠ¹æœã¯æ˜ç¢ºã‹ï¼Ÿ")


if __name__ == "__main__":
    main()
```
::::

# ã¾ã¨ã‚

vLLM Multi-LoRAã¨S-LoRAã¯ã€ãã‚Œãã‚Œç•°ãªã‚‹è¨­è¨ˆå“²å­¦ã¨ç›®æ¨™ã‚’æŒã¤Multi-LoRAæ¨è«–ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚vLLM Multi-LoRAã®å¼·ã¿ã¯ã€ã™ãã«ä½¿ãˆã‚‹å®Ÿç”¨çš„ãªå®Ÿè£…ã¨ã‚·ãƒ³ãƒ—ãƒ«ãªAPIã€ãã—ã¦vLLMã®æ—¢å­˜ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã¨ã®ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ãªçµ±åˆã«ã‚ã‚Šã¾ã™ã€‚é–‹ç™ºè€…ã¯æœ€å°é™ã®å­¦ç¿’ã‚³ã‚¹ãƒˆã§ã€è¤‡æ•°ã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’åŠ¹ç‡çš„ã«ç®¡ç†ã§ãã¾ã™ã€‚

ä¸€æ–¹ã€S-LoRAã¯æœ€é«˜ãƒ¬ãƒ™ãƒ«ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã¨é©æ–°çš„ãªãƒ¡ãƒ¢ãƒªç®¡ç†ã€ãã—ã¦å¤§è¦æ¨¡ç’°å¢ƒã§ã®å„ªã‚ŒãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚çµ±ä¸€ãƒ¡ãƒ¢ãƒªãƒ—ãƒ¼ãƒ«ã¨ã‚«ã‚¹ã‚¿ãƒ CUDAã‚«ãƒ¼ãƒãƒ«ã«ã‚ˆã‚Šã€å¾“æ¥ã§ã¯ä¸å¯èƒ½ã ã£ãŸæ•°åƒã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®åŒæ™‚æä¾›ã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚

ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®è¦ä»¶ã«å¿œã˜ã¦é©åˆ‡ãªã‚·ã‚¹ãƒ†ãƒ ã‚’é¸æŠã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚LLMã®åˆ©ç”¨ã‚³ã‚¹ãƒˆåŠ¹ç‡ãŒãƒ“ã‚¸ãƒã‚¹ã«ä¸ãˆã‚‹å½±éŸ¿ãŒç”šå¤§ã§ã‚ã‚Œã°S-LoRAã‚’vLLMã«ã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã™ã‚‹ã®ã‚‚è‰¯ã„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã­ã€‚

# å‚è€ƒæ–‡çŒ®

- [vLLM Multi-LoRA Inference Documentation](https://docs.vllm.ai/en/v0.4.1/getting_started/examples/multilora_inference.html)
- [vLLM GitHub Repository](https://github.com/vllm-project/vllm)
- [S-LoRA: Serving Thousands of Concurrent LoRA Adapters (arXiv:2311.03285)](https://arxiv.org/abs/2311.03285)
- [TinyLlama SQL LoRA Adapter](https://huggingface.co/sid321axn/tiny-llama-text2sql)
- [TinyLlama Math LoRA Adapter](https://huggingface.co/philimon/TinyLlama-gsm8k-lora)
- [TinyLlama Function Calling LoRA Adapter](https://huggingface.co/unclecode/tinyllama-function-call-lora-adapter-250424)
- [Glaive Function Calling Dataset](https://huggingface.co/datasets/unclecode/glaive-function-calling-llama3)
