---
title: "[翻訳] XQuant: KVキャッシュの再計算によりLLM推論のメモリウォールを打破する"
emoji: "📘"
type: "idea" # tech: 技術記事 / idea: アイデア
topics: ["KVCache", "LLM"]
published: true
---

https://arxiv.org/abs/2508.10395

## XQuantの理解(このセクションは翻訳ではありません)

ルーフラインモデルから考えるとDecodingはMemoryバウンドなので、余裕があるコンピュートにKとVの再計算をさせることでメモリーを節約しよう、という発想である。元々、KとVの毎回の全計算をするエフォートが大きいからKVキャッシュを持っていた。

XQuantの基本アイデア: KVを直接持つのではなく層の入力活性化Xを持っておいてKVをDecodingの際に作ることで、KVを持つよりXを持つ方が2倍メモリー削減できる。さらに、Xは低ビット量子化にも適している。一般的に再計算ロジックよりメモリーアクセスの方が時間かかるが、XQuantでメモリー操作も減るので高速化も期待できる。

GQA対応: GQAの場合だとXを単純にキャッシュするとむしろメモリを多く使うことになるので、重み行列W_k/W_vにSVD分解をオフラインで一度だけ適用して、Xを潜在空間にダウンプロジェクトする（XU_k/XU_v）ことでKVキャッシュと同じくらいのサイズになる。この潜在空間の分布は外れ値が最初のチャネルに集中するという特性を持ち、低ビット量子化に適している。SVD分解を既存手法のKVに適用する先行研究も存在するが、XQuantは同じメモリサイズでより高精度な量子化を実現している。

XQuant-CLの価値：連続する層間でXの分布が類似しているという観察に基づき、各層のXそのものではなく層間の差分ΔXを圧縮する。差分は元のXより範囲が小さいため、極めて低ビット（2〜3ビット）量子化でも高精度を保てる。これにより、単純な均一量子化のみでメモリー圧縮率を10〜12.5倍にできるのにPerplexityがほとんど劣化しない（0.01〜0.1程度）という結果を達成している。この層間圧縮はSVDとは独立した技術である。

こんな感じの理解をした。

## 著者

Aditya Tomar¹, Coleman Hooper¹†, Minjae Lee², Haocheng Xi¹,  
Rishabh Tiwari¹, Wonjun Kang², Luca Manolache¹,  
Michael W. Mahoney¹,³,⁴, Kurt Keutzer¹, Amir Gholami¹,³  

¹ UC Berkeley  ² FuriosaAI  ³ ICSI  ⁴ LBNL  
† 共同筆頭著者

## 概要

LLM推論は多くの下流アプリケーションにとって重要なワークロードとなっていますが、LLMの効率的な推論は、膨大なメモリフットプリントと帯域幅要件のため困難です。並行して、計算能力は過去数十年にわたってメモリ容量と帯域幅の両方を着実に上回っており、この傾向は現代のGPUハードウェアでも明らかであり、LLM推論の課題を悪化させています。そのため、メモリ操作を削減するために計算量を増やす新しいアルゴリズムが登場しています。この目的のため、私たちはXQuantを提示します。これはこの傾向を利用し、低ビット量子化を通じてメモリ消費を桁違いに削減し、最先端のKVキャッシュ量子化手法と比較して大幅な精度向上を実現します。私たちは、標準的なKVキャッシングを使用する代わりに、層の入力活性化Xを量子化してキャッシュし、推論中にKeysとValuesをオンザフライで再計算することでこれを実現します。これにより、KVキャッシングと比較して即座に2倍のメモリ節約が得られます。XQuantを適用することで、FP16ベースラインと比較して<0.1のパープレキシティ劣化で最大〜7.7倍のメモリ節約を達成します。さらに、私たちのアプローチは、X値が層間で類似しているという事実を活用します。この観察に基づいて、私たちはXQuant-CLを導入します。これは、X埋め込みの層間類似性を極端な圧縮に利用します。異なるモデル全体で、XQuant-CLはFP16ベースラインと比較して、わずか0.01のパープレキシティ劣化で最大10倍のメモリ節約を達成し、わずか0.1のパープレキシティ劣化で12.5倍のメモリ節約を達成します。注目すべきことに、標準的な均一量子化を使用しているにもかかわらず、XQuant-CLは、外れ値を考慮した戦略を用いた非均一量子化を採用する複雑なKVキャッシュ量子化手法を上回ることができます。将来世代のハードウェアプラットフォームにおける計算とメモリのスケーリングの前述の傾向を考えると、XQuantはLLM推論を加速するための前向きな視点を採用しています：XQuantは、急速に増加する計算能力を活用してメモリボトルネックを排除しながら、最先端のKVキャッシュ量子化手法を上回り、幅広いモデルにわたってFP16に近い精度を達成することを目指しています。

## 1 はじめに

![図1を参照](https://arxiv.org/html/2508.10395v1/x1.png)

**図1:** WikiText-2でLlama-2-7Bを使用して評価した、最先端のKVキャッシュ量子化手法と私たちのXQuantの{4,3,2}ビット幅にわたるパープレキシティ劣化（低いほど良い）対メモリ圧縮率（高いほど良い）。プロットの右上の端は、最も多くのメモリ圧縮と最も少ないパープレキシティ劣化を達成する最適な構成を表します。メモリ圧縮率とパープレキシティ劣化はFP16ベースラインに対するものです。表4に示すように、XQuant-CLは3ビット量子化で10倍のメモリ節約を得ながらわずか0.01のパープレキシティ劣化を達成し、2ビット量子化で12.5倍のメモリ圧縮を得ながら0.1のパープレキシティ劣化を達成します。

![図2を参照](https://arxiv.org/html/2508.10395v1/x2.png)

**図2:** XQuantが、KVキャッシュの代わりに入力埋め込み(X)をキャッシュすることでメモリフットプリントを削減する方法の視覚化。アテンションを計算するために、キャッシュされた入力を使用してKeysとValuesを再計算します。これにより、アテンションを計算する際に必要な計算量が増加します。しかし、LLM推論は通常メモリ帯域幅バウンドであるため、追加の計算操作を犠牲にしても、メモリ操作を削減することで推論を加速できます。

大規模言語モデル(LLM)は、幅広い自然言語処理(NLP)アプリケーションにわたって標準的なパラダイムとして広く採用されています。LLMはこれらのタスクで顕著なパフォーマンスを達成していますが、大きなパラメータ数と生成実行時に必要なメモリ操作の数により、推論コストが大きくなります。先行研究では、LLM推論が計算バウンドではなくメモリ帯域幅バウンドになる傾向があることが実証されており、したがって、LLMのメモリフットプリントを削減することは、下流アプリケーションを可能にするために重要です。短いコンテキスト長と小さなバッチサイズの場合、モデルの重みが通常メモリボトルネックです。しかし、長いコンテキスト長と大きなバッチサイズの場合、LLM推論の主なメモリボトルネックは、自己アテンション機構で使用される全シーケンスの埋め込み表現であり、シーケンス長に対して線形に増加するKey-Value (KV)キャッシュです。推論中、新しいトークンを生成するたびに、KVキャッシュ全体を繰り返しロードおよび保存する必要があり、これは法外にコストがかかり、大幅な速度低下につながります。これにより、推論プロセスを高速化するためにKVキャッシュのメモリ操作を削減する取り組みが動機付けられます。KVキャッシュを圧縮する有望な解決策の1つは、KVキャッシュ量子化です。KVキャッシュを量子化すると、KeysとValuesを表現するために使用するビット数を減らすことで、デコード中に必要なメモリフットプリントとメモリ操作の数が削減されます。しかし、既存の手法はKVキャッシュを低精度（例：4ビット量子化）に量子化しても精度を保持していますが、KV活性化のビット幅をさらに削減すると、モデルのパフォーマンスが低下することがよくあります。

この研究では、メモリ消費を削減するために、KVキャッシュではなく各層の入力活性化Xを量子化するXQuantという手法を提示します。私たちの手法は図2で視覚化されています。Xを量子化すると、別々のKeysとValuesを保存するのではなく、層ごとに1つのテンソルのみを保存すればよいため、KVキャッシュの量子化と比較して2倍のメモリ節約が得られます。興味深いことに、Xは極めて低ビット量子化にKVキャッシュよりも適していることもわかりました。さらに、デコード中にXからKVキャッシュを再計算することは追加の計算を必要としますが、LLM推論が徐々にメモリ帯域幅バウンドになりつつあるため、このコストを負担できます。この現象は、計算能力の向上率がメモリ帯域幅と容量の増加を上回り続けるため、将来のハードウェアプラットフォームでますます普及するでしょう。この研究では、以下の貢献を行います（これらは図1に要約されています）：

* LLM推論のメモリ消費を削減するために、XQuantは入力X活性化を量子化し、KVキャッシュを直接量子化するのに比べて2倍のメモリ節約を提供し、推論中にKV活性化をオンザフライで再計算します（セクション3.1を参照）。同じメモリフットプリント（FP16と比較して〜7.7倍の節約）で、XQuantはKVキャッシュ量子化と比較して最大〜0.9少ないパープレキシティ劣化を達成し、FP16ベースラインと比較して<0.1のパープレキシティ劣化を達成することを示します（セクション4.1を参照）。

* FP16ベースラインに匹敵する精度で超低精度量子化を実現するために、X埋め込みの層間類似性を利用するXQuant-CLという手法を提示します。私たちのアプローチは、Transformerアーキテクチャの残差ストリームの結果として範囲がはるかに小さい、連続する層間のXの差分を圧縮します（セクション3.2を参照）。標準的な非対称均一量子化を使用して、3ビット量子化でFP16ベースラインと比較して10倍のメモリ節約を達成しながらわずか0.01のパープレキシティ劣化を観察し、2ビット量子化とFP16に対して12.5倍のメモリ節約でわずか0.1のパープレキシティ劣化を観察します（セクション4.3を参照）。注目すべきことに、XQuant-CLは、非均一量子化と外れ値を考慮した密かつスパースな戦略などの複雑な技術を使用する最先端のKVキャッシュ量子化手法であるKVQuantを上回ります。Wikitext-2およびC4データセットで、KVQuantと比較してパープレキシティ劣化を〜0.4削減しながら、1.9倍少ないメモリを使用します（セクション4.3を参照）。

* Grouped Query Attention (GQA)を使用する新しいモデルをサポートするためにXQuantとXQuant-CLを拡張します。GQAモデルの場合、特異値分解(SVD)を使用してKeyとValue投影重み行列をオフラインで分解し、入力活性化をより小さい潜在空間にダウンプロジェクトして、メモリ消費を削減します。興味深いことに、この潜在空間のX分布は極めて低精度の量子化に適していることを観察します（セクション3.3を参照）。2ビット量子化の場合、XQuantはKVキャッシュ量子化と比較して2.2未満のパープレキシティ劣化を達成します（セクション4.1を参照）。XQuant-CLはさらにパフォーマンスを向上させ、6.7倍のメモリを節約しながらFP16ベースラインと比較してわずか〜0.1のパープレキシティ劣化をもたらします（セクション4.3を参照）。

* 最後に、再計算による計算オーバーヘッドとメモリ節約のシステムレベル分析を提供します。XQuantが精度を維持しながらメモリ消費を大幅に削減できることを示します。これは、計算がメモリ容量と帯域幅を支配し続けるため、追加の計算を犠牲にしても最終的に高速化をもたらします（セクション3.4を参照）。

## 2 関連研究

### 2.1 LLM推論のメモリウォール

ターゲットハードウェアプラットフォームでのカーネルのパフォーマンスを評価する場合、実行時間は次の2つのいずれかによって決定されます：1) 実行する必要がある計算操作の量、または 2) 実行する必要があるメモリ操作の量。これら2つの要因のどちらがボトルネックであるかは、ターゲットハードウェアの特性とカーネルの特性に依存します。Arithmetic Intensity（算術強度）は、メモリとの間で転送される1バイトあたりに実行する必要がある計算操作の比として定義され、カーネルを評価するための典型的なメトリックであり、通常、バイトあたりの浮動小数点演算(FLOP)の単位で表されます：

$$
\text{算術強度} = \frac{\text{実行される計算操作}}{\text{転送されたバイト数}} \tag{1}
$$

ターゲットハードウェアプラットフォームは、デバイスのピーク計算パフォーマンスと提供されるメモリ帯域幅の比の観点から特徴付けることができます。算術強度と同様にバイトあたりのFLOPで表されるピーク計算とメモリ帯域幅の比は、ここではそのハードウェアプラットフォームのリッジポイントと呼ばれます（この用語を使用するのは、ターゲットハードウェアプラットフォームの計算バウンドとメモリ帯域幅バウンドのカーネルを区切るRooflineプロット上のポイントと一致するためです）：

$$
\text{リッジポイント} = \frac{\text{ピーク計算スループット}}{\text{ピークメモリ帯域幅}} \tag{2}
$$

提供されたカーネルがメモリ帯域幅バウンドか計算バウンドかを評価する場合、カーネルの算術強度をターゲットハードウェアプラットフォームのリッジポイントと比較するだけで済みます。算術強度が大きい場合、カーネルはターゲットハードウェアで実行すると計算バウンドになります。算術強度が小さい場合、カーネルはメモリ帯域幅バウンドになります。

LLM推論を考慮する場合、主要な課題は、ほとんどのバッチサイズとコンテキスト長の領域で算術強度が低いことです。これは、各トークンを生成するには低強度の行列ベクトル乗算を実行するだけで済むのに対し、これらの行列をロードするにははるかに多くのメモリ操作が必要であるためです。したがって、LLM推論は、メモリからロードされる1バイトあたりに実行される浮動小数点演算が非常に少なくなります。さらに、LLMと、ハードウェアの計算とメモリでスケーリングトレンドを評価すると、既存のデバイスのメモリ能力とLLMの要求との間に拡大する不一致があります。この現象はメモリウォール問題と呼ばれています。この問題には2つの主要なコンポーネントがあります：

1. 現代のLLMのメモリ要件に関するスケーリングトレンドは、ハードウェアのメモリ容量と帯域幅の両方の増加を劇的に上回っています。

2. ピーク計算パフォーマンスのスケーリングは、ハードウェアのメモリ容量と帯域幅の対応する増加よりも桁違いに大きくなっています。

これらの要因を総合すると、LLM推論に必要なメモリ操作の数を削減することが重要です。計算量を増やすことでメモリ要件を削減できる場合、ハードウェアプラットフォームのピーク計算パフォーマンスとメモリ能力との間のギャップが拡大しているため、これは有益です。

**動機** 今日のハードウェアでは、メモリ使用量を削減するために計算を増やすとレイテンシが導入される可能性がありますが、このトレードオフはますます有利になると予想されます。私たちの研究はこの傾向を活用し、追加の計算を利用してメモリボトルネックを削減し、最終的に将来のハードウェア世代でより高速なLLM推論を可能にすることを目指しています。

### 2.2 活性化の再計算

先行研究では、活性化をオンザフライでより小さなチェックポイント状態から再計算する活性化の再計算を、メモリウォール問題の部分的な解決策として探求しています。Checkmateは、中間活性化のサブセットをチェックポイントとして保持し、他の活性化を破棄してからこれらのチェックポイント状態から再計算することで、メモリ制約のあるGPUで大規模なDNNをトレーニングすることを可能にしました。Checkmateは、提供されたメモリ制約を持つターゲットハードウェアプラットフォームの最適な再計算設定を解決します。最近の研究では、一部のKVエントリをCPUにオフロードするサービングシステムのコンテキストで、入力埋め込みXからKVキャッシュを再計算することも探求されています。HCacheはXをCPUメモリにオフロードし、XをCPUからGPUに移動してから再計算を実行してKeysとValuesを回復することで復元を実行します。また、計算とメモリの制約の組み合わせを考慮して、実行する再計算の最適量を決定する再計算も実行します。これらの研究は、実行できる再計算量を決定するためのシステムレベルの最適化に焦点を当てていますが、私たちの研究は、既存のKVキャッシュ圧縮手法と比較して改善された節約を達成するために、KVキャッシュ活性化ではなくXを圧縮することに焦点を当て、次にXの層間類似性を利用してより大きなメモリ節約を達成します。

KVキャッシュ用に別々にKeysとValuesを保存することを避ける試みに関する先行研究もあります。El-AttentionはKeyとValue投影行列をモデル内の他の行列にマージし、それにより入力埋め込みを直接使用してアテンションを計算できるようにしました。しかし、この手法は回転位置埋め込み(RoPE)エンコーディングや、Grouped Query Attentionを使用するモデルと互換性がありません。Slim-AttentionもKeysのみをキャッシュし、逆Key投影行列を乗算してValuesを回復する（そしてこの逆行列をモデル内の他の行列にオフラインでマージする）ことを目指しました。このアプローチでは、推論中にオンザフライでRoPEを適用する必要もあり、Grouped Query Attentionを使用するモデルと互換性がありません。さらに、そのような逆数は数値的に安定することが保証されていません。

### 2.3 KVキャッシュ量子化

KVキャッシュ量子化は、各KVキャッシュエントリの各浮動小数点要素を表現するために使用するビット数を減らすことで、KVキャッシュのメモリ要件を削減する有望な手法として登場しました。以前の研究では、Keysの外れ値チャネルに適応するために、Keyの分布をチャネルごとに、Valueの分布をトークンごとに量子化しています。Keyキャッシュ量子化の重要な側面は、RoPEの処理です。以前の手法では、RoPE前のKey量子化を適用するか、極形式表現を使用してKeyキャッシュを量子化して精度を保持しています。混合精度KVキャッシュ量子化は、特に敏感なトークンを高精度で保持することでモデルの精度を維持するために使用されています。以前の研究では、初期のピボットトークンを無傷のまま保持することも探求されています。これは、初期のトークンをモデルの精度を維持するために不釣り合いに重要な「アテンションシンク」トークンとして識別した以前の研究に基づいています。

### 2.4 KVキャッシュとKV再計算のための低ランク分解

また、元のKVエントリを再計算する前に削減された潜在次元でKVエントリをキャッシュするために、KVキャッシュまたは対応する投影行列に低ランク分解を適用した先行研究もあります。xKVは、連続する層のKVキャッシュエントリの特異ベクトルが整列していることを利用して、層間でKVキャッシュエントリをグループ化し、連結されたKVキャッシュにSVDを適用します。LokiはKeysが低ランクであることを発見し、低次元でアテンションを実行して最も重要なKeysを識別し、その後、完全次元のスパースアテンションのためにそれらのKeysのみをロードします。Lokiはメモリ操作のみを削減しますが、Eigen AttentionはKVキャッシュを実際に圧縮し、低ランク空間でのみアテンションを実行します。KVキャッシュをより低い次元に投影するために重みに低ランク分解を適用した複数の研究もあります。LoRCは重み行列にSVDを適用し、誤差増幅を最小限に抑えるために初期の層でより多くの特異値を保持します。Paluは推論前にオフラインで重みの低ランク分解を実行し、中間KVキャッシュエントリをキャッシュします。ReCalKVは、精度を保持し再計算オーバーヘッドを削減するために、SVDをヘッドのグループに適用する前にヘッドを並べ替えます。低ランク分解を使用してKVキャッシュを圧縮することを目的とするこれらの研究とは対照的に、XQuantは低ランク分解を適用せずに2倍のメモリ節約を得るためにX埋め込みを量子化することを目指しています。XQuant-CLはまた、層間でXの差分を圧縮することでXの分布の層間類似性を利用し、KVキャッシュを直接圧縮するのに比べて同じ精度で大幅なメモリ削減を提供します。

## 3 アルゴリズム：メモリボトルネックを軽減するために計算を犠牲にする

このセクションでは、アルゴリズムXQuant（セクション3.1）とXQuant-CL（セクション3.2）を紹介し、Multi-Head Attention (MHA)モデルのコンテキストでそれらについて議論します。次に、セクション3.3で、Grouped Query Attention (GQA)モデルをサポートするためにアルゴリズムを拡張する方法について議論します。GQAは、Llama、Mistral、Gemma、Qwenなどの多くのモデルファミリーの間でKVキャッシュ削減のための最適化技術として広く採用され普及しているため、特にGQAに対処することに注意してください。最後に、セクション3.4で、手法が行う計算とメモリ操作のトレードオフについて議論します。

### 3.1 XQuant: KVの代わりにXを量子化する

XQuantのコアアイデアは、入力活性化をチェックポイントし、アテンションを計算するために使用する必要があるときにこれらの小さなチェックポイントからKeysとValuesを再生成することにより、KVキャッシングのメモリ要件を削減することです。これは図2に示されており、標準的なKVキャッシングを使用するよりも2倍少ないメモリを必要とする入力埋め込みXを量子化してキャッシュします。Xをキャッシュすることの欠点は、KVキャッシュをオンザフライで再計算する必要があることです。これには、入力埋め込みに投影行列$W_k$と$W_v$を乗算する必要があります。しかし、セクション2.1と3.4で概説されているように、LLMデコードでアテンションの算術強度は通常低いため、メモリ操作の数を削減するために再計算のための追加計算を実行する余裕があります。Xに言及するときは常に、それらに層正規化が適用された後の入力活性化を意味することに注意してください。

### 3.2 XQuant-CL: Xの層間類似性を活用する

![図3を参照](https://arxiv.org/html/2508.10395v1/x3.png)

**図3:** Llama-3.1-8Bモデルの連続する層のpost-norm入力埋め込みX、pre-RoPE Keys、Valuesの比較。分布は、Wikitext-2の2Kシーケンス長のテストサンプルを使用して収集されました。KeysとValuesは連続する層間で明確な違いを示しますが、X埋め込みは顕著な類似性を示します。XQuant-CLで層間圧縮を使用して、この類似性を利用します。

図3に示すように、連続する層間のX埋め込みは（層間のKVキャッシュ埋め込みの類似性と比較して）驚くほど類似しています。この観察された特性は、各層の入力からその層内のアテンションと多層パーセプトロンブロックの出力に流れる残差ストリームに起因します。これらの出力は残差ストリームに追加されます。このように、各層の機能は単に入力を洗練させるものとして理解でき、この洗練は徐々に発生するため、連続する層の入力が実質的に類似していないことは直感的です。これは、層間類似性を利用できれば、さらなる圧縮の有望な機会を表します。

Xの層間類似性を利用するために、私たちはXQuant-CLを提案します。これは、連続する層間のXの差分を圧縮します。プリフィル中の私たちのアルゴリズムは付録Aの図A.1に示されており、デコード用の私たちのアルゴリズムは図4で視覚化されています。層$i$のXを直接量子化する代わりに、範囲がはるかに小さく、したがって量子化がはるかに容易な$\Delta X_i = X_i - X_{i-1}$を量子化します。この方法では、最初の層$X_0$を高精度のままにし、次に$X_0$に対する差分を計算します。しかし、層0から遠い層$i$について$X_i - X_0$を直接計算すると、$X_i$は$X_0$から十分に変化を蓄積して実質的に分岐しているため、このデルタは量子化が容易ではなくなります。これに対処するために、私たちは$\Delta \hat{X}_i = Q(X_i - \hat{X}_{i-1})$を量子化してキャッシュします。ここで、$Q$は量子化関数であり、$\hat{X}_{i-1}$は前の層のXの層間近似です。次に、$X_i$を$\hat{X}_i = X_0 + \sum_{j=1}^i \Delta \hat{X}_j$として近似します。この逐次的なデルタ合計により、各層が入力に適用する洗練を明示的に蓄積でき、極端な圧縮のためにこれらの個々のデルタの簡単な量子化特性を利用できます。しかし、上記の定式化では、層$i$の$\hat{X}$を計算するには、以前のすべての$i-1$デルタをロードする必要があり、これは非常にコストがかかります。したがって、図4で強調表示されているように、すべての前の層のデルタを合計するアキュムレータを維持します。これにより、$\hat{X}_i$の計算にはアキュムレータ（$\hat{X}_{i-1} = X_0 + \sum_{j=1}^{i-1} \Delta \hat{X}_j$）と単一のデルタ$\Delta \hat{X}_i$をロードするだけで済みます。

![図4を参照](https://arxiv.org/html/2508.10395v1/x4.png)

**図4:** デコード中のXQuant-CLアルゴリズムの図解。レイヤー0以外では、すべての他のレイヤーへの入力は、すべての前のレイヤーのデルタとレイヤー0の入力を使用して計算される層間近似です。レイヤー0の入力は各レイヤーのデルタと合計されるため、アキュムレータとして扱うことができ、レイヤーNのXを計算するためにすべてのN-1デルタをロードすることを回避できます。レイヤーの処理が完了した後、最後のトークンのレイヤーへの入力埋め込みがレイヤーの出力活性化（単一のトークンと同じ形状）から減算され、このデルタは量子化されてΔX̂キャッシュに追加されます。プリフィル中のXQuant-CLは付録Aの図A.1で視覚化されており、各レイヤーの完全なΔX̂がどのように計算されキャッシュされるかを示しています。

### 3.3 Grouped Query Attentionモデルのサポート

![図5を参照](https://arxiv.org/html/2508.10395v1/x5.png)

**図5:** GQAベースのモデルにXQuantを適用する方法を概説する図。GQAは、KeysとValuesを計算する際に入力埋め込み(X)をより小さいd/g次元にダウンプロジェクトします。したがって、KVキャッシュではなく入力Xを単純に量子化すると、メモリ消費が増加する可能性があります。これに対処するために、まずW_kおよびW_v行列にオフラインでSVDを適用します。プリフィル中のオンラインでは、XQuantを適用する前にU_kでXをダウンプロジェクトしてX_kを取得し、U_vでX_vを取得し、それによりメモリ消費を削減します。生成の場合、各新しいトークンもこの潜在空間にダウンプロジェクトされ、X_kおよびX_vキャッシュに追加され、量子化されます。連結された[X_k|x⃗_k]に(Σ_k B_k^T)を乗算してKeysを再計算し、連結された[X_k|x⃗_v]に(Σ_v B_v^T)を乗算してValuesを再計算します。グループサイズgは通常4以上であることに注意してください。つまり、単純にXをキャッシュすると、KVキャッシングよりも2倍以上のメモリを使用します。

多くの新しいLLMにXQuantを拡張する際の課題の1つは、これらのモデルが通常、これまで対処してきたモデルで使用される標準的なMulti-Headed Attention (MHA)ではなく、Grouped Query Attention (GQA)を使用していることです。前述したように、GQAはLLMのKVキャッシュのサイズを削減する方法として広く採用されているため、特にGQAモデルに対処します。GQAは、アテンションヘッドのサブセット間でKeysとValuesを共有することでKVキャッシュのメモリ消費を削減します。GQAモデルへの手法の拡張の課題は、KeysとValuesを計算する際に入力活性化をダウンプロジェクトすることです。これは、X埋め込みが形状$l \times d$（$l$はシーケンス長、$d$は隠れ次元）を持つのに対し、KとVはそれぞれ$l \times d/g$であり、$g$はKeysとValuesを共有するクエリヘッドの数であることを意味します。例えば、Llama-3.1-8Bモデルは隠れ次元4,096を持ち、$g=4$を使用します。したがって、Xは次元$l \times 4K$を持ち、KeyとValue活性化は次元$l \times 1K$を持つため、KVキャッシュは形状$l \times 2K$です。KVをキャッシュするには、各トークンに対してサイズ1Kの2つのベクトルを保存する必要がありますが、Xをキャッシュするには、各トークンに対してサイズ4Kの単一のベクトルを保存する必要があります。これは、GQAモデルにXQuantを単純に適用すると、同じ精度で2倍のメモリオーバーヘッドが発生することを意味し、私たちのアプローチの利点を否定します。

#### 3.3.1 GQAをサポートするためのXQuantの拡張

GQAを使用するモデルにXQuantを適用するために、重み行列にオフラインでSingular Value Decomposition (SVD)を適用して、入力活性化を低次元の潜在空間に保存できるようにします。私たちのアルゴリズムは図5で視覚化されています。$W_k$および$W_v$行列にSVDを適用して$U_k \Sigma_k B_k^T$および$U_v \Sigma_v B_v^T$を取得します。プリフィル中、入力埋め込みを$XU_k$および$XU_v$としてダウンプロジェクトします（$U_k$と$U_v$はそれぞれ$d \times d/g$）。これによりXの次元が$g/2$だけ低下し、GQA KVキャッシュと同じメモリフットプリントになります。生成中、各新しいトークンの入力活性化$\vec{x}$は同様に$U_k$と$U_v$によってダウンプロジェクトされて$\vec{x}_k$と$\vec{x}_v$を取得します。これらはキャッシュされた$XU_k$と$XU_v$に追加され、次に$\Sigma_k B_k^T$と$\Sigma_v B_v^T$をそれぞれ乗算してKeysとValuesを取得します。さらに、$\Sigma_k B_k^T$と$\Sigma_v B_v^T$はそれぞれ融合されて（$\Sigma$と$B^T$が乗算されて単一の行列にマージされる）、潜在的な$U_k$と$U_v$がそれぞれ乗算されてKVキャッシュを再計算するための新しい重み行列として機能します。これにより、融合された$\Sigma_k B_k^T$と$\Sigma_v B_v^T$が$d/g \times d/g$のより小さい正方行列であるため、推論中の再計算コストも削減されます。関連するコストについては、セクション3.4で詳しく説明します。重要なのは、SVDと重みの融合がオフラインで行われるため、レイテンシのオーバーヘッドが追加されないことです。

重要なことに、このアプローチは同じビット幅のKV量子化と同じメモリ消費量を持ちますが、ダウンプロジェクトされたX分布は量子化が容易であり、同じビット幅でより高い精度が得られます（セクション4.1を参照）。実際、潜在的なX分布は非常に興味深い構造を明らかにします：潜在的な$XU_k$がすべての外れ値を最初のチャネルにグループ化していることがわかり、これは異なるデータセットの複数の異なるモデルのすべての層で観察されます（図B.2、B.3を参照）。正方行列でない$W_{kv}$の場合、SVDは直交する列を持つ非正方$U_{kv}$を生成し、$U_{kv}^T U_{kv} = I_{2d/g}$ですが、$U_{kv} U_{kv}^T \neq I_d$です。$\Sigma_k$行列の特異値を適用せずに$XU_k$を量子化してキャッシュします。$U_k$は直交列を持つ行列であるため、すべての外れ値が最初のチャネルにある$XU_k$でこの構造を観察することは非常に興味深いです。ただし、$XU_v$分布では同様の興味深い構造は観察されません。これは、KeysにOutlierチャネルがあるのに対し、Valuesには外れ値がある明確に構造化された軸がないことを発見した他の研究の観察と一致しています。$XU_k$の最初のチャネルに存在する外れ値は、潜在的な分布が$B_v^T$によって変換されると、単に他のチャネルに分散され、Keysで見つかる外れ値チャネルを生じさせると考えています。これについては付録Bでさらに議論します。$XU_k$にはチャネルごとの量子化を使用し、$XU_v$にはトークンごとの量子化を使用します。これが最小の精度劣化をもたらす最良の構成であることがわかりました。これは、Keysをチャネルごとに、Valuesをトークンごとに量子化する手法と同様です。

#### 3.3.2 GQAをサポートするためのXQuant-CLの拡張

また、GQAモデルをサポートするために層間方法を拡張します。GQAモデルの場合、XQuant-CLはセクション3.3.1で対処したのと同じ問題に直面します。具体的には、GQAモデルは形状$l \times d$のXを形状$l \times 2d/g$のKVキャッシュにダウンプロジェクトします。$\Delta X$はXと同じ形状を持つため、GQAモデルのこのデルタを単純にキャッシュすると、KVキャッシュを保存するよりも多くのメモリオーバーヘッドが発生します。これに対処するために、$W_k$と$W_v$投影行列を列方向に連結し、$W_{kv} = [W_k|W_v] \in \mathbb{R}^{d \times 2d/g}$を生成します。これにSVDを実行して$U_{kv} \Sigma_{kv} B_{kv}^T$を取得します。ここで、$U_{kv} \in \mathbb{R}^{d \times 2d/g}$は個々の投影行列の共有部分空間であり、$\Sigma_{kv} B_{kv}^T$は破棄されます。$U_{kv}$によって$\Delta X$をダウンプロジェクトし、KVキャッシュと同じメモリフットプリントになり、この潜在的な分布を量子化してキャッシュします。このアプローチは、KV量子化と同じメモリ消費量を持ちますが、潜在的な投影にもかかわらずデルタは量子化が容易であり、同じビット幅でより高い精度が得られます（セクション4.3を参照）。重要なのは、SVDはオフラインで実行されるため、推論中に追加のレイテンシオーバーヘッドはありません。層$i$のXの近似である$\hat{X}_i$を計算する場合、形状$l \times d$のアキュムレータ$\hat{X}_{i-1}$を潜在的な$\Delta X_i U_{kv}$とマージする必要があります。これを行うために、$U_{kv}^T$を使用して$\Delta X_i U_{kv}$を上方投影し、この結果をアキュムレータ$\hat{X}_{i-1}$に追加し、更新されたアキュムレータ$\hat{X}_i$に$W_{kv}$を乗算してKVキャッシュの再計算を完了します。懸念事項の1つは、$U_{kv}^T$によって潜在的なデルタを上方投影することで元のデルタを取得できるかどうかです：非正方行列$W_{kv}$の場合、SVDは直交する列を持つ非正方$U_{kv}$を生成し、$U_{kv}^T U_{kv} = I_{2d/g}$ですが、$U_{kv} U_{kv}^T \neq I_d$です。しかし、量子化関数$Q$が恒等関数である場合、$U_{kv}^T$によって潜在的な$\Delta X_i U_{kv}$を上方投影することは、層$i$のKVキャッシュを計算する際の元の$\Delta X_i$の無損失再構成です：

$$
\begin{align}
[K|V]_i &= (\hat{X}_{i-1} + Q(\Delta X_i U_{kv}) \cdot U_{kv}^T) \cdot U_{kv} \Sigma_{kv} B_{kv}^T \\
&\text{ここで } U_{kv}^T U_{kv} = I_{2d/g}, \quad U_{kv} U_{kv}^T \neq I_d, \quad Q(x) = x \\
&= (\hat{X}_{i-1} U_{kv} + \Delta X_i U_{kv}) \cdot \Sigma_{kv} B_{kv}^T \\
&= (\hat{X}_{i-1} + \Delta X_i) \cdot U_{kv} \Sigma_{kv} B_{kv}^T \\
&= (\hat{X}_{i-1} + \Delta X_i) \cdot W_{kv} \\
&= \hat{X}_i \cdot [W_k|W_v]
\end{align}
$$

### 3.4 再計算のシステムレベル分析

ここでは、XQuantとXQuant-CLによる再計算の計算およびメモリオーバーヘッドを概説するシステムレベルのモデリングを提示します。この分析では、乗算累算演算を2FLOPとしてカウントします。

隠れ次元$d$とシーケンス長$l$のモデルを使用していると仮定します。MHAモデルにXQuantを適用する場合、単一の層の再計算に必要な計算量は$2 \cdot 2 \cdot l \cdot d^2$です。Xに$e$ビット量子化を適用するとします。すると、バイト単位のメモリ操作の総数は$e/8 \cdot l \cdot d$に等しくなります。KVキャッシュ量子化の場合は$2 \cdot e/8 \cdot l \cdot d$になります。GQAモデルにXQuantを適用する場合、単一の層の再計算に必要な計算量は$2 \cdot 2 \cdot l \cdot (d/g)^2$です。ここで、$g$はKeysとValuesを共有するクエリヘッドの数です。これはMHAモデルと比較して$g^2$倍少ない浮動小数点演算です。Xに$e$ビット量子化を適用するとします。すると、バイト単位のメモリ操作の総数は$2 \cdot e/8 \cdot l \cdot d/g$に等しく、これはKVキャッシュ量子化と同じです。しかし、同じ$e$ビット量子化の場合、XQuantはKVキャッシュ量子化よりもはるかに高い精度を達成します（セクション4.1を参照）。同等に、XQuantは、より小さい$e$でKVキャッシュ量子化のより大きい実効$e$と同様の精度を達成します。つまり、同じ精度の場合、XQuantはより少ないメモリ操作を実行します。

ここでは、追加の計算操作がレイテンシボトルネックにならずに実行できる再計算の量を示す例も提供します。ここでは、ターゲットハードウェアとしてNVIDIA H100 GPUを想定します。これはリッジポイント$P = \text{PeakFLOPs}/\text{MemoryBW} = 756\text{TFLOPs}/2\text{TB/s} = 378$を持ちます。また、KVキャッシュの再計算をモデルの重みのロード（Llama-2-7Bモデルの単一の層に対して$2 \cdot 12 \cdot d^2$の追加のメモリ操作に対応）とオーバーラップできると仮定します。計算がボトルネックにならずに再構築できる最大量を解くことができます：

$$
P = \frac{2 \cdot 2 \cdot l \cdot d^2}{\frac{e}{8} \cdot l \cdot d + 2 \cdot 12 \cdot d^2} \tag{3}
$$

$P=378$、$d=4K$、$e=2$でこの方程式を解くと、計算操作がボトルネックにならずに再計算できる最大シーケンス長$2.3K$が得られます。

Llama-3.1-8Bモデル（単一の層の重みに対して$2 \cdot 13 \cdot d^2 + 2 \cdot 2 \cdot (d/g)^2$のメモリ操作が必要で、SVD分解形式で$W_k/W_v$行列をロードするオーバーヘッドを含み、$g=4$を持つ）について同様の分析を実行できます：

$$
P = \frac{2 \cdot 2 \cdot l \cdot \left(\frac{d}{g}\right)^2}{\frac{e}{8} \cdot l \cdot \frac{d}{g} + 2 \cdot 13 \cdot d^2 + 2 \cdot 2 \cdot \left(\frac{d}{g}\right)^2} \tag{4}
$$

$P=378$、$d=4K$、$g=4$、$e=2$でこの方程式を解くと、計算操作がボトルネックにならずに再計算できる最大シーケンス長$40.6K$が得られます。

XQuant-CLでは、各層で$2 \cdot l \cdot d$の追加の計算操作を実行します。これは、キャッシュされたデルタ$\Delta \hat{X}_i$をアキュムレータ$\hat{X}_{i-1}$に追加する必要があるためです。ただし、このコストは無視できます：前述の再計算コストと組み合わせると、$2 \cdot 2 \cdot l \cdot d^2 + 2 \cdot l \cdot d = 2 \cdot 2 \cdot l \cdot d \cdot (d + 1/2)$になります。XQuant-CLでは、各層でアキュムレータとキャッシュされたデルタの両方をロードおよび保存する必要があるため、追加のメモリ操作も必要です。アキュムレータを高精度で保持するため、各層で$e_b/8 \cdot l \cdot d$バイトのメモリ操作を実行します。ここで、$e_b$はアキュムレータの精度です（通常$e_b=4$ビット；セクション4.3を参照）。さらに、キャッシュされたデルタをロードするために必要な$e/8 \cdot l \cdot d$メモリ操作を実行します。これはXQuantと同じです。GQAモデルの場合、各層で量子化されたダウンプロジェクトされた$\Delta X_i U_{kv}$をキャッシュするため、デルタのメモリ操作数は$2 \cdot e/8 \cdot l \cdot d/g$です。これは、GQAモデルに適用されるXQuantと同じです。このデルタをロードした後、$U_{kv}^T$で上方投影してアキュムレータ$\hat{X}_{i-1}$とマージし、次に更新されたアキュムレータにKeyとValue重み投影を適用します。合計すると、これには$2 \cdot 4 \cdot l \cdot d/g \cdot d$の計算操作が必要です。

## 4 実験結果

手法を評価するために、Llama-2-7B/13B、Llama-3.1-8B、およびMistral-7B-v0.3モデルを使用します。WikiText-2およびC4データセットでパープレキシティを測定し、LongBenchおよびGSM8Kデータセットで下流タスク評価を実行します。Llama-2-7B/13BモデルはMHAを使用し、Llama-3.1-8BとMistral-7BはGQAを使用することに注意してください。手法をKIVI（KIVI\*と呼ぶ）およびKVQuantと比較します。これらは2つの代表的な最先端のKVキャッシュ量子化手法です。

KIVIの場合、より強力なベースライン（ここではKIVI\*と呼ぶ）を使用します。元のKIVI手法は、回転位置埋め込み(RoPE)が適用された後にKeysを量子化します。しかし、KeyにRoPEを適用すると構造化されていない分布になるのに対し、Keysはpre-RoPEでより構造化された外れ値チャネルを持つことがわかりました。したがって、RoPEを適用する前にKeysを量子化します。

KVQuantの場合、最良の構成を使用します。これは、ベクトルごとの非均一な密およびスパース量子化を実行します。KVQuantの非均一量子化では、オフラインでキャリブレーションデータセットから層ごとの感度加重非均一データ型を導出して、分布をより適切に表現する必要があります。WikiText-2から2Kのシーケンス長の16個のキャリブレーションサンプルを使用します。密およびスパース量子化は、各ベクトルの外れ値を個別に分離し、高精度で保存します。

XQuantとXQuant-CLは、特別な外れ値処理やキャリブレーションを必要としない単純な均一量子化を使用することに注意してください。KVキャッシュ量子化ベースラインの場合、Keysをチャネルごとに、Valuesをトークンごとに量子化します。すべての量子化実験でグループサイズ128を使用します。生成タスクの場合、デコード中にチャネルごとの量子化を活用できるようにするために、最終的な残差トークン（最大グループサイズのトークン）を量子化せずに残します。これは、KIVI\*の残差方法と同様です。公平な比較のために、すべての生成実験でこの残差方法を採用します。

### 4.1 主な結果

**表1:** WikiText-2およびC4でのパープレキシティを使用したXQuant評価。左：(MHA) Llama-2-7B/13B；右：(GQA) Llama-3.1-8BおよびMistral-7B。KVキャッシュサイズの推定値（FP16ベースラインのKVキャッシュサイズに正規化）を提供し、同様のメモリ消費量で行をグループ化します。MHAモデルの場合、各グループはKIVI\*と比較してXQuantがわずかに少ないメモリを使用することを示します。XQuantは、単一のXテンソルのスケール係数とゼロポイントのみを保存すればよいのに対し、KIVI\*はKとVの両方に対して同じものを保存する必要があるためです。

#### Llama-2-7B/13B (MHA)

| Method | KV (MHA) | Llama-2-7B |  | Llama-2-13B |  |
|--------|----------|------------|------------|-------------|------------|
|        |          | Wiki2 | C4 | Wiki2 | C4 |
| Baseline | 1.00 | 5.47 | 7.26 | 4.88 | 6.73 |
| KIVI*-4bit | 0.27 | 5.49 | 7.30 | 4.90 | 6.75 |
| XQuant-8bit | 0.26 | **5.47** | **7.26** | **4.88** | **6.73** |
| KIVI*-3bit | 0.20 | 5.58 | 7.40 | 4.97 | 6.83 |
| KIVI*-2bit | 0.14 | 6.42 | 8.46 | 5.61 | 7.67 |
| XQuant-4bit | 0.13 | **5.54** | **7.36** | **4.94** | **6.79** |
| XQuant-3bit | 0.10 | 6.65 | 8.65 | 5.22 | 7.07 |

#### Llama-3.1-8B / Mistral-7B (GQA)

| Method | KV (GQA) | Llama-3.1-8B |  | Mistral-7B |  |
|--------|----------|------------|------------|-------------|------------|
|        |          | Wiki2 | C4 | Wiki2 | C4 |
| Baseline | 1.00 | 6.24 | 9.54 | 5.32 | 8.47 |
| KIVI*-4bit | 0.27 | 6.31 | 9.66 | 5.34 | 8.51 |
| XQuant-4bit | 0.27 | **6.28** | **9.60** | **5.33** | **8.49** |
| KIVI*-3bit | 0.20 | 6.59 | 10.13 | 5.43 | 8.62 |
| XQuant-3bit | 0.20 | **6.43** | **9.89** | **5.37** | **8.59** |
| KIVI*-2bit | 0.14 | 9.95 | 15.98 | 6.36 | 10.10 |
| XQuant-2bit | 0.14 | **7.71** | **12.27** | **5.79** | **9.12** |

表1では、WikiText-2およびC4でのLlama-2-7B/13B、Llama-3.1-8B、およびMistral-7Bモデルのパープレキシティ結果を報告します。パープレキシティは、すべての入力トークンの出力ロジットを使用したteacher forcingで測定されています。MHAモデルに適用されるXQuantの場合、Xにトークンごとの量子化を適用します。一方、KIVI\*の場合は、pre-RoPE Keysにチャネルごとの量子化を、Valuesにトークンごとの量子化を適用します。GQAモデルに適用されるXQuantの場合、$XU_k$と$XU_v$をキャッシュし、潜在的な$XU_k$にチャネルごとの量子化を、潜在的な$XU_v$にはトークンごとの量子化を適用するのが最良の構成であることがわかりました。

全体として、XQuantは同じメモリフットプリントでKIVI\*を大幅に上回るだけでなく、FP16ベースラインに非常に近い精度を保ちながら大幅なメモリ節約を実現することがわかります。同じメモリフットプリントで、XQuantはLlama-2-7BおよびLlama-2-13Bモデルに対してKIVI\*と比較してそれぞれ0.88および0.67少ないパープレキシティ劣化を達成します。FP16ベースラインと比較して、XQuantは7.7倍少ないメモリを使用しながら0.1未満のパープレキシティ劣化を達成します。同様に、GQAモデルの場合、XQuantは2ビット量子化の限界を押し広げ、Llama-3.1-8Bで2.2未満のパープレキシティ劣化をKIVI\*と比較して達成します。Mistral-7Bの場合、FP16ベースラインと比較して、XQuantは5倍のメモリ節約で3ビットで<0.1のパープレキシティ劣化を、3.7倍のメモリ節約で4ビットでわずか0.01のパープレキシティ劣化を達成します。2ビット精度では、XQuantはKIVI\*と比較して0.57少ないパープレキシティ劣化を達成し、FP16ベースラインと比較して7.1倍のメモリ節約を獲得します。

### 4.2 下流タスク評価

**表2:** LongBenchでのXQuant評価。(MHA) Llama-2-7B-chatおよび(GQA) Llama-3.1-8B-Instructモデルの結果を報告します。KIVI\*とのベースライン比較を含みます（この構成の詳細はセクション4.1に記載）。各タスクの精度とタスク間の平均精度を報告します。また、KVキャッシュサイズの推定値（FP16ベースラインのKVキャッシュサイズに正規化）も提供します。

#### Llama-2-7B-Chat

| Config | KV Budget | Single-Doc. QA |  |  | Multi-Doc. QA |  |  | Summarization |  |  |
|--------|-----------|----------------|----------------|----------------|---------------|----------------|----------------|---------------|----------------|----------------|
|        |           | NQA | Qspr | MFQA | HPQA | 2Wiki | MSQ | GRep | QMSM | MNews |
| All KV | 1.00 | 19.4 | 22.1 | 36.7 | 27.8 | 31.5 | 8.3 | 26.9 | 21.4 | 25.9 |
| KIVI*-4bit | 0.27 | 18.7 | 21.1 | 37.3 | 28.7 | 31.9 | 8.4 | 27.3 | 21.4 | 25.9 |
| XQuant-4bit | 0.13 | 18.1 | 21.2 | 37.1 | 28.5 | 31.4 | 8.2 | 27.0 | 21.3 | 25.8 |
| KIVI*-3bit | 0.20 | 17.4 | 20.3 | 36.0 | 26.2 | 29.5 | 7.8 | 26.7 | 20.8 | 25.3 |
| XQuant-3bit | 0.10 | 11.9 | 16.2 | 35.3 | 21.4 | 19.0 | 7.3 | 27.1 | 20.5 | 25.2 |

#### Llama-3.1-8B-Instruct

| Config | KV Budget | Single-Doc. QA |  |  | Multi-Doc. QA |  |  | Summarization |  |  |
|--------|-----------|----------------|----------------|----------------|---------------|----------------|----------------|---------------|----------------|----------------|
|        |           | NQA | Qspr | MFQA | HPQA | 2Wiki | MSQ | GRep | QMSM | MNews |
| All KV | 1.00 | 31.2 | 45.5 | 53.8 | 55.0 | 47.1 | 31.4 | 34.8 | 25.3 | 27.5 |
| KIVI*-4bit | 0.27 | 30.0 | 46.0 | 54.4 | 55.7 | 45.5 | 31.0 | 34.6 | 25.3 | 27.2 |
| XQuant-4bit | 0.27 | 30.8 | 46.7 | 55.1 | 54.5 | 47.9 | 30.5 | 34.9 | 25.7 | 27.5 |
| KIVI*-3bit | 0.20 | 29.6 | 45.8 | 55.4 | 54.0 | 43.0 | 31.4 | 34.3 | 25.2 | 27.4 |
| XQuant-3bit | 0.20 | 30.9 | 46.3 | 52.0 | 54.4 | 43.9 | 30.5 | 34.6 | 25.1 | 27.4 |
| KIVI*-2bit | 0.14 | 23.7 | 36.8 | 41.1 | 44.4 | 30.0 | 21.9 | 30.5 | 24.4 | 26.0 |
| XQuant-2bit | 0.14 | 25.3 | 38.3 | 49.6 | 48.6 | 35.5 | 28.4 | 34.0 | 24.8 | 27.3 |

**表3:** GSM8KでのXQuant評価。Llama-2-7B-chatモデルの結果を報告します。KIVI\*とのベースライン比較を含みます（この構成の詳細はセクション4.1に記載）。また、KVキャッシュサイズの推定値（FP16ベースラインのKVキャッシュサイズに正規化）も提供します。

| Config | Accuracy | KV Cache Size (Normalized) |
|--------|----------|----------------------------|
| All KV | 0.132 | 1.00 |
| KIVI*-4bit | 0.132 | 0.27 |
| XQuant-4bit | 0.129 | 0.13 |
| XQuant-3bit | 0.086 | 0.10 |

また、さまざまな下流タスクに対する戦略の適用可能性を示すために、タスク間評価も提供します。表2は、文脈内学習、文書Q/A、要約、コーディングタスクを含む一連の長いコンテキスト長タスクを含むベンチマークスイートであるLongBenchでの評価を提供します。Llama-2-7B-Chat (MHA)およびLlama-3.1-8B (GQA)モデルのXQuant結果を報告し、KIVI\*との基準比較も提供します。Llama-2-7B-Chatの場合、XQuant-4bitはKIVI\*-4bitと同じ精度を達成し、ベースラインと同じ精度を達成しながら、2倍の追加メモリ圧縮を提供します。Llama-3.1-8B-Instructの場合、XQuantはすべての精度設定でビット幅が同じ場合、平均して精度が向上します。

また、複雑な推論のための手法の適用可能性を実証するために、長い生成推論タスクの結果も含めます。表3は、算術推論能力を評価するGSM8Kデータセット（lm-eval-harnessを使用）の評価を提供します。Chain-of-Thought (CoT)構成を使用し、厳密一致精度を報告します。Llama-2-7B-Chatモデルの結果を報告し、KIVI\*との基準比較も提供します。XQuant-4bitはKIVI\*-3bitと同様の精度を達成しながら、1.5倍の追加メモリ節約を提供し、2倍少ないメモリを使用しながらKIVI\*-2bitを上回ることがわかります。

### 4.3 層間圧縮法による結果

**表4:** (MHA) Llama-2-7B/13Bおよび(GQA) Llama-3.1-8BとMistral-7BモデルのWikiText-2およびC4でのパープレキシティ(PPL)を使用したXQuant評価。KVキャッシュサイズの推定値（FP16ベースラインのKVキャッシュサイズに正規化）を提供します。KIVI\*、XQuant、およびXQuant-CLの最初の3層は4ビットで量子化されることに注意してください。これは、これらの手法がKVQuantと同等のメモリフットプリントを持つようにするために行われます。KVQuantは外れ値を高精度で保持するために追加のメモリを使用します。

#### Llama-2-7B / Llama-2-13B (MHA)

| Method | KV | Llama-2-7B |  | Llama-2-13B |  |
|--------|-----|------------|------------|-------------|------------|
|        |     | Wiki2 | C4 | Wiki2 | C4 |
| baseline | 1.00 | 5.47 | 7.26 | 4.88 | 6.73 |
| KIVI*-4bit | 0.27 | 5.49 | 7.30 | 4.90 | 6.75 |
| XQuant-4bit | 0.13 | **5.48** | **7.27** | **4.89** | **6.74** |
| KIVI*-3bit | 0.21 | 5.57 | 7.39 | 4.96 | 6.82 |
| KVQuant-3bit-1% | 0.21 | 5.56 | 7.36 | 4.96 | 6.81 |
| XQuant-3bit | 0.10 | 6.10 | 8.20 | 5.10 | 6.95 |
| XQuant-CL-3bit | 0.10 | **5.48** | **7.28** | **4.92** | **6.78** |
| KIVI*-2bit | 0.15 | 6.20 | 8.22 | 5.46 | 7.49 |
| KVQuant-2bit-1% | 0.15 | 5.99 | 7.83 | 5.34 | 7.23 |
| XQuant-2bit | 0.08 | 11.21 | 15.27 | 7.00 | 10.25 |
| XQuant-CL-2bit | 0.08 | **5.57** | **7.39** | **5.11** | **7.09** |

#### Llama-3.1-8B / Mistral-7B (GQA)

| Method | KV | Llama-3.1-8B |  | Mistral-7B |  |
|--------|-----|------------|------------|-------------|------------|
|        |     | Wiki2 | C4 | Wiki2 | C4 |
| baseline | 1.00 | 6.24 | 9.54 | 5.32 | 8.47 |
| KIVI*-4bit | 0.27 | 6.30 | 9.65 | 5.34 | 8.51 |
| XQuant-4bit | 0.27 | **6.29** | **9.61** | **5.32** | **8.49** |
| KIVI*-3bit | 0.21 | 6.55 | 10.05 | 5.41 | 8.62 |
| KVQuant-3bit-1% | 0.21 | 6.48 | 9.90 | 5.41 | 8.62 |
| XQuant-3bit | 0.21 | 6.43 | 9.87 | 5.37 | 8.59 |
| XQuant-CL-3bit | 0.21 | **6.32** | **9.67** | **5.34** | **8.51** |
| KIVI*-2bit | 0.15 | 8.72 | 13.54 | 6.14 | 9.96 |
| KVQuant-2bit-1% | 0.15 | 7.45 | 11.49 | 5.87 | 9.30 |
| XQuant-2bit | 0.15 | 7.38 | 11.54 | 5.69 | 9.21 |
| XQuant-CL-2bit | 0.15 | **6.60** | **10.15** | **5.46** | **8.61** |

表4では、(MHA) Llama-2-7B/13および(GQA) Llama-3.1-8BとMistral-7Bモデルに対する層間圧縮法XQuant-CLの評価を提供します。また、非対称均一量子化も使用するKIVI\*、および非均一、ベクトルごとの密およびスパース量子化を使用するKVQuantとの基準比較も提供します（すべての構成の詳細についてはセクション4.1を参照）。結果では、KVQuantをKVQuant-⟨e⟩bit-⟨o⟩%としてリストします。ここで、⟨e⟩は量子化ビット幅、⟨o⟩%は層ごとにスパースで高精度形式で保存される外れ値の割合です。外れ値しきい値として1%を使用します。これはKVQuantで示された最良の構成です。KIVI\*、XQuant、およびXQuant-CLの場合、最初の3層を高精度（4ビット）で保持することに注意してください。これらの初期層を高精度で保持すると、パープレキシティの劣化が著しく軽減され、外れ値を高精度で保持するために追加のメモリを使用するKVQuantと同等のメモリフットプリントになることがわかります。これは、残差接続を持つネットワークの最初の数層が実質的な表現学習（入力の大きな変換）を行うのに対し、後の層はネットワークの入力に小さな反復的な改良を適用することを示した研究結果と一致しています。したがって、最初の数層のデルタは量子化が困難ですが、ネットワークの残りの層のデルタは低ビット精度で量子化がはるかに容易です。XQuant-CLの場合、3番目の層をアキュムレータになる高精度ベース層として使用します。

Llama-2-7Bの場合、FP16ベースラインと比較して、XQuant-CLは12.5倍のメモリ節約で2ビットでわずか0.1のパープレキシティ劣化を、10倍のメモリ節約で3ビットでわずか0.01のパープレキシティ劣化をもたらします。KVQuant-2bit-1%と比較して、XQuant-CL-2bitは1.9倍少ないメモリを使用しながら0.42少ないパープレキシティ劣化をもたらします。同様に、Llama-2-13Bの場合、XQuant-CL-2bitは1.9倍少ないメモリを使用しながらWikiText-2でKVQuant-2bit-1%と比較して0.23少ないパープレキシティ劣化を達成します。Llama-3.1-8Bでは、2ビット精度で、XQuant-CLはC4で10.15のパープレキシティを保持します。これはKIVI\*-2bit (13.54)より3ポイント以上低く、KVQuant-2bit-1% (11.49)より1ポイント以上低いです。Mistral-7Bでは、XQuant-CL-2bitは6.7倍のメモリを節約しながら、WikiText-2でFP16ベースラインと比較してわずか0.14のパープレキシティ劣化に直面します。印象的なことに、WikiText-2でKVQuant-4bit-1%と比較してわずか0.12のパープレキシティ劣化で、XQuant-CL-2bitはKVQuant-4bit-1%より1.8倍少ないメモリを使用します。全体として、XQuant-CLは超低精度ビット幅で最先端のKVキャッシュ量子化手法を上回り、KVキャッシュ量子化より1.5-2倍のメモリ節約とFP16ベースラインより6-12倍のメモリ節約でFP16に近い精度を達成します。

## 5 結論

計算能力が現代のGPUハードウェアのメモリ帯域幅と容量を上回り続けるにつれて、LLM推論はますますメモリ帯域幅バウンドになりつつあります。このハードウェアスケーリングトレンドを踏まえ、自然な戦略は、メモリ要件を削減するために追加の計算操作を実行することです。この研究では、新世代のハードウェアで計算スケーリングトレンドを活用してLLMの推論を高速化するための前向きなビジョンを採用します。具体的には、LLM推論は通常、各トークンを生成する際に大きなKVキャッシュをロードするためにメモリ帯域幅バウンドです。これに対処するために、より高い計算コストと引き換えにKVキャッシュ活性化のサイズを削減することでLLM推論のメモリ要件を削減し、各トークンを生成するために必要なメモリ操作の数を削減して推論を高速化することを目指します。私たちはXQuantを提案します。これは層入力活性化を量子化してKVキャッシングと比較して2倍のメモリ削減を実現し、推論中にKeysとValuesをオンザフライで再計算します。次に、基本的な手法を拡張してXQuant-CLを提案します。これは連続する層間のX埋め込みの層間圧縮性を活用します。単純な均一量子化を使用して、XQuantとXQuant-CLは、非均一量子化と密およびスパース量子化を使用するKVQuantのような最先端のKVキャッシュ量子化手法を上回りながら、FP16ベースラインに近い精度も保持することがわかります。FP16ベースラインと比較して、XQuantはLlama-2-7BおよびLlama-2-13Bモデルで7.7倍少ないメモリを使用しながら0.1未満のパープレキシティ劣化を達成します。XQuant-CLは、2ビット精度で12.5倍のメモリ節約でわずか0.1のパープレキシティ劣化を、3ビット精度で10倍のメモリ節約でわずか0.01のパープレキシティ劣化をFP16ベースラインと比較して達成します。XQuantとXQuant-CLの両方は、KIVI\*およびKVQuantの低ビット精度量子化と比較して数ポイントのパープレキシティ劣化を削減します。ハードウェアプラットフォームの計算能力とメモリ能力の間の拡大する不一致により、XQuantのような再計算手法は、低ビット精度でもFP16に近い精度を保ちながら、メモリ帯域幅バウンドのLLM推論を加速するために利用可能な計算を活用するのに役立ちます。

### 制限事項

私たちの研究は、入力X埋め込みを量子化してKVキャッシュ活性化を再計算することでLLM推論のメモリ要件を削減することに焦点を当てています。このアプローチは精度の損失を最小限に抑えながら積極的なメモリ圧縮を可能にしますが、再計算を実行するために追加の計算操作を必要とするため、特定のハードウェアプラットフォームでレイテンシが増加する可能性があります。さらに、XQuant-CLはメモリ容量要件を削減しますが、アキュムレータをロードする必要があるため、KVキャッシュ活性化を再計算するために追加の計算とメモリ操作を必要とします。ただし、目標がFP16に近い精度を達成することであるメモリ制約のあるシナリオでは、XQuant-CLが最適な選択になる可能性があります。

### 謝辞

FuriosaAIチーム（Jihoon Yoon、Kevin Galim、Heeju Kim、Hyung Il Kooを含む）、Intel、Apple、NVIDIA、Mozillaからの寛大なサポートに感謝します。また、Accelerating Foundation Model Researchを通じたMicrosoftのサポートにも感謝します。さらに、Google Cloud、Google TRCチーム、およびDavid Patterson教授からのサポートに感謝します。Keutzer教授の研究室は、Intel社、UC Berkeley oneAPI Center of Excellence、Intel VLABチーム、およびBDDとBAIRを通じた資金提供によってスポンサーされています。MWMはまた、DARPA、DOE、NSF、およびONRに感謝したいと思います。この研究は、契約番号DE-AC02-05CH11231の下での米国エネルギー省の先進科学計算研究部門の科学局長によって部分的にサポートされました。私たちの結論は必ずしもスポンサーの立場や方針を反映するものではなく、公式な承認を推測すべきではありません。

## 参考文献

略

## 付録A XQuant-CLのプリフィル

ここでは、XQuant-CLの推論のプリフィル段階の視覚化を含めます。特に、連続する層のX埋め込み間のデルタがどのように計算され、量子化され、キャッシュされるかを示します。Layer 0の入力X_0以外は、すべての他の層の入力は、X_0と前の量子化されたデルタの合計を使用して計算される近似であることに注意してください。

![図A.1を参照](https://arxiv.org/html/2508.10395v1/x6.png)

**図A.1:** プリフィル中のXQuant-CLアルゴリズムの図解。Layer 0以外は、すべての他の層への入力は、すべての前の層のデルタとLayer 0の入力を使用して計算される層間近似です。Layer 0の入力は各層のデルタと合計されるため、アキュムレータとして扱うことができ、Layer NのXを計算するためにすべてのN-1デルタをロードすることを回避できます。層の処理が完了した後、シーケンス内のすべてのトークンの層への入力埋め込みが層の出力活性化（入力埋め込みと同じ形状）から減算され、このデルタは量子化されてΔX̂としてキャッシュされます。

## 付録B GQAモデルにXQuantを適用する際の観察された外れ値特性

### 要約

前述のように、$W_k$および$W_v$行列に対してSVDを実行し、$U_k \Sigma_k B_k^T$および$U_v \Sigma_v B_v^T$を取得します。$XU_k$および$XU_v$をキャッシュし、これらの潜在分布を量子化しますが、スケーリング項$\Sigma$を適用する前です（$U_k$と$U_v$のみを乗算します - これらはそれぞれ直交列を持つ行列です）。GQAモデルにXQuantを適用する際の1つの課題は、GQAがXをより小さい次元にダウンプロジェクトするため、単純にXを量子化すると、KVキャッシュの量子化よりも多くのメモリを必要とすることです（セクション3.3.1を参照）。

### 観察された外れ値特性

$W_k$に投影した後、私たちは$XU_k$のすべての層で興味深い特性を観察します。ここで、$U_k$は直交列を持つ行列です。$XU_k$の最初のチャネルに大規模な外れ値がすべて集中していることがわかり、これは異なるデータセット上の複数の異なるモデルのすべての層で観察されます（図B.2、B.3を参照）。図B.1は、この観察された外れ値特性を視覚化し、Keysの外れ値チャネルとの接続を説明しています。$X \in \mathbb{R}^{L \times d}$（$L$はシーケンス長、$d$は隠れ次元）。$U_k \in \mathbb{R}^{d \times d/g}$および$\Sigma_k \in \mathbb{R}^{d/g \times d/g}$、ここで$g$はGQAモデルで使用されるグループサイズです。$\sigma_0 \geq \sigma_1 \geq ... \geq \sigma_{d/g} > 0$であることに注意してください。異なるデータセット上の異なるモデルのすべての層で、$XU_k$の最初のチャネルに大規模な外れ値があることがわかります（図B.2、B.3を参照）。$XU_k$に$\Sigma_k$を乗算すると、外れ値チャネルの順序が保持されます（最初のチャネルには依然として最大の大きさの外れ値があります）。$XU_k \Sigma_k$に$B_v^T$を乗算すると、$B_v^T$の最初の行が$XU_k \Sigma_k$の最初の外れ値チャネルと相互作用し、上記の図で赤で強調表示されています。この最初の行のtop-k最大大きさ値を使用して、Keysのどのチャネルが外れ値チャネルであるかを特定できます。これは、キャリブレーションデータなしで、重みのみに基づいてオフラインで達成できます。

![図B.1を参照](https://arxiv.org/html/2508.10395v1/x7.png)

**図B.1:** $X \in \mathbb{R}^{L \times d}$、$L$はシーケンスLengthで、$d$は隠れ次元です。$U_k \in \mathbb{R}^{d \times d/g}$および$\Sigma_k \in \mathbb{R}^{d/g \times d/g}$、ここで$g$はGQAモデルで使用されるグループサイズです。$\sigma_0 \geq \sigma_1 \geq ... \geq \sigma_{d/g} > 0$であることに注意してください。異なるデータセット上の異なるモデルのすべての層で、$XU_k$の最初のチャネルに大規模な外れ値があることがわかります（図B.2、B.3を参照）。$XU_k$に$\Sigma_k$を乗算すると、外れ値チャネルの順序が保持されます（最初のチャネルには依然として最大の大きさの外れ値があります）。$XU_k \Sigma_k$に$B_v^T$を乗算すると、$B_v^T$の最初の行が$XU_k \Sigma_k$の最初の外れ値チャネルと相互作用します。この最初の行のtop-k最大大きさ値を使用して、Keysのどのチャネルが外れ値チャネルであるかを特定できます。

### KVキャッシュ量子化手法との接続

以前のKVキャッシュ量子化手法（KVQuantなど）は、量子化による精度劣化を削減するために、Keysの外れ値を高精度で保持します。Keysには明確な外れ値チャネルがあることが観察されているため（図3を参照）、Keys全体の外れ値を保持するには、これらの外れ値チャネルを見つける必要があります。外れ値チャネルを特定するために、KVQuantのような手法はキャリブレーションデータセット上でモデルを実行し、多様なデータサンプルにわたって外れ値になる傾向があるチャネルに注目します。

$XU_k$の最初のチャネルの外れ値動作を観察したことで、キャリブレーションを実行せずにオフラインで$W_k$行列のSVD分解を検査することでKeysの外れ値チャネルを特定できるかどうかを探求します。この推論の線に従います：$XU_k$は順序付けられた分布であり、最初のチャネル（つまり最初の列）が$W_k$の最大の特異値に関連付けられ、2番目のチャネルが2番目に大きい特異値に関連付けられ、以下同様です。$XU_k$に$\Sigma_k$を乗算すると、$XU_k$の最初のチャネルが最大の特異値$\sigma_0$でスケーリングされ、$XU_k$の2番目のチャネルが2番目に大きい特異値$\sigma_1$でスケーリングされ、以下同様です。この行列乗算は図B.1で視覚化されています。したがって、$XU_k \cdot \Sigma_k$は外れ値チャネルの順序を保持します（つまり、$XU_k$の最初のチャネルは依然として外れ値チャネルです）。最後に、Keysの再計算を完了するために、$(XU_k \Sigma_k)$に$B_v^T$を乗算します。これは必然的に最初のチャネルの外れ値を他のチャネルに分散します。これらは、キャリブレーションを実行することでKVキャッシュ量子化手法がKeys分布で特定しようとするチャネルです。

しかし、$B_v^T$の列で最初の要素が他のすべての列の最初の要素と比較して最大の大きさを持つものも、Keysの外れ値チャネルを含む列になる傾向があると仮定します。例えば、図B.1では、$B_v^T$の2番目の列に$b_{\max}$が含まれていることが示されています。これは$B_v^T$の最初の行の最大の大きさの値です。これは、Keysの2番目の列が外れ値チャネルになる傾向があることを意味します。これは、$B_v^T$の最初の行ベクトルの値が、行列乗算を実行する際に$(XU_k \Sigma_k)$の最初の外れ値チャネルと相互作用するスカラーであるためです。したがって、$(XU_k \Sigma_k)$の最初の列を「ヒットする」$B_v^T$の最初の行ベクトルの最大のスカラーを持つ列が、Keysで最大の値をもたらします。図B.1では、最大の外れ値チャネル$\sigma_0 X\vec{x}_0$が赤で強調表示されており、この外れ値チャネルを乗算するスカラーを含む$B_v^T$の最初の行ベクトルも赤で強調表示されています。

ただし、上記の仮説が成立しない2つのケースがあります：1) $B_v^T$の他の行ベクトルに、$XU_k$の最初のチャネルではない別のチャネルが大きさで爆発的に増加するスカラーが含まれている場合、2) $B_v^T$の最初の行ベクトルの最大のスカラーが、外れ値チャネルのほとんどの値を$(XU_k \Sigma_k)$の次に大きい外れ値チャネルと反対の符号にして、それらの合計が小さい大きさになる場合。それにもかかわらず、他のチャネル自体が最初のチャネルよりもはるかに小さい大きさであるため、これらのシナリオは可能性が低いです。したがって、1) 他のチャネルが最初の外れ値チャネルを大きさで上回るほど爆発的に増加することは unlikely であり、2) 最初の外れ値チャネルを、反対符号の要素を持つ別のチャネルに追加することが最初の外れ値チャネルに大きな影響を与えることは unlikely です。

この仮説をテストするために、上記の方法を使用してKeysの外れ値チャネルを決定し、次に真値と比較することでこれらの予測が正しいかどうかを確認します。真値については、最大の平均大きさを持つKeys行列の列を選択します。Keysには複数の外れ値チャネルがある可能性があるため、$B_v^T$の最初の行のtop-k最大大きさ値の列インデックスを予測として選択します。次に、真値インデックスが上記の方法で予測されたインデックスのいずれかに現れるかどうかを確認するだけです。予測のいずれかに真値インデックスが含まれている場合、予測を正しいものとしてマークし、モデルのすべての層のKeysにわたる正しい予測の割合として最終的な精度を報告します。2つの異なるデータセット、WikiText-2とC4でこの手法をテストして、この重みのみベースの分析が異なるデータに対してロバストであるかどうかを確認します。また、Llama-3.1-8BとMistral-7Bモデルでテストして、異なるモデルにわたる手法の効果を確認します。結果は表B.2にリストされています。

Llama-3.1-8Bの場合、$B_v^T$の最初の行の8つの最大大きさ値のみがKeysの外れ値チャネルを決定するために必要であることがわかります。Mistral-7Bの場合、top-8最大大きさ値を使用すると96.88%の精度が得られます。さらに、両方のモデルで、精度はデータセット間で一致しており、Keysの外れ値インデックスを決定するこの手法がデータに対してロバストであることを示しています。

**表B.1:** 最初の外れ値チャネルをFP16で保持することでXU_kの構造化された分布を活用しようとする試み。表はLlama-3.1-8BとMistral-7BのWikiText-2とC4でのパープレキシティを使用したXQuant評価を示しています。参照ポイントとして機能するように、表1からKIVI\*とXQuantの結果を複製します。2ビット精度の場合、最初のチャネルをFP16で保持すると、いくらかのパープレキシティ改善が得られます。

[表の内容は元の論文を参照]

**表B.2:** 重みのみを分析してオフラインでキャリブレーションデータなしで、重み行列B_v^Tのtop-k値を分析することで正しく予測された外れ値チャネルの割合。Llama-3.1-8Bの場合、B_v^Tの最初の行の8つの最大大きさ値のみがKeysの外れ値チャネルを決定するために必要です。

| top-k | Llama-3.1-8B | Mistral-7B |
|-------|--------------|------------|
|       | WikiText2 | C4 | WikiText2 | C4 |
| k=1   | 66.14% | 71.15% | 75.35% | 71.91% |
| k=2   | 72.08% | 75.08% | 87.55% | 83.48% |
| k=4   | 87.71% | 90.62% | 93.75% | 93.75% |
| k=8   | 100% | 100% | 96.88% | 96.88% |

![図B.2を参照](https://arxiv.org/html/2508.10395v1/x8.png)
![図B.2を参照](https://arxiv.org/html/2508.10395v1/x9.png)

**図B.2:** WikiText-2（上）とC4（下）のサンプルでのLlama-3.1-8BのX（左）、XU_k（中央）、XU_v（右）の分布。

![図B.3を参照](https://arxiv.org/html/2508.10395v1/x10.png)
![図B.3を参照](https://arxiv.org/html/2508.10395v1/x11.png)

**図B.3:** WikiText-2（上）とC4（下）のサンプルでのMistral-7BのX（左）、XU_k（中央）、XU_v（右）の分布。

---

**翻訳完了**

この文書は、UC BerkeleyおよびFuriosaAIの研究者による「XQuant: Breaking the Memory Wall for LLM Inference with KV Cache Rematerialization」（arXiv:2508.10395v1）の日本語翻訳です。

原文URL: https://arxiv.org/html/2508.10395v1
