---
title: "Amazon SageMaker HyperPod -- Elastic Training"
emoji: "➰"
type: "tech"
topics: ["AWS", "SageMaker", "HyperPod", "DistributedTraining", "PyTorch"]
published: false
---

## はじめに

本記事は SageMaker HyperPod 機能解説シリーズの一部です。以下もすでに記事として書いてあるので参考にしてください。

https://zenn.dev/tosshi/articles/45a746434b2090

Amazon SageMaker HyperPod Elastic Training は、クラスターの容量変化に応じて学習ジョブのノード数を**動的に増減**する機能で、ハードウェア障害時の自動縮退運転や、新規ノード追加時の自動スケールアップを実現します。

本記事では、Elastic Training の技術的詳細を包括的に整理します。

:::message alert
本記事は 2026 年 2 月時点の公式ドキュメント、オープンソースコード、などに基づく調査記事です。間違っている可能性もあるため必ず最新の公式ドキュメントを正として確認してください。間違いがあればコメントください。
:::

## 概要

![HyperPod Elastic Training のスケーリング動作を示すアニメーション](https://d2908q01vomqb2.cloudfront.net/da4b9237bacccdf19c0760cab7aec4a8359010b0/2025/11/18/2025-sageamker-hyperpod-elastic-training.gif)

:::message
本機能は EKS 環境でのみ利用可能です。Slurm 環境では利用できません（[HyperPod CLI ソースコード](https://github.com/aws/sagemaker-hyperpod-cli)）。
:::

Elastic Training は、クラスターの容量変化に応じて学習ジョブのノード数を**動的に増減**する機能です。ハードウェア障害でノードが減少した場合は自動的に縮小運転を継続し、新しいノードが利用可能になれば自動的にスケールアップします。

## PyTorch Elastic との関係

HyperPod Elastic Training は、PyTorch Elastic（[`torch.distributed.elastic`](https://github.com/pytorch/pytorch/tree/main/torch/distributed/elastic)）を基盤技術として採用しています。ベースとなる PyTorch Elastic は以下の 4 層のアーキテクチャで動作します。

参照: [HyperPod CLI ソースコード](https://github.com/aws/sagemaker-hyperpod-cli)、[PyTorch Elastic ソースコード](https://github.com/pytorch/pytorch/tree/main/torch/distributed/elastic)

各レイヤーの役割は以下の通りです。

- **Layer 4**: ユーザーが CLI または SDK でジョブを作成
- **Layer 3**: Kubernetes Operator が CRD を監視し、Pod を管理
- **Layer 2**: Pod 内で torchrun がワーカープロセスを起動・監視
- **Layer 1**: ワーカー間の合流・調整を Rendezvous が担当

```mermaid
graph TB
    subgraph L4["L4: UI"]
        direction LR
        CLI["hyp CLI"]
        SDK["Python SDK"]
    end

    subgraph L3["L3: k8s Orchestration"]
        direction TB
        Operator["HyperPod Operator"]
        CRD["HyperPodPyTorchJob<br/>(CRD)"]
        Operator --> CRD
    end

    subgraph L2["L2: Distributed Agent"]
        direction TB
        Torchrun["torchrun"]
        Agent["SimpleElasticAgent"]
        Torchrun -.起動.-> Agent
    end

    subgraph L1["L1: Rendezvous"]
        direction TB
        Handler["DynamicRendezvousHandler"]
        Store["TCPStore"]
        Handler --> Store
    end

    CLI -->|"ジョブ作成"| Operator
    SDK -->|"ジョブ作成"| Operator
    CRD -->|"Pod 起動"| Torchrun
    Agent -->|"ワーカー調整"| Handler
```

HyperPod は、Kubeflow の [`PyTorchJob`](https://github.com/kubeflow/training-operator)（`kubeflow.org/v1`）ではなく、独自の CRD を使用する点に注意が必要です。

| 項目 | Kubeflow PyTorchJob | HyperPod PyTorchJob |
|------|---|---|
| apiVersion | `kubeflow.org/v1` | `sagemaker.amazonaws.com/v1` |
| kind | `PyTorchJob` | `HyperPodPyTorchJob` |
| ElasticPolicy | `rdzvBackend` 等を手動指定 | Operator が自動管理 |
| レプリカ種別 | Master/Worker 分離 | 単一 ReplicaSpec（"pod"） |
| スケーリング | ユーザー管理 | Operator による自動管理 |

## HyperPodPyTorchJob CRD の仕様

HyperPod Elastic Training のジョブは、以下の CRD で定義されます（[HyperPod CLI ソースコード](https://github.com/aws/sagemaker-hyperpod-cli)の [schema.json](https://github.com/aws/sagemaker-hyperpod-cli/blob/main/hyperpod-pytorch-job-template/hyperpod_pytorch_job_template/v1_1/schema.json)、[model.py](https://github.com/aws/sagemaker-hyperpod-cli/blob/main/hyperpod-pytorch-job-template/hyperpod_pytorch_job_template/v1_1/model.py)、および [ElasticPolicy クラス定義](https://github.com/aws/sagemaker-hyperpod-cli/blob/main/src/sagemaker/hyperpod/training/config/hyperpod_pytorch_job_unified_config.py)で確認）。

https://docs.aws.amazon.com/ja_jp/sagemaker/latest/dg/sagemaker-eks-operator-usage.html


```yaml
apiVersion: sagemaker.amazonaws.com/v1
kind: HyperPodPyTorchJob
metadata:
  name: my-elastic-job
  namespace: my-namespace
spec:
  nprocPerNode: "8"
  replicaSpecs:
    - name: "pod"        # HyperPod は単一 ReplicaSpec を採用（Rendezvous 管理を Operator に委任するため Master/Worker の区別が不要）
      replicas: 4          # 初期ノード数
      maxReplicas: 8        # 最大ノード数
      template:
        spec:
          containers:
            - name: "pytorch-job-container"
              image: "my-registry/training:latest"
              resources:
                requests:
                  nvidia.com/gpu: 8
                limits:
                  nvidia.com/gpu: 8
          nodeSelector:
            node.kubernetes.io/instance-type: "ml.p5.48xlarge"
  elasticPolicy:
    minReplicas: 4
    maxReplicas: 8
    replicaIncrementStep: 2     # 2 ノード単位でスケーリング
    scalingTimeoutInSeconds: 300
    gracefulShutdownTimeoutInSeconds: 120
  runPolicy:
    cleanPodPolicy: "None"
    jobMaxRetryCount: 3
    restartPolicy:
      scaleUpSnoozeTimeInSeconds: 600  # 再起動後のスケールアップ抑止期間
```

### ElasticPolicy の主要フィールド

| フィールド | 型 | 説明 |
|---|---|---|
| `minReplicas` | int | 最小レプリカ数（CLI 経由では `node_count` と同値に設定される） |
| `maxReplicas` | int | 最大レプリカ数（`max_node_count` と同値） |
| `replicaIncrementStep` | int | ステップサイズ（例: 2 = 2 ノードずつ増減） |
| `replicaDiscreteValues` | list[int] | 離散的なレプリカ数（`replicaIncrementStep` と[相互排他](https://github.com/aws/sagemaker-hyperpod-cli/blob/9ffefae22ff108141579f4b4d7bb1ddc7b243d61/hyperpod-pytorch-job-template/hyperpod_pytorch_job_template/v1_1/model.py#L419-L425)） |
| `scalingTimeoutInSeconds` | int | スケーリング操作のタイムアウト |
| `gracefulShutdownTimeoutInSeconds` | int | グレースフルシャットダウンのタイムアウト |
| `faultyScaleDownTimeoutInSeconds` | int | 障害 Pod のスケールダウンまでの待機時間 |

## Rendezvous メカニズム

Elastic Training の中核となる Rendezvous は、分散学習に参加するワーカーの**合流・調整**メカニズムです。PyTorch Elastic の [`DynamicRendezvousHandler`](https://github.com/pytorch/pytorch/blob/main/torch/distributed/elastic/rendezvous/dynamic_rendezvous.py) が以下のフローで動作します。

```mermaid
sequenceDiagram
    autonumber
    participant W1 as Worker A
    participant W2 as Worker B
    participant W3 as Worker C (新規)
    participant RH as RendezvousHandler
    participant BE as Backend (C10d Store)

    Note over W1, BE: 初期状態: min_nodes=2, max_nodes=4

    rect rgb(230, 245, 255)
        Note over W1, W2: Phase 1: 初回 Rendezvous
        W1 ->> RH: next_rendezvous()
        RH ->> BE: ADD_TO_PARTICIPANTS(A)
        W2 ->> RH: next_rendezvous()
        RH ->> BE: ADD_TO_PARTICIPANTS(B)
        Note over RH: min_nodes=2 到達 → last_call 30 秒
        Note over RH: deadline 到達 → MARK_COMPLETE
        RH -->> W1: rank=0, world_size=2
        RH -->> W2: rank=1, world_size=2
        Note over W1, W2: 学習開始
    end

    rect rgb(255, 245, 220)
        Note over W3, RH: Phase 2: ワーカー追加
        W3 ->> RH: next_rendezvous()
        RH ->> BE: ADD_TO_WAIT_LIST(C)
        Note over RH: complete=True なので wait_list へ
    end

    rect rgb(255, 230, 230)
        Note over W1, BE: Phase 3: Re-rendezvous
        Note over W1: num_nodes_waiting() > 0 を検出
        Note over W1, W2: （ユーザーコードで定期的にチェックポイントを保存済み）
        W1 ->> RH: next_rendezvous() (再参加)
        W2 ->> RH: next_rendezvous() (再参加)
        W3 ->> RH: (wait_list から参加)
        Note over RH: 新ラウンド: 3 ノードで MARK_COMPLETE
        RH -->> W1: rank=0, world_size=3
        RH -->> W2: rank=1, world_size=3
        RH -->> W3: rank=2, world_size=3
        Note over W1, W3: 学習再開 (world_size: 2→3)
    end
```

### Rendezvous の状態管理

[`_RendezvousState`](https://github.com/pytorch/pytorch/blob/main/torch/distributed/elastic/rendezvous/dynamic_rendezvous.py) が管理する主要なフィールド

https://github.com/pytorch/pytorch/blob/704b8d34b25fcb73489cc9834276b0e569244578/torch/distributed/elastic/rendezvous/dynamic_rendezvous.py#L272-L296

状態管理は [`C10dRendezvousBackend`](https://github.com/pytorch/pytorch/blob/main/torch/distributed/elastic/rendezvous/c10d_rendezvous_backend.py) のバックエンドで行われ、Store の `compare_set()` 操作（CAS: Compare-And-Swap）による楽観的ロックで一貫性が保証されます。

::::details Compare-And-Swap（CAS）とは
Compare-And-Swap（CAS）は、並行処理での競合を回避するアトミックな操作パターンです。

**基本動作**:
```python
def compare_set(key, expected_value, new_value):
    if current_value == expected_value:
        current_value = new_value
        return True  # 成功
    else:
        return False  # 競合発生（他のプロセスが先に更新した）
```

**Rendezvous での使用例**:
1. プロセス A が状態を読み取る（`round=1, participants=2`）
2. プロセス B も同じ状態を読み取る
3. プロセス A が `compare_set(expected=round1, new=round1+参加者追加)` を実行 → 成功
4. プロセス B が `compare_set(expected=round1, new=round1+参加者追加)` を実行 → **失敗**（既に round1 ではない）
5. プロセス B は最新状態を再読み込みしてリトライ

この仕組みにより、複数ワーカーが同時に状態を更新しても一貫性が保たれます。
::::

### ハートビートと死亡検出

| パラメータ | デフォルト値 | 説明 |
|---|---|---|
| `keep_alive_interval` | 5 秒 | ハートビート送信間隔 |
| `keep_alive_max_attempt` | 3 回 | 最大失敗回数 |
| **死亡検出までの最大遅延** | **15 秒** | 5 秒 x 3 回（`_sanitize()` メソッドで実装） |

## Agent のメインループ

[`SimpleElasticAgent._invoke_run()`](https://github.com/pytorch/pytorch/blob/704b8d34b25fcb73489cc9834276b0e569244578/torch/distributed/elastic/agent/server/api.py#L892) が Elastic Training の中核ループとして動作します。

```mermaid
flowchart TD
    Start([開始]) --> Init[_initialize_workers<br/>初回 Rendezvous + 起動]
    Init --> Sleep[monitor_interval<br/>スリープ]
    Sleep --> Monitor[_monitor_workers<br/>ワーカー状態チェック]
    Monitor --> CheckState{state<br/>判定}

    CheckState -->|SUCCEEDED| Exit[_exit_barrier]
    Exit --> ReturnSuccess([正常終了])

    CheckState -->|UNHEALTHY<br/>FAILED| CheckRestarts{remaining_restarts: 0 より大きい?}
    CheckRestarts -->|YES| DecRestarts[remaining_restarts--]
    DecRestarts --> RestartFail[_restart_workers<br/>Re-rendezvous]
    RestartFail --> Sleep
    CheckRestarts -->|NO| Stop[_stop_workers]
    Stop --> ReturnFail([異常終了])

    CheckState -->|HEALTHY| CheckWaiting{num_nodes_waiting<br/>> 0?}
    CheckWaiting -->|YES| RestartScale[_restart_workers<br/>Re-rendezvous<br/>membership changes は<br/>remaining_restarts に<br/>カウントされない]
    RestartScale --> Sleep
    CheckWaiting -->|NO| Sleep

    style Init fill: #e1f5ff
    style RestartFail fill: #ffe1e1
    style RestartScale fill: #fff4e1
    style Exit fill: #e1ffe1
    style Stop fill: #ffe1e1
```

このループにより、ワーカーの障害検出と新規ワーカーの参加検出の両方が自動的に処理されます。

### 再起動カウントの重要な仕様

Agent には再起動回数の上限（`remaining_restarts`）がありますが、**障害による再起動とスケーリングによる再起動では扱いが異なります**。

| 再起動の種類 | `remaining_restarts` の消費 | 説明 |
|------------|------------------------|------|
| **障害による再起動**<br/>（UNHEALTHY/FAILED → `_restart_workers()`） | 1 減る | 無限ループを防ぐため、上限に達したら学習を停止 |
| **スケーリングによる再起動**<br/>（新ノード参加で `_restart_workers()`） | **カウントされない** | ノード追加/削除は正常な動作なので、何度でも実行可能 |

## CLI によるジョブ作成

HyperPod CLI（[`hyp`](https://github.com/aws/sagemaker-hyperpod-cli)）を使用して Elastic Training ジョブを作成します。

```bash
hyp create hyp-pytorch-job \
  --job-name my-elastic-job \
  --image my-registry/training:latest \
  --node-count 4 \
  --max-node-count 8 \
  --instance-type ml.p5.48xlarge \
  --tasks-per-node 8 \
  --elastic-replica-increment-step 2 \
  --elastic-scaling-timeout-in-seconds 300 \
  --elastic-graceful-shutdown-timeout-in-seconds 120 \
  --elastic-scale-up-snooze-time-in-seconds 600
```

## ユーザーコードでの対応

Elastic Training を使用する場合、ユーザーの学習スクリプト側でも `world_size` の動的変更に対応する必要があります。

Re-rendezvous 後、`world_size` が変更されると実効バッチサイズも変化します。学習率を調整することが考えられますが、実験再現性や学習安定性に影響があるため、学習率は固定したまま実効バッチサイズを一定に保つアプローチが推奨されます。この点については、モデルやタスクに応じた実験的な検証が必要です。

:::message
**重要**: PyTorch Elastic（torchrun）は Re-rendezvous 時にワーカープロセスを再起動します。そのため、ユーザーコードの初期化部分が自動的に再実行され、新しい `world_size` が `torch.distributed.get_world_size()` で取得できます。
:::

Re-rendezvous 時にはワーカープロセスが再起動されるため、チェックポイントの保存はユーザーコード側で定期的に実装する必要があります。最新のチェックポイントから学習を再開できるよう、適切な保存間隔を設定してください。

:::message
**Checkpointless Training との関係**: Checkpointless Training（メモリ内レプリカによる高速復旧）と Elastic Training は、理論的には階層的な障害復旧として併用可能です。Checkpointless が第一防衛線（In-Process Recovery による高速復旧）を提供し、Elastic が最終防衛線（ノード交換・縮退運転）を提供する構成が考えられます。ただし、world_size が変更されるシナリオでは Megatron-Core の冗長レプリカ再配置が必要であり、AWS 公式ドキュメントでも併用に関する明示的なサポートは記載されていません。詳細は後述の「Checkpointless Training との比較」セクションを参照してください。
:::

:::message alert
Re-rendezvous 時には**全ワーカープロセスが一時停止**します。これは数秒から数十秒の中断を伴うため、頻繁なスケーリングは学習効率を低下させます。`scaleUpSnoozeTimeInSeconds` を適切に設定して、再起動直後のスケールアップを抑止することが重要です。
:::


## GRPO マルチフェーズ学習への適用可能性

上述した通り Elastic Training ではスケーリングの際に中断を伴うため頻繁なスケーリングが発生するようなケースには向いておらず、障害復旧や実験単位などの中断頻度が低いようなケースでの利用が良さそうです。

::::details GRPO フェーズ連動型 Elastic Training の詳細分析


### Elastic Training の有効なユースケース

**頻度が低い場合は Elastic Training が有効**です：

| ユースケース | 変更頻度 | 再シャーディング回数 | 判定 |
|-------------|---------|---------------------|------|
| **障害復旧** | 数日に 1 回 | 1〜数回 | [OK] 実用的 |
| **実験単位の変更**（事前学習→ファインチューニング等） | 実験の区切りで 1 回 | 1 回 | [OK] 実用的 |
| **GRPO フェーズ連動型**（毎イテレーション） | 1000 回以上 | 1000 回以上 | [NG] 非実用的 |

例えば、事前学習（128 ノード）からファインチューニング（16 ノード）への移行は、実験の区切りで 1 回のみノード数を変更するため、再シャーディングのコストは十分に許容できます。

### Slurm と Elastic Training（EKS）の比較

実験単位のノード数変更は Slurm でも可能ですが、実現方法が異なります。

| 機能 | Slurm | Elastic Training（EKS） |
|------|-------|----------------------|
| 実験単位のノード数変更 | 可能（別ジョブ投入） | 可能（同一ジョブ内） |
| 障害時の自動縮小運転 | 不可 | 可能 |
| 同一ジョブ内での動的変更 | 不可 | 可能 |

**Slurm 環境**:
```bash
# 事前学習: 128 ノード
sbatch --nodes=128 --job-name=pretrain pretrain.sh

# ファインチューニング: 16 ノード（別ジョブとして投入）
sbatch --nodes=16 --job-name=finetune finetune.sh
```

**Elastic Training（EKS 環境）**:
```bash
# 同一ジョブ内でレプリカ数を変更（概念的な例）
# 注: HyperPodPyTorchJob CRD が scale subresource をサポートしている場合のみ動作
kubectl scale hyperpodjob/my-job --replicas=16

# 代替方法（より確実）
kubectl patch hyperpodjob my-job --type='json' -p='[{"op": "replace", "path": "/spec/replicas", "value": 16}]'
```

Slurm では異なるノード数で別々のジョブを投入しますが、Elastic Training では同一ジョブ内でノード数を動的に変更できます。

### 推奨アプローチ

現実的なアプローチは、**学習クラスタは固定ノード数で運用し、推論プール（vLLM サーバー）のみを弾力的にスケーリング**する方式です。vLLM サーバーはステートレスに近く、Kubernetes HPA や KubeRay Autoscaler で制御できます。Generation Phase で推論ノードをスケールアップし、Training Phase で最小構成にスケールダウンすることで、10-14% 程度のコスト削減が見込めます[^grpo_cost]。

[^grpo_cost]: Generation Phase 80%、Training Phase 20% の比率を前提とし、推論プールを Training Phase で最小構成（1 ノード）にスケールダウンした場合の推定値。実際の削減率は、推論ノード数、インスタンスタイプ、スポット割引率等に依存します。

一方、Elastic Training は**障害復旧目的と実験単位のノード数変更**での活用に適しています。Health Monitoring Agent と組み合わせることで、ノード障害時の自動縮小運転と復旧後のスケールアップを実現し、学習の継続性を確保できます。

### trn2 を推論フェーズに使用する構成

GRPO の Generation Phase に AWS Trainium2（trn2）を推論アクセラレータとして使用し、p5（H100 GPU）と混在させる構成も検討に値します。

### 技術的な実現方法

HyperPodPyTorchJob の Elastic Training は単一インスタンスタイプのみサポートするため、GPU + trn2 の混在には使用できません（[HyperPod CLI のソースコード](https://github.com/aws/sagemaker-hyperpod-cli)で `--instance-type` パラメータが単一値のみを受け付けることを確認済み）。代わりに、以下の分離アーキテクチャが現実的です:

- **学習クラスタ**: p5.48xlarge（固定ノード数）で HyperPodPyTorchJob を使用
- **推論プール**: trn2.48xlarge 上の vLLM Server を弾力的に管理

vLLM on Neuron（NxD Inference ライブラリ）は Neuron SDK 2.27.1 時点で trn2 をサポートしており、Llama 3.x 等の主要モデルでの推論が可能です。推論サーバーはステートレスであるため、以下のいずれかの方法で弾力的にスケーリングできます。

### オプション A: Kubernetes Deployment + HPA

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-trn2-inference
spec:
  replicas: 2
  # ... (Pod template with trn2 nodeSelector)
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: vllm-trn2-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: vllm-trn2-inference
  minReplicas: 2
  maxReplicas: 16
```

### オプション B: VERL + Ray Autoscaler

VERL（Versatile RLHF Library）を使用する場合、vLLM を Ray Actor として管理し、Ray Autoscaler による弾力的スケーリングが可能です。ただし、VERL の `ResourcePoolManager` は現状では静的なリソース割り当てのため、動的スケーリングにはカスタマイズが必要です（研究課題レベル）。

**利点**:
- trn2 の推論コスト効率（GPU 比で価格性能優位性が期待される）
- 推論プールの独立したスケーリングにより、Generation Phase でのリソース効率を向上
- 学習クラスタの安定性を維持しつつ、推論リソースを弾力的に管理

**課題**:
- GPU で学習したモデルの trn2 推論サーバーへの配布と Neuron コンパイル
- Neuron コンパイルキャッシュの管理（初回コンパイルに 15 分〜数時間）
- 異種アクセラレータ間の数値精度の微小な差異

:::message
GRPO のフェーズ連動型 Elastic Training は、PyTorch の将来バージョンでの Elastic FSDP サポートに依存する研究課題です。一方、VERL（Versatile RLHF Library）の Colocate Placement（3D-HybridEngine による効率的なフェーズ切り替えを実装）は、同一 GPU セット上で Generation と Training を切り替えることで、フェーズ間のリソース非効率性を設計レベルで解消する別のアプローチを提供します。VERL を使用しても学習ノード数の動的変更は依然として困難ですが、そもそも Elastic Training の必要性自体が低下します。推論プールの弾力的スケーリングと VERL の Colocate Placement を組み合わせるアプローチが現時点では最も実用的です。
:::

::::

## VERL の Slurm 環境対応状況

GRPO のようなマルチフェーズ RLHF 学習を実装する上で、VERL（Versatile RLHF Library）は有力な選択肢です。前セクションの最後に VERL の Colocate Placement について言及しましたが、ここでは HyperPod 環境における VERL の Slurm 対応状況について整理します。

### VERL とは

VERL は Volcano Engine が開発するオープンソースの RLHF 学習フレームワークです。分散学習基盤として PyTorch FSDP と Megatron-LM をサポートし、Ray を分散制御基盤として使用します。推論エンジンには vLLM / SGLang を採用し、高速なロールアウト生成を実現します。また、Colocate Placement により Actor、Critic、Reference、Reward モデルを効率的に GPU に配置し、メモリ冗長性を削減します。

### Slurm での動作可能性

VERL は **Ray on Slurm** の仕組みを利用して、Slurm クラスター上で動作可能です。公式ドキュメント（`docs/start/multinode.rst`）では、以下の 4 つの起動方法が記載されており、**Option 3 として Slurm が公式サポート**されています：

| 方法 | 概要 | Slurm 依存 |
|------|------|-----------|
| Option 1: Manual Ray Cluster | 手動で `ray start --head` でクラスタ起動 | なし |
| Option 2: SkyPilot | SkyPilot で Kubernetes / クラウド起動 | なし |
| **Option 3: Slurm** | **Slurm + Ray で起動** | **Slurm 使用** |
| Option 4: dstack | dstack オーケストレーター | なし |

公式リポジトリには `examples/slurm/ray_on_slurm.slurm` というサンプルスクリプトも提供されています。

### HyperPod での制約（重要）

しかし、**AWS SageMaker HyperPod では VERL の Slurm 対応に制約**があります。`sagemaker-hyperpod-recipes` リポジトリの README に以下の明確な記述があります：

> "Only LLMFT recipes are supported on Slurm clusters. **VERL recipes are not supported on Slurm** but are available on EKS and SageMaker training jobs."

つまり、**HyperPod 上での VERL は EKS と SageMaker Training Job のみで公式サポート**されており、Slurm クラスターでは非サポートです。

| プラットフォーム | VERL サポート | コンテナイメージ |
|---------------|------------|----------------|
| **HyperPod EKS** | [OK] 公式サポート | `hyperpod-recipes: verl-v1.0.0-eks` |
| **SageMaker Training Job** | [OK] 公式サポート | `hyperpod-recipes: verl-v1.0.0-smtj` |
| **HyperPod Slurm** | [NG] 非サポート | 提供なし |

### 既知の問題

コミュニティの GitHub Issues では、Slurm 環境での VERL 動作に関する複数の問題が報告されています：

| Issue | 概要 | ステータス |
|-------|------|-----------|
| #523 | Slurm 環境で Ray の CPU 割り当てが不正。`ray.init(num_cpus=<固定値>)` で明示的に指定が必要 | PR #1009 で修正済み |
| #548 | サンプルスクリプトのノード配列パースバグ。マルチノード構成で失敗 | 2026-02-18 時点で未修正 |
| #3406 | Slurm + Ray 環境で DAPO 学習が極端に遅い（3 ノード x 4 H100 構成） | 未解決 |
| (コメント) | "Ray was always uncooperative on Slurm" とのユーザー指摘 | - |

### 推奨事項

:::message alert
HyperPod で VERL を使用する場合は、**EKS 環境を選択することを強く推奨**します。Slurm 環境では AWS 公式サポートがなく、Ray on Slurm の既知の問題（CPU 割り当て、パフォーマンス低下）に直面する可能性があります。VERL は Ray に強く依存しているため、Ray の Slurm サポートの成熟度が使用体験に直結します。
:::

### Checkpointless Training との比較

Spot Instance での運用において、Elastic Training と Checkpointless Training を比較します。

| 機能 | Elastic Training（Spot） | Checkpointless Training |
|------|----------------------|----------------------|
| Spot 中断への対応 | チェックポイント保存後に縮小運転 | メモリ内レプリカで即座に復旧 |
| ダウンタイム | 約 70 秒（Re-rendezvous + 復元） | 約 120 秒（ノード交換を伴う場合。IPR/PLR による復旧は数秒〜数十秒） |
| メモリオーバーヘッド | なし | 約 1.5 倍（冗長レプリカ） |
| ノード障害への耐性 | minReplicas まで（複数ノード対応可能） | 複数ノード同時障害に対応 |
| 適用範囲 | すべての分散学習フレームワーク | NVIDIA NeMo Framework のみ |
| 最小ノード数 | 1 ノード | 通常 8 ノード以上 |

:::message
Checkpointless Training は Spot 中断に対応していますが、**NVIDIA NeMo Framework が必須**であり、メモリオーバーヘッドが約 1.5 倍発生します。Elastic Training は汎用的で、PyTorch FSDP / DeepSpeed / Megatron-LM など任意のフレームワークで使用できます。
:::

### 併用可能性

Checkpointless Training と Elastic Training は、理論的には**階層的な障害復旧**として併用可能です：

| 防衛線 | 技術 | 復旧時間 | 適用シナリオ |
|-------|------|---------|------------|
| 第一防衛線 | Checkpointless (IPR) | 数秒 | GPU エラー、軽微な障害 |
| 第二防衛線 | Checkpointless (PLR) | 数十秒 | プロセスレベル障害 |
| 最終防衛線 | Elastic Training | 約 70 秒 | ノード交換、縮退運転 |

**併用が成立するシナリオ**:
- ノード故障時に同数の新ノードで交換される場合（world_size が変わらない）
- Checkpointless が高速復旧を試み、失敗した場合に Elastic がノード交換に対応

**併用が困難なシナリオ**:
- 頻繁なスケールアップ/ダウン（world_size の変更が頻繁）
- world_size 変更時に Megatron-Core の冗長レプリカ再配置が必要

**重要**: AWS 公式ドキュメントには併用に関する明示的なサポート記載はありません。NeMo Framework + PyTorch Elastic の統合については、実機での検証が必要です。

## まとめ

本記事では、Amazon SageMaker HyperPod Elastic Training の技術的詳細を解説しました。主要なポイントを以下にまとめます。

**アーキテクチャ**: Elastic Training は PyTorch Elastic（`torch.distributed.elastic`）を基盤とし、HyperPod 独自の CRD（`HyperPodPyTorchJob`）と Operator が Kubernetes 上でのジョブ管理を担当します。Rendezvous メカニズムによるワーカーの合流・調整と、`SimpleElasticAgent` のメインループによる障害検出・スケーリングが自動的に処理されます。

**スケーリング**: `ElasticPolicy` でノード数の範囲とステップサイズを定義し、Operator が自動的にスケールアップ/ダウンを実行します。Re-rendezvous には全ワーカーの一時停止を伴うため、`scaleUpSnoozeTimeInSeconds` による頻度制御が重要です。

**有効なユースケース**: 障害復旧（数日に 1 回程度の変更）や実験単位のノード数変更（事前学習→ファインチューニング）では実用的です。一方、GRPO のような毎イテレーションのフェーズ連動型スケーリングは、Re-rendezvous と再シャーディングの繰り返しコストにより現時点では非実用的です。

**GRPO への推奨アプローチ**: 学習クラスタは固定ノード数で運用し、推論プール（vLLM サーバー）のみを弾力的にスケーリングする方式が現実的です。VERL の Colocate Placement と組み合わせることで、リソース効率をさらに改善できます。

**VERL の環境選択**: HyperPod で VERL を使用する場合は EKS 環境を推奨します。Slurm 環境は公式非サポートであり、Ray on Slurm の既知の問題に注意が必要です。

## 参考資料

- [AWS SageMaker HyperPod 公式ドキュメント](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod.html)
- [HyperPod CLI ソースコード](https://github.com/aws/sagemaker-hyperpod-cli)
- [PyTorch Elastic ソースコード](https://github.com/pytorch/pytorch/tree/main/torch/distributed/elastic)
- [PyTorch Elastic: DynamicRendezvousHandler](https://github.com/pytorch/pytorch/blob/main/torch/distributed/elastic/rendezvous/dynamic_rendezvous.py)
- [PyTorch Elastic: C10dRendezvousBackend](https://github.com/pytorch/pytorch/blob/main/torch/distributed/elastic/rendezvous/c10d_rendezvous_backend.py)
- [PyTorch Elastic: SimpleElasticAgent](https://github.com/pytorch/pytorch/blob/main/torch/distributed/elastic/agent/server/api.py)
- [PyTorch DistributedSampler ソースコード](https://github.com/pytorch/pytorch/blob/main/torch/utils/data/distributed.py)
- [Kubeflow Training Operator](https://github.com/kubeflow/training-operator)
- [VERL (Versatile RLHF Library)](https://github.com/volcengine/verl)
- [VERL 公式ドキュメント](https://verl.readthedocs.io/)
- [SageMaker HyperPod Recipes](https://github.com/aws/sagemaker-hyperpod-recipes)
- [AWS Distributed Training サンプル](https://github.com/aws-samples/awsome-distributed-training)
- [FSx for Lustre 公式ドキュメント](https://docs.aws.amazon.com/fsx/latest/LustreGuide/)
- [Amazon EC2 Spot Instance](https://aws.amazon.com/ec2/spot/)
- [AWS Spot Instance Advisor](https://aws.amazon.com/ec2/spot/instance-advisor/)