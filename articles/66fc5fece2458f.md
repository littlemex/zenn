---
title: "[翻訳] 機械学習モデルのパフォーマンスを引き出す - ボトルネックからブレークスルーへ"
emoji: "🐥"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["AWS Neuron"]
published: true
---

https://prod-assets.cosmic.aws.dev/a/34Rd8k5C2MPeg6YzY6NESuQBrvU/blog.webp?imgSize=770x520 の翻訳

# 機械学習モデルのパフォーマンスを引き出す - ボトルネックからブレークスルーへ

機械学習モデルが数兆個のパラメータまでスケールするにつれ、小さな最適化の非効率性が数百万ドルもの不要なインフラコストに膨れ上がる可能性があり、持続可能なML展開にはパフォーマンスプロファイリングが不可欠となっています。AWS Neuron Profilerは、分散環境全体でほぼゼロオーバーヘッドの包括的なパフォーマンス分析を提供することでこの課題に対処し、開発者がモデルがメモリバウンドかコンピュートバウンドかを特定し、それに応じて最適化するのを支援します。

![Sadaf Rasool](https://avatars.builderprofile.aws.dev/34RcM8QFE4L3VhuNbKrSVxSnNy2.webp)

[Sadaf Rasool](/community/@sadafrasool)

**公開日: 2025年10月23日**

---

機械学習は近年急速に進歩しており、ディープラーニングモデルは現在数兆個のパラメータに達しています。このスケールの増大は驚くべき能力を解き放つ一方で、計算コストも押し上げます。2012年以降、最先端モデルのトレーニングコストは数ヶ月ごとにほぼ2倍になっています。慎重な最適化がなければ、これらのワークロードは持続不可能になる可能性があります。

最適化されていないワークロードの影響はスケールで複合的に増大し、インフラコストを膨張させ、スループットを低下させます。さらに悪いことに、十分に活用されていないハードウェアは、支払った分のパフォーマンスが得られていないことを意味します。これが、最適化が機械学習の中心にある理由です。より優れたカーネルやコンパイラ技術から、ハードウェアを考慮したチューニングまで、すべての改善により、モデルはより高速に実行され、より効率的にスケールし、展開が持続可能なものになります。

大規模言語モデル(LLM)推論を例に考えてみましょう。最適化されていないアテンション実装が、最適化されたバージョンと比較して生成トークンあたりわずか2ミリ秒のレイテンシを追加すると仮定します。これは些細に聞こえるかもしれませんが、スケール時の影響は大きくなる可能性があります:

- 典型的なユーザーリクエストは1,000トークンを生成する可能性があり、リクエストあたり2秒の余分なレイテンシが追加されます。
- システムが1日に100万リクエストを処理する場合、それは毎日200万秒(≈23日)の無駄な計算となります。
- MLアクセラレータのクラスタでは、これは月に数万時間の追加となり、クラウドコストを押し上げながらユーザーエクスペリエンスを低下させます。

この非効率性を月に数千億トークンにスケールした場合を想像してください—わずか数ミリ秒の追加レイテンシから始まったものが、数百万ドルもの不要なインフラコストに膨れ上がる可能性があります。モデルが基盤となるハードウェアをどのように活用しているかを理解することで、上記の最適化されていないアテンション実装のような、さらなる最適化が必要なモデルの非効率的な部分を明らかにすることができます。これらの活用不足の領域に焦点を当てて対処することで、機械学習ワークロードの全体的なコスト効率を改善できます。

パフォーマンス特性に基づいて、機械学習モデルは**メモリバウンド**または**コンピュートバウンド**に分類できます。メモリバウンドのシナリオは、頻繁なDMA転送が実行時間を支配し、HBMとSRAM間のデータのロードとアンロードに大部分の時間が費やされる場合に発生します。対照的に、コンピュートバウンドのシナリオは、メモリ帯域幅が十分であるが、コンピュートエンジンが様々な命令実行で完全に占有されている場合に発生します。

![](https://prod-assets.cosmic.aws.dev/a/34Rd8k5C2MPeg6YzY6NESuQBrvU/blog.webp?imgSize=770x520 "arithmetic_intensity")

メモリバウンドの機械学習モデルでは、パフォーマンスはハードウェアの純粋な計算能力によってではなく、メモリとプロセッサ間でデータを移動できる速度によって制限されます。このシナリオでは、MLアクセラレータが大規模な計算能力を持っていても、データを待ってアイドル状態になり、活用率の低下と推論またはトレーニング時間の膨張につながります。これは、アテンションや大規模なMLPレイヤーのような操作で特に問題となり、メモリへの繰り返しの読み書きが実行時間を支配する可能性があります。理想的には、ハードウェアがデータ転送を待つのではなく、実際の数学演算(FLOP)を実行することに大部分の時間を費やす、コンピュートバウンドのモデルを目指します。

では、これをどこから修正し始めればよいのでしょうか? そこでプロファイラの出番です。プロファイラは内部で何が起こっているかを明らかにします:どのレイヤーが最も時間を消費しているか、モデルがコンピュートバウンドかメモリバウンドか、非効率的なアテンションや冗長なMLP操作のようなボトルネックがどこに隠れているか。推測する代わりに、ML実践者は無駄なミリ秒の原因となる正確なホットスポットを特定できます。プロファイラは、操作に費やされた時間、メモリ使用量、データ転送、計算効率を分解することで、モデルがハードウェアとどのように相互作用するかを可視化します。

![](https://prod-assets.cosmic.aws.dev/a/34RdSQXM9ubGoKuHNMjJjDVbsML/blog.webp?imgSize=556x340 "performance_optimization_process")

機械学習モデルのプロファイリングは継続的なプロセスです。モデルをプロファイリングして時間とリソースがどこに費やされているかを把握し、次にそれらの結果を分析してボトルネックを特定します。それらの洞察に基づいて、ターゲットを絞った最適化を適用します—メモリ転送の削減、カーネルの融合、バッチサイズのチューニングなど。変更が加えられたら、再度プロファイリングして改善を検証し、新しい非効率性を明らかにします。

AWS Trainium/Inferentiaの世界では、Neuron profilerがモデルのトレーニングと展開時のボトルネックを特定する上で重要な役割を果たします。モデルのパフォーマンスを向上させるために最適化努力をどこに集中すべきかを導きます。この分析は、パフォーマンスの制限がメモリアクセスパターンによるものか計算強度によるものかを正確に特定するため、ハードウェア活用を最適化する際に特に重要になります。モデルプロファイリングが提供する洞察により、開発者はシステム効率を最大化し、最適なハードウェアパフォーマンスを達成するための最も影響力のある領域にリソースを集中できます。

私たちは、実践的な例を通じて[Neuron Profiler](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/tools/neuron-sys-tools/neuron-profile-user-guide.html)の機能を説明する一連のブログを公開する予定です。この強力なツールがモデルのパフォーマンスに関する貴重な洞察をどのように提供するかを紹介します。ML開発者が最適化の機会を特定し、モデルの動作を詳細に理解するのに役立つ主要な指標とメトリクスを説明します。

## Neuron Profilerの主要機能と能力

### 包括的なパフォーマンス分析

Neuron Profilerは、デバイスレベルとシステムレベルの両方でパフォーマンスプロファイルをキャプチャできます。デバイスレベルのプロファイリングは、コンピュートエンジン全体の命令処理とDMAデータ移動を追跡することで、モデル実行への深い可視性を提供します。対照的に、システムレベルのプロファイリングはより広い視野を提供し、ワークフロー全体のパフォーマンスボトルネックと非効率性を特定するのに役立ちます。

### ほぼゼロのオーバーヘッド

Neuron Profilerは、Neuronデバイスの組み込みハードウェア機能を活用して、最小限のオーバーヘッドでパフォーマンスデータを収集します。従来のソフトウェアベースのプロファイリング方法ではなく、専用のオンチップコンポーネントを利用することで、モデルの実行速度に影響を与えることなくパフォーマンスメトリクスをキャプチャし、最適なパフォーマンスを維持しながら正確な測定を保証します。

### マルチワーカー＆マルチノードサポート

Neuron Profilerは、複数のNeuronCore、デバイス、ノード全体のパフォーマンスを効果的に監視および分析できる、分散モデル展開用に設計された専門ツールです。シングルデバイス設定から複雑なマルチノード環境まで、様々な構成でパフォーマンスデータをシームレスにキャプチャし、正確なタイミングとメトリクスを維持します。これにより、大規模な機械学習実装、特に分散設定でcollectives対応のNEFFをプロファイリングする際に、開発者にとって不可欠なツールとなります。

### インタラクティブな可視化

Neuron ProfilerのWebベースビューアは、インタラクティブな可視化機能を通じてMLモデルの最適化を簡素化する強力なツールです。操作のタイミングとDMAアクティビティを示す詳細な実行タイムラインを表示し、ズームとナビゲーションのための直感的なコントロールを備えています。ユーザーはプロファイルデータを効率的に検索し、フレームワークレイヤーに基づいてビューをカスタマイズし、共同デバッグのための永続的な注釈を追加できます。これらの機能により、Neuronデバイス上のモデルパフォーマンスを分析および最適化するための不可欠なツールとなっています。

![](https://prod-assets.cosmic.aws.dev/a/34RdZiNzsDgcF0MjRFiX74Y3fV8/blog.webp?imgSize=1000x207 "interactive_visualization")

### Perfetto統合

高度なトレース分析と可視化のために、AWS Neuron Profilerは人気のオープンソース分析ツールキットであるPerfettoとシームレスに統合されます。この統合により、ユーザーはPerfettoの強力なユーザーインターフェイスを通じてプロファイリングデータを可視化および分析する代替方法を提供します。この機能を活用するには、単一のコマンドを使用してプロファイルデータをPerfettoのトレース形式に簡単に変換できます:

```
$ neuron-profile view -n file.neff -s profile.ntff --output-format perfetto
```

これによりプロファイルが処理され、`ntff.pftrace`ファイルが生成されます。プロファイルを可視化するには、`ui.perfetto.dev`にアクセスして、生成された`ntff.pftrace`ファイルをアップロードするだけです。

![](https://prod-assets.cosmic.aws.dev/a/34RdgUPXWGQ6B0OGgFukuVRcxYO/blog.webp?imgSize=1000x585 "perfetto_integration")

この投稿は、機械学習ワークロードのプロファイリングの重要性を強調し、Neuron Profilerの機能の高レベルな概要を提供します。次のステップとして、この基礎の上に実践的な例を構築します。このブログシリーズは以下のパートで構成されます:

1. [**AWS TrainiumとInferentiaでNeuron Profilerを使用したモデルプロファイリングの開始**](https://builder.aws.com/content/33FrJL2E97pPBNrqmYmvCRIHMew/getting-started-with-model-profiling-on-aws-trainium-and-inferentia-using-aws-neuron-profiler)
2. [**AWS Neuron ProfilerによるNKIカーネルパフォーマンスのデコード**](https://builder.aws.com/content/34Ru44lIq9QrlPgr16BvDm78F3G/decoding-nki-kernel-performance-using-aws-neuron-profiler)
3. **AWS Neuron Profilerによるトランスフォーマーベースモデルのパフォーマンス最適化機会の分析**
4. **最新のNeuron Profilerの解説: よりスマートに、より高速に、より強力に**

[今後の投稿](https://builder.aws.com/content/33FrJL2E97pPBNrqmYmvCRIHMew/getting-started-with-model-profiling-on-aws-trainium-and-inferentia-using-aws-neuron-profiler)では、Neuron Profilerを実際に機械学習モデルのプロファイリングにどのように適用できるかを詳しく説明します。

---

このブログは、Esha Lakhotia、Scott Perry、Sadaf Rasoolによって執筆されました。

**Esha Lakhotia**は、AWSのAnnapurna Labsのプロダクトマネージャーです。彼女は、AWSのAIチップ用の開発者ツールを構築することで、顧客がMLパフォーマンス最適化目標を達成できるようにしています。AWS TrainiumとAWS Inferentia上で顧客が機械学習ワークロードをプロファイル、デバッグ、加速するのに役立つ直感的なツール体験の作成に注力しています。

**Scott Perry**は、AWSのAnnapurna MLアクセラレータチームのソリューションアーキテクトです。カナダを拠点とし、AWS InferentiaとAWS Trainiumを使用したディープラーニングのトレーニングと推論ワークロードの展開と最適化を顧客が行うのを支援しています。彼の関心分野には、大規模言語モデル、深層強化学習、IoT、ゲノミクスが含まれます。

**Sadaf Rasool**は、AWSのAnnapurna Labsのソリューションアーキテクトです。Sadafは顧客と協力して、重要なビジネス課題に対処する機械学習ソリューションを設計しています。顧客がAWS TrainiumまたはAWS Inferentiaチップを活用して機械学習モデルをトレーニングおよび展開し、イノベーションの旅を加速するのを支援しています。

---

この翻訳には、以下の4つの画像URLが含まれています:
1. https://prod-assets.cosmic.aws.dev/a/34Rd8k5C2MPeg6YzY6NESuQBrvU/blog.webp?imgSize=770x520
2. https://prod-assets.cosmic.aws.dev/a/34RdSQXM9ubGoKuHNMjJjDVbsML/blog.webp?imgSize=556x340
3. https://prod-assets.cosmic.aws.dev/a/34RdZiNzsDgcF0MjRFiX74Y3fV8/blog.webp?imgSize=1000x207
4. https://prod-assets.cosmic.aws.dev/a/34RdgUPXWGQ6B0OGgFukuVRcxYO/blog.webp?imgSize=1000x585