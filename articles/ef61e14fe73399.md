---
title: "vllm-neuron ã‚’ç”¨ã„ãŸ LLM æ¨è«–ã¨ãã®æ¸¬å®šæ–¹æ³•ã®èª¿æŸ»"
emoji: "âš¡"
type: "tech"
topics: ["AWS", "Inferentia2", "vLLM", "Neuron", "æ©Ÿæ¢°å­¦ç¿’"]
published: true
---

## ã¯ã˜ã‚ã«

æœ¬è¨˜äº‹ã§ã¯ã€[vllm-neuron](https://github.com/vllm-project/vllm-neuron) ã‚’ä½¿ç”¨ã—ã¦æ¨è«–æ€§èƒ½ã‚’ç°¡æ˜“ã«è¨ˆæ¸¬ã™ã‚‹æ–¹æ³•ã‚’ã€å®Ÿéš›ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã®ä¸€ä¾‹ã¨ã¨ã‚‚ã«è§£èª¬ã—ã¾ã™ã€‚ç‰¹ã«ã€ç°¡æ˜“ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè£…ä¾‹ã«ç„¦ç‚¹ã‚’å½“ã¦ã€prefix caching ã‚„ bucketing ã¨ã„ã£ãŸæ‰‹æ³•ã®è¨­å®šã®å½±éŸ¿ã‚’å®Ÿéš›ã«è©•ä¾¡ã—ã¾ã™ã€‚

### vllm-neuron ã¨ã¯

vLLM ã¯ç¶™ç¶šçš„ãªãƒãƒƒãƒå‡¦ç†ãªã©ã®é«˜åº¦ãªæ¨è«–æ©Ÿèƒ½ã‚’æ´»ç”¨ã—ãŸ LLM æ¨è«–ãŠã‚ˆã³ã‚µãƒ¼ãƒ“ã‚¹æä¾›ã®ãŸã‚ã®äººæ°—ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚vllm-neuron ã¯ã€é«˜é€Ÿãª LLM æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã§ã‚ã‚‹ vLLM ã‚’ AWS Neuron SDK ã¨çµ±åˆã—ãŸã‚‚ã®ã§ã™ã€‚é€šå¸¸ã® vLLM ãŒ NVIDIA GPU ä¸Šã§ CUDA ã‚’ä½¿ç”¨ã™ã‚‹ã®ã«å¯¾ã—ã€vllm-neuron ã¯ AWS Inferentia ã¨ Trainium ãƒãƒƒãƒ—ä¸Šã§å‹•ä½œã—ã¾ã™ã€‚

vllm-neuron ã¯å†…éƒ¨ã§ NxD Inference (neuronx-distributed-inference) ã‚’ç”¨ã„ã¦ãŠã‚Šã€`inference_demo` ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ vLLM ã¨åŒã˜ã‚ˆã†ãªè¨­å®šæ–¹æ³•ã§ä½¿ãˆã‚‹ã‚ˆã†ã«ã—ãŸãƒ©ãƒƒãƒ‘ãƒ¼çš„ãªä½ç½®ä»˜ã‘ã§ã™ã€‚å…·ä½“çš„ã«ã¯ã€ä»¥ä¸‹ã®ãƒ—ãƒ­ã‚»ã‚¹ã§å‹•ä½œã—ã¾ã™ã€‚

1. **ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰**: Hugging Face å½¢å¼ã®ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
2. **ã‚°ãƒ©ãƒ•ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«**: Neuron Compiler ãŒ PyTorch ãƒ¢ãƒ‡ãƒ«ã‚’ Neuron å®Ÿè¡Œå¯èƒ½ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ (NEFF) ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
3. **äº‹å‰ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—**: æ§˜ã€…ãªå…¥åŠ›å½¢çŠ¶ã«å¯¾ã—ã¦ã‚°ãƒ©ãƒ•ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ« (bucketing ä½¿ç”¨æ™‚)
4. **æ¨è«–å®Ÿè¡Œ**: ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ã‚°ãƒ©ãƒ•ã‚’ä½¿ç”¨ã—ã¦é«˜é€Ÿæ¨è«–ã‚’å®Ÿè¡Œ

ã“ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ã€NVIDIA GPU ã«ãŠã‘ã‚‹ CUDA ã‚°ãƒ©ãƒ•ã®æ¦‚å¿µã¨é¡ä¼¼ã—ã¦ã„ã¾ã™ã€‚vLLM ãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ CUDA ã‚°ãƒ©ãƒ•ã‚’ä½¿ç”¨ã—ã¦ GPU ä¸Šã§ã‚«ãƒ¼ãƒãƒ«èµ·å‹•ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’å‰Šæ¸›ã™ã‚‹ã®ã¨åŒæ§˜ã«ã€vllm-neuron ã¯äº‹å‰ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã•ã‚ŒãŸ Neuron ã‚°ãƒ©ãƒ•ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€æ¨è«–æ™‚ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’æœ€å°åŒ–ã—ã¾ã™ã€‚

```mermaid
sequenceDiagram
    participant App as ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³
    participant vLLM as vllm-neuron
    participant Compiler as Neuron Compiler
    participant Runtime as Neuron Runtime
    participant Chip as Inferentia2

    App->>vLLM: LLM ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
    vLLM->>Compiler: ãƒ¢ãƒ‡ãƒ«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«è¦æ±‚
    Compiler->>Compiler: PyTorch â†’ NEFF å¤‰æ›
    Compiler-->>vLLM: ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ã‚°ãƒ©ãƒ•
    vLLM->>Runtime: ã‚°ãƒ©ãƒ•ãƒ­ãƒ¼ãƒ‰
    Runtime->>Chip: ã‚°ãƒ©ãƒ•é…ç½®

    App->>vLLM: æ¨è«–ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
    vLLM->>Runtime: ãƒã‚±ãƒƒãƒˆã‚µã‚¤ã‚ºã«ä¸¸ã‚ã¦å®Ÿè¡Œ
    Runtime->>Chip: æ¨è«–å®Ÿè¡Œ
    Chip-->>Runtime: çµæœ
    Runtime-->>vLLM: çµæœ
    vLLM-->>App: æ¨è«–çµæœ

    Note over Compiler,Runtime: åˆå›ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã¯æ•°åˆ†ï½æ•°ååˆ†<br/>2å›ç›®ä»¥é™ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨ã§é«˜é€Ÿ

    rect rgb(230, 245, 255)
        Note over vLLM,Chip: CUDA ã‚°ãƒ©ãƒ•ã¨åŒæ§˜ã®æ¦‚å¿µ:<br/>äº‹å‰ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ã‚°ãƒ©ãƒ•ã§<br/>å®Ÿè¡Œæ™‚ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰å‰Šæ¸›
    end
```

é‡è¦ãªç‚¹ã¨ã—ã¦ã€Neuron Compiler ã¯åˆå›å®Ÿè¡Œæ™‚ã«æ•°åˆ†ã‹ã‚‰æ•°ååˆ†ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚é–“ã‚’è¦ã—ã€ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«çµæœã¯ `/var/tmp/neuron-compile-cache/` ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã¾ã™ã€‚2 å›ç›®ä»¥é™ã®å®Ÿè¡Œã§ã¯ã€ã“ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå†åˆ©ç”¨ã•ã‚Œã‚‹ãŸã‚ã€èµ·å‹•æ™‚é–“ãŒå¤§å¹…ã«çŸ­ç¸®ã•ã‚Œã¾ã™ã€‚

## ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè£…

æœ¬ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€vllm-neuron ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè£…ã‚’è§£èª¬ã—ã¾ã™ã€‚ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€JSON è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚ˆã‚‹æŸ”è»Ÿãªãƒ†ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³å®šç¾©ã‚’ã‚µãƒãƒ¼ãƒˆã—ã€è¤‡æ•°ã®è¨­å®šã‚’è‡ªå‹•çš„ã«ãƒ†ã‚¹ãƒˆã—ã¦çµæœã‚’æ¯”è¼ƒã§ãã¾ã™ã€‚è§£èª¬ã®å®¹æ˜“ã•ã‚’é‡è¦–ã™ã‚‹ãŸã‚ã«ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã€ãƒ¡ã‚½ãƒƒãƒ‰æ•°ãªã©ã‚’æ¥µåŠ›å‰Šæ¸›ã™ã‚‹å½¢ã¨ã—ã¦ã„ã¾ã™ã€‚è¨ˆæ¸¬ã§è¿½åŠ ã™ã¹ããƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒã‚ã‚Œã° AI ã«ãŠé¡˜ã„ã—ã¦è¿½åŠ å®Ÿè£…ã—ã¦ã‚‚ã‚‰ã„ã¾ã—ã‚‡ã†ï¼

### æ¦‚è¦

ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚·ã‚¹ãƒ†ãƒ ã¯ã€ä»¥ä¸‹ã® 3 ã¤ã®ä¸»è¦ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‹ã‚‰æ§‹æˆã•ã‚Œã¾ã™ã€‚

1. **JSON è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«**: ãƒ†ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã¨å…±é€šè¨­å®šã‚’å®šç¾©
2. **Bash ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ** (`run_offline_benchmark.sh`): è¨­å®šã‚’èª­ã¿è¾¼ã¿ã€å„ãƒ†ã‚¹ãƒˆã‚’é †æ¬¡å®Ÿè¡Œ
3. **Python ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆ** (`offline_benchmark.py`): vLLM ã‚’ç›´æ¥ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã¦æ¸¬å®šã‚’å®Ÿè¡Œ

ã“ã®è¨­è¨ˆã«ã‚ˆã‚Šã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å¤‰æ›´ã™ã‚‹ã“ã¨ãªãã€JSON ãƒ•ã‚¡ã‚¤ãƒ«ã®ç·¨é›†ã®ã¿ã§æ§˜ã€…ãªãƒ†ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å®šç¾©ã§ãã¾ã™ã€‚

### JSON è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®æ§‹é€ 

JSON è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€`base_config` ã¨ `test_configs` ã® 2 ã¤ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§æ§‹æˆã•ã‚Œã¾ã™ã€‚ä»¥ä¸‹ã¯ã€prefix caching ã¨ bucketing ã® 4 é€šã‚Šã®çµ„ã¿åˆã‚ã›ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹è¨­å®šä¾‹ã§ã™ã€‚

::::details JSON è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä¾‹
```json:test_cache_bucketing_comparison.json
{
  "model": "./models/Qwen3-0.6B-Reranker",
  "tensor_parallel_size": 2,
  "base_config": {
    "max_model_len": 2048,
    "block_size": 32,
    "input_file": "./sample.csv",
    "search_num": 20,
    "top_n": 6,
    "batch_size": 8,
    "max_length": 1500
  },
  "test_configs": [
    {
      "name": "cache_ON_bucketing_ON",
      "max_num_seqs": 4,
      "max_num_batched_tokens": 256,
      "pa_num_blocks": 512,
      "enable_prefix_caching": true,
      "enable_bucketing": true
    },
    {
      "name": "cache_ON_bucketing_OFF",
      "max_num_seqs": 4,
      "max_num_batched_tokens": 256,
      "pa_num_blocks": 512,
      "enable_prefix_caching": true,
      "enable_bucketing": false
    },
    {
      "name": "cache_OFF_bucketing_ON",
      "max_num_seqs": 4,
      "max_num_batched_tokens": 256,
      "pa_num_blocks": 512,
      "enable_prefix_caching": false,
      "enable_bucketing": true
    },
    {
      "name": "cache_OFF_bucketing_OFF",
      "max_num_seqs": 4,
      "max_num_batched_tokens": 256,
      "pa_num_blocks": 512,
      "enable_prefix_caching": false,
      "enable_bucketing": false
    }
  ]
}
```

#### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è©³ç´°

**ãƒˆãƒƒãƒ—ãƒ¬ãƒ™ãƒ«è¨­å®š**:
- `model`: Hugging Face ãƒ¢ãƒ‡ãƒ« ID ã¾ãŸã¯ãƒ­ãƒ¼ã‚«ãƒ«ãƒ‘ã‚¹
- `tensor_parallel_size`: Tensor Parallelism ã®ä¸¦åˆ—åº¦

**base_config** (å…¨ãƒ†ã‚¹ãƒˆã§å…±é€š):
- `max_model_len`: ãƒ¢ãƒ‡ãƒ«ãŒå‡¦ç†å¯èƒ½ãªæœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·
- `block_size`: PagedAttention ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚µã‚¤ã‚º
- `input_file`: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã® CSV ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
- `search_num`: Rerank ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
- `top_n`: æœ€çµ‚çš„ã«è¿”ã™ä¸Šä½ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
- `batch_size`: æ¨è«–æ™‚ã®ãƒãƒƒãƒã‚µã‚¤ã‚º
- `max_length`: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°

**test_configs** (å„ãƒ†ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³):
- `name`: è¨­å®šã®è­˜åˆ¥å (çµæœãƒ•ã‚¡ã‚¤ãƒ«åã«ä½¿ç”¨)
- `max_num_seqs`: åŒæ™‚ã«å‡¦ç†ã™ã‚‹æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹æ•°
- `max_num_batched_tokens`: ãƒãƒƒãƒå†…ã®æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°
- `pa_num_blocks`: PagedAttention ã§ä½¿ç”¨ã™ã‚‹ãƒ¡ãƒ¢ãƒªãƒ–ãƒ­ãƒƒã‚¯æ•°
- `enable_prefix_caching`: Prefix Caching ã®æœ‰åŠ¹åŒ– (true/false)
- `enable_bucketing`: Bucketing ã®æœ‰åŠ¹åŒ– (true/false)
::::

### Bash ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

`run_offline_benchmark.sh` ã¯ã€JSON è¨­å®šã‚’èª­ã¿è¾¼ã¿ã€å„ãƒ†ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é †æ¬¡å®Ÿè¡Œã™ã‚‹ãƒ©ãƒƒãƒ‘ãƒ¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã™ã€‚ä»¥ä¸‹ã¯ã€ä¸»è¦éƒ¨åˆ†ã®å®Ÿè£…ã‚’æŠœç²‹ã—ãŸã‚‚ã®ã§ã™ã€‚

::::details Bash ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆä¾‹
```bash:run_offline_benchmark.sh
#!/bin/bash

# vllm-neuron ç’°å¢ƒã®æœ‰åŠ¹åŒ–ï¼ˆå¿…è¦ã«å¿œã˜ã¦æœ€é©ãªãƒ‘ã‚¹ã«å¤‰ãˆã¦ãã ã•ã„ï¼‰
source /opt/aws_neuronx_venv_pytorch_inference_vllm/bin/activate

CONFIG_FILE="${CONFIG_FILE:-}"

if [[ -n "$CONFIG_FILE" && -f "$CONFIG_FILE" ]]; then
    echo "JSON Configuration Mode"

    # çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    TIMESTAMP=$(date +%Y_%m_%d_%H_%M)
    RESULTS_DIR="./benchmark_results_${TIMESTAMP}"
    mkdir -p "$RESULTS_DIR"

    # åŸºæœ¬è¨­å®šã®æŠ½å‡º
    MODEL=$(python3 -c "import json; f=open('$CONFIG_FILE'); d=json.load(f); print(d.get('model', './models/Qwen3-0.6B-Reranker'))")
    TP=$(python3 -c "import json; f=open('$CONFIG_FILE'); d=json.load(f); print(d.get('tensor_parallel_size', 2))")

    # ãƒ†ã‚¹ãƒˆè¨­å®šæ•°ã®å–å¾—
    NUM_CONFIGS=$(python3 -c "import json; f=open('$CONFIG_FILE'); d=json.load(f); print(len(d.get('test_configs', [])))")

    # å„ãƒ†ã‚¹ãƒˆè¨­å®šã®å‡¦ç†
    for i in $(seq 0 $(($NUM_CONFIGS - 1))); do
        # è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æŠ½å‡º
        CONFIG_NAME=$(python3 -c "import json; f=open('$CONFIG_FILE'); d=json.load(f); print(d['test_configs'][$i].get('name', 'config_$i'))")
        MAX_NUM_SEQS=$(python3 -c "import json; f=open('$CONFIG_FILE'); d=json.load(f); print(d['test_configs'][$i]['max_num_seqs'])")

        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        python3 offline_benchmark.py \
            --model "$MODEL" \
            --tp "$TP" \
            --max-num-seqs "$MAX_NUM_SEQS" \
            --output-file "$RESULTS_DIR/${CONFIG_NAME}.json" \
            > "$RESULTS_DIR/${CONFIG_NAME}.log" 2>&1
    done

    # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    python3 << 'EOF'
import json
import glob
import os

# å…¨çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿
result_files = sorted(glob.glob(f"{results_dir}/*.json"))
results = []

for f in result_files:
    with open(f) as file:
        data = json.load(file)
        config_name = os.path.basename(f).replace('.json', '')
        results.append({
            'name': config_name,
            'metrics': data['metrics']
        })

# ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã§ã‚½ãƒ¼ãƒˆ
sorted_results = sorted(results, key=lambda x: x['metrics']['throughput_queries_per_sec'], reverse=True)

# Markdown ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
# (è©³ç´°ã¯çœç•¥)
EOF
fi
```

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ã€‚

1. **ãƒ«ãƒ¼ãƒ—å‡¦ç†**: `test_configs` é…åˆ—ã®å„è¦ç´ ã«å¯¾ã—ã¦é †æ¬¡ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
2. **çµæœã®é›†ç´„**: ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆå®Œäº†å¾Œã€Python åŸ‹ã‚è¾¼ã¿ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒçµæœã‚’èª­ã¿è¾¼ã¿ã€ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã§ã‚½ãƒ¼ãƒˆã—ãŸ Markdown ãƒ¬ãƒãƒ¼ãƒˆã‚’è‡ªå‹•ç”Ÿæˆã—ã¾ã™ã€‚
::::

::::details å®Œå…¨ç‰ˆ
ä»¥ä¸‹ã«å®Œå…¨ç‰ˆã‚’ç¤ºã—ã¦ãŠãã¾ã™ã€‚

```bash:run_offline_benchmark.sh
#!/bin/bash

# Activate vLLM-Neuron environment
source /opt/aws_neuronx_venv_pytorch_inference_vllm/bin/activate

# ==============================================================================
# Flexible Offline Benchmark Runner
# Supports both JSON configuration and environment variable modes
# ==============================================================================

CONFIG_FILE="${CONFIG_FILE:-}"

if [[ -n "$CONFIG_FILE" && -f "$CONFIG_FILE" ]]; then
    echo "================================================================================"
    echo "JSON Configuration Mode"
    echo "================================================================================"
    echo "Configuration file: $CONFIG_FILE"
    echo

    # Create results directory
    TIMESTAMP=$(date +%Y_%m_%d_%H_%M)
    RESULTS_DIR="./benchmark_results_${TIMESTAMP}"
    mkdir -p "$RESULTS_DIR"

    echo "Results directory: $RESULTS_DIR"
    echo

    # Extract base configuration
    MODEL=$(python3 -c "import json; f=open('$CONFIG_FILE'); d=json.load(f); print(d.get('model', './models/Qwen3-0.6B-Reranker'))")
    TP=$(python3 -c "import json; f=open('$CONFIG_FILE'); d=json.load(f); print(d.get('tensor_parallel_size', 2))")

    BASE_MAX_MODEL_LEN=$(python3 -c "import json; f=open('$CONFIG_FILE'); d=json.load(f); print(d.get('base_config', {}).get('max_model_len', 2048))")
    BASE_BLOCK_SIZE=$(python3 -c "import json; f=open('$CONFIG_FILE'); d=json.load(f); print(d.get('base_config', {}).get('block_size', 32))")
    BASE_INPUT_FILE=$(python3 -c "import json; f=open('$CONFIG_FILE'); d=json.load(f); print(d.get('base_config', {}).get('input_file', './sample.csv'))")
    BASE_SEARCH_NUM=$(python3 -c "import json; f=open('$CONFIG_FILE'); d=json.load(f); print(d.get('base_config', {}).get('search_num', 20))")
    BASE_TOP_N=$(python3 -c "import json; f=open('$CONFIG_FILE'); d=json.load(f); print(d.get('base_config', {}).get('top_n', 6))")
    BASE_BATCH_SIZE=$(python3 -c "import json; f=open('$CONFIG_FILE'); d=json.load(f); print(d.get('base_config', {}).get('batch_size', 8))")
    BASE_MAX_LENGTH=$(python3 -c "import json; f=open('$CONFIG_FILE'); d=json.load(f); print(d.get('base_config', {}).get('max_length', 1500))")

    # Get number of test configs
    NUM_CONFIGS=$(python3 -c "import json; f=open('$CONFIG_FILE'); d=json.load(f); print(len(d.get('test_configs', [])))")

    echo "Base configuration:"
    echo "  Model: $MODEL"
    echo "  Tensor Parallel: $TP"
    echo "  Max Model Len: $BASE_MAX_MODEL_LEN"
    echo "  Block Size: $BASE_BLOCK_SIZE"
    echo "  Input File: $BASE_INPUT_FILE"
    echo
    echo "Number of test configurations: $NUM_CONFIGS"
    echo

    # Process each test configuration
    for i in $(seq 0 $(($NUM_CONFIGS - 1))); do
        echo "================================================================================"
        echo "Test Configuration $((i + 1))/$NUM_CONFIGS"
        echo "================================================================================"

        # Extract configuration for this test
        CONFIG_NAME=$(python3 -c "import json; f=open('$CONFIG_FILE'); d=json.load(f); print(d['test_configs'][$i].get('name', 'config_$i'))")
        MAX_NUM_SEQS=$(python3 -c "import json; f=open('$CONFIG_FILE'); d=json.load(f); print(d['test_configs'][$i]['max_num_seqs'])")
        MAX_BATCHED_TOKENS=$(python3 -c "import json; f=open('$CONFIG_FILE'); d=json.load(f); print(d['test_configs'][$i]['max_num_batched_tokens'])")
        PA_NUM_BLOCKS=$(python3 -c "import json; f=open('$CONFIG_FILE'); d=json.load(f); print(d['test_configs'][$i]['pa_num_blocks'])")

        # Optional parameters with defaults
        ENABLE_PREFIX_CACHING=$(python3 -c "import json; f=open('$CONFIG_FILE'); d=json.load(f); print('--enable-prefix-caching' if d['test_configs'][$i].get('enable_prefix_caching', False) else '')")
        ENABLE_BUCKETING=$(python3 -c "import json; f=open('$CONFIG_FILE'); d=json.load(f); print('--enable-bucketing' if d['test_configs'][$i].get('enable_bucketing', False) else '')")
        KV_CACHE_QUANT=$(python3 -c "import json; f=open('$CONFIG_FILE'); d=json.load(f); print('--kv-cache-quant' if d['test_configs'][$i].get('kv_cache_quant', False) else '')")
        SEQUENCE_PARALLEL=$(python3 -c "import json; f=open('$CONFIG_FILE'); d=json.load(f); print('--sequence-parallel' if d['test_configs'][$i].get('sequence_parallel', False) else '')")
        FLASH_DECODING=$(python3 -c "import json; f=open('$CONFIG_FILE'); d=json.load(f); print('--flash-decoding' if d['test_configs'][$i].get('flash_decoding', False) else '')")

        OUTPUT_FILE="$RESULTS_DIR/${CONFIG_NAME}.json"
        LOG_FILE="$RESULTS_DIR/${CONFIG_NAME}.log"

        echo "Configuration: $CONFIG_NAME"
        echo "  max_num_seqs: $MAX_NUM_SEQS"
        echo "  max_batched_tokens: $MAX_BATCHED_TOKENS"
        echo "  pa_num_blocks: $PA_NUM_BLOCKS"
        echo "  enable_prefix_caching: $(echo $ENABLE_PREFIX_CACHING | grep -q 'enable-prefix-caching' && echo 'true' || echo 'false')"
        echo "  enable_bucketing: $(echo $ENABLE_BUCKETING | grep -q 'enable-bucketing' && echo 'true' || echo 'false')"
        echo "  Output: $OUTPUT_FILE"
        echo

        # Run benchmark
        python3 offline_benchmark.py \
            --model "$MODEL" \
            --tp "$TP" \
            --max-num-seqs "$MAX_NUM_SEQS" \
            --max-num-batched-tokens "$MAX_BATCHED_TOKENS" \
            --max-model-len "$BASE_MAX_MODEL_LEN" \
            --block-size "$BASE_BLOCK_SIZE" \
            --pa-num-blocks "$PA_NUM_BLOCKS" \
            $ENABLE_PREFIX_CACHING \
            $ENABLE_BUCKETING \
            $KV_CACHE_QUANT \
            $SEQUENCE_PARALLEL \
            $FLASH_DECODING \
            --input-file "$BASE_INPUT_FILE" \
            --output-file "$OUTPUT_FILE" \
            --search-num "$BASE_SEARCH_NUM" \
            --top-n "$BASE_TOP_N" \
            --batch-size "$BASE_BATCH_SIZE" \
            --max-length "$BASE_MAX_LENGTH" \
            > "$LOG_FILE" 2>&1

        if [[ $? -eq 0 ]]; then
            echo "âœ“ Test completed successfully"
        else
            echo "âœ— Test failed (check $LOG_FILE)"
        fi
        echo
    done

    # Generate summary report
    echo "================================================================================"
    echo "Generating Summary Report"
    echo "================================================================================"

    SUMMARY_FILE="$RESULTS_DIR/summary.md"

    # Pass variables to Python via environment
    export RESULTS_DIR_FOR_PYTHON="$RESULTS_DIR"
    export CONFIG_FILE_FOR_PYTHON="$CONFIG_FILE"
    export SUMMARY_FILE_FOR_PYTHON="$SUMMARY_FILE"

    python3 << 'EOF'
import json
import glob
import os
from datetime import datetime

results_dir = os.environ['RESULTS_DIR_FOR_PYTHON']
config_file = os.environ['CONFIG_FILE_FOR_PYTHON']

# Read all result JSON files
result_files = sorted(glob.glob(f"{results_dir}/*.json"))
results = []

for f in result_files:
    try:
        with open(f) as file:
            data = json.load(file)
            config_name = os.path.basename(f).replace('.json', '')
            results.append({
                'name': config_name,
                'config': data['configuration'],
                'metrics': data['metrics']
            })
    except Exception as e:
        print(f"Warning: Failed to read {f}: {e}")

if not results:
    print("No results found")
    exit(1)

# Generate markdown summary
summary = f"""# Benchmark Summary Report

**Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**Configuration File**: {config_file}
**Results Directory**: {results_dir}
**Total Configurations Tested**: {len(results)}

---

## Results Comparison

| Rank | Configuration | Throughput<br>(queries/sec) | Avg Time<br>(sec) | P50<br>(sec) | P95<br>(sec) | P99<br>(sec) |
|------|--------------|---------------------------|-----------------|----------|----------|----------|
"""

# Sort by throughput (descending)
sorted_results = sorted(results, key=lambda x: x['metrics']['throughput_queries_per_sec'], reverse=True)

medals = ['ğŸ¥‡', 'ğŸ¥ˆ', 'ğŸ¥‰']
for idx, r in enumerate(sorted_results):
    rank = idx + 1
    medal = medals[idx] if idx < 3 else ''
    name = r['name']
    metrics = r['metrics']

    summary += f"| {medal} **{rank}** | {name} | **{metrics['throughput_queries_per_sec']:.4f}** | {metrics['avg_time_per_query']:.4f} | {metrics['p50_time']:.4f} | {metrics['p95_time']:.4f} | {metrics['p99_time']:.4f} |\n"

summary += "\n---\n\n## Detailed Results\n\n"

# Detailed results for each configuration
for idx, r in enumerate(sorted_results):
    rank = idx + 1
    medal = medals[idx] if idx < 3 else ''

    summary += f"### {medal} Rank {rank}: {r['name']}\n\n"
    summary += "**Configuration:**\n"
    summary += f"- max_num_seqs: {r['config']['max_num_seqs']}\n"
    summary += f"- max_num_batched_tokens: {r['config']['max_num_batched_tokens']}\n"
    summary += f"- pa_num_blocks: {r['config']['pa_num_blocks']}\n"
    summary += f"- enable_prefix_caching: {r['config']['enable_prefix_caching']}\n"
    summary += f"- enable_bucketing: {r['config']['enable_bucketing']}\n"

    if r['config'].get('kv_cache_quant'):
        summary += f"- kv_cache_quant: {r['config']['kv_cache_quant']}\n"
    if r['config'].get('sequence_parallel'):
        summary += f"- sequence_parallel: {r['config']['sequence_parallel']}\n"
    if r['config'].get('flash_decoding'):
        summary += f"- flash_decoding: {r['config']['flash_decoding']}\n"

    summary += "\n**Performance Metrics:**\n"
    summary += f"- Throughput: **{r['metrics']['throughput_queries_per_sec']:.4f} queries/sec**\n"
    summary += f"- Average Time: {r['metrics']['avg_time_per_query']:.4f} sec\n"
    summary += f"- Min Time: {r['metrics']['min_time_per_query']:.4f} sec\n"
    summary += f"- Max Time: {r['metrics']['max_time_per_query']:.4f} sec\n"
    summary += f"- P50 Latency: {r['metrics']['p50_time']:.4f} sec\n"
    summary += f"- P95 Latency: {r['metrics']['p95_time']:.4f} sec\n"
    summary += f"- P99 Latency: {r['metrics']['p99_time']:.4f} sec\n"
    summary += f"- Total Queries: {r['metrics']['total_queries']}\n"
    summary += f"- Total Time: {r['metrics']['total_time']:.2f} sec\n"
    summary += "\n"

# Best configuration recommendation
best = sorted_results[0]
summary += f"""---

## Recommended Configuration

Based on the benchmark results, the optimal configuration is:

**{best['name']}**

"""

# Add bash config separately to avoid shell variable expansion issues
summary += "```bash\n"
summary += f"MAX_NUM_SEQS={best['config']['max_num_seqs']}\n"
summary += f"MAX_NUM_BATCHED_TOKENS={best['config']['max_num_batched_tokens']}\n"
summary += f"PA_NUM_BLOCKS={best['config']['pa_num_blocks']}\n"
summary += f"ENABLE_PREFIX_CACHING={str(best['config']['enable_prefix_caching']).lower()}\n"
summary += f"ENABLE_BUCKETING={str(best['config']['enable_bucketing']).lower()}\n"
summary += "```\n\n"

summary += f"""**Performance:**
- Throughput: **{best['metrics']['throughput_queries_per_sec']:.4f} queries/sec**
- P99 Latency: {best['metrics']['p99_time']:.4f} sec

---

**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""

# Write summary
summary_file = os.environ['SUMMARY_FILE_FOR_PYTHON']
with open(summary_file, 'w') as f:
    f.write(summary)

print(f"Summary report saved to: {summary_file}")
print()
print("=" * 80)
print("Top 3 Configurations:")
print("=" * 80)
for idx, r in enumerate(sorted_results[:3]):
    medal = medals[idx]
    print(f"{medal} {r['name']}: {r['metrics']['throughput_queries_per_sec']:.4f} queries/sec")
print()
EOF

    echo
    echo "================================================================================"
    echo "All Tests Complete"
    echo "================================================================================"
    echo "Results directory: $RESULTS_DIR"
    echo "Summary report: $SUMMARY_FILE"
    echo

else
    # ============================================================================
    # Environment Variable Mode (Legacy)
    # ============================================================================
    echo "================================================================================"
    echo "Environment Variable Mode (Legacy)"
    echo "================================================================================"
    echo

    MODEL="${MODEL:-./models/Qwen3-0.6B-Reranker}"
    TP="${TP:-2}"
    INPUT_LEN="${INPUT_LEN:-1000}"
    OUTPUT_LEN="${OUTPUT_LEN:-50}"
    MAX_MODEL_LEN="${MAX_MODEL_LEN:-2048}"
    PA_BLOCK_SIZE="${PA_BLOCK_SIZE:-32}"
    PA_NUM_BLOCKS="${PA_NUM_BLOCKS:-512}"
    ENABLE_BUCKETING="${ENABLE_BUCKETING:-true}"
    NUM_SEQS_LIST="${NUM_SEQS_LIST:-4 8}"
    NUM_BATCHED_TOKENS_LIST="${NUM_BATCHED_TOKENS_LIST:-256 512}"
    NEURON_PROFILES="${NEURON_PROFILES:-baseline kv-quant}"
    INPUT_FILE="${INPUT_FILE:-./sample.csv}"
    SEARCH_NUM="${SEARCH_NUM:-20}"
    TOP_N="${TOP_N:-6}"
    BATCH_SIZE="${BATCH_SIZE:-8}"
    MAX_LENGTH="${MAX_LENGTH:-1500}"

    bash auto_tune_neuron_offline.sh
fi
```
::::



### Python ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè£…

`offline_benchmark.py` ã¯ã€vLLM ã® LLM ã‚¯ãƒ©ã‚¹ã‚’ç›´æ¥ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã€ã‚µãƒ¼ãƒãƒ¼ã‚’èµ·å‹•ã›ãšã«ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

::::details ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
```python:offline_benchmark.py
#!/usr/bin/env python3
"""
Offline Benchmark Script for vLLM-Neuron
Measures performance using direct LLM class instantiation (no server)
"""

import argparse
import json
import time
import sys
from typing import List, Sequence
import pandas as pd
from tqdm import tqdm
import torch

from vllm import LLM, SamplingParams
from transformers import AutoTokenizer


def parse_args():
    parser = argparse.ArgumentParser(description='Offline vLLM-Neuron Benchmark')

    # Model configuration
    parser.add_argument('--model', type=str, required=True, help='Model path')
    parser.add_argument('--tp', type=int, default=2, help='Tensor parallel size')
    parser.add_argument('--max-num-seqs', type=int, default=8, help='Max number of sequences')
    parser.add_argument('--max-num-batched-tokens', type=int, default=512, help='Max batched tokens')
    parser.add_argument('--max-model-len', type=int, default=2048, help='Max model length')
    parser.add_argument('--block-size', type=int, default=32, help='Block size')
    parser.add_argument('--pa-num-blocks', type=int, default=2048, help='PA num blocks')

    # Neuron configuration
    parser.add_argument('--enable-prefix-caching', action='store_true', help='Enable prefix caching')
    parser.add_argument('--enable-bucketing', action='store_true', help='Enable bucketing')
    parser.add_argument('--kv-cache-quant', action='store_true', help='Enable KV cache quantization')
    parser.add_argument('--sequence-parallel', action='store_true', help='Enable sequence parallel')
    parser.add_argument('--flash-decoding', action='store_true', help='Enable flash decoding')

    # Benchmark configuration
    parser.add_argument('--input-file', type=str, required=True, help='Input CSV file')
    parser.add_argument('--output-file', type=str, required=True, help='Output JSON file')
    parser.add_argument('--search-num', type=int, default=20, help='Number of search results')
    parser.add_argument('--top-n', type=int, default=6, help='Top N results to return')
    parser.add_argument('--batch-size', type=int, default=8, help='Batch size for processing')
    parser.add_argument('--max-length', type=int, default=1500, help='Max prompt length')

    return parser.parse_args()


class RerankerBenchmark:
    def __init__(self, args):
        self.args = args

        # Build neuron config
        neuron_config = {
            "skip_warmup": True,
            "pa_num_blocks": args.pa_num_blocks,
            "pa_block_size": args.block_size,
            "enable_bucketing": args.enable_bucketing,
        }

        # Only set block KV layout and prefix caching if prefix caching is enabled
        if args.enable_prefix_caching:
            neuron_config["is_prefix_caching"] = True
            neuron_config["is_block_kv_layout"] = True

        if args.kv_cache_quant:
            neuron_config["kv_cache_quant"] = True
        if args.sequence_parallel:
            neuron_config["sequence_parallel_enabled"] = True
        if args.flash_decoding:
            neuron_config["flash_decoding_enabled"] = True

        print(f"Initializing LLM with neuron_config: {json.dumps(neuron_config, indent=2)}")

        # Initialize LLM
        start_init = time.perf_counter()
        self.llm = LLM(
            model=args.model,
            tensor_parallel_size=args.tp,
            max_num_seqs=args.max_num_seqs,
            max_num_batched_tokens=args.max_num_batched_tokens,
            max_model_len=args.max_model_len,
            block_size=args.block_size,
            num_gpu_blocks_override=args.pa_num_blocks,  # Required for Neuron despite "gpu" in name
            enable_prefix_caching=args.enable_prefix_caching,
            dtype="bfloat16",
            additional_config={
                "override_neuron_config": neuron_config,
            },
        )
        init_time = time.perf_counter() - start_init
        print(f"LLM initialization completed in {init_time:.2f}s")

        # Initialize tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained(args.model, padding_side='left')
        self.token_false_id = self.tokenizer.convert_tokens_to_ids("no")
        self.token_true_id = self.tokenizer.convert_tokens_to_ids("yes")

        # Prompt templates
        self.prefix = "<|im_start|>system\nJudge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be \"yes\" or \"no\".<|im_end|>\n<|im_start|>user\n"
        self.suffix = "<|im_end|>\n<|im_start|>assitant\n<think>\n\n</think>\n\n"
        self.prefix_tokens = self.tokenizer.encode(self.prefix, add_special_tokens=False)
        self.suffix_tokens = self.tokenizer.encode(self.suffix, add_special_tokens=False)

    def format_instruction(self, query, doc, instruction=None):
        if instruction is None:
            instruction = 'Given a query about insurance, retrieve relevant passages that answer the query'
        output = f"<Instruct>: {instruction}\n<Query>: {query}\n<Document>: {doc}"
        if len(output) >= 2000:
            output = output[:2000]
        return output

    def build_prompts_for_vllm(self, pairs: Sequence[str]) -> List[str]:
        prompts: List[str] = []
        budget = self.args.max_length - len(self.prefix_tokens) - len(self.suffix_tokens)

        enc = self.tokenizer(
            list(pairs),
            padding=False,
            truncation="longest_first",
            return_attention_mask=False,
            add_special_tokens=False,
            max_length=max(8, budget),
        )

        for ids in enc["input_ids"]:
            final_ids = self.prefix_tokens + ids + self.suffix_tokens
            text = self.tokenizer.decode(final_ids, skip_special_tokens=False)
            prompts.append(text)
        return prompts

    def compute_logits_vllm(self, prompts: Sequence[str]) -> List[float]:
        n = len(prompts)
        probs_true: List[float] = [0.5] * n

        sp = SamplingParams(
            max_tokens=1,
            temperature=0.0,
            logprobs=20,
            detokenize=True,
            allowed_token_ids=[self.token_true_id, self.token_false_id]
        )

        # Process in chunks
        chunk_size = self.args.batch_size
        for s in range(0, len(prompts), chunk_size):
            batch_prompts = prompts[s:s + chunk_size]
            outs = self.llm.generate(batch_prompts, sp, use_tqdm=False)

            for i, out in enumerate(outs):
                probs_true[s + i] = 0.5  # Placeholder score

        return probs_true

    def search_results_rerank(self, query, search_results, top_n):
        pairs = [self.format_instruction(query=query, doc=doc) for doc in search_results]
        prompts = self.build_prompts_for_vllm(pairs)
        scores = self.compute_logits_vllm(prompts)

        scored_documents = [
            {
                "relevance_score": score,
                "content": doc,
                "index": i
            }
            for i, (score, doc) in enumerate(zip(scores, search_results))
        ]

        sorted_documents = sorted(scored_documents, key=lambda x: x["relevance_score"], reverse=True)
        results = sorted_documents[:top_n] if len(sorted_documents) > top_n else sorted_documents

        return [
            {
                "query": query,
                "rerank_score": r["relevance_score"],
                "content": r["content"]
            }
            for r in results
        ]

    def run_benchmark(self):
        """Run the benchmark and collect metrics"""
        df = pd.read_csv(self.args.input_file)

        time_counts = []
        prompt_counts = []
        token_counts = []

        print(f"\nRunning benchmark on {len(df)} queries...")

        for index, row in tqdm(df.iterrows(), total=len(df)):
            query = row["query"]
            search_results = [row[f"answer_{i}"] for i in range(self.args.search_num)]

            start_time = time.perf_counter()
            results = self.search_results_rerank(
                query=query,
                search_results=search_results,
                top_n=self.args.top_n,
            )
            end_time = time.perf_counter()

            execution_time = end_time - start_time
            time_counts.append(execution_time)
            prompt_counts.append(self.args.search_num)
            token_counts.append(self.args.search_num * self.args.max_length)  # Approximate

            # Free memory
            torch.cuda.empty_cache()

        # Calculate metrics
        import numpy as np
        time_counts = np.array(time_counts)

        results = {
            "configuration": {
                "model": self.args.model,
                "tensor_parallel_size": self.args.tp,
                "max_num_seqs": self.args.max_num_seqs,
                "max_num_batched_tokens": self.args.max_num_batched_tokens,
                "max_model_len": self.args.max_model_len,
                "block_size": self.args.block_size,
                "pa_num_blocks": self.args.pa_num_blocks,
                "enable_prefix_caching": self.args.enable_prefix_caching,
                "enable_bucketing": self.args.enable_bucketing,
                "kv_cache_quant": self.args.kv_cache_quant,
                "sequence_parallel": self.args.sequence_parallel,
                "flash_decoding": self.args.flash_decoding,
            },
            "metrics": {
                "avg_time_per_query": float(time_counts.mean()),
                "min_time_per_query": float(time_counts.min()),
                "max_time_per_query": float(time_counts.max()),
                "p50_time": float(np.percentile(time_counts, 50)),
                "p95_time": float(np.percentile(time_counts, 95)),
                "p99_time": float(np.percentile(time_counts, 99)),
                "total_queries": len(df),
                "total_time": float(time_counts.sum()),
                "throughput_queries_per_sec": len(df) / float(time_counts.sum()),
            },
            "raw_times": time_counts.tolist(),
        }

        # Save results
        with open(self.args.output_file, 'w') as f:
            json.dump(results, f, indent=2)

        print(f"\nBenchmark Results:")
        print(f"  Average time per query: {results['metrics']['avg_time_per_query']:.4f}s")
        print(f"  Min time: {results['metrics']['min_time_per_query']:.4f}s")
        print(f"  Max time: {results['metrics']['max_time_per_query']:.4f}s")
        print(f"  P50: {results['metrics']['p50_time']:.4f}s")
        print(f"  P95: {results['metrics']['p95_time']:.4f}s")
        print(f"  P99: {results['metrics']['p99_time']:.4f}s")
        print(f"  Throughput: {results['metrics']['throughput_queries_per_sec']:.2f} queries/sec")
        print(f"\nResults saved to: {self.args.output_file}")

        return results


def main():
    args = parse_args()

    print("=" * 80)
    print("vLLM-Neuron Offline Benchmark")
    print("=" * 80)

    benchmark = RerankerBenchmark(args)
    results = benchmark.run_benchmark()

    return 0


if __name__ == "__main__":
    sys.exit(main())
```

#### å®Ÿè£…ã®é‡è¦ãƒã‚¤ãƒ³ãƒˆ

```python
if args.enable_prefix_caching:
    neuron_config["is_prefix_caching"] = True
    neuron_config["is_block_kv_layout"] = True
```

ã“ã®æ¡ä»¶åˆ†å²ã¯éå¸¸ã«é‡è¦ã§ã™ã€‚`is_block_kv_layout` ã‚’å¸¸ã« `True` ã«è¨­å®šã™ã‚‹ã¨ã€prefix caching ã‚’ç„¡åŠ¹åŒ–ã—ãŸéš›ã« `BlockKVCacheManager` ã®åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã™ã€‚vllm-neuron ã®å†…éƒ¨å®Ÿè£…ã§ã¯ã€prefix caching ãŒç„¡åŠ¹ã®å ´åˆã€ç•°ãªã‚‹ã‚³ãƒ¼ãƒ‰ãƒ‘ã‚¹ã‚’é€šã‚‹ãŸã‚ã€`is_block_kv_layout` ã®è¨­å®šãŒä¸è¦ã¨ãªã‚Šã¾ã™ã€‚
::::

### å®Ÿè¡Œæ–¹æ³•

ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®å®Ÿè¡Œã¯éå¸¸ã«ã‚·ãƒ³ãƒ—ãƒ«ã§ã™ã€‚JSON è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã€ç’°å¢ƒå¤‰æ•° `CONFIG_FILE` ã«æŒ‡å®šã—ã¦ `run_offline_benchmark.sh` ã‚’å®Ÿè¡Œã™ã‚‹ã ã‘ã§ã™ã€‚

```bash
cd /path/to/your/project

# Prefix caching ã¨ bucketing ã®æ¯”è¼ƒãƒ†ã‚¹ãƒˆ (4 è¨­å®šã€ç´„ 20 åˆ†)
CONFIG_FILE=test_cache_bucketing_comparison.json bash run_offline_benchmark.sh
```

å®Ÿè¡ŒãŒå®Œäº†ã™ã‚‹ã¨ã€`benchmark_results_YYYY_MM_DD_HH_MM/` ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã™ã€‚

- `summary.md`: å…¨è¨­å®šã®æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ (Markdown å½¢å¼)
- `<config_name>.json`: å„è¨­å®šã®è©³ç´°ãªçµæœ
- `<config_name>.log`: å„è¨­å®šã®å®Ÿè¡Œãƒ­ã‚°

ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«ã¯ã€ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆä¸Šä½ 3 ã¤ã®è¨­å®šãŒè‡ªå‹•çš„ã«è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚

```
================================================================================
Top 3 Configurations:
================================================================================
ğŸ¥‡ cache_OFF_bucketing_ON: 3.0975 queries/sec
ğŸ¥ˆ cache_ON_bucketing_ON: 2.3225 queries/sec
ğŸ¥‰ cache_ON_bucketing_OFF: 0.8718 queries/sec
```

## ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã¨åˆ†æ

æœ¬ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€inf2.xlarge ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä¸Šã§å®Ÿæ–½ã—ãŸãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’å®Ÿéš›ã«ç´¹ä»‹ã—ã¾ã™ã€‚Prefix caching ã¨ bucketing ã® 4 é€šã‚Šã®çµ„ã¿åˆã‚ã›ã‚’ãƒ†ã‚¹ãƒˆã—ã€ãã‚Œãã‚Œã®æœ€é©åŒ–æ‰‹æ³•ãŒã‚¿ã‚¹ã‚¯ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’å®šé‡çš„ã«è©•ä¾¡ã—ã¾ã—ãŸã€‚ï¼ˆã‚¿ã‚¹ã‚¯å†…å®¹ã‚„ãƒ¢ãƒ‡ãƒ«è©³ç´°ã®èª¬æ˜ã¯ä¸»çœ¼ã§ã¯ãªã„ãŸã‚å‰²æ„›ã—ã¾ã™ï¼‰

inf2.xlarge ã®èµ·å‹•ã¯ä»¥ä¸‹ã‚’å‚è€ƒã«ã—ã¦ã¿ã¦ãã ã•ã„ã€‚ï¼ˆUbuntu24 ã® DLAMI ã«å¯¾å¿œã—ã¾ã—ãŸï¼‰

https://zenn.dev/tosshi/articles/eb54037328d2ef

### å®Ÿé¨“è¨­å®š

- **ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹**: inf2.xlarge (2 Neuron ã‚³ã‚¢ã€Ubuntu 24.04)
- **ãƒ¢ãƒ‡ãƒ«**: Qwen/Qwen3-Reranker-0.6B
- **ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿**: 10 ã‚¯ã‚¨ãƒªã€å„ã‚¯ã‚¨ãƒªã§ 20 ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ Rerank
- **å…±é€šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:
  - `max_num_seqs`: 4
  - `max_num_batched_tokens`: 256
  - `pa_num_blocks`: 512
  - `tensor_parallel_size`: 2

### æ€§èƒ½æ¯”è¼ƒçµæœ

ä»¥ä¸‹ã®è¡¨ã¯ã€4 ã¤ã®è¨­å®šãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚æ³¨æ„ç‚¹ã¨ã—ã¦ã€ã“ã‚Œã‚‰ã®çµæœã¯æœ¬å®Ÿé¨“ã§ä½¿ç”¨ã—ãŸç‰¹å®šã®ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«é™å®šã•ã‚ŒãŸã‚‚ã®ã§ã™ã€‚

| ãƒ©ãƒ³ã‚¯ | è¨­å®š | Throughput<br>(queries/sec) | å¹³å‡æ™‚é–“<br>(sec) | P50<br>(sec) | P95<br>(sec) | P99<br>(sec) | å¯¾æœ€é€Ÿæ¯” |
|--------|------|---------------------------|-----------------|----------|----------|----------|----------|
| ğŸ¥‡ **1** | prefix caching OFF<br>bucketing ON | **3.0975** | 0.3228 | 0.3071 | 0.3972 | 0.4496 | 100% |
| ğŸ¥ˆ **2** | prefix caching ON<br>bucketing ON | **2.3225** | 0.4306 | 0.4137 | 0.5090 | 0.5690 | 75.0% |
| ğŸ¥‰ **3** | prefix caching ON<br>bucketing OFF | **0.8718** | 1.1470 | 1.1262 | 1.2401 | 1.3114 | 28.1% |
|  **4** | prefix caching OFF<br>bucketing OFF | **0.8151** | 1.2268 | 1.2029 | 1.3342 | 1.4159 | 26.3% |

ã“ã‚Œã‚‰ã®æ•°å€¤ã¯ã€æœ¬å®Ÿé¨“ã®ç‰¹å®šæ¡ä»¶ä¸‹ã§å¾—ã‚‰ã‚ŒãŸå‚è€ƒå€¤ã§ã‚ã‚Šã€ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã€å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®ç‰¹æ€§ã€ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢æ§‹æˆã«ã‚ˆã£ã¦çµæœã¯å¤‰å‹•ã—ã¾ã™ã€‚

### é‡è¦ãªç™ºè¦‹

#### 1. Prefix Caching ã¯ Reranker ã‚¿ã‚¹ã‚¯ã§ã¯é€†åŠ¹æœ

ä»Šå›ã®å®Ÿé¨“ã§æœ€ã‚‚é‡è¦ãªç™ºè¦‹ã¯ã€prefix caching ã‚’ç„¡åŠ¹åŒ–ã—ãŸè¨­å®šãŒã€æœ‰åŠ¹åŒ–ã—ãŸè¨­å®šã¨æ¯”è¼ƒã—ã¦ **33.4% é«˜ã„ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ**ã‚’é”æˆã—ãŸã“ã¨ã§ã™ã€‚ã“ã®çµæœã¯ã€ä¸€èˆ¬çš„ãª LLM æ¨è«–ã«ãŠã„ã¦ prefix caching ãŒæ¨å¥¨ã•ã‚Œã‚‹ã“ã¨ã¨å¯¾ç…§çš„ã§ã™ã€‚

**åŸå› äºˆæƒ³ï¼ˆå½“ã¦ãšã£ã½ã†ã§ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã¾ã§ã‚„ã£ã¦ã¾ã›ã‚“ï¼‰**:

Prefix caching ã¯ã€è¤‡æ•°ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã§å…±é€šã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã® KV cache ã‚’å†åˆ©ç”¨ã™ã‚‹ã“ã¨ã§ã€Time To First Token (TTFT) ã‚’çŸ­ç¸®ã™ã‚‹æœ€é©åŒ–æ‰‹æ³•ã§ã™ã€‚ã—ã‹ã—ã€Reranker ã‚¿ã‚¹ã‚¯ã§ã¯ã€ä»¥ä¸‹ã®ç†ç”±ã«ã‚ˆã‚Šé€†åŠ¹æœã¨ãªã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚

1. **ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã®å†åˆ©ç”¨é »åº¦ãŒä½ã„**: Reranker ã¯å„ã‚¯ã‚¨ãƒªã¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒšã‚¢ã«å¯¾ã—ã¦ç•°ãªã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã€å…±é€šãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã®é•·ã•ãŒçŸ­ãã€å†åˆ©ç”¨æ©Ÿä¼šãŒé™ã‚‰ã‚Œã‚‹ã¨æ€ã‚ã‚Œã¾ã™ã€‚
2. **ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰**: KV ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç®¡ç†ã—ã€ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°ã€ãªã©ã®å‡¦ç†ã‚’å®Ÿè£…ã—ã¦ã„ã‚‹ã¯ãšãªã®ã§ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆã—ãªã„å ´åˆã¯ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã®æ‚ªå½±éŸ¿ãŒå¤§ãã„ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚

#### 2. Bucketing ã¯å¿…é ˆã®æœ€é©åŒ–æ‰‹æ³•

Bucketing ã®æœ‰ç„¡ã«ã‚ˆã‚‹æ€§èƒ½å·®ã¯æ¥µã‚ã¦é¡•è‘—ã§ã—ãŸã€‚Bucketing ã‚’ç„¡åŠ¹åŒ–ã™ã‚‹ã¨ã€prefix caching ã®è¨­å®šã«é–¢ã‚ã‚‰ãšã€ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãŒ **70-75% ä½ä¸‹**ã—ã¾ã™ã€‚

**Bucketing ã®ä»•çµ„ã¿**:

AWS Neuron ã¯é™çš„ã‚°ãƒ©ãƒ•ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚’æ¡ç”¨ã—ã¦ã„ã¾ã™ã€‚ã“ã‚Œã¯ã€NVIDIA GPU ã«ãŠã‘ã‚‹ CUDA ã‚°ãƒ©ãƒ•ã¨åŒæ§˜ã®æ¦‚å¿µã§ã™ãŒã€ã‚ˆã‚Šå³å¯†ãªå…¥åŠ›å½¢çŠ¶ã®å›ºå®šãŒå¿…è¦ã§ã™ã€‚Bucketing ã‚’æœ‰åŠ¹åŒ–ã™ã‚‹ã¨ã€vLLM ã¯å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’äº‹å‰å®šç¾©ã•ã‚ŒãŸãƒã‚±ãƒƒãƒˆã‚µã‚¤ã‚º (ä¾‹: 128, 256, 512, 1024) ã«ä¸¸ã‚ã¾ã™ã€‚

:::message
AWS Neuron ã¨ NVIDIA GPU ã§ã¯å®Ÿè£…ãƒ¬ãƒ™ãƒ«ã§ã¯å¤§ããªé•ã„ãŒã‚ã‚Šã€Neuron ã¯ 2026/01/25 æ™‚ç‚¹ã§ã¯äº‹å‰ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«å¿…é ˆã§ã€CUDA ã‚°ãƒ©ãƒ•ã¯å®Ÿè¡Œæ™‚ã‚­ãƒ£ãƒ—ãƒãƒ£ã€ã§ã™ã€‚ä»Šå¾Œ AWS Neuron ã¯ PyTorch Eager ãƒ¢ãƒ¼ãƒ‰ã«å¯¾å¿œã™ã‚‹ã®ã§ã„ãšã‚Œã‚³ãƒ³ãƒ‘ã‚¤ãƒ«å¿…é ˆã§ã¯ãªããªã‚‹ã¨æ€ã‚ã‚Œã¾ã™ãŒã€Eager ãƒ¢ãƒ¼ãƒ‰ã¯äº‹å‰ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚ˆã‚Šãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒåŠ£åŒ–ã™ã‚‹å¯èƒ½æ€§ãŒé«˜ã„ãŸã‚æ…é‡ã«æ¤œè¨¼ã™ã‚‹ã¹ãã§ã—ã‚‡ã†ã€‚
:::

Bucketing ã«ã‚ˆã‚Šã€ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ã‚°ãƒ©ãƒ•ã®å†åˆ©ç”¨ç‡ãŒæœ€å¤§åŒ–ã•ã‚Œã€å‹•çš„ãªå…¥åŠ›å½¢çŠ¶å¤‰åŒ–ã«å¯¾ã™ã‚‹ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒå‰Šæ¸›ã•ã‚Œã¾ã™ã€‚æœ¬å®Ÿé¨“ã§ã¯ã€bucketing ON ã¨ OFF ã§æœ€å¤§ 3.8 å€ã®æ€§èƒ½å·®ãŒè¦³æ¸¬ã•ã‚Œã¾ã—ãŸã€‚

#### 3. æœ€é©è¨­å®šã®çµ„ã¿åˆã‚ã›

ä»Šå›ã®å®Ÿé¨“ã«ã‚ˆã‚Šã‚¿ã‚¹ã‚¯ã«ãŠã‘ã‚‹æœ€é©ãªè¨­å®šã¯ä»¥ä¸‹ã¨ãªã‚Šã¾ã—ãŸã€‚

```python
from vllm import LLM, SamplingParams

llm = LLM(
    model="Qwen/Qwen3-Reranker-0.6B",
    tensor_parallel_size=2,
    max_num_seqs=4,
    max_num_batched_tokens=256,
    max_model_len=2048,
    block_size=32,
    num_gpu_blocks_override=512,
    enable_prefix_caching=False,
    dtype="bfloat16",
    additional_config={
        "override_neuron_config": {
            "skip_warmup": True,
            "pa_num_blocks": 512,
            "pa_block_size": 32,
            "enable_bucketing": True,
        }
    }
)
```

ã“ã®è¨­å®šã«ã‚ˆã‚Šã€3.0975 queries/sec ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã¨ 0.4496 ç§’ã® P99 ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ã‚’é”æˆã—ã¾ã—ãŸã€‚ãŸã ã—ã€ã“ã®è¨­å®šã¯æœ¬å®Ÿé¨“ã®æ¡ä»¶ä¸‹ã§ã®æœ€é©è§£ã§ã‚ã‚Šã€ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã‚„ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã§ã¯å†è©•ä¾¡ãŒå¿…è¦ã§ã™ã€‚

ä»Šå›ã®çµæœã¯ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã®ç‰¹æ€§ã‚’ç†è§£ã—ãŸä¸Šã§æ§˜ã€…ãªè¨­å®šã®å½±éŸ¿ã‚’ç¢ºèªã™ã‚‹ã“ã¨ã®é‡è¦æ€§ã‚’ç¤ºã—ã¦ãŠã‚Šã€ä»Šå›ä¾‹ç¤ºã—ãŸã‚ˆã†ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®ãŸã‚ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®é‡è¦æ€§ãŒç¢ºèªã§ãã¾ã—ãŸã€‚vLLM ã«ã¯ [Benchmarking](https://docs.vllm.ai/en/stable/benchmarking/) ãƒ„ãƒ¼ãƒ«ãŒæä¾›ã•ã‚Œã¦ã„ã¾ã™ãŒã€vllm-neuron ã§ã¯ç¾çŠ¶å‹•ä½œã—ã¾ã›ã‚“ã€‚


## å®Ÿè£…ä¸Šã®æ³¨æ„ç‚¹ã¨ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

vllm-neuron ã‚’ä½¿ç”¨ã™ã‚‹éš›ã€ã„ãã¤ã‹ã®é‡è¦ãªè½ã¨ã—ç©´ãŒã‚ã‚Šã¾ã—ãŸã€‚æœ¬ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€å®Ÿè£…éç¨‹ã§é­é‡ã—ãŸå•é¡Œã¨ãã®è§£æ±ºç­–ã‚’å…±æœ‰ã—ã¾ã™ã€‚

::::details 1. is_block_kv_layout ã®è¨­å®šã‚¿ã‚¤ãƒŸãƒ³ã‚°

**å•é¡Œ**:

`override_neuron_config` ã§ `is_block_kv_layout: True` ã‚’å¸¸ã«è¨­å®šã™ã‚‹ã¨ã€prefix caching ã‚’ç„¡åŠ¹åŒ–ã—ãŸéš›ã«ä»¥ä¸‹ã®ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã™ã€‚

```
AttributeError: 'BlockKVCacheManager' object has no attribute 'k_shape'
```

**åŸå› **:

vllm-neuron ã®å†…éƒ¨å®Ÿè£…ã§ã¯ã€prefix caching ãŒç„¡åŠ¹ã®å ´åˆã€BlockKVCacheManager ã®åˆæœŸåŒ–ãŒç•°ãªã‚‹ã‚³ãƒ¼ãƒ‰ãƒ‘ã‚¹ã‚’é€šã‚Šã¾ã™ã€‚`is_block_kv_layout` ãŒæ˜ç¤ºçš„ã«è¨­å®šã•ã‚Œã¦ã„ã‚‹ã¨ã€k_shape å±æ€§ã‚’æŒãŸãªã„ BlockKVCacheManager ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¯¾ã—ã¦ã‚¢ã‚¯ã‚»ã‚¹ãŒè©¦ã¿ã‚‰ã‚Œã€ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã™ã€‚

**è§£æ±ºç­–**:

Prefix caching ãŒæœ‰åŠ¹ãªå ´åˆã®ã¿ `is_block_kv_layout` ã‚’è¨­å®šã—ã¾ã™ã€‚

```python
neuron_config = {
    "skip_warmup": True,
    "pa_num_blocks": args.pa_num_blocks,
    "pa_block_size": args.block_size,
    "enable_bucketing": args.enable_bucketing,
}

# Prefix caching ãŒæœ‰åŠ¹ãªå ´åˆã®ã¿è¨­å®š
if args.enable_prefix_caching:
    neuron_config["is_prefix_caching"] = True
    neuron_config["is_block_kv_layout"] = True
```

ã“ã®æ¡ä»¶åˆ†å²ã«ã‚ˆã‚Šã€prefix caching ã®æœ‰ç„¡ã«é–¢ã‚ã‚‰ãšæ­£å¸¸ã«å‹•ä½œã—ã¾ã™ã€‚
::::

::::details 2. åˆå›ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚é–“

**å•é¡Œ**:

ãƒ¢ãƒ‡ãƒ«ã®åˆå›ãƒ­ãƒ¼ãƒ‰æ™‚ã«ã€30 åˆ†ä»¥ä¸Šã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚é–“ãŒã‹ã‹ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚

**å¯¾å‡¦æ³•**:

ã“ã‚Œã¯ Neuron Compiler ãŒãƒ¢ãƒ‡ãƒ«ã‚’ NEFF å½¢å¼ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¦ã„ã‚‹ãŸã‚ã§ã‚ã‚Šã€é¿ã‘ã‚‰ã‚Œãªã„ overhead ã§ã™ã€‚ä»¥ä¸‹ã®ç‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

1. **ãƒ­ã‚°ã®ç¢ºèª**: "Compiling" ã‚„ "Generating HLO" ãŒãƒ­ã‚°ã«è¡¨ç¤ºã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¾ã™ã€‚
2. **ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æ´»ç”¨**: ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«çµæœã¯ `/var/tmp/neuron-compile-cache/` ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã•ã‚Œã¾ã™ã€‚2 å›ç›®ä»¥é™ã®å®Ÿè¡Œã§ã¯ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå†åˆ©ç”¨ã•ã‚Œã‚‹ãŸã‚ã€èµ·å‹•æ™‚é–“ãŒå¤§å¹…ã«çŸ­ç¸®ã•ã‚Œã¾ã™ã€‚
3. **ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æ°¸ç¶šåŒ–**: EC2 ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’åœæ­¢ã™ã‚‹å ´åˆã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ EBS ãƒœãƒªãƒ¥ãƒ¼ãƒ ã«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã™ã‚‹ã“ã¨ã§ã€å†èµ·å‹•å¾Œã‚‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åˆ©ç”¨ã§ãã¾ã™ã€‚

```bash
# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ç¢ºèª
ls -lh /var/tmp/neuron-compile-cache/

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚µã‚¤ã‚ºã®ç¢ºèª
du -sh /var/tmp/neuron-compile-cache/
```
::::

::::details 3. ãƒ¡ãƒ¢ãƒªä¸è¶³ã‚¨ãƒ©ãƒ¼

**ç—‡çŠ¶**:

ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰æ™‚ã« OOM (Out of Memory) ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã™ã€‚

**è§£æ±ºç­–**:

ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã®å¯¾ç­–ã‚’å®Ÿæ–½ã—ã¦ãã ã•ã„ã€‚

1. **pa_num_blocks ã‚’æ¸›ã‚‰ã™**: ä¾‹ãˆã° `2048` â†’ `512` ã«æ¸›ã‚‰ã™ã“ã¨ã§ã€PagedAttention ãŒä½¿ç”¨ã™ã‚‹ãƒ¡ãƒ¢ãƒªã‚’å‰Šæ¸›ã—ã¾ã™ã€‚
2. **max_model_len ã‚’æ¸›ã‚‰ã™**: ãƒ¢ãƒ‡ãƒ«ãŒå‡¦ç†ã™ã‚‹æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚’çŸ­ãã—ã¾ã™ã€‚
3. **tensor_parallel_size ã‚’å¢—ã‚„ã™**: ãƒ¡ãƒ¢ãƒªã‚’è¤‡æ•°ã® Neuron ã‚³ã‚¢ã«åˆ†æ•£ã—ã¾ã™ã€‚ãŸã ã—ã€inf2.xlarge ã¯ 2 ã‚³ã‚¢ã®ã¿ã®ãŸã‚ã€ã‚ˆã‚Šå¤§ãã„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—ã¸ã®å¤‰æ›´ãŒå¿…è¦ã§ã™ã€‚
4. **ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—ã‚’å¤§ããã™ã‚‹**: inf2.8xlarge ã‚„ inf2.24xlarge ã«å¤‰æ›´ã—ã¾ã™ã€‚

```python
# ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›ã™ã‚‹è¨­å®šä¾‹
llm = LLM(
    model="Qwen/Qwen3-Reranker-0.6B",
    tensor_parallel_size=2,
    max_num_seqs=2,  # 4 ã‹ã‚‰ 2 ã«å‰Šæ¸›
    max_num_batched_tokens=128,  # 256 ã‹ã‚‰ 128 ã«å‰Šæ¸›
    max_model_len=1024,  # 2048 ã‹ã‚‰ 1024 ã«å‰Šæ¸›
    num_gpu_blocks_override=256,  # 512 ã‹ã‚‰ 256 ã«å‰Šæ¸›
    # ...
)
```
::::

## vLLM Benchmarking ãƒ„ãƒ¼ãƒ«

vLLM ã«ã¯ Benchmarking ãƒ„ãƒ¼ãƒ«ãŒã‚ã‚‹ã®ã«ãªãœãã‚Œã‚’ä½¿ã‚ãªã„ã®ã‹ã€ã¨æ€ã‚ã‚ŒãŸæ–¹ã‚‚ã„ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ãŸã‚ã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆã™ã‚‹å‰ã« vLLM Benchmarking ãƒ„ãƒ¼ãƒ«ã«ã¤ã„ã¦å‹•ä½œç¢ºèªã—ãŸçµæœã‚’å…±æœ‰ã—ã¦ãŠãã¾ã™ã€‚ãŸã ã—ã€å®Ÿè£…ã¾ã§æ·±ã¼ã£ã¦èª¿æŸ»ã—ãŸã‚ã‘ã§ã¯ãªã„ãŸã‚å®Ÿã¯å‹•ãã€ã¨ã„ã†ã“ã¨ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚æœ€çµ‚çš„ã«ã¯ã”è‡ªèº«ã§ã—ã£ã‹ã‚Šã¨ç¢ºèªãã ã•ã„ã€‚

vLLM å…¬å¼ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ CLI (`vllm bench`) ã¯ **vllm-neuron ç’°å¢ƒã§ã¯å‹•ä½œã—ã¾ã›ã‚“ã§ã—ãŸ**ã€‚CLI ã¨ã—ã¦ 3 ã¤ã®ã‚³ãƒãƒ³ãƒ‰ï¼ˆ`serve`ã€`throughput`ã€`latency`ï¼‰ãŒå­˜åœ¨ã—ã¾ã™ãŒã€ä»Šå›åˆ©ç”¨ã—ãŸã‹ã£ãŸ `throughput` ã‚’å®Ÿéš›ã«å®Ÿè¡Œã™ã‚‹ã¨åˆæœŸåŒ–æ™‚ã«ãƒãƒ³ã‚°ã—ã€æ­£å¸¸ã«å‹•ä½œã—ã¾ã›ã‚“ã§ã—ãŸã€‚`startup`ã€`mm-processor`ã€`sweep` ã¯å®Ÿè£…ã•ã‚Œã¦ãªã•ãã†ã§ã™ã€‚

### vLLM å…¬å¼ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ CLI

vLLM ã¯ã€ã™ã¹ã¦ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ©Ÿèƒ½ã‚’çµ±ä¸€ã•ã‚ŒãŸ CLI ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ `vllm bench` ã«ç§»è¡Œã—ã¦ã„ã¾ã™ã€‚å¾“æ¥ã® standalone ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆ`benchmark_serving.py`ã€`benchmark_throughput.py` ãªã©ï¼‰ã¯ deprecated ã¨ãªã‚Šã€ã™ã¹ã¦ã®æ©Ÿèƒ½ãŒ `vllm bench` ã‚µãƒ–ã‚³ãƒãƒ³ãƒ‰ã«é›†ç´„ã•ã‚Œã¾ã—ãŸã€‚

| ã‚³ãƒãƒ³ãƒ‰ | ç”¨é€” | ä¸»è¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ |
|---------|------|--------------|
| `vllm bench serve` | ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚µãƒ¼ãƒ“ãƒ³ã‚° | TTFTã€TPOTã€ITLã€E2ELã€throughputã€goodput |
| `vllm bench throughput` | ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒãƒƒãƒæ¨è«– | æœ€å¤§ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ |
| `vllm bench latency` | ã‚·ãƒ³ã‚°ãƒ«ãƒãƒƒãƒãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ | ãƒãƒƒãƒå‡¦ç†æ™‚é–“ |
| `vllm bench startup` | ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—æ™‚é–“ | ã‚³ãƒ¼ãƒ«ãƒ‰/ã‚¦ã‚©ãƒ¼ãƒ ã‚¹ã‚¿ãƒ¼ãƒˆæ™‚é–“ |
| `vllm bench mm-processor` | ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ« | ãƒ—ãƒªãƒ—ãƒ­ã‚»ãƒƒã‚·ãƒ³ã‚°ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ |
| `vllm bench sweep` | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¹ã‚¤ãƒ¼ãƒ— | è¨­å®šä¾å­˜ |

### ä¸»è¦æ©Ÿèƒ½

::::details æ©Ÿèƒ½ 1: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚µãƒãƒ¼ãƒˆ

**ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**:
- ShareGPT - ãƒãƒ«ãƒã‚¿ãƒ¼ãƒ³ä¼šè©±ãƒ‡ãƒ¼ã‚¿
- Random - åˆæˆãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆ
- BurstGPT - CSV ãƒ™ãƒ¼ã‚¹ GPT-4 ãƒ•ã‚£ãƒ«ã‚¿ãƒ‡ãƒ¼ã‚¿
- Sonnet - è©©ãƒ™ãƒ¼ã‚¹æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿
- HuggingFace - HF dataset hub
- Custom - JSONL ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ
- Prefix repetition - ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ãƒ†ã‚¹ãƒˆç”¨

**ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ**:
- Random-MM - åˆæˆç”»åƒãƒ»å‹•ç”»
- VisionArena - ç”»åƒä»˜ãä¼šè©±
- MMVU - å‹•ç”» QA
::::

::::details æ©Ÿèƒ½ 2: ãƒ¡ãƒˆãƒªã‚¯ã‚¹

**ã‚µãƒ¼ãƒ“ãƒ³ã‚°ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯**:
- **TTFT** (Time to First Token) - æœ€åˆã®ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆã¾ã§ã®æ™‚é–“
- **TPOT** (Time Per Output Token) - ãƒˆãƒ¼ã‚¯ãƒ³ã‚ãŸã‚Šã®å¹³å‡æ™‚é–“
- **ITL** (Inter-Token Latency) - ãƒˆãƒ¼ã‚¯ãƒ³é–“ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼
- **E2EL** (End-to-End Latency) - ãƒªã‚¯ã‚¨ã‚¹ãƒˆå…¨ä½“ã®å‡¦ç†æ™‚é–“
- **Throughput** - ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/ãƒˆãƒ¼ã‚¯ãƒ³ per second
- **Goodput** - SLA åŸºæº–ã‚’æº€ãŸã™ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°
::::

::::details æ©Ÿèƒ½ 3: ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ãƒ‘ã‚¿ãƒ¼ãƒ³

- Poisson arrivals - æ¨™æº–åˆ†æ•£ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
- Bursty traffic - ãƒãƒ¼ã‚¹ãƒˆæ€§åˆ¶å¾¡
- Ramp-up strategies - æ®µéšçš„ãªè² è·å¢—åŠ 
- Instant load - ä¸€æ–‰ãƒªã‚¯ã‚¨ã‚¹ãƒˆ (`request-rate=inf`)
::::

### vllm-neuron ã®å¯¾å¿œçŠ¶æ³

vllm-neuron ç’°å¢ƒï¼ˆ`/opt/aws_neuronx_venv_pytorch_inference_vllm`ï¼‰ã§ä»¥ä¸‹ã® 3 ã¤ã®ã‚³ãƒãƒ³ãƒ‰ãŒ **CLI ã¨ã—ã¦ã¯å­˜åœ¨**ã—ã¾ã™ã€‚

:::message alert
ãŸã ã—å®Ÿéš›ã«ã¯å‹•ä½œã—ã¾ã›ã‚“ã§ã—ãŸã€‚
:::

```bash
$ vllm bench --help

INFO 01-25 05:32:37 [__init__.py:36] Available plugins for group vllm.platform_plugins:
INFO 01-25 05:32:37 [__init__.py:38] - neuron -> vllm_neuron:register
INFO 01-25 05:32:37 [__init__.py:41] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 01-25 05:32:37 [__init__.py:207] Platform plugin neuron is activated
Could not import vLLM config module for overrides: cannot import name 'ModelConfig' from partially initialized module 'vllm.config' (most likely due to a circular import) (/opt/aws_neuronx_venv_pytorch_inference_vllm/lib/python3.12/site-packages/vllm/config/__init__.py)
WARNING 01-25 05:32:39 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
Could not import required modules for chat completion stream override: No module named 'vllm_neuron.patches'
usage: vllm bench <bench_type> [options]

vLLM bench subcommand.

positional arguments:
  {latency,serve,throughput}
    latency             Benchmark the latency of a single batch of requests.
    serve               Benchmark the online serving throughput.
    throughput          Benchmark offline inference throughput.

options:
  -h, --help            show this help message and exit
```

### å„ã‚³ãƒãƒ³ãƒ‰ã®ç†è«–ä¸Šã®ä»•æ§˜ï¼ˆå®Ÿéš›ã«ã¯å‹•ä½œã›ãšï¼‰

::::details vllm bench serve

**ç”¨é€”**: ã‚ªãƒ³ãƒ©ã‚¤ãƒ³ã‚µãƒ¼ãƒ“ãƒ³ã‚°ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯

**ä¸»è¦ã‚ªãƒ—ã‚·ãƒ§ãƒ³**:
- Dataset è¨­å®šï¼ˆShareGPTã€Randomã€Sonnet ãªã©ï¼‰
- Request rate è¨­å®š
- Sampling parameters
- Latency ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼ˆTTFTã€TPOTã€E2EL ãªã©ï¼‰

**åˆ©ç”¨å¯èƒ½ãªè¨­å®šã‚°ãƒ«ãƒ¼ãƒ—**:
```
- options
- custom dataset options
- spec bench dataset options
- sonnet dataset options
- sharegpt dataset options
- blazedit dataset options
- random dataset options
- random multimodal dataset options
- hf dataset options
- prefix repetition dataset options
- sampling parameters
```
::::

::::details vllm bench throughput

**ç”¨é€”**: ã‚ªãƒ•ãƒ©ã‚¤ãƒ³ãƒãƒƒãƒæ¨è«–ã®æœ€å¤§ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆæ¸¬å®š

**ä¸»è¦ã‚ªãƒ—ã‚·ãƒ§ãƒ³**:
- Prompt æ•°ã®æŒ‡å®š
- Input/Output length è¨­å®š
- Model configuration
- Prefix caching è¨­å®š
- **additional-config ã«ã‚ˆã‚‹ Neuron å›ºæœ‰è¨­å®š**

**åˆ©ç”¨å¯èƒ½ãªè¨­å®šã‚°ãƒ«ãƒ¼ãƒ—**:
```
- options
- prefix repetition dataset options
- ModelConfig
- LoadConfig
- StructuredOutputsConfig
- ParallelConfig
- CacheConfig
- MultiModalConfig
- LoRAConfig
- ObservabilityConfig
- SchedulerConfig
- VllmConfig
```
::::

::::details vllm bench latency

**ç”¨é€”**: ã‚·ãƒ³ã‚°ãƒ«ãƒãƒƒãƒã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼æ¸¬å®š

**ä¸»è¦ã‚ªãƒ—ã‚·ãƒ§ãƒ³**:
- Batch size è¨­å®š
- Input/Output length è¨­å®š
- Warmup iterations
- Profiling ã‚µãƒãƒ¼ãƒˆ

**åˆ©ç”¨å¯èƒ½ãªè¨­å®šã‚°ãƒ«ãƒ¼ãƒ—**:
```
- options
- ModelConfig
- LoadConfig
- StructuredOutputsConfig
- ParallelConfig
- CacheConfig
- MultiModalConfig
- LoRAConfig
- ObservabilityConfig
- SchedulerConfig
- VllmConfig
```
::::

### Neuron å›ºæœ‰è¨­å®šã®æŒ‡å®šæ–¹æ³•

vllm-neuron ã§ã¯ã€`--additional-config` ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ Neuron å›ºæœ‰ã®è¨­å®šã‚’ JSON å½¢å¼ã§æ¸¡ã™ã“ã¨ãŒã§ãã¾ã™ã€‚

```bash
--additional-config ADDITIONAL_CONFIG
    Additional config for specified platform. Different
    platforms may support different configs. Make sure the
    configs are valid for the platform you are using.
    Contents must be hashable. (default: {})
```

ä»¥ä¸‹ã®ä¾‹ã®ã‚ˆã†ã«å®Ÿè¡Œã§ãã¾ã™ã€‚

```bash
vllm bench throughput \
  --model ./models/Qwen3-0.6B-Reranker \
  --tensor-parallel-size 2 \
  --num-prompts 100 \
  --input-len 512 \
  --output-len 128 \
  --max-model-len 2048 \
  --enable-prefix-caching \
  --additional-config '{"override_neuron_config": {"enable_bucketing": true, "pa_num_blocks": 512, "pa_block_size": 32, "skip_warmup": true}}'
```

### vllm-neuron ã§ã® Benchmarking ãƒ„ãƒ¼ãƒ«ã®å‹•ä½œç¢ºèª

Benchmarking ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ãŠã†ã¨è©¦ã¿ã¾ã—ãŸãŒã€1. LLM åˆæœŸåŒ–æ™‚ã«ãƒ—ãƒ­ã‚»ã‚¹ãŒãƒãƒ³ã‚°ã™ã‚‹ã€2. å¾ªç’° import ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã€3. å®Ÿè¡Œãƒ†ã‚¹ãƒˆã§ JSON å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œãªã„ã€ãªã©ã®ç†ç”±ã‹ã‚‰å‹•ä½œã—ã¾ã›ã‚“ã§ã—ãŸã€‚ãã‚‚ãã‚‚ vllm-neuron ã®**å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã‚‚ Benchmarking CLI ã«å¯¾å¿œã—ã¦ã„ã‚‹ã¨ã„ã†è¨˜è¼‰ãŒãã‚‚ãã‚‚è¦‹å½“ãŸã‚‰ãªã„**ã®ã§ vllm-neuron ã¸ã®ç§»æ¤ãŒã¾ã ä¸å®Œå…¨ãªã®ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚


::::details ãƒ†ã‚¹ãƒˆ 1: vllm bench throughput å®Ÿè¡Œ

```bash
python -m vllm.benchmarks.throughput \
  --model ./models/Qwen3-0.6B-Reranker \
  --tensor-parallel-size 2 \
  --num-prompts 3 \
  --input-len 100 \
  --output-len 10
```

**çµæœ**: âŒ **å¤±æ•—**

- LLM ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆæ™‚ã«ãƒ—ãƒ­ã‚»ã‚¹ãŒãƒãƒ³ã‚°
- åˆæœŸåŒ–ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿å‡ºåŠ›ã•ã‚Œã¦åœæ­¢:
  ```
  INFO 01-24 18:59:19 [__init__.py:207] Platform plugin neuron is activated
  Could not import vLLM config module for overrides: cannot import name 'ModelConfig' from partially initialized module 'vllm.config' (most likely due to a circular import)
  WARNING 01-24 18:59:19 [_custom_ops.py:20] Failed to import from vllm._C with ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
  ```
- JSON å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œãªã„
- ãƒ—ãƒ­ã‚»ã‚¹ã¯å®Œäº†ã‚‚å¤±æ•—ã‚‚ã›ãšã€ç„¡å¿œç­”çŠ¶æ…‹
::::

::::details ãƒ†ã‚¹ãƒˆ 2: ãƒ†ã‚¹ãƒˆ 2: vllm bench throughput å®Ÿè¡Œã€Neuron è¨­å®šã‚ã‚Š

```bash
python -m vllm.benchmarks.throughput \
  --model ./models/Qwen3-0.6B-Reranker \
  --tensor-parallel-size 2 \
  --num-prompts 3 \
  --input-len 100 \
  --output-len 10 \
  --max-model-len 2048 \
  --block-size 32 \
  --max-num-seqs 4 \
  --max-num-batched-tokens 256 \
  --enable-prefix-caching \
  --additional-config '{"override_neuron_config": {"skip_warmup": true, "is_prefix_caching": true, "is_block_kv_layout": true, "pa_num_blocks": 512, "pa_block_size": 32, "enable_bucketing": true}}'
```

**çµæœ**: âŒ **å¤±æ•—**

- ãƒ†ã‚¹ãƒˆ 1 ã¨åŒã˜æŒ™å‹•ï¼ˆLLM åˆæœŸåŒ–æ™‚ã«ãƒãƒ³ã‚°ï¼‰
- `--additional-config` ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒèªè­˜ã•ã‚Œã¦ã„ã‚‹ã‹ã‚‚ä¸æ˜
- ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãªã—
::::

::::details ãƒ†ã‚¹ãƒˆ 3: vllm bench throughputã€CLI ã«ã‚ˆã‚‹å®Ÿè¡Œ

```bash
vllm bench throughput --model ./models/Qwen3-0.6B-Reranker --tensor-parallel-size 2
```

**çµæœ**: âŒ **å¤±æ•—**

```
usage: vllm [-h] [-v] {chat,complete,serve,bench,collect-env,run-batch} ...
vllm: error: unrecognized arguments:
```

**çŸ¥è¦‹**:
- vLLM-Neuron ã® `vllm bench` CLI ã¯å®Ÿè£…ãŒä¸å®Œå…¨ã¾ãŸã¯å‹•ä½œã—ã¦ã„ãªã„
- Python module syntax (`python -m vllm.benchmarks.throughput`) ã‚’ä½¿ã†å¿…è¦ãŒã‚ã‚‹
::::

::::details ã‚¨ãƒ©ãƒ¼æƒ…å ±: å¾ªç’° import ã‚¨ãƒ©ãƒ¼

```
Could not import vLLM config module for overrides: cannot import name 'ModelConfig' from partially initialized module 'vllm.config'
```

ã“ã® warning ã¯ vllm-neuron ã®åˆæœŸåŒ–æ™‚ã«ç™ºç”Ÿã—ã¦ãŠã‚Šã€ä¸€éƒ¨ã®è¨­å®šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒæ­£ã—ããƒ­ãƒ¼ãƒ‰ã•ã‚Œã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
::::

## ã¾ã¨ã‚

æœ¬è¨˜äº‹ã§ã¯ã€vllm-neuron ã‚’ä½¿ç”¨ã—ãŸãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«ã¤ã„ã¦å®Ÿè£…ä¾‹ã®è§£èª¬ã¨å®Ÿéš›ã«çµæœã‚’ç¢ºèªã—ã¾ã—ãŸã€‚ã“ã‚Œã‹ã‚‰ vllm-neuron ã§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã—ã‚ˆã†ã¨ã—ã¦ã„ã‚‹æ–¹ã®ä¸€åŠ©ã«ãªã‚Œã°å¹¸ã„ã§ã™ã€‚ä»Šå¾Œã¯ä»Šå›ã®çµæœã‚’ã‚ˆã‚Šè©³ç´°ã«ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã™ã‚‹æ–¹æ³•ã«ã¤ã„ã¦æ¤œè¨¼ã‚’é€²ã‚ã‚‹äºˆå®šã§ã™ã€‚

## å‚è€ƒè³‡æ–™

- [Qwen3-Reranker-0.6B - Hugging Face](https://huggingface.co/Qwen/Qwen3-Reranker-0.6B)
- [AWS Neuron SDK Documentation](https://awsdocs-neuron.readthedocs-hosted.com/)
- [vLLM Documentation](https://docs.vllm.ai/)
- [Amazon EC2 Inf2 Instances](https://aws.amazon.com/ec2/instance-types/inf2/)
- [AWS Inferentia2 ã§ Inf2 å®Ÿé¨“ç’°å¢ƒã‚’æ§‹ç¯‰ã™ã‚‹](https://zenn.dev/tosshi/articles/eb54037328d2ef)
