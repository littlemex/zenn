---
title: "AWS SageMaker HyperPod ã® Checkpointless/Elastic Training å¾¹åº•è§£èª¬"
emoji: "ğŸ”¥"
type: "tech"
topics: ["AWS", "SageMaker", "HyperPod", "Kubernetes", "æ©Ÿæ¢°å­¦ç¿’"]
published: false
---

# ã¯ã˜ã‚ã«

AWS SageMaker HyperPod ã¯ã€å¤§è¦æ¨¡ãªæ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã«ãŠã„ã¦ã€**éšœå®³ã‹ã‚‰ã®è‡ªå‹•å›å¾©**ã¨**ãƒªã‚½ãƒ¼ã‚¹ã®å‹•çš„ç®¡ç†**ã‚’å®Ÿç¾ã™ã‚‹çµ±åˆçš„ãªãƒ¬ã‚¸ãƒªã‚¨ãƒ³ã‚·ãƒ¼æ©Ÿèƒ½ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚

æœ¬è¨˜äº‹ã§ã¯ã€HyperPod ã®ä¸»è¦ 4 æ©Ÿèƒ½ï¼ˆ**Checkpointless Training**ã€**Elastic Training**ã€**Managed Tiered Checkpointing**ã€**Health Monitoring Agent**ï¼‰ã®æŠ€è¡“çš„ãªå®Ÿè£…æ–¹æ³•ã¨ç›¸äº’é–¢ä¿‚ã«ã¤ã„ã¦è©³ã—ãè§£èª¬ã—ã¾ã™ã€‚

## æœ¬è¨˜äº‹ã§æ‰±ã†å†…å®¹

1. HyperPod ã® 4 ã¤ã®ä¸»è¦æ©Ÿèƒ½ã¨ãã®ç›¸äº’é–¢ä¿‚
2. Checkpointless Training ã¨ Elastic Training ã®å®Ÿè£…è©³ç´°
3. Managed Tiered Checkpointing ã¨ Health Monitoring Agent ã®å½¹å‰²
4. å¯¾å¿œã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—ã¨ OSï¼ˆTrainium trn2 ã®å¯¾å¿œçŠ¶æ³ã‚’å«ã‚€ï¼‰
5. å¾“æ¥æ‰‹æ³•ã¨ã®é•ã„ã¨åˆ†æ•£è¨“ç·´æ‰‹æ³•ã¨ã®äº’æ›æ€§
6. PyTorch Elastic Rendezvous ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã¨ HyperPod CLI ã®å®Ÿè£…ãƒ¬ãƒ™ãƒ«è§£æ
7. Kubernetes ã§ã®å…·ä½“çš„ãªå®Ÿè£…ä¾‹ã¨ GRPO ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰

## å‰ææ¡ä»¶

æœ¬è¨˜äº‹ã§ã¯ä»¥ä¸‹ã‚’å‰æã¨ã—ã¦ã„ã¾ã™ï¼š

- **SageMaker Pipelines ã‚„ Training Job ã¯ä½¿ç”¨ã—ãªã„**
- **Kubernetes (k8s) ãƒã‚¤ãƒ†ã‚£ãƒ–ãªãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨**
- å¯¾è±¡ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ : **SageMaker HyperPod on EKS**

## ç”¨èªã®æ˜ç¢ºåŒ–: "Elastic Training"

**é‡è¦**: AWS ã®å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯ã€ŒElastic Trainingã€ã¨ã„ã†ç”¨èªãŒç’°å¢ƒã«ã‚ˆã£ã¦ç•°ãªã‚‹æ„å‘³ã§ä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

### Slurm ç’°å¢ƒã§ã® "Elastic Training"

Slurm ãƒ™ãƒ¼ã‚¹ã® HyperPod ã§ã¯ã€**Auto-Resumeï¼ˆéšœå®³æ™‚ã®è‡ªå‹•å†é–‹ï¼‰** ã‚’æŒ‡ã—ã¾ã™ã€‚

- ãƒãƒ¼ãƒ‰éšœå®³ã‚„ãƒ—ãƒªã‚¨ãƒ³ãƒ—ã‚·ãƒ§ãƒ³æ™‚ã«ã‚¸ãƒ§ãƒ–ã‚’è‡ªå‹•çš„ã«å†ã‚­ãƒ¥ãƒ¼ã‚¤ãƒ³ã‚°
- Slurm ã® `--requeue` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚‹å®Ÿè£…
- è¨“ç·´ã‚¸ãƒ§ãƒ–å…¨ä½“ã®å†èµ·å‹•ãŒå¯¾è±¡

### EKS ç’°å¢ƒã§ã® "Elastic Training"ï¼ˆæœ¬è¨˜äº‹ã®å¯¾è±¡ï¼‰

Kubernetes ãƒ™ãƒ¼ã‚¹ã® HyperPod ã§ã¯ã€**Elastic Scalingï¼ˆå‹•çš„ãªãƒãƒ¼ãƒ‰å¢—æ¸›ï¼‰** ã‚’æŒ‡ã—ã¾ã™ã€‚

- è¨“ç·´ä¸­ã«ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰æ•°ã‚’å‹•çš„ã«å¢—æ¸›
- PyTorch Elastic ã¨ Kubeflow Training Operator ã«ã‚ˆã‚‹å®Ÿè£…
- `minReplicas` ã‹ã‚‰ `maxReplicas` ã®ç¯„å›²ã§è‡ªå‹•èª¿æ•´

**æœ¬è¨˜äº‹ã§ã¯ EKS ç’°å¢ƒã® Elastic Scalingï¼ˆå‹•çš„ãªãƒãƒ¼ãƒ‰å¢—æ¸›ï¼‰ã«ç„¦ç‚¹ã‚’å½“ã¦ã¾ã™ã€‚**

---

# HyperPod ã®ä¸»è¦æ©Ÿèƒ½

SageMaker HyperPod ã¯ã€å¤§è¦æ¨¡ãªæ©Ÿæ¢°å­¦ç¿’è¨“ç·´ã®ãƒ¬ã‚¸ãƒªã‚¨ãƒ³ã‚·ãƒ¼ï¼ˆresilienceã€å›å¾©åŠ›ï¼‰ã‚’é«˜ã‚ã‚‹ãŸã‚ã€è¤‡æ•°ã®æ©Ÿèƒ½ã‚’çµ±åˆçš„ã«æä¾›ã—ã¦ã„ã¾ã™ã€‚

## 4 ã¤ã®ä¸»è¦æ©Ÿèƒ½

### 1. Checkpointless Training

**ç›®çš„**: éšœå®³å¾©æ—§æ™‚é–“ã®åŠ‡çš„ãªçŸ­ç¸®

- GPU ãƒ¡ãƒ¢ãƒªå†…ã®å†—é•·æ€§ã‚’åˆ©ç”¨ã—ãŸé«˜é€Ÿå¾©æ—§
- ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ã‚’ä¸è¦ã«ã™ã‚‹
- **å¾©æ—§æ™‚é–“**: æ•°åˆ†ï¼ˆAWS å…¬å¼ã§ã¯ "minutes" ã¨è¨˜è¼‰ã€å¾“æ¥ã¯æ•°æ™‚é–“ï¼‰
- **è¿½åŠ ã‚³ã‚¹ãƒˆ**: GPU ãƒ¡ãƒ¢ãƒªã®è¿½åŠ ä½¿ç”¨ã‚ã‚Šï¼ˆå…·ä½“çš„ãªå‰²åˆã¯å…¬å¼æœªå…¬è¡¨ï¼‰
- **å¯¾å¿œç’°å¢ƒ**: HyperPod EKS ã®ã¿
- **å¿…é ˆã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ**: NeMo ãƒ™ãƒ¼ã‚¹ã®ã‚«ã‚¹ã‚¿ãƒ å®Ÿè£…ï¼ˆhyperpod_checkpointless_nemoï¼‰

å‚ç…§: https://aws.amazon.com/blogs/aws/introducing-checkpointless-and-elastic-training-on-amazon-sagemaker-hyperpod/

### 2. Elastic Training / Auto-Resume

**ç›®çš„**: ãƒãƒ¼ãƒ‰éšœå®³æ™‚ã®è‡ªå‹•å†é–‹

#### EKS ç’°å¢ƒ: Elastic Training
- è¨“ç·´ä¸­ã«ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰æ•°ã‚’å‹•çš„ã«å¢—æ¸›
- PyTorch Elastic ã® Rendezvous ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã‚’ä½¿ç”¨
- `minReplicas` ã‹ã‚‰ `maxReplicas` ã®ç¯„å›²ã§è‡ªå‹•èª¿æ•´

#### Slurm ç’°å¢ƒ: Auto-Resume
- ãƒãƒ¼ãƒ‰éšœå®³ã‚„ãƒ—ãƒªã‚¨ãƒ³ãƒ—ã‚·ãƒ§ãƒ³æ™‚ã«ã‚¸ãƒ§ãƒ–ã‚’è‡ªå‹•å†ã‚­ãƒ¥ãƒ¼ã‚¤ãƒ³ã‚°
- Slurm ã® `--requeue` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚‹å®Ÿè£…
- Health Monitoring Agent ã¨é€£æºã—ã¦è‡ªå‹•å¾©æ—§

### 3. Managed Tiered Checkpointing

**ç›®çš„**: ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸éšå±¤ã®æœ€é©åŒ–

[æ³¨æ„] ã“ã®ç”¨èªã¯ AWS å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«æ˜ç¤ºçš„ãªè¨˜è¼‰ãŒãªãã€ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã®ã‚³ãƒ¡ãƒ³ãƒˆã§ã®ã¿è¨€åŠã•ã‚Œã¦ã„ã¾ã™ã€‚æ¨æ¸¬ã•ã‚Œã‚‹æ¦‚å¿µã¯ä»¥ä¸‹ã®é€šã‚Šã§ã™ã€‚

- **Tier 1**: é«˜é€Ÿãƒ­ãƒ¼ã‚«ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ï¼ˆFSx for Lustreã€ãƒ­ãƒ¼ã‚«ãƒ« NVMeï¼‰
- **Tier 2**: æ°¸ç¶šçš„ãƒªãƒ¢ãƒ¼ãƒˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ï¼ˆAmazon S3ï¼‰
- éåŒæœŸä¿å­˜ã«ã‚ˆã‚Šå­¦ç¿’ã‚’ãƒ–ãƒ­ãƒƒã‚¯ã—ãªã„
- å®šæœŸçš„ã« S3 ã«è‡ªå‹•åŒæœŸ

**SageMaker ã®æ¨™æº–æ©Ÿèƒ½ã¨ã®é–¢é€£**:
- `/opt/ml/checkpoints` ã‹ã‚‰ S3 ã¸ã®è‡ªå‹•åŒæœŸ
- ã“ã®æ©Ÿèƒ½ãŒ "Managed Tiered checkpointing" ã®å®Ÿæ…‹ã®å¯èƒ½æ€§

å‚ç…§: https://docs.aws.amazon.com/sagemaker/latest/dg/model-checkpoints.html

### 4. Health Monitoring Agent (HMA)

**ç›®çš„**: ãƒãƒ¼ãƒ‰éšœå®³ã®æ—©æœŸæ¤œå‡ºã¨è‡ªå‹•å¾©æ—§

- ã™ã¹ã¦ã® HyperPod EKS ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã§è‡ªå‹•çš„ã«æœ‰åŠ¹åŒ–
- DaemonSet ã¨ã—ã¦å„ãƒãƒ¼ãƒ‰ä¸Šã§å‹•ä½œ
- GPU/Trainium ãƒ‡ãƒã‚¤ã‚¹ã®éšœå®³ã‚’ç¶™ç¶šçš„ã«ç›£è¦–
- **æ¤œå‡ºæ™‚é–“**: è¿…é€Ÿã«æ¤œå‡ºï¼ˆå…·ä½“çš„ãªæ™‚é–“ã¯å…¬å¼æœªå…¬è¡¨ï¼‰
- **å¯¾å¿œ**: Node Label/Taint è¨­å®šã€è‡ªå‹•å†èµ·å‹•ã¾ãŸã¯äº¤æ›

**ç›£è¦–é …ç›®**:
- NVIDIA GPU: DCGM ãƒãƒªã‚·ãƒ¼é•åã€nvidia-smi ã‚¨ãƒ©ãƒ¼
- AWS Trainium: Neuron Monitor ã‚¨ãƒ©ãƒ¼ã€ãƒ‡ãƒã‚¤ã‚¹ã‚«ã‚¦ãƒ³ãƒˆæ¤œè¨¼
- EC2 Platform Log ã‚¨ãƒ©ãƒ¼

å‚ç…§: https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-eks-resiliency-health-monitoring-agent.html

## æ©Ÿèƒ½ã®ç›¸äº’é–¢ä¿‚

ã“ã‚Œã‚‰ã®æ©Ÿèƒ½ã¯ç‹¬ç«‹ã—ã¦å‹•ä½œã—ã¾ã™ãŒã€çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§æœ€å¤§ã®åŠ¹æœã‚’ç™ºæ®ã—ã¾ã™ã€‚

### çµ±åˆã•ã‚ŒãŸéšœå®³å¾©æ—§ãƒ•ãƒ­ãƒ¼

```
[ãƒãƒ¼ãƒ‰éšœå®³ç™ºç”Ÿ]
        â†“
[Health Monitoring Agent ãŒæ¤œå‡º] (è¿…é€Ÿã«æ¤œå‡º)
        â†“
[Node ã« Taint è¨­å®šã€Pod Eviction]
        â†“
[Node Recovery System ãŒè‡ªå‹•å¾©æ—§] (5-10 åˆ†)
        â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â†“                                     â†“
[Checkpointless Training]           [å¾“æ¥ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ]
GPU ãƒ¡ãƒ¢ãƒªã‹ã‚‰çŠ¶æ…‹å¾©å…ƒ (æ•°åˆ†)         S3 ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰ (60-120 åˆ†)
    â†“                                     â†“
[å­¦ç¿’å†é–‹]                           [å­¦ç¿’å†é–‹]

åˆè¨ˆå¾©æ—§æ™‚é–“: æ•°åˆ†ï½20 åˆ†ç¨‹åº¦        åˆè¨ˆ: 65-130 åˆ†ç¨‹åº¦
```

### æ©Ÿèƒ½ã®çµ„ã¿åˆã‚ã›ãƒ‘ã‚¿ãƒ¼ãƒ³

[æ³¨æ„] ä»¥ä¸‹ã¯å„æ©Ÿèƒ½ã®å¾©æ—§æ™‚é–“ã®æ¦‚ç®—ã§ã™ã€‚å®Ÿéš›ã®å¾©æ—§æ™‚é–“ã¯ç’°å¢ƒã‚„ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«ã‚ˆã£ã¦å¤‰å‹•ã—ã¾ã™ã€‚

| æ©Ÿèƒ½ã®çµ„ã¿åˆã‚ã› | å¾©æ—§æ™‚é–“ï¼ˆæ¨å®šï¼‰ | ç”¨é€” |
|----------------|-----------------|------|
| HMA ã®ã¿ | æ•°ååˆ† | åŸºæœ¬çš„ãªè‡ªå‹•å¾©æ—§ |
| HMA + Checkpointless | æ•°åˆ†ï½20 åˆ† | é«˜é€Ÿå¾©æ—§ãŒå¿…è¦ãªæœ¬ç•ªç’°å¢ƒ |
| HMA + Managed Tiered Checkpointing | æ•°ååˆ† | é•·æœŸè¨“ç·´ã§ã‚³ã‚¹ãƒˆæœ€é©åŒ– |
| HMA + Checkpointless + Managed Tiered | æ•°åˆ†ï½20 åˆ† | æœ€é«˜ãƒ¬ãƒ™ãƒ«ã®å¯ç”¨æ€§ã¨ã‚³ã‚¹ãƒˆæœ€é©åŒ– |
| HMA + Elastic Training | å¯å¤‰ | å‹•çš„ãƒªã‚½ãƒ¼ã‚¹èª¿æ•´ãŒå¿…è¦ãªç’°å¢ƒ |

### Checkpointless vs Managed Tiered Checkpointing

| é …ç›® | Checkpointless Training | Managed Tiered Checkpointing |
|------|------------------------|------------------------------|
| **ç›®çš„** | éšœå®³å¾©æ—§æ™‚é–“ã®çŸ­ç¸® | ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆç®¡ç†ã®åŠ¹ç‡åŒ– |
| **å¾©æ—§æ™‚é–“** | æ•°åˆ† | æ•°æ™‚é–“ï¼ˆS3 ã‹ã‚‰ã®å–å¾—ãŒå¿…è¦ãªå ´åˆï¼‰ |
| **è¿½åŠ ã‚³ã‚¹ãƒˆ** | GPU ãƒ¡ãƒ¢ãƒªã®è¿½åŠ ä½¿ç”¨ | ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚³ã‚¹ãƒˆ |
| **ãƒ‡ã‚£ã‚¹ã‚¯ I/O** | ãªã— | å®šæœŸçš„ãª S3 ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ |
| **åŒæ™‚ä½¿ç”¨** | å¯èƒ½ | Checkpointless ã‚’è£œå®Œ |
| **å¯¾å¿œç’°å¢ƒ** | HyperPod EKS ã®ã¿ | EKSã€Slurmã€é€šå¸¸ã® EC2 |

---

# Checkpointless Training ã¨ã¯ï¼ˆè©³ç´°ï¼‰

## æ¦‚è¦

Checkpointless Training ã¯ã€AWS SageMaker HyperPod ã«å®Ÿè£…ã•ã‚ŒãŸæ©Ÿèƒ½ã§ã€**ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ã®éšœå®³ã‹ã‚‰æ•°åˆ†ä»¥å†…ã«è‡ªå‹•å›å¾©**ã™ã‚‹ä»•çµ„ã¿ã§ã™ã€‚

å¾“æ¥ã®è¨“ç·´ã§ã¯ã€å®šæœŸçš„ã«ãƒ¢ãƒ‡ãƒ«ã®çŠ¶æ…‹ï¼ˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆï¼‰ã‚’ãƒ‡ã‚£ã‚¹ã‚¯ã«ä¿å­˜ã—ã€éšœå®³ç™ºç”Ÿæ™‚ã«ã¯ãã“ã‹ã‚‰å¾©å…ƒã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã—ãŸã€‚ã—ã‹ã—ã€å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§ã¯ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ä¿å­˜ã«æ•°åˆ†ã‹ã‹ã‚Šã€ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚‚å¤§é‡ã«æ¶ˆè²»ã—ã¾ã™ï¼ˆä¾‹: Llama 3 70B ã§ 100GB ä»¥ä¸Šï¼‰ã€‚

Checkpointless Training ã¯ã€ã“ã®**ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ã‚’ä¸è¦ã«ã—**ã€ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸åŠ¹ç‡ã‚’ä¸¡ç«‹ã•ã›ã¾ã™ã€‚

å‚ç…§ï¼šhttps://aws.amazon.com/sagemaker/hyperpod/

## ä¸»ãªåˆ©ç‚¹

1. **I/O ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã®å‰Šæ¸›**: ãƒ‡ã‚£ã‚¹ã‚¯ã¸ã®æ›¸ãè¾¼ã¿ãŒä¸è¦
2. **ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚³ã‚¹ãƒˆã®å‰Šæ¸›**: å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ãŒä¸è¦
3. **å¾©æ—§æ™‚é–“ã®çŸ­ç¸®**: éšœå®³ã‹ã‚‰æ•°åˆ†ä»¥å†…ã«è‡ªå‹•å›å¾©
4. **ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚é–“ã®å‰Šæ¸›**: Checkpointless Training ã‚’å«ã‚€ HyperPod ã®ãƒ¬ã‚¸ãƒªã‚¨ãƒ³ã‚·ãƒ¼æ©Ÿèƒ½å…¨ä½“ã§æœ€å¤§ 40% ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚é–“å‰Šæ¸›ï¼ˆAWS å…¬å¼æƒ…å ±ï¼‰
5. **å¸‚å ´æŠ•å…¥æ™‚é–“ã®çŸ­ç¸®**: æ•°é€±é–“ã®æ™‚é–“ç¯€ç´„ãŒå¯èƒ½

## å¯¾è±¡ãƒ¢ãƒ‡ãƒ«

ç¾æ™‚ç‚¹ã§ä»¥ä¸‹ã®ãƒ¢ãƒ‡ãƒ«ãŒã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™ï¼š

- **Llama 3 70B**ï¼ˆLoRAã€äº‹å‰å­¦ç¿’ï¼‰
- **GPT-OSS 120B**ï¼ˆå®Œå…¨å¾®èª¿æ•´ã€LoRAï¼‰

å‚ç…§ï¼šhttps://github.com/aws/sagemaker-hyperpod-recipes

---

# Elastic Training ã¨ã¯

## æ¦‚è¦

Elastic Training ã¯ã€è¨“ç·´ã‚¸ãƒ§ãƒ–å®Ÿè¡Œä¸­ã«**ãƒãƒ¼ãƒ‰æ•°ã‚’å‹•çš„ã«å¢—æ¸›ã§ãã‚‹**æ©Ÿèƒ½ã§ã™ã€‚PyTorch Elastic ã¨ Kubeflow Training Operator ã®çµ±åˆã«ã‚ˆã‚Šã€ä»¥ä¸‹ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ï¼š

- ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ã‚§ãƒ¼ã‚ºã«åˆã‚ã›ãŸè¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã®æœ€é©åŒ–
- ã‚³ã‚¹ãƒˆåŠ¹ç‡ã®å‘ä¸Šï¼ˆå¿…è¦ãªã¨ãã ã‘ãƒªã‚½ãƒ¼ã‚¹ã‚’è¿½åŠ ï¼‰
- ãƒãƒ¼ãƒ‰éšœå®³æ™‚ã®è‡ªå‹•å›å¾©ã¨å†é–‹

## æŠ€è¡“çš„åŸºç›¤

Elastic Training ã¯ä»¥ä¸‹ã®æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯ã§å®Ÿç¾ã•ã‚Œã¦ã„ã¾ã™ï¼š

### PyTorch Elastic

PyTorch ã® `torch.distributed.elastic` ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã€**Rendezvous** ã¨å‘¼ã°ã‚Œã‚‹ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã§ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰ã®å‹•çš„ãªå¢—æ¸›ã‚’ç®¡ç†ã—ã¾ã™ã€‚

- **Rendezvous Backend**: etcd ã¾ãŸã¯ c10d ã‚’ä½¿ç”¨
- **å‹•çš„ãªãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚µã‚¤ã‚ºå¤‰æ›´**: `WORLD_SIZE` ã¨ `RANK` ã‚’è‡ªå‹•æ›´æ–°
- **Fault Tolerance**: ãƒãƒ¼ãƒ‰éšœå®³æ™‚ã®è‡ªå‹•å†èµ·å‹•

å‚ç…§ï¼šhttps://pytorch.org/docs/stable/distributed.html

### Kubeflow Training Operator

Kubernetes ã‚«ã‚¹ã‚¿ãƒ ãƒªã‚½ãƒ¼ã‚¹ `PyTorchJob` ã‚’ä½¿ç”¨ã—ã€Kubernetes ãƒã‚¤ãƒ†ã‚£ãƒ–ãªæ–¹æ³•ã§åˆ†æ•£è¨“ç·´ã‚’ç®¡ç†ã—ã¾ã™ã€‚

- **ElasticPolicy**: `minReplicas` ã¨ `maxReplicas` ã§å‹•çš„ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’å®šç¾©
- **Auto-Resume**: Pod éšœå®³æ™‚ã®è‡ªå‹•å†é–‹
- **kubectl çµ±åˆ**: æ¨™æº–çš„ãª Kubernetes ãƒ„ãƒ¼ãƒ«ã§ç®¡ç†

å‚ç…§ï¼šhttps://www.kubeflow.org/docs/components/training/pytorch/

---

# å¯¾å¿œã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—ã¨ OS

## å¯¾å¿œã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—

SageMaker HyperPod ã§ã¯ä»¥ä¸‹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—ã§ Checkpointless/Elastic Training ãŒåˆ©ç”¨å¯èƒ½ã§ã™ã€‚

### Trainium ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆAWS Neuronï¼‰

| ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ— | ä¸–ä»£ | ãƒãƒƒãƒ—æ•° | HyperPod å¯¾å¿œ |
|-------------------|------|---------|--------------|
| ml.trn1.32xlarge | ç¬¬ 1 ä¸–ä»£ | 16 | å¯¾å¿œ |
| ml.trn1n.32xlarge | ç¬¬ 1 ä¸–ä»£æ‹¡å¼µ | 16 | å¯¾å¿œ |
| **ml.trn2.3xlarge** | **ç¬¬ 2 ä¸–ä»£** | **1** (æ¨å®š) | **å¯¾å¿œ** |
| **ml.trn2.48xlarge** | **ç¬¬ 2 ä¸–ä»£** | **16** | **å¯¾å¿œ** |

[é‡è¦] **trn2 ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã¯ SageMaker HyperPod ã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã¾ã™**ï¼ˆAPI ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ç¢ºèªæ¸ˆã¿ï¼‰ã€‚ãŸã ã—ã€Checkpointless Training ç­‰ã®å€‹åˆ¥æ©Ÿèƒ½ã® trn2 ã§ã®å‹•ä½œæ¤œè¨¼çŠ¶æ³ã¯å…¬å¼ã«æ˜ç¤ºã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚

å‚ç…§ï¼š
- https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_ClusterInstanceGroupSpecification.html
- https://aws.amazon.com/machine-learning/trainium/

### GPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹

| ã‚·ãƒªãƒ¼ã‚º | ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä¾‹ | GPU | HyperPod å¯¾å¿œ |
|---------|--------------|-----|--------------|
| P5 | ml.p5.48xlarge | H100 x8 | å¯¾å¿œ |
| P4 | ml.p4d.24xlarge | A100 x8 | å¯¾å¿œ |
| P6 | ml.p6e-gb200.36xlarge | Blackwell GB200 | å¯¾å¿œ |

## å¯¾å¿œ OS

### Trainium ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆNeuron SDKï¼‰

æ¨å¥¨ OSï¼ˆ2024 å¹´ 12 æœˆä»¥é™ï¼‰ï¼š

- **Ubuntu 22.04 LTS**ï¼ˆæ¨å¥¨ï¼‰
- **Amazon Linux 2023**

æ³¨æ„: Neuron SDK 2.21.0 ä»¥é™ã€ã™ã¹ã¦ã® Deep Learning Container (DLC) ãŒ Ubuntu 22.04 ã«ç§»è¡Œã—ã¾ã—ãŸã€‚

å‚ç…§ï¼šhttps://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/containers/neuron-dlc.html

### GPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆCUDAï¼‰

å¯¾å¿œ OSï¼š

- Ubuntu 24.04 LTS, 22.04 LTSï¼ˆæ¨å¥¨ï¼‰
- Amazon Linux 2023
- Red Hat Enterprise Linux 10, 9, 8
- Debian 12, 13

å‚ç…§ï¼šhttps://docs.nvidia.com/cuda/cuda-installation-guide-linux/

## SDK/ãƒ‰ãƒ©ã‚¤ãƒè¦ä»¶

### Trainiumï¼ˆNeuron SDKï¼‰

- **Neuron SDK**: 2.27.1ï¼ˆ2026 å¹´ 1 æœˆãƒªãƒªãƒ¼ã‚¹ï¼‰
- **PyTorch NeuronX**: 2.5.1 æ¨å¥¨ï¼ˆ2.9 ãŒ NxDT æœ€çµ‚ã‚µãƒãƒ¼ãƒˆï¼‰
- **Python**: 3.11, 3.12
- **é‡è¦**: PyTorch 2.10 ä»¥é™ã¯ native PyTorch + TorchTitan ã¸ã®ç§»è¡ŒãŒæ¨å¥¨

å‚ç…§ï¼šhttps://github.com/aws-neuron/aws-neuron-sdk/releases

### GPUï¼ˆCUDAï¼‰

- **CUDA**: æœ€æ–°æ¨å¥¨ï¼ˆ12.x ç³»ã¾ãŸã¯ 13.x ç³»ï¼‰
- **NVIDIA Driver**: æœ€æ–°æ¨å¥¨
- **cuDNN/TensorRT**: CUDA å¯¾å¿œç‰ˆ

å‚ç…§ï¼šhttps://docs.nvidia.com/cuda/cuda-installation-guide-linux/

---

# å¾“æ¥æ‰‹æ³•ã¨ã®é•ã„

## ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å´ã§ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ã¨ã®é•ã„

### å¾“æ¥ã® PyTorch ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ

```python
# å¾“æ¥ã®æ‰‹æ³•
torch.save({
    'epoch': epoch,
    'model_state_dict': model.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': loss,
}, checkpoint_path)
```

#### å¾“æ¥æ‰‹æ³•ã®èª²é¡Œ

| é …ç›® | å¾“æ¥ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ | Checkpointless Training |
|-----|-------------------------|------------------------|
| **I/O ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰** | å¤§ï¼ˆãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«æ¯”ä¾‹ã€æ•°åˆ†ï¼‰ | ãªã—ï¼ˆãƒ¡ãƒ¢ãƒªå†…ä¿æŒï¼‰ |
| **ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ä½¿ç”¨é‡** | å¤§ï¼ˆ70B ãƒ¢ãƒ‡ãƒ«ã§ 100GB+ï¼‰ | æœ€å° |
| **å¾©æ—§æ™‚é–“** | ä¸­ï¼ˆãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ­ãƒ¼ãƒ‰ + åˆæœŸåŒ–ï¼‰ | çŸ­ï¼ˆæ•°åˆ†ä»¥å†…ï¼‰ |
| **ä¿å­˜é »åº¦** | Epoch ã”ã¨ã€ã¾ãŸã¯æ•°ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ | ä¸è¦ |

å‚ç…§ï¼šhttps://docs.pytorch.org/tutorials/beginner/saving_loading_models.html

## Slurm Auto-Resume ã¨ã®é•ã„

### Slurm ã® `--requeue` ã‚ªãƒ—ã‚·ãƒ§ãƒ³

```bash
sbatch --requeue --time=48:00:00 train.sh
```

Slurm ã® auto-resume ã¯**ã‚¸ãƒ§ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¬ãƒ™ãƒ«**ã§ã®å†èµ·å‹•ã§ã™ï¼š

- ãƒãƒ¼ãƒ‰éšœå®³ã‚„ãƒ—ãƒªã‚¨ãƒ³ãƒ—ã‚·ãƒ§ãƒ³æ™‚ã«ã‚¸ãƒ§ãƒ–ã‚’å†ã‚­ãƒ¥ãƒ¼ã‚¤ãƒ³ã‚°
- **ãƒãƒƒãƒã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’æœ€åˆã‹ã‚‰å®Ÿè¡Œ**
- ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãŒãªã„å ´åˆã¯è¨“ç·´ãŒæœ€åˆã‹ã‚‰ã‚„ã‚Šç›´ã—

| é …ç›® | Slurm Auto-Resume | HyperPod Checkpointless |
|-----|------------------|------------------------|
| **åˆ¶å¾¡ãƒ¬ãƒ™ãƒ«** | ã‚¸ãƒ§ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ© | ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ |
| **å¾©æ—§ç²’åº¦** | ã‚¸ãƒ§ãƒ–å…¨ä½“ | ã‚¹ãƒ†ãƒ¼ãƒˆä¿æŒ |
| **è¨“ç·´é€²æ—** | æœ€åˆã‹ã‚‰ã‚„ã‚Šç›´ã— | ç¶™ç¶šå®Ÿè¡Œ |
| **ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰** | ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°é…å»¶ | æœ€å° |

å‚ç…§ï¼šhttps://slurm.schedmd.com/sbatch.html

## Kubernetes Job ã® Restart Policy ã¨ã®é•ã„

### Kubernetes Job

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: training-job
spec:
  backoffLimit: 4
  template:
    spec:
      containers:
      - name: trainer
        image: training: latest
      restartPolicy: OnFailure
```

Kubernetes Job ã¯**å˜ç´”ãªãƒªãƒˆãƒ©ã‚¤ãƒ¡ã‚«ãƒ‹ã‚ºãƒ **ã®ã¿ã‚’æä¾›ï¼š

- Pod å¤±æ•—æ™‚ã«æ–°ã—ã„ Pod ã‚’èµ·å‹•
- `backoffLimit` ã«é”ã™ã‚‹ã¾ã§å†è©¦è¡Œ
- **ã‚¹ãƒ†ãƒ¼ãƒˆä¿æŒãªã—**ï¼ˆæœ€åˆã‹ã‚‰å®Ÿè¡Œï¼‰

| é …ç›® | K8s Job | K8s StatefulSet | HyperPod |
|-----|---------|----------------|----------|
| **è‡ªå‹•å†èµ·å‹•** | ã‚ã‚Šï¼ˆOnFailureï¼‰ | ã‚ã‚Šï¼ˆAlwaysï¼‰ | ã‚ã‚Šï¼ˆAuto-Resumeï¼‰ |
| **çŠ¶æ…‹ä¿æŒ** | ãªã— | PVC ã«ã‚ˆã‚‹æ°¸ç¶šåŒ– | ãƒ¡ãƒ¢ãƒªå†…ä¿æŒ |
| **ãƒãƒ¼ãƒ‰éšœå®³å¯¾å¿œ** | æ–° Pod èµ·å‹• | æ–° Pod + PVC | è‡ªå‹•å›å¾© |
| **å¾©æ—§æ™‚é–“** | é…ã„ï¼ˆåˆæœŸåŒ–ã‹ã‚‰ï¼‰ | ä¸­ç¨‹åº¦ï¼ˆãƒ­ãƒ¼ãƒ‰æ™‚é–“ï¼‰ | çŸ­ã„ï¼ˆæ•°åˆ†ä»¥å†…ï¼‰ |

å‚ç…§ï¼šhttps://kubernetes.io/docs/concepts/workloads/controllers/job/

---

# ãƒãƒ¼ãƒ‰å¢—æ¸›ã®ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 

## å¢—æ¸›ã®å˜ä½

Elastic Training ã§ã¯**ãƒãƒ¼ãƒ‰å˜ä½**ã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãŒè¡Œã‚ã‚Œã¾ã™ã€‚

### Kubeflow PyTorchJob ã® ElasticPolicy

```yaml
apiVersion: kubeflow.org/v1
kind: PyTorchJob
metadata:
  name: elastic-training
spec:
  elasticPolicy:
    rdzvBackend: etcd
    rdzvHost: etcd
    rdzvPort: 2379
    minReplicas: 1      # æœ€å°ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
    maxReplicas: 64     # æœ€å¤§ãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
    maxRestarts: 100    # æœ€å¤§ãƒªã‚¹ã‚¿ãƒ¼ãƒˆå›æ•°
```

- **minReplicas**: è¨“ç·´ã‚’é–‹å§‹ã™ã‚‹ãŸã‚ã®æœ€å°ãƒãƒ¼ãƒ‰æ•°
- **maxReplicas**: ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—å¯èƒ½ãªæœ€å¤§ãƒãƒ¼ãƒ‰æ•°
- **å‹•çš„ç¯„å›²**: `minReplicas` ã‹ã‚‰ `maxReplicas` ã®é–“ã§è‡ªå‹•èª¿æ•´

å‚ç…§ï¼šhttps://github.com/aws-samples/awsome-distributed-training/blob/main/3.test_cases/pytorch/neuronx-distributed/llama3/kubernetes/llama3_train.yaml-template

## ãƒˆãƒªã‚¬ãƒ¼æ–¹å¼

### 1. æ‰‹å‹•ãƒˆãƒªã‚¬ãƒ¼

Kubernetes ã®æ¨™æº–ã‚³ãƒãƒ³ãƒ‰ã§ãƒãƒ¼ãƒ‰æ•°ã‚’å¤‰æ›´ï¼š

```bash
# PyTorchJob ã® Worker ãƒ¬ãƒ—ãƒªã‚«æ•°ã‚’å¤‰æ›´
kubectl scale pytorchjob elastic-training --replicas=8
```

### 2. ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹ãƒˆãƒªã‚¬ãƒ¼ï¼ˆHPAï¼‰

Horizontal Pod Autoscaler (HPA) ã‚’ä½¿ç”¨ï¼š

```yaml
elasticPolicy:
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 90
```

- CPU/GPU ä½¿ç”¨ç‡ã«åŸºã¥ã„ã¦è‡ªå‹•ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
- ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚‚è¨­å®šå¯èƒ½ï¼ˆä¾‹: ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã€loss å€¤ï¼‰

### 3. è‡ªå‹•ãƒˆãƒªã‚¬ãƒ¼ï¼ˆRendezvousï¼‰

PyTorch Elastic ã® Rendezvous ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ï¼š

- æ–°è¦ãƒãƒ¼ãƒ‰å‚åŠ ã‚’è‡ªå‹•æ¤œå‡º
- ãƒãƒ¼ãƒ‰éšœå®³ã‚’è‡ªå‹•æ¤œå‡º
- æ¤œå‡ºå¾Œã€è‡ªå‹•çš„ã«å† rendezvous ã‚’å®Ÿè¡Œ

## æŠ€è¡“çš„ãªå®Ÿç¾æ–¹æ³•

### PyTorch Elastic ã® Rendezvous ãƒ¡ã‚«ãƒ‹ã‚ºãƒ 

```bash
torchrun \
  --nnodes=2:8 \
  --nproc-per-node=8 \
  --rdzv-backend=c10d \
  --rdzv-endpoint=$MASTER_ADDR: $MASTER_PORT \
  --rdzv-id=$JOB_ID \
  --max-restarts=3 \
  train.py
```

#### Rendezvous ã®å‹•ä½œ

1. **ãƒãƒªã‚¢æ©Ÿæ§‹ï¼ˆBarrierï¼‰**
   - æœ€å°ãƒãƒ¼ãƒ‰æ•°ï¼ˆ`minReplicas`ï¼‰ã«é”ã™ã‚‹ã¾ã§ãƒ–ãƒ­ãƒƒã‚¯
   - æœ€å¤§ãƒãƒ¼ãƒ‰æ•°ï¼ˆ`maxReplicas`ï¼‰åˆ°é”ã§å³åº§ã«å®Œäº†

2. **æ’ä»–æ€§ï¼ˆExclusivityï¼‰**
   - è¤‡æ•°ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—ãŒä¸¦è¡Œå½¢æˆã•ã‚Œã‚‹ã“ã¨ã‚’é˜²æ­¢
   - æ—¢å­˜ã® rendezvous å®Œäº†æ™‚ã€æ–°è¦ãƒãƒ¼ãƒ‰ã¯å¾…æ©Ÿ

3. **ä¸€è²«æ€§ï¼ˆConsistencyï¼‰**
   - å…¨ãƒ¡ãƒ³ãƒãƒ¼ãŒçµ±ä¸€ã•ã‚ŒãŸ `RANK` å‰²ã‚Šå½“ã¦ã«åŒæ„
   - å† rendezvous æ™‚ã« `RANK` ãŒå¤‰æ›´ã•ã‚Œã‚‹å¯èƒ½æ€§

4. **è€éšœå®³æ€§ï¼ˆFault Toleranceï¼‰**
   - ãƒãƒ¼ãƒ‰éšœå®³æ™‚ã«è‡ªå‹•çš„ã«å† rendezvous å®Ÿè¡Œ
   - ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã‚„æ¥ç¶šå–ªå¤±ã‚’è¨±å®¹

å‚ç…§ï¼š
- PyTorch Elastic: https://pytorch.org/docs/stable/elastic/run.html
- Rendezvous å®Ÿè£…: https://github.com/pytorch/pytorch (torch/distributed/elastic/rendezvous/)

### etcd ãƒ™ãƒ¼ã‚¹ã®å®Ÿè£…

```yaml
# etcd Serviceï¼ˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆé€šä¿¡ç”¨ï¼‰
apiVersion: v1
kind: Service
metadata:
  name: etcd
spec:
  ports:
    - name: etcd-client-port
      port: 2379
      protocol: TCP
      targetPort: 2379
  selector:
    app: etcd
---
# etcd Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  name: etcd
spec:
  replicas: 1
  template:
    spec:
      containers:
        - name: etcd
          image: quay.io/coreos/etcd: v3.5.19
          command:
            - /usr/local/bin/etcd
          args:
            - "--enable-v2"
            - "--listen-client-urls=http://0.0.0.0:2379"
            - "--advertise-client-urls=http://etcd:2379"
      restartPolicy: Always
```

etcd ã¯**åˆ†æ•£åˆæ„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ **ï¼ˆRaftï¼‰ã‚’ä½¿ç”¨ã—ã€ä»¥ä¸‹ã‚’ç®¡ç†ï¼š

- ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰ã®ãƒ¡ãƒ³ãƒãƒ¼ã‚·ãƒƒãƒ—æƒ…å ±
- ç¾åœ¨ã®ãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚µã‚¤ã‚ºã¨å„ãƒãƒ¼ãƒ‰ã®ãƒ©ãƒ³ã‚¯
- Rendezvous ã®çŠ¶æ…‹ï¼ˆwaiting, running, completedï¼‰

## ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚¤ãƒ™ãƒ³ãƒˆã®å‡¦ç†

### ãƒãƒ¼ãƒ‰å¢—åŠ æ™‚

1. æ–°ã—ã„ãƒãƒ¼ãƒ‰ãŒ etcd ã«å‚åŠ ã‚’ç™»éŒ²
2. etcd ãŒ rendezvous å®Œäº†ã‚’é€šçŸ¥
3. **æ—¢å­˜ãƒ¯ãƒ¼ã‚«ãƒ¼ãŒä¸€æ™‚åœæ­¢**
4. æ–°ã—ã„ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—ãŒå½¢æˆï¼ˆ`WORLD_SIZE` ãŒå¢—åŠ ï¼‰
5. å…¨ãƒ¯ãƒ¼ã‚«ãƒ¼ãŒæ–°ã—ã„ `RANK` ã¨ `WORLD_SIZE` ã§å†èµ·å‹•

### ãƒãƒ¼ãƒ‰æ¸›å°‘æ™‚

1. ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒé›¢è„±ã‚’æ¤œå‡ºï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¾ãŸã¯æ˜ç¤ºçš„ãªé›¢è„±ï¼‰
2. **æ—¢å­˜ãƒ¯ãƒ¼ã‚«ãƒ¼ãŒä¸€æ™‚åœæ­¢**
3. æ®‹ã‚Šã®ãƒãƒ¼ãƒ‰ã§æ–°ã—ã„ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚°ãƒ«ãƒ¼ãƒ—å½¢æˆï¼ˆ`WORLD_SIZE` ãŒæ¸›å°‘ï¼‰
4. å…¨ãƒ¯ãƒ¼ã‚«ãƒ¼ãŒæ›´æ–°ã•ã‚ŒãŸ `RANK` ã¨ `WORLD_SIZE` ã§å†èµ·å‹•

### ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¸ã®å½±éŸ¿

- **ä¸€æ™‚åœæ­¢ã®æœ‰ç„¡**: ç¾åœ¨ã®å®Ÿè£…ã§ã¯ä¸€æ™‚åœæ­¢ãŒç™ºç”Ÿã—ã¾ã™ï¼ˆå…¨ãƒ—ãƒ­ã‚»ã‚¹å†èµ·å‹•ï¼‰
- **ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ **: æ•°ç§’ï½æ•°åç§’ï¼ˆrendezvous + ãƒ—ãƒ­ã‚»ã‚¹èµ·å‹•æ™‚é–“ï¼‰
- **çŠ¶æ…‹å¾©å…ƒ**: Checkpointless Training ã®å ´åˆã¯ãƒ¡ãƒ¢ãƒªå†…çŠ¶æ…‹ã‹ã‚‰å¾©å…ƒ
- **ãƒãƒƒãƒã‚µã‚¤ã‚ºèª¿æ•´**: `WORLD_SIZE` å¤‰æ›´ã«ã‚ˆã‚Šã€ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒãƒƒãƒã‚µã‚¤ã‚ºãŒå¤‰ã‚ã‚‹å¯èƒ½æ€§

---

# åˆ†æ•£è¨“ç·´æ‰‹æ³•ã¨ã®äº’æ›æ€§

Elastic Trainingï¼ˆå‹•çš„ãƒãƒ¼ãƒ‰å¢—æ¸›ï¼‰ã¯ã€ä½¿ç”¨ã™ã‚‹åˆ†æ•£è¨“ç·´æ‰‹æ³•ã«ã‚ˆã£ã¦å¯¾å¿œçŠ¶æ³ãŒç•°ãªã‚Šã¾ã™ã€‚

## ä¸»è¦ãªåˆ†æ•£è¨“ç·´æ‰‹æ³•

### DDP (Distributed Data Parallel)

| é …ç›® | å¯¾å¿œçŠ¶æ³ |
|------|---------|
| **Elastic Training** | âœ… å®Œå…¨å¯¾å¿œ |
| **Checkpointless Training** | âœ… å¯¾å¿œ |
| **å‚™è€ƒ** | PyTorch Elastic ã®æ¨™æº–ã‚µãƒãƒ¼ãƒˆ |

### FSDP (Fully Sharded Data Parallel)

| é …ç›® | å¯¾å¿œçŠ¶æ³ |
|------|---------|
| **Elastic Training** | âœ… å¯¾å¿œ |
| **Checkpointless Training** | âœ… å¯¾å¿œ |
| **å‚™è€ƒ** | FSDP2 ã§ã¯åˆ¶ç´„ã‚ã‚Šï¼ˆã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æˆ¦ç•¥ã®å†æ§‹æˆãŒå¿…è¦ï¼‰ |

### DeepSpeed ZeRO

| é …ç›® | å¯¾å¿œçŠ¶æ³ |
|------|---------|
| **Elastic Training** | â–³ éƒ¨åˆ†å¯¾å¿œ |
| **Checkpointless Training** | âœ… å¯¾å¿œ |
| **å‚™è€ƒ** | ZeRO Stage 3 ã§ã¯ãƒ¢ãƒ‡ãƒ«ã®å†ã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãŒå¿…è¦ |

### NxD (NeuronX Distributed Training)

| é …ç›® | å¯¾å¿œçŠ¶æ³ |
|------|---------|
| **Elastic Training** | âŒ æœªå¯¾å¿œ |
| **Checkpointless Training** | âœ… å¯¾å¿œ |
| **å‚™è€ƒ** | Neuron SDK ã¯å›ºå®šãƒãƒ¼ãƒ‰æ•°ã§ã®è¨“ç·´ã‚’å‰æ |

### GRPO (Group Relative Policy Optimization)

| é …ç›® | å¯¾å¿œçŠ¶æ³ |
|------|---------|
| **Elastic Training** | âŒ å›°é›£ |
| **Checkpointless Training** | â–³ ä¸æ˜ï¼ˆå…¬å¼ã®æ¤œè¨¼æƒ…å ±ãªã—ï¼‰ |
| **å‚™è€ƒ** | è¤‡æ•°ãƒ¢ãƒ‡ãƒ«ï¼ˆActor, Ref, Rewardï¼‰ç®¡ç†ã¨ç”Ÿæˆãƒ•ã‚§ãƒ¼ã‚ºåˆ†é›¢ãŒèª²é¡Œ |

**GRPO å®Ÿè£…ä¾‹**:
- **TRL å®Ÿè£…**: https://github.com/aws-samples/awsome-distributed-training/tree/main/3.test_cases/pytorch/trl/grpo
  - Slurm ãƒ™ãƒ¼ã‚¹ã€DeepSpeed ZeRO-3ã€Qwen2.5-72B ãƒ¢ãƒ‡ãƒ«ã§æ¤œè¨¼æ¸ˆã¿
- **VERL å®Ÿè£…**: https://github.com/aws-samples/awsome-distributed-training/tree/main/3.test_cases/pytorch/verl/rlvr
  - EKS + Ray ãƒ™ãƒ¼ã‚¹ã€FSDPã€p5en.48xlarge 4 ãƒãƒ¼ãƒ‰ç’°å¢ƒã§æ¤œè¨¼

## Elastic Training ã®åˆ¶ç´„

ãƒãƒ¼ãƒ‰æ•°ã®å‹•çš„å¤‰æ›´ãŒå›°é›£ãªç†ç”±ï¼š

1. **ãƒ¢ãƒ‡ãƒ«ã®ã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°å†æ§‹æˆ**: ZeRO Stage 3 ã‚„ FSDP ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒåˆ†æ•£ã•ã‚Œã¦ã„ã‚‹å ´åˆã€ãƒãƒ¼ãƒ‰æ•°å¤‰æ›´æ™‚ã«å†ã‚·ãƒ£ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãŒå¿…è¦
2. **å›ºå®šçš„ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: Tensor Parallel ã‚„ Pipeline Parallel ã¯å›ºå®šãƒãƒ¼ãƒ‰æ•°ã‚’å‰æ
3. **ç”Ÿæˆãƒ•ã‚§ãƒ¼ã‚ºã¨ã®ä¾å­˜**: GRPO ã®ã‚ˆã†ãª RL æ‰‹æ³•ã§ã¯ã€ç”Ÿæˆã‚µãƒ¼ãƒãƒ¼ï¼ˆvLLMï¼‰ã¨ã®å¯†çµåˆãŒèª²é¡Œ
4. **ã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆ¶ç´„**: Ray ã‚¯ãƒ©ã‚¹ã‚¿ã‚„ Neuron Runtime ã¯å®Ÿè¡Œä¸­ã®ãƒãƒ¼ãƒ‰æ•°å¤‰æ›´ã«å¯¾å¿œã—ã¦ã„ãªã„

---

# å®Ÿè£…ãƒ¬ãƒ™ãƒ«ã®è©³ç´°è§£æ

## PyTorch Elastic Rendezvous ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®å®Ÿè£…

PyTorch Elastic ã® Rendezvous ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã¯ã€åˆ†æ•£è¨“ç·´ã«ãŠã„ã¦è¤‡æ•°ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰ãŒã€Œé›†åˆã€ã—ã€äº’ã„ã‚’ç™ºè¦‹ã—ã€å½¹å‰²ï¼ˆrankï¼‰ã‚’åˆæ„ã™ã‚‹ãŸã‚ã®ä»•çµ„ã¿ã§ã™ã€‚å†…éƒ¨å®Ÿè£…ã«ã¯ 2 ã¤ã®æ–¹å¼ãŒã‚ã‚Šã¾ã™ã€‚

### DynamicRendezvousHandlerï¼ˆæ¨å¥¨å®Ÿè£…ï¼‰

æœ€æ–°ã®æ¨å¥¨å®Ÿè£…ã¯ `DynamicRendezvousHandler` ã§ã€çŠ¶æ…‹æ©Ÿæ¢°ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚

ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰: https://github.com/pytorch/pytorch/blob/main/torch/distributed/elastic/rendezvous/dynamic_rendezvous.py

#### çŠ¶æ…‹ãƒ¢ãƒ‡ãƒ«

Rendezvous ã®çŠ¶æ…‹ã¯ `_RendezvousState` ã‚¯ãƒ©ã‚¹ã§ç®¡ç†ã•ã‚Œã¾ã™ï¼ˆ272-316 è¡Œç›®ï¼‰ã€‚

```python
class _RendezvousState:
    round: int              # Rendezvous ã®ãƒ©ã‚¦ãƒ³ãƒ‰ç•ªå·
    complete: bool          # ç¾åœ¨ã®ãƒ©ã‚¦ãƒ³ãƒ‰ãŒå®Œäº†ã—ãŸã‹
    deadline: datetime | None  # last_call ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã®ãƒ‡ãƒƒãƒ‰ãƒ©ã‚¤ãƒ³
    closed: bool            # Rendezvous ãŒé–‰ã˜ã‚‰ã‚ŒãŸã‹
    participants: dict[_NodeDesc, int]  # å‚åŠ è€…ãƒãƒ¼ãƒ‰ â†’ rank ã®ãƒãƒƒãƒ”ãƒ³ã‚°
    wait_list: set[_NodeDesc]          # æ¬¡ã®ãƒ©ã‚¦ãƒ³ãƒ‰ã®å¾…æ©Ÿãƒªã‚¹ãƒˆ
    redundancy_list: set[_NodeDesc]    # å†—é•·ãƒãƒ¼ãƒ‰ãƒªã‚¹ãƒˆï¼ˆmax_nodes è¶…éæ™‚ï¼‰
    last_heartbeats: dict[_NodeDesc, datetime]  # ãƒãƒ¼ãƒ‰ã”ã¨ã®æœ€çµ‚ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆæ™‚åˆ»
```

#### ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å®šç¾©

13 ç¨®é¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã§çŠ¶æ…‹é·ç§»ã‚’åˆ¶å¾¡ã—ã¾ã™ï¼ˆ526-541 è¡Œç›®ï¼‰ï¼š

- `ADD_TO_PARTICIPANTS`: å‚åŠ è€…ãƒªã‚¹ãƒˆã¸ã®è¿½åŠ 
- `REMOVE_FROM_PARTICIPANTS`: å‚åŠ è€…ãƒªã‚¹ãƒˆã‹ã‚‰ã®å‰Šé™¤
- `ADD_TO_WAIT_LIST`: å¾…æ©Ÿãƒªã‚¹ãƒˆã¸ã®è¿½åŠ 
- `MARK_RENDEZVOUS_COMPLETE`: Rendezvous å®Œäº†ãƒãƒ¼ã‚¯
- `KEEP_ALIVE`: ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆé€ä¿¡
- `SYNC`: ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¨ã®åŒæœŸå¾…ã¡
- `FINISH`: æ“ä½œå®Œäº†
- ãã®ä»–ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³

#### ãƒ¯ãƒ¼ã‚«ãƒ¼å‚åŠ ã®ãƒ­ã‚¸ãƒƒã‚¯

`_RendezvousJoinOp.__call__` ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆ870-971 è¡Œç›®ï¼‰ã§å‚åŠ åˆ¤å®šã‚’è¡Œã„ã¾ã™ï¼š

1. **Rendezvous ãŒé–‰é–ã•ã‚Œã¦ã„ã‚‹å ´åˆ**: `ERROR_CLOSED` ã‚’è¿”ã™
2. **å®Œäº†æ¸ˆã¿ & æ—¢ã«å‚åŠ è€…ã®å ´åˆ**: `FINISH` ã‚’è¿”ã™
3. **æœªå®Œäº† & æœªå‚åŠ ã®å ´åˆ**: `ADD_TO_PARTICIPANTS` ã‚’è¿”ã™
4. **å®Œäº†æ¸ˆã¿ & æœªå‚åŠ ã®å ´åˆ**:
   - å‚åŠ è€…æ•° < max_nodes ãªã‚‰ `ADD_TO_WAIT_LIST`ï¼ˆæ¬¡ã®ãƒ©ã‚¦ãƒ³ãƒ‰å¾…ã¡ï¼‰
   - å‚åŠ è€…æ•° >= max_nodes ãªã‚‰ `ADD_TO_REDUNDANCY_LIST`

#### minReplicas/maxReplicas ã®é©ç”¨

`_add_to_participants` ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆ722-749 è¡Œç›®ï¼‰ã§ä»¥ä¸‹ã®ãƒ­ã‚¸ãƒƒã‚¯ãŒå®Ÿè£…ã•ã‚Œã¦ã„ã¾ã™ï¼š

```python
def _add_to_participants(self) -> None:
    state.participants[self._node] = 0  # rank ã¯å¾Œã§è¨­å®š

    # min_nodes ã«é”ã—ãŸã‚‰ last_call deadline ã‚’è¨­å®š
    if len(state.participants) == self._settings.min_nodes:
        state.deadline = datetime.now(timezone.utc) + self._settings.timeout.last_call

    # max_nodes ã«é”ã—ãŸã‚‰å³åº§ã« Rendezvous å®Œäº†
    if len(state.participants) == self._settings.max_nodes:
        self._mark_rendezvous_complete()
```

**é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ**: min_nodes ã«é”ã™ã‚‹ã¨ `last_call` ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ 30 ç§’ï¼‰ãŒé–‹å§‹ã•ã‚Œã€ã“ã®æœŸé–“ä¸­ã«è¿½åŠ ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ãŒå‚åŠ å¯èƒ½ã€‚max_nodes ã«é”ã™ã‚‹ã¨å³åº§ã«å®Œäº†ã€‚

#### ãƒ¯ãƒ¼ã‚«ãƒ¼æ­»äº¡æ¤œå‡º

`_sanitize` ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆ472-513 è¡Œç›®ï¼‰ã§å®šæœŸçš„ã«ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆã‚’ãƒã‚§ãƒƒã‚¯ï¼š

```python
def _sanitize(self) -> None:
    expire_time = datetime.now(timezone.utc) - (
        keep_alive_interval * keep_alive_max_attempt
    )

    # ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆãŒ expire_time ã‚ˆã‚Šå¤ã„ãƒãƒ¼ãƒ‰ã‚’ã€Œæ­»äº¡ã€ã¨åˆ¤å®š
    dead_nodes = [
        node for node, last_heartbeat in state.last_heartbeats.items()
        if last_heartbeat < expire_time
    ]

    for dead_node in dead_nodes:
        del state.participants[dead_node]  # å‚åŠ è€…ã‹ã‚‰å‰Šé™¤
```

- **ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆé–“éš”**: 5 ç§’ã”ã¨ï¼ˆ`keep_alive_interval`ï¼‰
- **æ­»äº¡åˆ¤å®š**: 15 ç§’ï¼ˆ5 ç§’ Ã— 3 å›ï¼‰ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆãŒãªã„ãƒãƒ¼ãƒ‰ã‚’æ­»äº¡ã¨åˆ¤å®š
- **Re-rendezvous ãƒˆãƒªã‚¬ãƒ¼**: å‚åŠ è€…ãŒ 0 ã«ãªã‚‹ã¨ `round++` ã§æ–°ãƒ©ã‚¦ãƒ³ãƒ‰é–‹å§‹ï¼ˆ318-337 è¡Œç›®ï¼‰

å‚ç…§: https://github.com/pytorch/pytorch/blob/main/torch/distributed/elastic/rendezvous/dynamic_rendezvous.py

### EtcdRendezvousï¼ˆãƒ¬ã‚¬ã‚·ãƒ¼å®Ÿè£…ï¼‰

ãƒ¬ã‚¬ã‚·ãƒ¼å®Ÿè£…ã¯ etcd ã‚’ç›´æ¥ä½¿ç”¨ã—ãŸ CASï¼ˆCompare-and-Swapï¼‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã™ã€‚

ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰: https://github.com/pytorch/pytorch/blob/main/torch/distributed/elastic/rendezvous/etcd_rendezvous.py

#### çŠ¶æ…‹é·ç§»ãƒ¢ãƒ‡ãƒ«

4 ã¤ã®çŠ¶æ…‹ã‚’é·ç§»ã—ã¾ã™ï¼ˆ221-260 è¡Œç›®ï¼‰ï¼š

```
[ç©º] â†’ setup â†’ joinable â†’ frozen â†’ final
```

- **setup**: Rendezvous åˆæœŸåŒ–ï¼ˆTTL 5 ç§’ï¼‰
- **joinable**: ãƒ¯ãƒ¼ã‚«ãƒ¼å‚åŠ å—ä»˜ä¸­ï¼ˆTTL 10 ç§’ã€min_workers åˆ°é”å¾Œï¼‰
- **frozen**: å…¨å“¡æƒã£ãŸçŠ¶æ…‹ï¼ˆTTL 10 ç§’ï¼‰
- **final**: ç¢ºèªå®Œäº†ã€è¨“ç·´é–‹å§‹å¯èƒ½

#### CAS ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚ˆã‚‹å‚åŠ 

`join_rendezvous` ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆ517-565 è¡Œç›®ï¼‰ã§æ¥½è¦³çš„ãƒ­ãƒƒã‚¯ã‚’å®Ÿç¾ï¼š

```python
def join_rendezvous(self, expected_version):
    while True:
        cas_delay()  # ãƒ©ãƒ³ãƒ€ãƒ é…å»¶
        active_version, state = self.get_rdzv_state()

        this_rank = len(state["participants"])
        state["participants"].append(this_rank)

        # CAS ã§æ›¸ãè¾¼ã¿
        active_version = self.client.test_and_set(
            key=..., value=json.dumps(state),
            prev_value=active_version.value, ttl=set_ttl
        )
```

#### ãƒ¯ãƒ¼ã‚«ãƒ¼æ­»äº¡æ¤œå‡º

`wait_for_rendezvous_to_free` ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆ680-745 è¡Œç›®ï¼‰ã§ keep-alive ã‚­ãƒ¼ã®å­˜åœ¨ã‚’ç¢ºèªï¼š

```python
# keep-alive ã‚­ãƒ¼ã®å­˜åœ¨ã‚’ç¢ºèª
for key in state["keep_alives"]:
    if key not in keep_alive_keys:
        # ã“ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ã¯ãƒªãƒ¼ã‚¹ã‚’æ›´æ–°ã—ãªã‹ã£ãŸ = æ­»äº¡
        # CAS ã§ Rendezvous ã‚’ç ´å£Š â†’ æ–°ã—ã„ Rendezvous ã®ä½œæˆã‚’è¨±å¯
        self.client.delete(
            key="/rdzv/active_version",
            prevValue=active_version.value,
        )
```

å‚ç…§: https://github.com/pytorch/pytorch/blob/main/torch/distributed/elastic/rendezvous/etcd_rendezvous.py

---

## HyperPod CLI ã® elastic ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å®Ÿè£…

HyperPod CLIï¼ˆ`sagemaker-hyperpod` ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ï¼‰ã¯ `hyp` ã‚³ãƒãƒ³ãƒ‰ã¨ã—ã¦æä¾›ã•ã‚Œã€elastic training ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ã¾ã™ã€‚

ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰: https://github.com/aws/sagemaker-hyperpod-cli

### CLI ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å®šç¾©

ä»¥ä¸‹ã® 6 ã¤ã® elastic ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒ JSON Schemaï¼ˆv1_1ï¼‰ã§å®šç¾©ã•ã‚Œã¦ã„ã¾ã™ï¼š

| CLI ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ | å‹ | ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ | èª¬æ˜ |
|---|---|---|---|
| `--elastic-replica-increment-step` | INTEGER (>=1) | null | ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚µã‚¤ã‚º |
| `--max-node-count` | INTEGER (>=1) | null | elastic training ã®æœ€å¤§ãƒãƒ¼ãƒ‰æ•° |
| `--elastic-graceful-shutdown-timeout-in-seconds` | INTEGER (>=0) | null | ã‚°ãƒ¬ãƒ¼ã‚¹ãƒ•ãƒ«ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ |
| `--elastic-scaling-timeout-in-seconds` | INTEGER (>=0) | null | ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°æ“ä½œã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ |
| `--elastic-scale-up-snooze-time-in-seconds` | INTEGER (>=0) | null | å†èµ·å‹•å¾Œã®ã‚¹ã‚±ãƒ¼ãƒ«ã‚¢ãƒƒãƒ—ç¦æ­¢æœŸé–“ |
| `--elastic-replica-discrete-values` | ARRAY[int] | null | ãƒ¬ãƒ—ãƒªã‚«æ•°ã®é›¢æ•£å€¤ãƒªã‚¹ãƒˆ |

å‚ç…§: https://github.com/aws/sagemaker-hyperpod-cli/blob/main/hyperpod-pytorch-job-template/hyperpod_pytorch_job_template/v1_1/schema.json

### JSON Schema ã‹ã‚‰ã®å‹•çš„ç”Ÿæˆ

HyperPod CLI ã¯ JSON Schema ã‹ã‚‰ CLI ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’å‹•çš„ã«ç”Ÿæˆã—ã¾ã™ï¼ˆ`training_utils.py: generate_click_command()`ï¼‰ï¼š

```python
def wrapped_func(*args, **kwargs):
    Model = registry.get(version)       # PyTorchJobConfig ã‚¯ãƒ©ã‚¹ã‚’å–å¾—
    flat = Model(**filtered_kwargs)      # ãƒ•ãƒ©ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã§ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
    domain = flat.to_domain()            # HyperPodPytorchJob ã«å¤‰æ›
    return func(version, debug, domain)  # ã‚¸ãƒ§ãƒ–ä½œæˆ
```

å‚ç…§: https://github.com/aws/sagemaker-hyperpod-cli/blob/main/src/sagemaker/hyperpod/cli/training_utils.py

### ElasticPolicy ã¸ã®å¤‰æ›

`to_domain()` ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆmodel.py:560-582 è¡Œç›®ï¼‰ã§ã€ãƒ•ãƒ©ãƒƒãƒˆãª CLI ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒ ElasticPolicy ã«å¤‰æ›ã•ã‚Œã¾ã™ï¼š

```python
elastic_policy = None
if any([
    self.elastic_replica_increment_step is not None,
    self.max_node_count is not None,
    ...
]):
    elastic_policy_kwargs = build_dict(
        min_replicas=self.node_count,           # node_count ãŒ min_replicas ã«ãªã‚‹
        max_replicas=self.max_node_count,       # max_node_count ãŒ max_replicas ã«ãªã‚‹
        ...
    )
    elastic_policy = ElasticPolicy(**elastic_policy_kwargs)
```

**é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ**:
- `--node-count` ãŒ `ElasticPolicy.minReplicas` ã«è‡ªå‹•ãƒãƒƒãƒ”ãƒ³ã‚°ã•ã‚Œã‚‹
- `--max-node-count` ãŒ `ElasticPolicy.maxReplicas` ã«è¨­å®šã•ã‚Œã‚‹

å‚ç…§: https://github.com/aws/sagemaker-hyperpod-cli/blob/main/hyperpod-pytorch-job-template/hyperpod_pytorch_job_template/v1_1/model.py

### HyperPodPyTorchJob CRD ã®ä½œæˆ

`HyperPodPytorchJob.create()` ãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆhyperpod_pytorch_job.py:228-287 è¡Œç›®ï¼‰ã§ Kubernetes CRD ã‚’ä½œæˆï¼š

```python
def create(self, debug=False):
    config = {
        "apiVersion": "sagemaker.amazonaws.com/v1",
        "kind": "HyperPodPyTorchJob",
        "spec": spec.model_dump(exclude_none=True),
    }

    custom_api = client.CustomObjectsApi()
    custom_api.create_namespaced_custom_object(
        group="sagemaker.amazonaws.com",
        version="v1",
        namespace=self.metadata.namespace,
        plural="hyperpodpytorchjobs",
        body=config,
    )
```

**é‡è¦ãªç™ºè¦‹**: HyperPod CLI ã¯ Kubeflow ã® `PyTorchJob` (`kubeflow.org/v1`) ã§ã¯ãªãã€**SageMaker ç‹¬è‡ªã® `HyperPodPyTorchJob` CRD**ï¼ˆ`sagemaker.amazonaws.com/v1`ï¼‰ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

å‚ç…§: https://github.com/aws/sagemaker-hyperpod-cli/blob/main/src/sagemaker/hyperpod/training/hyperpod_pytorch_job.py

### Rendezvous è¨­å®šã®è‡ªå‹•ç®¡ç†

`rdzvBackend`, `rdzvHost`, `rdzvPort` ã¯ HyperPod ã® ElasticPolicy ã«ã¯å«ã¾ã‚Œãšã€**HyperPod Training Operator ãŒè‡ªå‹•è¨­å®š**ã—ã¾ã™ã€‚Operator ã¯ `masterAddr` ã¨ `masterPort` ã‚’ status ã«è¨­å®šã—ã€å„ Pod ã«ç’°å¢ƒå¤‰æ•°ã¨ã—ã¦æ³¨å…¥ã—ã¾ã™ï¼ˆæ¨æ¸¬ï¼‰ã€‚

---

## å®Ÿè£…è©³ç´°: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å›³

ä»¥ä¸‹ã® 4 ã¤ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å›³ã§ã€Rendezvous ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã¨ HyperPod CLI ã®å‡¦ç†ãƒ•ãƒ­ãƒ¼ã‚’è©³ç´°ã«èª¬æ˜ã—ã¾ã™ã€‚

### 1. Rendezvous åˆæœŸåŒ–ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å›³

ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰ãŒèµ·å‹•ã—ã¦ã‹ã‚‰ Rendezvous ãŒå®Œäº†ã—ã€åˆ†æ•£è¨“ç·´ãŒé–‹å§‹ã•ã‚Œã‚‹ã¾ã§ã®ãƒ•ãƒ­ãƒ¼ã€‚

```mermaid
sequenceDiagram
    autonumber
    participant W1 as Worker A<br/>(Agent)
    participant W2 as Worker B<br/>(Agent)
    participant W3 as Worker C<br/>(Agent)
    participant DRH as DynamicRendezvous<br/>Handler
    participant Exec as OpExecutor
    participant SH as StateHolder
    participant BE as RendezvousBackend<br/>(etcd / C10d)

    Note over W1, BE: [è¨­å®š] min_nodes=2, max_nodes=4, last_call_timeout=30s

    rect rgb(230, 245, 255)
        Note over W1, DRH: Worker A èµ·å‹•
        W1 ->> DRH: next_rendezvous()
        DRH ->> DRH: _stop_heartbeats()
        Note right of DRH: ãƒ©ãƒ³ãƒ€ãƒ é…å»¶ (0ã€œ0.3 ç§’)<br/>ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰è² è·åˆ†æ•£

        DRH ->> Exec: run(exit_op, deadline)
        Exec ->> SH: sync()
        SH ->> BE: get_state()
        BE -->> SH: state (ç©º)
        Note right of Exec: å‰ãƒ©ã‚¦ãƒ³ãƒ‰ã«æœªå‚åŠ <br/>â†’ å³ FINISH

        DRH ->> Exec: run(join_op, deadline)
        Exec ->> SH: sync()
        SH ->> BE: get_state()
        BE -->> SH: state
        SH ->> SH: _sanitize() æ­»äº¡ãƒãƒ¼ãƒ‰é™¤å»
        Exec ->> Exec: _RendezvousJoinOp()
        Note right of Exec: participants=0 < min_nodes<br/>â†’ ADD_TO_PARTICIPANTS
        Exec ->> SH: _add_to_participants(A)
        Exec ->> SH: mark_dirty()
        Exec ->> SH: sync()
        SH ->> BE: set_state(participants={A})
        BE -->> SH: OK
        Note right of Exec: participants=1 < min_nodes=2<br/>â†’ SYNC (å¾…æ©Ÿ)
    end

    rect rgb(230, 255, 230)
        Note over W2, DRH: Worker B èµ·å‹•
        W2 ->> DRH: next_rendezvous()
        DRH ->> Exec: run(exit_op, deadline)
        Note right of Exec: å‰ãƒ©ã‚¦ãƒ³ãƒ‰ã«æœªå‚åŠ  â†’ FINISH

        DRH ->> Exec: run(join_op, deadline)
        Exec ->> SH: sync()
        SH ->> BE: get_state()
        BE -->> SH: state (participants={A})
        Exec ->> Exec: _RendezvousJoinOp()
        Note right of Exec: participants=1 < min_nodes=2<br/>â†’ ADD_TO_PARTICIPANTS
        Exec ->> SH: _add_to_participants(B)
        Exec ->> SH: mark_dirty()
        Exec ->> SH: sync()
        SH ->> BE: set_state(participants={A,B})
        BE -->> SH: OK
    end

    rect rgb(255, 255, 220)
        Note over W1, BE: min_nodes=2 åˆ°é” â†’ last_call æœŸé–“é–‹å§‹ (deadline è¨­å®š)
        Exec ->> Exec: deadline = now + last_call_timeout (30s)
        Note right of Exec: è¿½åŠ ãƒ¯ãƒ¼ã‚«ãƒ¼ã‚’å¾…æ©Ÿä¸­...<br/>deadline ã¾ã§å‚åŠ å¯èƒ½
    end

    rect rgb(230, 255, 230)
        Note over W3, DRH: Worker C èµ·å‹• (last_call æœŸé–“ä¸­)
        W3 ->> DRH: next_rendezvous()
        DRH ->> Exec: run(join_op, deadline)
        Exec ->> SH: sync()
        SH ->> BE: get_state()
        BE -->> SH: state (participants={A,B})
        Exec ->> Exec: _RendezvousJoinOp()
        Note right of Exec: complete=False ã‹ã¤<br/>participants < max_nodes<br/>â†’ ADD_TO_PARTICIPANTS
        Exec ->> SH: _add_to_participants(C)
        Exec ->> SH: mark_dirty()
        Exec ->> SH: sync()
        SH ->> BE: set_state(participants={A,B,C})
        BE -->> SH: OK
    end

    rect rgb(255, 230, 230)
        Note over W1, BE: deadline åˆ°é” â†’ Rendezvous å®Œäº†
        Exec ->> Exec: _mark_rendezvous_complete()
        Note right of Exec: complete=True<br/>rank ã‚’ã‚½ãƒ¼ãƒˆé †ã§å‰²ã‚Šå½“ã¦<br/>Aâ†’0, Bâ†’1, Câ†’2
        Exec ->> SH: mark_dirty()
        Exec ->> SH: sync()
        SH ->> BE: set_state(complete=True, ranks å‰²ã‚Šå½“ã¦æ¸ˆã¿)
        BE -->> SH: OK
        Exec -->> DRH: FINISH
    end

    rect rgb(240, 230, 255)
        Note over W1, BE: è¨“ç·´é–‹å§‹æº–å‚™
        DRH ->> DRH: _start_heartbeats()<br/>keep_alive_interval=5s
        DRH ->> DRH: _get_world() â†’ (rank, world_size)
        DRH ->> DRH: _get_store() â†’ PrefixStore
        DRH -->> W1: RendezvousInfo(rank=0, world_size=3, store)
        DRH -->> W2: RendezvousInfo(rank=1, world_size=3, store)
        DRH -->> W3: RendezvousInfo(rank=2, world_size=3, store)

        W1 ->> W1: init_process_group()
        W2 ->> W2: init_process_group()
        W3 ->> W3: init_process_group()
        Note over W1, W3: åˆ†æ•£è¨“ç·´é–‹å§‹ (world_size=3)
    end
```

**è£œè¶³**:
- `min_nodes` åˆ°é”å‰ã¯ SYNC ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã§å¾…æ©Ÿãƒ«ãƒ¼ãƒ—ã—ã€å®šæœŸçš„ã«ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã¨çŠ¶æ…‹ã‚’åŒæœŸ
- `max_nodes` ã«åˆ°é”ã—ãŸå ´åˆã¯ deadline ã‚’å¾…ãŸãšå³åº§ã« `_mark_rendezvous_complete()` ãŒå®Ÿè¡Œã•ã‚Œã‚‹
- rank ã¯ãƒãƒ¼ãƒ‰ ID ã®ã‚½ãƒ¼ãƒˆé †ã§ 0 ã‹ã‚‰å‰²ã‚Šå½“ã¦ã‚‰ã‚Œã‚‹

---

### 2. ãƒ¯ãƒ¼ã‚«ãƒ¼è¿½åŠ æ™‚ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å›³

Rendezvous å®Œäº†å¾Œã«æ–°ã—ã„ãƒ¯ãƒ¼ã‚«ãƒ¼ãŒå‚åŠ ã—ã€Re-rendezvous ãŒç™ºç”Ÿã™ã‚‹ãƒ•ãƒ­ãƒ¼ã€‚

```mermaid
sequenceDiagram
    autonumber
    participant W1 as Worker A<br/>(rank=0)
    participant W2 as Worker B<br/>(rank=1)
    participant W_new as Worker D<br/>(æ–°è¦)
    participant DRH as DynamicRendezvous<br/>Handler
    participant Exec as OpExecutor
    participant SH as StateHolder
    participant BE as RendezvousBackend<br/>(etcd / C10d)

    Note over W1, BE: [ç¾åœ¨ã®çŠ¶æ…‹] round=0, complete=True<br/>participants={A: rank0, B: rank1}, max_nodes=4

    rect rgb(230, 245, 255)
        Note over W1, W2: è¨“ç·´å®Ÿè¡Œä¸­...
        W1 ->> W1: è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œ
        W2 ->> W2: è¨“ç·´ã‚¹ãƒ†ãƒƒãƒ—å®Ÿè¡Œ
    end

    rect rgb(255, 245, 220)
        Note over W_new, DRH: Worker D ãŒèµ·å‹•ãƒ»å‚åŠ ã‚’è©¦ã¿ã‚‹
        W_new ->> DRH: next_rendezvous()
        DRH ->> DRH: _stop_heartbeats()
        DRH ->> Exec: run(exit_op, deadline)
        Note right of Exec: å‰ãƒ©ã‚¦ãƒ³ãƒ‰ã«æœªå‚åŠ  â†’ FINISH

        DRH ->> Exec: run(join_op, deadline)
        Exec ->> SH: sync()
        SH ->> BE: get_state()
        BE -->> SH: state (complete=True, participants={A,B})

        Exec ->> Exec: _RendezvousJoinOp()
        Note right of Exec: complete=True ã‹ã¤<br/>participants < max_nodes<br/>â†’ ADD_TO_WAIT_LIST
        Exec ->> SH: _add_to_wait_list(D)
        Exec ->> SH: mark_dirty()
        Exec ->> SH: sync()
        SH ->> BE: set_state(wait_list={D})
        BE -->> SH: OK
        Note right of Exec: ç¾åœ¨ã®ãƒ©ã‚¦ãƒ³ãƒ‰ãŒçµ‚äº†ã™ã‚‹ã¾ã§<br/>SYNC ã§å¾…æ©Ÿãƒ«ãƒ¼ãƒ—
    end

    rect rgb(255, 230, 230)
        Note over W1, BE: æ—¢å­˜ãƒ¯ãƒ¼ã‚«ãƒ¼ãŒãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜å¾Œã« Re-rendezvous ã‚’é–‹å§‹
        W1 ->> W1: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
        W2 ->> W2: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜

        W1 ->> DRH: next_rendezvous()
        DRH ->> DRH: _stop_heartbeats()
        DRH ->> Exec: run(exit_op, deadline)
        Exec ->> SH: sync()
        SH ->> BE: get_state()
        BE -->> SH: state
        Exec ->> Exec: _RendezvousExitOp()
        Note right of Exec: è‡ªåˆ†ãŒ participants ã«å­˜åœ¨<br/>â†’ REMOVE_FROM_PARTICIPANTS
        Exec ->> SH: participants ã‹ã‚‰ A ã‚’é™¤å»
        Exec ->> SH: mark_dirty()
        Exec ->> SH: sync()
        SH ->> BE: set_state(participants={B})
        BE -->> SH: OK

        W2 ->> DRH: next_rendezvous()
        DRH ->> Exec: run(exit_op, deadline)
        Exec ->> SH: sync()
        Exec ->> Exec: _RendezvousExitOp()
        Exec ->> SH: participants ã‹ã‚‰ B ã‚’é™¤å»
        Exec ->> SH: mark_dirty()
        Exec ->> SH: sync()
        SH ->> BE: set_state(participants={})
        BE -->> SH: OK
        Note right of Exec: participants ãŒç©º<br/>â†’ round++, complete=False<br/>wait_list={D} ãŒæ¬¡ãƒ©ã‚¦ãƒ³ãƒ‰ã«å‚åŠ å¯èƒ½
    end

    rect rgb(230, 255, 230)
        Note over W1, BE: æ–°ã—ã„ãƒ©ã‚¦ãƒ³ãƒ‰ (round=1) ã§å…¨ãƒ¯ãƒ¼ã‚«ãƒ¼ãŒå‚åŠ 

        W1 ->> DRH: run(join_op, deadline)
        W2 ->> DRH: run(join_op, deadline)
        W_new ->> DRH: run(join_op, deadline)
        Note right of DRH: SYNC ãƒ«ãƒ¼ãƒ—ã‹ã‚‰æŠœã‘ãŸ D ã‚‚å‚åŠ 

        DRH ->> Exec: ADD_TO_PARTICIPANTS (A, B, D)
        Exec ->> SH: mark_dirty()
        Exec ->> SH: sync()
        SH ->> BE: set_state(participants={A,B,D})
        BE -->> SH: OK

        Note over W1, BE: min_nodes åˆ°é” â†’ last_call â†’ å®Œäº†
        Exec ->> Exec: _mark_rendezvous_complete()
        Note right of Exec: complete=True<br/>Aâ†’rank0, Bâ†’rank1, Dâ†’rank2
    end

    rect rgb(240, 230, 255)
        Note over W1, BE: æ–°ã—ã„ world_size=3 ã§è¨“ç·´å†é–‹
        DRH ->> DRH: _start_heartbeats()
        DRH -->> W1: RendezvousInfo(rank=0, world_size=3)
        DRH -->> W2: RendezvousInfo(rank=1, world_size=3)
        DRH -->> W_new: RendezvousInfo(rank=2, world_size=3)

        W1 ->> W1: init_process_group()
        W2 ->> W2: init_process_group()
        W_new ->> W_new: init_process_group()
        Note over W1, W_new: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰è¨“ç·´å†é–‹<br/>world_size: 2â†’3
    end
```

**è£œè¶³**:
- æ–°ãƒ¯ãƒ¼ã‚«ãƒ¼ã¯ `complete=True` ã®çŠ¶æ…‹ã§ã¯ç›´æ¥å‚åŠ ã§ããšã€`wait_list` ã«è¿½åŠ ã•ã‚Œã‚‹
- `max_nodes` ã«é”ã—ã¦ã„ã‚‹å ´åˆã¯ `redundancy_list` ã«è¿½åŠ ã•ã‚Œã€ç©ºããŒå‡ºã‚‹ã¾ã§å¾…æ©Ÿ
- Re-rendezvous ã¯æ—¢å­˜ãƒ¯ãƒ¼ã‚«ãƒ¼ãŒ `next_rendezvous()` ã‚’å†å‘¼ã³å‡ºã—ã™ã‚‹ã“ã¨ã§ç™ºç”Ÿ
- Agentï¼ˆelastic_agentï¼‰ãŒãƒ¯ãƒ¼ã‚«ãƒ¼ã®çŠ¶æ…‹å¤‰åŒ–ã‚’æ¤œçŸ¥ã—ã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜å¾Œã« Re-rendezvous ã‚’ãƒˆãƒªã‚¬ãƒ¼

---

### 3. ãƒ¯ãƒ¼ã‚«ãƒ¼é›¢è„±æ™‚ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹å›³

ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰ã®éšœå®³ç™ºç”Ÿã‹ã‚‰ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ¤œå‡ºã€Re-rendezvousã€è¨“ç·´å†é–‹ã¾ã§ã®ãƒ•ãƒ­ãƒ¼ã€‚

```mermaid
sequenceDiagram
    autonumber
    participant W1 as Worker A<br/>(rank=0)
    participant W2 as Worker B<br/>(rank=1)
    participant W3 as Worker C<br/>(rank=2)
    participant DRH as DynamicRendezvous<br/>Handler
    participant SH as StateHolder
    participant BE as RendezvousBackend<br/>(etcd / C10d)
    participant Timer as PeriodicTimer<br/>(keep_alive)

    Note over W1, BE: [ç¾åœ¨ã®çŠ¶æ…‹] round=0, complete=True<br/>participants={A:0, B:1, C:2}

    rect rgb(230, 245, 255)
        Note over W1, Timer: æ­£å¸¸ç¨¼åƒä¸­: ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆé€ä¿¡
        loop keep_alive_interval = 5 ç§’ã”ã¨
            Timer ->> DRH: _keep_alive() ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
            DRH ->> SH: sync()
            SH ->> BE: get_state()
            BE -->> SH: state
            DRH ->> DRH: last_heartbeats[self] = now
            DRH ->> SH: mark_dirty()
            DRH ->> SH: sync()
            SH ->> BE: set_state(heartbeat æ›´æ–°æ¸ˆã¿)
            BE -->> SH: OK
        end
    end

    rect rgb(255, 200, 200)
        Note over W2: [éšœå®³ç™ºç”Ÿ] Worker B ãŒã‚¯ãƒ©ãƒƒã‚·ãƒ¥
        W2 ->> W2: ãƒ—ãƒ­ã‚»ã‚¹ç•°å¸¸çµ‚äº†
        Note right of W2: PeriodicTimer ã‚‚åœæ­¢<br/>ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆé€ä¿¡ãŒé€”çµ¶ãˆã‚‹
    end

    rect rgb(255, 245, 220)
        Note over W1, BE: ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆæœŸé™åˆ‡ã‚Œã®æ¤œå‡º
        Note right of Timer: æ¤œå‡ºæ¡ä»¶: <br/>expire_time = now - (5s x 3 å›) = 15 ç§’
        Timer ->> DRH: _keep_alive() (Worker A ã®å®šæœŸå®Ÿè¡Œ)
        DRH ->> SH: sync()
        SH ->> BE: get_state()
        BE -->> SH: state
        SH ->> SH: _sanitize()
        Note right of SH: B ã® last_heartbeat < expire_time<br/>â†’ B ã‚’æ­»äº¡ãƒãƒ¼ãƒ‰ã¨åˆ¤å®š
        SH ->> SH: participants ã‹ã‚‰ B ã‚’å‰Šé™¤
        SH ->> SH: last_heartbeats ã‹ã‚‰ B ã‚’å‰Šé™¤
        SH ->> SH: _remove_participant_epilogue()
        Note right of SH: complete=True ã‹ã¤<br/>participants={A,C} ãŒæ®‹å­˜<br/>â†’ complete ã®ã¾ã¾ç¶­æŒ
        SH ->> BE: set_state(participants={A,C})
        BE -->> SH: OK
    end

    rect rgb(255, 230, 230)
        Note over W1, BE: Agent ãŒéšœå®³ã‚’æ¤œå‡º â†’ Re-rendezvous é–‹å§‹
        W1 ->> W1: NCCL ã‚¨ãƒ©ãƒ¼ / é€šä¿¡éšœå®³ã‚’æ¤œå‡º
        W3 ->> W3: NCCL ã‚¨ãƒ©ãƒ¼ / é€šä¿¡éšœå®³ã‚’æ¤œå‡º

        W1 ->> W1: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
        W3 ->> W3: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜

        W1 ->> DRH: next_rendezvous()
        DRH ->> DRH: _stop_heartbeats()

        DRH ->> DRH: run(exit_op)
        Note right of DRH: è‡ªåˆ†ãŒ participants ã«å­˜åœ¨<br/>â†’ REMOVE_FROM_PARTICIPANTS
        DRH ->> SH: sync()
        SH ->> BE: set_state(participants={C})
        BE -->> SH: OK

        W3 ->> DRH: next_rendezvous()
        DRH ->> DRH: run(exit_op)
        DRH ->> SH: sync()
        SH ->> BE: set_state(participants={})
        BE -->> SH: OK
        Note right of SH: participants ãŒç©º<br/>â†’ round=1, complete=False
    end

    rect rgb(230, 255, 230)
        Note over W1, BE: æ–°ãƒ©ã‚¦ãƒ³ãƒ‰ (round=1) ã§ Re-rendezvous
        W1 ->> DRH: run(join_op)
        DRH ->> SH: sync()
        SH ->> BE: get_state()
        BE -->> SH: state (round=1, complete=False)
        DRH ->> SH: _add_to_participants(A)
        DRH ->> SH: mark_dirty()
        DRH ->> SH: sync()
        SH ->> BE: set_state(participants={A})
        BE -->> SH: OK

        W3 ->> DRH: run(join_op)
        DRH ->> SH: sync()
        SH ->> BE: get_state()
        BE -->> SH: state (participants={A})
        DRH ->> SH: _add_to_participants(C)
        DRH ->> SH: mark_dirty()
        DRH ->> SH: sync()
        SH ->> BE: set_state(participants={A,C})
        BE -->> SH: OK

        Note over W1, BE: min_nodes=2 åˆ°é” â†’ last_call â†’ å®Œäº†
        DRH ->> DRH: _mark_rendezvous_complete()
        Note right of DRH: complete=True<br/>Aâ†’rank0, Câ†’rank1<br/>(rank å†å‰²ã‚Šå½“ã¦)
    end

    rect rgb(240, 230, 255)
        Note over W1, BE: ç¸®å°ã—ãŸ world ã§è¨“ç·´å†é–‹
        DRH ->> DRH: _start_heartbeats()
        DRH -->> W1: RendezvousInfo(rank=0, world_size=2)
        DRH -->> W3: RendezvousInfo(rank=1, world_size=2)

        W1 ->> W1: init_process_group()
        W3 ->> W3: init_process_group()
        W1 ->> W1: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ãƒ¢ãƒ‡ãƒ«å¾©å…ƒ
        W3 ->> W3: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ãƒ¢ãƒ‡ãƒ«å¾©å…ƒ
        Note over W1, W3: è¨“ç·´å†é–‹ (world_size: 3â†’2)<br/>Worker B ã¯é›¢è„±æ¸ˆã¿
    end
```

**è£œè¶³**:
- ãƒãƒ¼ãƒˆãƒ“ãƒ¼ãƒˆã®æœŸé™åˆ‡ã‚Œæ¤œå‡ºã¯ `_sanitize()` ã§è¡Œã‚ã‚Œã€`keep_alive_interval(5s) x keep_alive_max_attempt(3) = 15 ç§’` ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå€¤
- éšœå®³ãƒãƒ¼ãƒ‰ã®é™¤å»ã¯ä»–ãƒ¯ãƒ¼ã‚«ãƒ¼ã® `sync()` æ™‚ã«è‡ªå‹•çš„ã«å®Ÿè¡Œã•ã‚Œã‚‹
- `min_nodes` æœªæº€ã«æ¸›å°‘ã—ãŸå ´åˆã¯ `deadline` ãŒã‚¯ãƒªã‚¢ã•ã‚Œã€æ–°ã—ã„ãƒ¯ãƒ¼ã‚«ãƒ¼ã®å‚åŠ ã‚’å¾…ã¤çŠ¶æ…‹ã«æˆ»ã‚‹
- ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ã®å¾©å…ƒã¯ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å´ã®è²¬å‹™ã§ã‚ã‚Šã€Rendezvous ãƒ¡ã‚«ãƒ‹ã‚ºãƒ è‡ªä½“ã¯ rank ã¨ world_size ã®å†å‰²ã‚Šå½“ã¦ã®ã¿ã‚’è¡Œã†

---

### 4. HyperPod CLI ã¨ HyperPodPyTorchJob ã®é€£æºå›³

`hyp` CLI ã‚³ãƒãƒ³ãƒ‰ã‹ã‚‰ HyperPod Training Operator ã‚’çµŒç”±ã—ã¦åˆ†æ•£è¨“ç·´ãŒé–‹å§‹ã•ã‚Œã‚‹ã¾ã§ã®å‡¦ç†ãƒ•ãƒ­ãƒ¼ã€‚

```mermaid
sequenceDiagram
    autonumber
    participant User as ãƒ¦ãƒ¼ã‚¶ãƒ¼
    participant CLI as hyp CLI<br/>(Python)
    participant Valid as PyTorchJobConfig<br/>(Pydantic)
    participant K8sAPI as Kubernetes<br/>API Server
    participant Operator as HyperPod Training<br/>Operator<br/>(aws-hyperpod)
    participant Pod as Worker Pods<br/>({job}-pod-{N})
    participant RDZV as Rendezvous<br/>Backend

    rect rgb(230, 245, 255)
        Note over User, CLI: Step 1: CLI ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œ
        User ->> CLI: hyp create hyp-pytorch-job<br/>--job-name my-job<br/>--node-count 4<br/>--max-node-count 8<br/>--instance-type ml.p4d.24xlarge<br/>--tasks-per-node 8<br/>--elastic-replica-increment-step 2
    end

    rect rgb(255, 245, 220)
        Note over CLI, Valid: Step 2: ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã¨å¤‰æ›
        CLI ->> CLI: generate_click_command()<br/>schema.json (v1_1) ã‹ã‚‰ã‚ªãƒ—ã‚·ãƒ§ãƒ³å‹•çš„ç”Ÿæˆ
        CLI ->> Valid: PyTorchJobConfig(**params)
        Valid ->> Valid: å‹ãƒã‚§ãƒƒã‚¯ (integer >= 1 ç­‰)
        Valid ->> Valid: elastic_replica_increment_step ã¨<br/>elastic_replica_discrete_values ã®<br/>ç›¸äº’æ’ä»–ãƒã‚§ãƒƒã‚¯
        Note right of Valid: node_count=4 â†’ replicas=4, minReplicas=4<br/>max_node_count=8 â†’ maxReplicas=8

        Valid ->> Valid: to_domain() å¤‰æ›
        Note right of Valid: Resources (GPU/CPU/Memory/EFA) æ§‹ç¯‰<br/>Container æ§‹ç¯‰<br/>NodeSelector (instance-type) æ§‹ç¯‰<br/>ReplicaSpec (replicas=4, maxReplicas=8)<br/>ElasticPolicy (min=4, max=8, step=2)<br/>RunPolicy æ§‹ç¯‰
    end

    rect rgb(230, 255, 230)
        Note over CLI, K8sAPI: Step 3: CRD ä½œæˆ
        CLI ->> CLI: config.load_kube_config()
        CLI ->> CLI: allocate_quotas_if_applicable()<br/>ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—ã‹ã‚‰ãƒªã‚½ãƒ¼ã‚¹è‡ªå‹•è¨ˆç®—

        CLI ->> K8sAPI: CustomObjectsApi()<br/>.create_namespaced_custom_object()
        Note right of K8sAPI: apiVersion: sagemaker.amazonaws.com/v1<br/>kind: HyperPodPyTorchJob<br/>ElasticPolicy: <br/>  minReplicas: 4<br/>  maxReplicas: 8<br/>  replicaIncrementStep: 2
        K8sAPI -->> CLI: CRD ä½œæˆå®Œäº†
        CLI -->> User: ã‚¸ãƒ§ãƒ–ä½œæˆæˆåŠŸ
    end

    rect rgb(255, 230, 230)
        Note over K8sAPI, Operator: Step 4: Operator ãŒ CRD ã‚’æ¤œçŸ¥
        K8sAPI ->> Operator: Watch: HyperPodPyTorchJob ä½œæˆã‚¤ãƒ™ãƒ³ãƒˆ
        Operator ->> Operator: CRD spec ã‚’è§£æ
        Operator ->> Operator: ElasticPolicy ã‚’èª­ã¿å–ã‚Š<br/>minReplicas=4, maxReplicas=8
        Operator ->> Operator: Rendezvous è¨­å®šã‚’è‡ªå‹•ç”Ÿæˆ<br/>(masterAddr, masterPort)
    end

    rect rgb(240, 230, 255)
        Note over Operator, Pod: Step 5: Worker Pod ä½œæˆ
        Operator ->> K8sAPI: Pod ä½œæˆ (my-job-pod-0 ã€œ my-job-pod-3)
        K8sAPI ->> Pod: 4 ã¤ã® Worker Pod ã‚’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«

        Operator ->> Operator: status.masterAddr = Pod-0 ã® IP<br/>status.masterPort = å‰²ã‚Šå½“ã¦ãƒãƒ¼ãƒˆ
        Operator ->> K8sAPI: status æ›´æ–° (masterAddr, masterPort)

        Note over Pod: å„ Pod ã«ç’°å¢ƒå¤‰æ•°ã‚’æ³¨å…¥ (æ¨æ¸¬): <br/>MASTER_ADDR, MASTER_PORT,<br/>WORLD_SIZE, RANK, LOCAL_WORLD_SIZE ç­‰

        Pod ->> Pod: pytorch-job-container èµ·å‹•
        Pod ->> Pod: torchrun --nproc_per_node=8 ...
    end

    rect rgb(230, 245, 255)
        Note over Pod, RDZV: Step 6: Rendezvous ã¨è¨“ç·´é–‹å§‹
        Pod ->> RDZV: DynamicRendezvousHandler<br/>.next_rendezvous()
        RDZV -->> Pod: RendezvousInfo<br/>(rank, world_size, store)
        Pod ->> Pod: init_process_group()
        Note over Pod: åˆ†æ•£è¨“ç·´é–‹å§‹<br/>world_size=32 (4 nodes x 8 GPUs)
    end

    rect rgb(255, 245, 220)
        Note over Operator, Pod: [ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚¤ãƒ™ãƒ³ãƒˆç™ºç”Ÿæ™‚]
        Operator ->> Operator: ElasticScalingStatus æ›´æ–°<br/>targetReplicas å¤‰æ›´
        Operator ->> K8sAPI: Pod è¿½åŠ /å‰Šé™¤
        Note right of Operator: replicaIncrementStep=2 ã«å¾“ã„<br/>2 ãƒãƒ¼ãƒ‰å˜ä½ã§ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
        Operator ->> Operator: scalingTimeoutInSeconds=300 å†…ã«<br/>ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°å®Œäº†ã‚’è¦æ±‚
        Pod ->> RDZV: Re-rendezvous
        RDZV -->> Pod: æ–°ã—ã„ RendezvousInfo
        Pod ->> Pod: æ–°ã—ã„ world_size ã§è¨“ç·´å†é–‹
    end
```

**è£œè¶³**:
- HyperPod ã¯ Kubeflow ã® PyTorchJob (`kubeflow.org/v1`) ã§ã¯ãªãã€SageMaker ç‹¬è‡ªã® CRD (`sagemaker.amazonaws.com/v1` / `HyperPodPyTorchJob`) ã‚’ä½¿ç”¨
- `rdzvBackend`, `rdzvHost`, `rdzvPort` ã¯ HyperPod ã® ElasticPolicy ã«ã¯å­˜åœ¨ã›ãšã€Operator ãŒè‡ªå‹•çš„ã«ç®¡ç†
- `--node-count` ã¯ `ReplicaSpec.replicas`ï¼ˆåˆæœŸãƒ¬ãƒ—ãƒªã‚«æ•°ï¼‰ã¨ `ElasticPolicy.minReplicas`ï¼ˆæœ€å°ãƒ¬ãƒ—ãƒªã‚«æ•°ï¼‰ã®ä¸¡æ–¹ã«è¨­å®šã•ã‚Œã‚‹
- `--max-node-count` ã¯ `ReplicaSpec.maxReplicas` ã¨ `ElasticPolicy.maxReplicas` ã®ä¸¡æ–¹ã«è¨­å®šã•ã‚Œã‚‹
- HyperPod Training Operator ã®ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã¯éå…¬é–‹ã®ãŸã‚ã€Operator å†…éƒ¨ã®å‡¦ç†ã¯ CLI ã‚³ãƒ¼ãƒ‰ã‹ã‚‰ã®æ¨æ¸¬ã‚’å«ã‚€

---

# Kubernetes ã§ã®å®Ÿè£…ä¾‹

## SageMaker HyperPod on EKS ã®æ§‹æˆ

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Amazon EKS         â”‚
â”‚  (Control Plane)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ 1:1 mapping
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SageMaker HyperPod â”‚
â”‚  (Worker Nodes)     â”‚
â”‚  - trn1/trn2        â”‚
â”‚  - p4/p5/p6         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

- EKS ã‚¯ãƒ©ã‚¹ã‚¿ã¨ HyperPod ã‚¯ãƒ©ã‚¹ã‚¿ãŒ 1 å¯¾ 1 ã§ãƒãƒƒãƒ”ãƒ³ã‚°
- `kubectl` ã§ç›´æ¥æ“ä½œå¯èƒ½
- SSM/SSH ã§ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½

å‚ç…§ï¼šhttps://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-eks.html

## PyTorchJob with ElasticPolicy ã®ä¾‹

ä»¥ä¸‹ã¯ã€AWS Trainium (trn1) ã§ Llama 3 ã‚’è¨“ç·´ã™ã‚‹å®Œå…¨ãª YAML ä¾‹ã§ã™ã€‚

```yaml
---
# etcd Serviceï¼ˆRendezvous ç”¨ï¼‰
apiVersion: v1
kind: Service
metadata:
  name: etcd
spec:
  ports:
    - name: etcd-client-port
      port: 2379
      protocol: TCP
      targetPort: 2379
  selector:
    app: etcd
---
# etcd Deployment
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: etcd
  name: etcd
spec:
  replicas: 1
  selector:
    matchLabels:
      app: etcd
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: etcd
    spec:
      containers:
        - command:
            - /usr/local/bin/etcd
          args:
            - "--enable-v2"
            - "--listen-client-urls=http://0.0.0.0:2379"
            - "--advertise-client-urls=http://etcd:2379"
          image: quay.io/coreos/etcd: v3.5.19
          name: etcd
          ports:
            - containerPort: 2379
              name: client
              protocol: TCP
            - containerPort: 2380
              name: server
              protocol: TCP
      restartPolicy: Always
---
# PyTorchJobï¼ˆElastic Trainingï¼‰
apiVersion: "kubeflow.org/v1"
kind: PyTorchJob
metadata:
  name: trn1-llama3-elastic
spec:
  elasticPolicy:
    rdzvBackend: etcd
    rdzvHost: etcd
    rdzvPort: 2379
    minReplicas: 1
    maxReplicas: 64
    maxRestarts: 100
    metrics:
      - type: Resource
        resource:
          name: cpu
          target:
            type: Utilization
            averageUtilization: 90
  pytorchReplicaSpecs:
    Worker:
      replicas: 4  # åˆæœŸãƒ¯ãƒ¼ã‚«ãƒ¼æ•°
      restartPolicy: OnFailure
      template:
        metadata:
          labels:
            app: trn1-llama3
        spec:
          containers:
            - name: pytorch
              image: <AWS-ACCOUNT-ID>.dkr.ecr.<REGION>.amazonaws.com/neuron-training: latest
              imagePullPolicy: Always
              command:
                - torchrun
                - --nproc_per_node=32
                - --nnodes=$NUM_NODES
                - train.py
                - --checkpoint_freq=1000
                - --num_kept_checkpoint=5
                - --checkpoint_dir=/fsx/checkpoints
              env:
                - name: NEURON_RT_NUM_CORES
                  value: "32"
                - name: FI_PROVIDER
                  value: efa
                - name: FI_EFA_USE_DEVICE_RDMA
                  value: "1"
                - name: NEURON_CC_FLAGS
                  value: "--model-type transformer --distribution-strategy=llm-training"
              resources:
                requests:
                  aws.amazon.com/neuron: 16
                  vpc.amazonaws.com/efa: 8
                limits:
                  aws.amazon.com/neuron: 16
                  vpc.amazonaws.com/efa: 8
              volumeMounts:
                - name: persistent-storage
                  mountPath: /fsx
          volumes:
            - name: persistent-storage
              persistentVolumeClaim:
                claimName: fsx-claim
```

å‚ç…§ï¼šhttps://github.com/aws-samples/awsome-distributed-training/blob/main/3.test_cases/pytorch/neuronx-distributed/llama3/kubernetes/llama3_train.yaml-template

## Helm Chart ã«ã‚ˆã‚‹ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

SageMaker HyperPod CLI ã¯ Helm Chart ã‚’æä¾›ã—ã¦ã„ã¾ã™ï¼š

```bash
# ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³
git clone https://github.com/aws/sagemaker-hyperpod-cli.git
cd sagemaker-hyperpod-cli/helm_chart

# ä¾å­˜é–¢ä¿‚ã®æ›´æ–°
helm dependencies update HyperPodHelmChart

# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
helm install hyperpod-dependencies HyperPodHelmChart \
  --namespace kube-system \
  --set health-monitoring-agent.enabled=true \
  --set health-monitoring-agent.region=us-west-2 \
  --set deep-health-check.enabled=true \
  --set job-auto-restart.enabled=true
```

ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã‚‹ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆï¼š

- **Kubeflow Training Operator**: PyTorchJob ã®ç®¡ç†
- **MPI Operator**: MPI ãƒ™ãƒ¼ã‚¹ã®åˆ†æ•£è¨“ç·´
- **EFA Device Plugin**: Elastic Fabric Adapter ã®æœ€é©åŒ–
- **Neuron Device Plugin**: Trainium/Inferentia ã‚¢ã‚¯ã‚»ã‚¹
- **Health Monitoring Agent**: ãƒãƒ¼ãƒ‰ã®ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
- **Job Auto-Restart**: PyTorchJob ã®è‡ªå‹•å†é–‹

å‚ç…§ï¼šhttps://github.com/aws/sagemaker-hyperpod-cli/tree/main/helm_chart

## HyperPod CLI ã®ä½¿ç”¨

```bash
# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install sagemaker-hyperpod

# ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ¥ç¶š
hyp set-cluster-context --cluster-name my-hyperpod-cluster

# PyTorchJob ã®ä½œæˆ
hyp create hyp-pytorch-job \
  --job-name llama3-training \
  --image <ECR-IMAGE> \
  --command '[python, train.py]' \
  --instance-type ml.trn2.48xlarge \
  --tasks-per-node 32

# ã‚¸ãƒ§ãƒ–ã®ç¢ºèª
kubectl get pytorchjobs -n hyperpod

# ãƒ­ã‚°ã®ç¢ºèª
kubectl logs <pod-name> -n hyperpod
```

å‚ç…§ï¼šhttps://github.com/aws/sagemaker-hyperpod-cli

## Resilience æ©Ÿèƒ½ã®è¨­å®š

```yaml
# values.yamlï¼ˆHelm Chartï¼‰
health-monitoring-agent:
  enabled: true
  region: us-west-2

deep-health-check:
  enabled: true
  interval: 300  # 5 åˆ†ã”ã¨

job-auto-restart:
  enabled: true
  max-retry: 10
```

Resilience æ©Ÿèƒ½ã«ã‚ˆã‚Šä»¥ä¸‹ãŒæä¾›ã•ã‚Œã¾ã™ï¼š

- **Basic Health Checks**: åŸºæœ¬çš„ãªãƒãƒ¼ãƒ‰ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
- **Deep Health Checks**: è©³ç´°ãªãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒã‚§ãƒƒã‚¯ï¼ˆGPU/Neuronï¼‰
- **Automatic Node Recovery**: éšœå®³ãƒãƒ¼ãƒ‰ã®è‡ªå‹•äº¤æ›
- **Job Auto-Restart**: PyTorchJob ã®è‡ªå‹•å†é–‹

å‚ç…§ï¼šhttps://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-eks-resiliency.html

---

# ã¾ã¨ã‚

## HyperPod ã® 4 ã¤ã®ä¸»è¦æ©Ÿèƒ½

### 1. Checkpointless Training

| é …ç›® | è©³ç´° |
|-----|-----|
| **ç›®çš„** | ã‚¤ãƒ³ãƒ•ãƒ©éšœå®³ã‹ã‚‰ã®è‡ªå‹•å›å¾© |
| **å¾©æ—§æ™‚é–“** | æ•°åˆ†ï¼ˆå¾“æ¥ã¯æ•°æ™‚é–“ï¼‰ |
| **å¯¾è±¡** | å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆLlama 3 70Bã€GPT-OSS 120Bï¼‰ |
| **å‰Šæ¸›åŠ¹æœ** | HyperPod å…¨ä½“ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚é–“æœ€å¤§ 40%ã€ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚³ã‚¹ãƒˆå‰Šæ¸› |
| **å¯¾å¿œç’°å¢ƒ** | HyperPod EKS ã®ã¿ |

### 2. Elastic Training / Auto-Resume

| é …ç›® | è©³ç´° |
|-----|-----|
| **ç›®çš„** | ãƒãƒ¼ãƒ‰éšœå®³æ™‚ã®è‡ªå‹•å†é–‹ãƒ»å‹•çš„ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚° |
| **EKS** | å‹•çš„ãªãƒãƒ¼ãƒ‰å¢—æ¸›ï¼ˆminReplicasï½maxReplicasï¼‰ |
| **Slurm** | ã‚¸ãƒ§ãƒ–ã®è‡ªå‹•å†ã‚­ãƒ¥ãƒ¼ã‚¤ãƒ³ã‚°ï¼ˆ--requeueï¼‰ |
| **æŠ€è¡“** | PyTorch Elastic + Kubeflow Training Operator |

### 3. Managed Tiered Checkpointing

| é …ç›® | è©³ç´° |
|-----|-----|
| **ç›®çš„** | ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸éšå±¤ã®æœ€é©åŒ– |
| **Tier 1** | é«˜é€Ÿãƒ­ãƒ¼ã‚«ãƒ«ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ï¼ˆFSxã€NVMeï¼‰ |
| **Tier 2** | æ°¸ç¶šçš„ãƒªãƒ¢ãƒ¼ãƒˆã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ï¼ˆS3ï¼‰ |
| **ç‰¹å¾´** | éåŒæœŸä¿å­˜ã€è‡ªå‹•åŒæœŸ |
| **æ³¨æ„** | å…¬å¼ç”¨èªã§ã¯ãªãã€å®Ÿè£…çŠ¶æ³ä¸æ˜ç¢º |

### 4. Health Monitoring Agent

| é …ç›® | è©³ç´° |
|-----|-----|
| **ç›®çš„** | ãƒãƒ¼ãƒ‰éšœå®³ã®æ—©æœŸæ¤œå‡ºã¨è‡ªå‹•å¾©æ—§ |
| **æ¤œå‡ºæ™‚é–“** | è¿…é€Ÿã«æ¤œå‡º |
| **ç›£è¦–å¯¾è±¡** | GPU/Trainium ãƒ‡ãƒã‚¤ã‚¹ã€EC2 Platform Log |
| **å¯¾å¿œ** | Node Label/Taint è¨­å®šã€è‡ªå‹•å†èµ·å‹•ã¾ãŸã¯äº¤æ› |
| **å¯¾å¿œç’°å¢ƒ** | HyperPod EKSï¼ˆè‡ªå‹•æœ‰åŠ¹åŒ–ï¼‰ |

## å¯¾å¿œã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã¨ OS

### ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹

- **Trainium**: trn1, trn1n, **trn2**ï¼ˆml.trn2.3xlarge, ml.trn2.48xlargeï¼‰
- **GPU**: p4d, p5, p6

### OS

- **æ¨å¥¨**: Ubuntu 22.04 LTS, Amazon Linux 2023
- **SDK**: Neuron SDK 2.27.1, PyTorch NeuronX 2.5.1+

## å¾“æ¥æ‰‹æ³•ã¨ã®é•ã„

| æ‰‹æ³• | åˆ¶å¾¡ãƒ¬ãƒ™ãƒ« | å¾©æ—§æ™‚é–“ | ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ |
|-----|-----------|---------|----------|
| **ã‚¢ãƒ—ãƒªå´ Checkpoint** | ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ | ä¸­ï¼ˆæ•°åˆ†ï¼‰ | å¤§ï¼ˆ100GB+ï¼‰ |
| **Slurm Auto-Resume** | ã‚¸ãƒ§ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ© | é…ã„ï¼ˆåˆæœŸåŒ–ã‹ã‚‰ï¼‰ | ä¸­ |
| **K8s Job Restart** | Kubernetes | é…ã„ï¼ˆåˆæœŸåŒ–ã‹ã‚‰ï¼‰ | æœ€å° |
| **HyperPod Checkpointless** | ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ | **çŸ­ã„ï¼ˆæ•°åˆ†ä»¥å†…ï¼‰** | **æœ€å°** |

## æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  kubectl, hyp CLI               â”‚  æ“ä½œãƒ„ãƒ¼ãƒ«
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Kubeflow Training Operator     â”‚  ã‚¸ãƒ§ãƒ–ç®¡ç†
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  PyTorch Elastic (Rendezvous)   â”‚  å‹•çš„ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  HyperPod Resilience            â”‚  è‡ªå‹•å›å¾©
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Amazon EKS                     â”‚  Kubernetes
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  SageMaker HyperPod             â”‚  ãƒãƒãƒ¼ã‚¸ãƒ‰ã‚¯ãƒ©ã‚¹ã‚¿
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## å‚è€ƒ URL ä¸€è¦§

### AWS å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- SageMaker HyperPod è£½å“ãƒšãƒ¼ã‚¸ï¼šhttps://aws.amazon.com/sagemaker/hyperpod/
- HyperPod on EKSï¼šhttps://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-eks.html
- HyperPod Resilienceï¼šhttps://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-eks-resiliency.html
- ClusterInstanceGroupSpecification APIï¼šhttps://docs.aws.amazon.com/sagemaker/latest/APIReference/API_ClusterInstanceGroupSpecification.html

### GitHub ãƒªãƒã‚¸ãƒˆãƒª

- HyperPod CLIï¼šhttps://github.com/aws/sagemaker-hyperpod-cli
- HyperPod Recipesï¼šhttps://github.com/aws/sagemaker-hyperpod-recipes
- awsome-distributed-trainingï¼šhttps://github.com/aws-samples/awsome-distributed-training
- Llama 3 Kubernetes å®Ÿè£…ä¾‹ï¼šhttps://github.com/aws-samples/awsome-distributed-training/blob/main/3.test_cases/pytorch/neuronx-distributed/llama3/kubernetes/llama3_train.yaml-template
- Kubeflow Training Operatorï¼šhttps://github.com/kubeflow/training-operator
- AWS Neuron SDKï¼šhttps://github.com/aws-neuron/aws-neuron-sdk

### PyTorch/Kubernetes å…¬å¼

- PyTorch Distributedï¼šhttps://pytorch.org/docs/stable/distributed.html
- PyTorch Elasticï¼šhttps://pytorch.org/docs/stable/elastic/run.html
- Kubernetes Jobï¼šhttps://kubernetes.io/docs/concepts/workloads/controllers/job/
- Kubernetes StatefulSetï¼šhttps://kubernetes.io/docs/concepts/workloads/controllers/statefulset/
- Kubeflow PyTorchJobï¼šhttps://www.kubeflow.org/docs/components/training/pytorch/

### ãã®ä»–æŠ€è¡“è³‡æ–™

- Neuron DLC ãƒªãƒªãƒ¼ã‚¹ãƒãƒ¼ãƒˆï¼šhttps://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/containers/neuron-dlc.html
- PyTorch FSDPï¼ˆMeta Engineering Blogï¼‰ï¼šhttps://engineering.fb.com/2021/07/15/open-source/fsdp/
- DeepSpeed ZeROï¼šhttps://www.microsoft.com/en-us/research/blog/zero-deepspeed-new-system-optimizations-enable-training-models-with-over-100-billion-parameters/
- NVIDIA CUDA Installation Guideï¼šhttps://docs.nvidia.com/cuda/cuda-installation-guide-linux/
- Slurm sbatchï¼šhttps://slurm.schedmd.com/sbatch.html

---

# ãŠã‚ã‚Šã«

AWS SageMaker HyperPod ã¯ã€å¤§è¦æ¨¡æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã«ãŠã„ã¦ã€**4 ã¤ã®çµ±åˆçš„ãªãƒ¬ã‚¸ãƒªã‚¨ãƒ³ã‚·ãƒ¼æ©Ÿèƒ½**ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚

## ä¸»è¦ãªæˆæœ

1. **Checkpointless Training**: GPU ãƒ¡ãƒ¢ãƒªå†…ã®å†—é•·æ€§ã‚’æ´»ç”¨ã—ã€å¾©æ—§æ™‚é–“ã‚’æ•°æ™‚é–“ã‹ã‚‰æ•°åˆ†ã«çŸ­ç¸®
2. **Elastic Training / Auto-Resume**: ãƒãƒ¼ãƒ‰éšœå®³æ™‚ã®è‡ªå‹•å†é–‹ã¨å‹•çš„ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã‚’å®Ÿç¾
3. **Managed Tiered Checkpointing**: FSx ã¨ S3 ã®éšå±¤çš„ãªã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ç®¡ç†ã§ã‚³ã‚¹ãƒˆæœ€é©åŒ–ï¼ˆãŸã ã—å…¬å¼ç”¨èªã§ã¯ãªãã€å®Ÿè£…çŠ¶æ³ã¯ä¸æ˜ç¢ºï¼‰
4. **Health Monitoring Agent**: éšœå®³ã‚’è¿…é€Ÿã«æ¤œå‡ºã—ã€è‡ªå‹•å¾©æ—§ã‚’å®Ÿç¾

## ç‰¹ã«æ³¨ç›®ã™ã¹ãç‚¹

1. **çµ±åˆã•ã‚ŒãŸéšœå®³å¾©æ—§**: 4 ã¤ã®æ©Ÿèƒ½ãŒé€£æºã—ã€å¾“æ¥æ‰‹æ³•ã¨æ¯”è¼ƒã—ã¦å¤§å¹…ãªå¾©æ—§æ™‚é–“çŸ­ç¸®ã‚’å®Ÿç¾
2. **trn2 ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ã‚µãƒãƒ¼ãƒˆ**: ç¬¬ 2 ä¸–ä»£ Trainium ã§ã‚‚åˆ©ç”¨å¯èƒ½
3. **Kubernetes ãƒã‚¤ãƒ†ã‚£ãƒ–ãªå®Ÿè£…**: æ¨™æº–çš„ãª k8s ãƒ„ãƒ¼ãƒ«ã§ç®¡ç†å¯èƒ½
4. **å®Ÿè£…ä¾‹ã®å……å®Ÿ**: aws-samples/awsome-distributed-training ã« TRL/VERL ã® GRPO ã‚µãƒ³ãƒ—ãƒ«ã‚’å«ã‚€è±Šå¯Œãªå®Ÿè£…ä¾‹

## ä»Šå¾Œã®å±•æœ›

- ã•ã‚‰ã«å¤šãã®ãƒ¢ãƒ‡ãƒ«ã¨ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®ã‚µãƒãƒ¼ãƒˆ
- GRPO ãªã©ã®å¼·åŒ–å­¦ç¿’æ‰‹æ³•ã¨ã®çµ±åˆ
- Managed Tiered Checkpointing ã®å…¬å¼ã‚µãƒãƒ¼ãƒˆï¼ˆç¾åœ¨ã¯å®Ÿè£…çŠ¶æ³ä¸æ˜ç¢ºï¼‰
- Checkpointless Training ã® Slurm ç’°å¢ƒã¸ã®å¯¾å¿œ

ã“ã‚Œã‚‰ã®æ©Ÿèƒ½ã«ã‚ˆã‚Šã€å¾“æ¥ã¯æ•°é€±é–“ã‹ã‹ã£ã¦ã„ãŸå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ãŒã€ã‚ˆã‚ŠçŸ­æœŸé–“ã§ã€ã‚ˆã‚Šã‚³ã‚¹ãƒˆåŠ¹ç‡è‰¯ãå®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚å®Ÿéš›ã®ç’°å¢ƒã§ã®æ¤œè¨¼ã¨ã€å…·ä½“çš„ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã®å…¬é–‹ãŒå¾…ãŸã‚Œã¾ã™ã€‚
