---
title: "Advanced tracing and evaluation of generative AI agents using LangChain and Amazon SageMaker AI MLFlow"
emoji: "🔥"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: []
published: false
---

Translation: https://aws.amazon.com/jp/blogs/machine-learning/advanced-tracing-and-evaluation-of-generative-ai-agents-using-langchain-and-amazon-sagemaker-ai-mlflow/

# LangChain と Amazon SageMaker AI MLFlow を使用した生成 AI エージェントの高度なトレーシングと評価

Sandeep Raveesh-Babu著（2025年4月7日）

実世界のタスクに対応できる生成 AI エージェントの開発は複雑であり、本番環境レベルのエージェントアプリケーションを構築するには、ユーザーインターフェース、評価フレームワーク、継続的改善メカニズムなどの追加ツールとエージェントを統合する必要があります。開発者は、予測不可能な動作、複雑なワークフロー、複雑な相互作用に直面することがよくあります。エージェントの実験フェーズは特に困難で、多くの場合、面倒でエラーが発生しやすい状況です。堅牢な追跡メカニズムがないと、開発者はボトルネックの特定、エージェントの推論の理解、複数のツール間のシームレスな連携の確保、パフォーマンスの最適化など、困難なタスクに直面します。これらの課題により、効果的で信頼性の高い AI エージェントを作成するプロセスは困難な取り組みとなり、開発を効率化しシステムの信頼性を向上させるための革新的なソリューションが必要となります。

このような状況において、[MLflow を搭載した Amazon SageMaker AI](https://docs.aws.amazon.com/sagemaker/latest/dg/mlflow.html) は、生成 AI エージェントの実験を効率化するための強力なソリューションを提供します。本記事では、LangChain の人気のあるオープンソース [LangGraph](https://langchain-ai.github.io/langgraph/tutorials/introduction/) エージェントフレームワークを使用してエージェントを構築し、LangGraph 生成 AI エージェントの詳細なトレーシングと評価を有効にする方法を示します。この記事では、開発者および機械学習（ML）実践者として、MLflow を搭載した Amazon SageMaker AI を使用して生成 AI エージェントのパフォーマンスを効率的に実験、評価し、本番環境に向けてアプリケーションを最適化する方法を探ります。また、Retrieval Augmented Generation Assessment（[RAGAS](https://docs.ragas.io/en/stable/)）による高度な評価指標を導入し、RAGAS のようなサードパーティの指標をトラッキングするための MLflow のカスタマイズ方法も示します。

### 生成 AI エージェント開発における高度なトレーシングと評価の必要性

実験において重要な機能は、リクエストを処理する際のエージェントの内部実行パスを観察、記録、分析する能力です。これは、エラーの特定、意思決定プロセスの評価、システムの全体的な信頼性の向上に不可欠です。ワークフローのトレーシングは、デバッグを支援するだけでなく、エージェントが多様なシナリオで一貫して実行されることを確実にします。

さらなる複雑さは、生成 AI エージェントが実行するテキスト生成、要約、質問応答などのオープンエンドなタスクの性質から生じます。従来のソフトウェアテストとは異なり、生成 AI エージェントの評価には、基本的な精度や遅延の測定を超えた新しい指標と方法論が必要です。正確性、有害性、関連性、一貫性、ツールの呼び出し、根拠の確実性など、複数の次元を評価する必要があり、同時に実行パスを追跡してエラーやボトルネックを特定する必要があります。

### なぜ MLflow を搭載した SageMaker AI なのか？

[Amazon SageMaker AI](https://aws.amazon.com/sagemaker-ai/) は、人気のあるオープンソース MLflow の完全マネージド版を提供し、機械学習の実験と生成 AI 管理のための堅牢なプラットフォームを提供します。この組み合わせは、生成 AI エージェントの作業において特に強力です。MLflow を搭載した SageMaker AI は、実験のトラッキング、モデルレジストリ、デプロイメント、視覚化による指標比較を含む機械学習ワークフローの管理に広く採用されている MLflow のオープンソースの遺産を基盤としています。

* **スケーラビリティ**: SageMaker AI により、生成 AI エージェントの実験を簡単にスケールアップし、複数の反復を同時に実行できます。
* **統合トラッキング**: MLflow の統合により、実験のトラッキング、バージョン管理、エージェントワークフローの効率的な管理が可能です。
* **視覚化**: MLflow の組み込み機能により、各実験実行のパフォーマンスを監視および視覚化できます。
* **ML チームの継続性**: すでに従来の ML に MLflow を使用している組織は、MLOps スタックを大幅に変更することなくエージェントを採用できるため、生成 AI の導入における摩擦が軽減されます。
* **AWS エコシステムの利点**: MLflow を超えて、SageMaker AI は基盤モデル、多数のマネージドサービス、簡素化されたインフラストラクチャ、統合されたセキュリティへのアクセスを含む、生成 AI 開発のための包括的なエコシステムを提供します。

この進化により、MLflow を搭載した SageMaker AI は、従来の ML と最先端の生成 AI エージェント開発の両方のための統一されたプラットフォームとして位置付けられています。

### MLflow を搭載した SageMaker AI の主要機能

MLflow を搭載した SageMaker AI の機能は、エージェントの動作のトレース、エージェントのパフォーマンスの評価、統一されたガバナンスという、エージェント実験の中核的な課題に直接対応します。

1. **実験のトラッキング:** LangGraph エージェントの異なる実行を比較し、反復間のパフォーマンスの変化を追跡します。
2. **エージェントのバージョン管理:** 開発ライフサイクルを通じてエージェントの異なるバージョンを追跡し、エージェントを反復的に改良および改善します。
3. **統一されたエージェントガバナンス:** MLflow を搭載した SageMaker AI に登録されたエージェントは自動的に [MLflow を搭載した SageMaker AI コンソール](https://docs.aws.amazon.com/sagemaker/latest/dg/mlflow-launch-ui.html) に表示され、チーム間での管理、評価、ガバナンスの協調的なアプローチが可能になります。
4. **スケーラブルなインフラストラクチャ:** SageMaker AI のマネージドインフラストラクチャを使用して、リソース管理を心配することなく大規模な実験を実行できます。

### LangGraph 生成 AI エージェント

[LangGraph](https://www.langchain.com/langgraph) は、企業固有のニーズに合わせた生成 AI エージェントを設計するための強力で柔軟なアプローチを提供します。LangGraph の制御可能なエージェントフレームワークは本番環境での使用を想定して設計されており、カスタムソリューションを作成するための低レベルのカスタマイズオプションを提供します。

この記事では、データストアから財務データを取得するツールを備えたシンプルな財務アシスタントエージェントを作成する方法を示します。以下の図は、この記事のサンプルエージェントを示しています。必要なコードとともに、[GitHub リポジトリ](https://github.com/aws-samples/genai-ml-platform-examples/tree/main/operations/sagemaker-mlflow-trace-evaluate-langgraph-agent) で利用可能で、独自のアプリケーションに複製および適応できます。

![](https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/03/28/Picture1-15.jpg)

### ソリューションコード

[aws-samples GitHub リポジトリ](https://github.com/aws-samples/genai-ml-platform-examples/tree/main/operations/sagemaker-mlflow-trace-evaluate-langgraph-agent) から完全なサンプルコードをフォローして実行できます。この記事の残りの部分では、評価とトラッキングのアプローチを説明するために、リポジトリのコードからスニペットを使用します。

**前提条件**

* 課金が有効な AWS アカウント。
* SageMakerAI ドメイン。詳細については、[Amazon SageMaker AI のクイックセットアップを使用する](https://docs.aws.amazon.com/sagemaker/latest/dg/onboard-quick-start.html) を参照してください。
* Amazon SageMaker Studio で実行中の SageMaker AI with MLflow トラッキングサーバーへのアクセス。詳細については、[新しい MLflow トラッキングサーバーのセットアップ](https://docs.aws.amazon.com/sagemaker/latest/dg/mlflow-create-tracking-server.html) の手順を参照してください。
* エージェントと評価タスクのための [Amazon Bedrock 基盤モデル](https://docs.aws.amazon.com/bedrock/latest/userguide/bedrock-marketplace-subscribe-to-a-model.html) へのアクセス。

#### SageMaker AI with MLflow でのエージェントのトレース

MLflow のトレース機能は、LangGraph エージェントの動作を理解するために不可欠です。MLflow トラッキングは、機械学習コードを実行する際のパラメータ、コードバージョン、指標、出力ファイルのログ記録と、後で結果を視覚化するための API および UI です。

MLflow トレースは、エージェントサービス、ノード、ツールの実行に関する詳細な情報を取得することで、生成 AI エージェントの観察可能性を向上させる機能です。トレースは、リクエストの各中間ステップに関連する入力、出力、メタデータを記録する方法を提供し、バグや予期しない動作の原因を簡単に特定できるようにします。

MLfow トラッキング UI は、選択された MLflow 実験の MLflow Traces タブの下にエクスポートされたトレースを表示します（以下の画像参照）。

![](https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/03/28/Picture2-3.jpg)

さらに、Request ID を選択することで、エージェント入力またはプロンプト呼び出しの詳細なトレースを確認できます。Request ID を選択すると、入力から最終出力までの呼び出しワークフローの各ステップで取得された結果を含む折りたたみ可能なビューが開きます（以下の画像参照）。

SageMaker AI with MLflow は、LangGraph エージェントのすべてのノードをトレースし、各ノードの詳細な入力、出力、使用トークン、および起源タイプ（人間、ツール、AI）を含むマルチシーケンスメッセージを MLflow UI に表示します。また、エージェントワークフロー全体の実行時間をノードごとの時間分解とともに取得します。全体として、トレースは生成 AI エージェントにとって以下の理由で重要です：

* **パフォーマンス監視:** トレースにより、エージェントの動作を監視し、効果的に動作することを確認でき、誤動作、不正確さ、偏った出力の特定に役立ちます。
* **タイムアウト管理:** タイムアウト付きのトレースは、エージェントが長時間実行される操作や無限ループに陥るのを防ぎ、より良いリソース管理と応答性を確保するのに役立ちます。
* **デバッグとトラブルシューティング:** ユーザー入力に基づいて複数のステップと異なるシーケンスを持つ複雑なエージェントの場合、トレースは実行プロセスで問題が発生する場所を特定するのに役立ちます。
* **説明可能性:** トレースは、エージェントの意思決定プロセスに関する洞察を提供し、その行動の背後にある理由を理解するのに役立ちます。例えば、どのツールが呼び出され、処理タイプが人間、ツール、または AI のいずれであるかを確認できます。
* **最適化:** AI システムの実行トレースを取得および伝播することで、プロンプトやメタデータなどの異種パラメータを含む AI システムのエンドツーエンドの最適化が可能になります。
* **コンプライアンスとセキュリティ:** トレースは、監査ログとリアルタイム監視機能を提供することで、規制コンプライアンスと安全な運用の維持に役立ちます。
* **コストトラッキング:** トレースは、リソース使用量（入力トークン、出力トークン）とそれに関連する AI エージェントの実行コストの推定分析に役立ちます。
* **適応と学習:** トレースにより、エージェントがプロンプトとデータとどのように相互作用するかを観察でき、時間とともにエージェントのパフォーマンスを改善および適応するための洞察を提供します。

MLflow UI では、入力リクエストプロンプトまたは呼び出しを処理する際のエージェントのステップで取得された詳細を、タスク名を選択して確認できます（以下の画像参照）。

![](https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/03/28/Picture4-5.jpg)

適切なトレースを実装することで、生成 AI エージェントの動作についてより深い洞察を得て、パフォーマンスを最適化し、確実に信頼性とセキュリティを持って運用できるようになります。

### エージェントのトレース設定

きめ細かな制御と柔軟なトラッキングのために、MLflow のトレースデコレータ API を使用できます。これらの API を使用すると、最小限の変更で特定のエージェントノード、関数、またはコードブロックにトレースを追加できます。

```python
@mlflow.trace(name="assistant", attributes={"workflow": "agent_assistant"}, span_type="graph.py")
def assistant(state: GraphState):
    ...
```

この設定により、ユーザーは以下のことが可能になります：

* LangGraph エージェントのパフォーマンスボトルネックを特定する
* 意思決定プロセスを追跡する
* エラー率とタイプを監視する
* 異なるシナリオでのエージェントの動作パターンを分析する

このアプローチにより、実験で追跡したい内容を正確に指定できます。さらに、MLflow は MLflow の自動ログ機能 `mlflow.langchain.autolog()` を通じて、基本的なトレースのために LangChain との互換性をすぐに利用できます。MLflow を搭載した SageMaker AI を使用することで、開発環境と本番環境の両方で、LangGraph エージェントのパフォーマンスと動作について深い洞察を得て、より簡単なデバッグ、最適化、監視が可能になります。

### MLflow での評価

[MLflow の評価機能](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html) を使用して、LangGraph 大規模言語モデル（LLM）エージェントのパフォーマンスを評価し、さまざまなシナリオでの効果を客観的に測定できます。評価の重要な側面は以下の通りです：

* **評価指標:** MLflow は、[LLM-as-a-Judge](https://mlflow.org/docs/latest/llms/llm-evaluate/index.html#llm-as-a-judge-metrics)、精度、レイテンシー指標など、多くのデフォルト指標を提供し、評価用に指定できます。また、エージェント用にカスタマイズされた LLM 固有の指標を定義する柔軟性もあります。例えば、正しい財務アドバイス、規制ガイドラインの遵守、ツール呼び出しの有用性などのカスタム指標を導入できます。
* **評価データセット:** 実世界のクエリとシナリオを反映した評価用データセットを準備します。データセットには、サンプルの質問、期待される回答、関連するコンテキストデータを含める必要があります。
* **MLflow evaluate ライブラリを使用した評価の実行:** MLflow の `mlflow.evaluate()` は包括的な評価結果を返し、コードで直接表示するか、より視覚的な表現のために SageMaker AI with MLflow UI を通じて表示できます。

以下は、エージェントの評価に `mlflow.evaluate()` を使用する方法のスニペットです。[aws-samples GitHub リポジトリ](https://github.com/aws-samples/genai-ml-platform-examples/tree/main/operations/sagemaker-mlflow-trace-evaluate-langgraph-agent) のコードを実行することで、この例に従うことができます。

```python
results = mlflow.evaluate(
            agent_responses,  # テストクエリに対するエージェントが生成した回答
            targets="ground_truth",    # 比較用の "正解" 回答
            model_type="question-answering",  # QA タスク用の事前定義された指標
            extra_metrics=metrics   # 含める評価指標
        )
```

このコードスニペットは、MLflow の `evaluate()` 関数を使用して、LangGraph LLM エージェントのパフォーマンスを厳密に評価し、[aws-samples GitHub リポジトリ](https://github.com/aws-samples/genai-ml-platform-examples/tree/main/operations/sagemaker-mlflow-trace-evaluate-langgraph-agent) の `golden_questions_answer.jsonl` ファイルに保持されている事前定義された ground truth データセットと応答を比較します。"model_type":"question-answering" を指定することで、MLflow は精度や一貫性など、質問応答タスクに関連する評価指標を適用します。さらに、`extra_metrics` パラメータにより、標準的なベンチマークを超えた包括的で細かい評価を可能にする、エージェントの特定のアプリケーションに合わせたカスタムのドメイン固有の指標を組み込むことができます。この評価の結果は MLflow にログ記録され（以下の画像参照）、エージェントのパフォーマンスの一元化された追跡可能な記録を提供し、反復的な改善と情報に基づいたデプロイメントの決定を容易にします。MLflow の評価は、指定された MLflow 実験の MLflow 実行の一部として取得されます。

SageMaker AI with MLflow トラッキングサーバーを開き、指定された MLflow 実験の MLflow 実行のリストを確認できます（以下の画像参照）。

![](https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/03/28/Picture5-3.jpg)

評価指標は、モデル指標および付随するアーティファクトとともに MLflow 実行内で取得されます（以下の画像参照）。

![](https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/03/28/Picture6-2.jpg)

さらに、評価指標は選択された MLflow 実行内の Model metrics タブの下にも表示されます（以下の画像参照）。

![](https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/03/28/Picture7-2.jpg)

最後に、以下の画像に示すように、MLflow UI で選択された MLflow 実行実験間の compare チェックボックスオプションを選択することで、開発フェーズ中のエージェントのさまざまなバリエーションとバージョンを比較できます。これは、デプロイメントまたはエージェント開発のための他の意思決定プロセスに最適なパフォーマンスを発揮するエージェントバージョンを比較して選択するのに役立ちます。

![](https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/03/28/Picture8-3.jpg)

### LangGraph エージェントの登録

SageMaker AI with MLflow アーティファクトを使用して、必要なまたは生成した他の項目とともに LangGraph エージェントを登録できます。すべてのアーティファクトは、SageMaker AI with MLflow トラッキングサーバーの設定された Amazon Simple Storage Service（Amazon S3）バケットに保存されます。LangGraph エージェントの登録は、ガバナンスとライフサイクル管理にとって重要です。これは、エージェントのトラッキング、バージョン管理、デプロイメントのための一元化されたリポジトリを提供します。検証済みの AI アセットのカタログと考えてください。

以下の図に示すように、MLflow 実行内の Artifact タブの下で取得されたアーティファクトを確認できます。

![](https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/03/28/Picture9-3.jpg)

MLflow は、評価結果や requirements.txt ファイルで消費されたライブラリなど、エージェント関連の情報ファイルを自動的に取得してログに記録します。さらに、MLflow モデルとして正常にログ記録された LangGraph エージェントは、`mlflow.langchain.load_model(model_uri)` を使用して推論用にロードできます。厳密な評価後のエージェントの登録は、実証済みで検証済みのエージェントを本番環境にプロモートすることを確実にするのに役立ちます。この実践により、パフォーマンスの低いまたは信頼性の低いエージェントのデプロイメントを防ぎ、ユーザーエクスペリエンスとアプリケーションの整合性を保護するのに役立ちます。評価後の登録は、最良の結果を持つ実験が本番環境にプロモートされることを確実にするために重要です。

### 外部ライブラリ（RAGAS など）を使用した MLflow での実験と評価

MLflow の柔軟性により、外部ライブラリとのシームレスな統合が可能で、LangChain LangGraph エージェントの開発、評価、デプロイメントの能力が向上します。SageMaker MLflow を拡張して、RAGAS などの外部評価ライブラリを含めることで、LangGraph エージェントの包括的な評価が可能になります。この統合により、ML 実践者は MLflow の実験トラッキングと視覚化機能を活用しながら、RAGAS の特殊な LLM 評価指標を使用できます。RAGAS 指標を SageMaker AI with MLflow に直接ログ記録することで、複数の実行にわたって LangGraph エージェントのさまざまなバージョンを簡単に比較し、そのパフォーマンスについてより深い洞察を得ることができます。

RAGAS は、LLM アプリケーションと生成 AI エージェントの評価用のツールを提供するオープンソースライブラリです。RAGAS には、評価のスコアリングのための LLM モデル（評価者）の選択と、広範なデフォルト指標を備えた [ragas.evaluate()](https://docs.ragas.io/en/stable/references/evaluate/) メソッドが含まれています。MLflow 実験に RAGAS 指標を組み込むには、以下のアプローチを使用できます。

GitHub リポジトリの `additional_evaluations_with_ragas.ipynb` ノートブックを実行することで、この例に従うことができます。

```python
from ragas import EvaluationDataset
from ragas import evaluate
from ragas.llms import LangchainLLMWrapper
evaluation_dataset = EvaluationDataset.from_list(ragas_dataset)
evaluator_llm = LangchainLLMWrapper(llm_for_evaluation)
result = evaluate(
    dataset=evaluation_dataset,
    metrics=metrics_final,
    llm=evaluator_llm,
    embeddings=bedrock_embeddings,
    )
result
```

上記のコードによる RAGAS 指標を使用した評価結果を以下の図に示します。

![](https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/03/28/Picture10-1.jpg)

その後、計算された RAGAS 評価指標を MLflow 実験実行の一部として SageMaker AI with MLflow トラッキングサーバーにエクスポートしてトラッキングできます。説明のために以下のコードスニペットを示します。完全なコードは、同じ aws-samples GitHub リポジトリのノートブックにあります。

```python
with mlflow.start_run(
    experiment_id=get_experiment_id(_MLFLOW_RAGAS_EXPERIMENT_NAME), 
    run_name=timestamp, 
    tags={
        "project": os.getenv('PROJECT'),
        "model": os.getenv('MODELID'),
        "version": os.getenv('VERSION')
    }
):
    # データセットを MLflow にログ記録
    mlflow.log_input(dataset, context="ragas_eval_results")

    for ragas_metric in [faithfulness, answer_relevancy, answer_correctness]:
        print(ragas_metric.name)
        mean = ragas_result_ds[ragas_metric.name].mean()
        p90 = ragas_result_ds[ragas_metric.name].quantile(0.9)
        variance = ragas_result_ds[ragas_metric.name].var()
        print(mean, p90, variance)
        mlflow.log_metric(f"ragas_{ragas_metric.name}_score/v1/mean", mean)
        mlflow.log_metric(f"ragas_{ragas_metric.name}_score/v1/p90", p90)
        mlflow.log_metric(f"ragas_{ragas_metric.name}_score/v1/variance", variance)
mlflow.end_run()
```

MLflow によってログ記録された RAGAS 指標は、SageMaker AI with MLflow UI の Model metrics タブで確認できます（以下の画像参照）。

![](https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/03/28/Picture11-1.jpg)

### 実験から本番へ：MLflow のトレースと評価による協調的な承認

実際のデプロイメントシナリオでは、LangGraph エージェントを使用した MLflow のトレースと評価機能により、実験から本番環境への移行プロセスが大幅に効率化されます。

以下の画像に示すように、エージェントプラットフォームに取り組む大規模なデータサイエンティストと ML エンジニアのチームを想像してみてください。MLflow を使用することで、複雑なクエリを処理し、返品を処理し、製品の推奨事項を提供できる洗練されたエージェントを作成できます。実験フェーズでは、チームは MLflow を使用してエージェントのさまざまなバージョンをログに記録し、応答の正確性、レイテンシー、その他の指標などのパフォーマンスと評価指標を追跡できます。MLflow のトレース機能により、エージェントの意思決定プロセスを分析し、改善が必要な領域を特定できます。数多くの実験にわたる結果は自動的に SageMaker AI with MLflow にログ記録されます。チームは MLflow UI を使用して協力し、エージェントの最もパフォーマンスの高いバージョンを比較して選択し、SageMaker AI with MLflow にログ記録された多様なデータに基づいて本番環境に適したバージョンを決定できます。

![](https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/03/28/Picture12.jpg)

このデータを使用して、チームはエージェントを本番環境にプロモートするための明確なデータ駆動型のケースをステークホルダーに提示できます。マネージャーとコンプライアンス担当者は、エージェントのパフォーマンス履歴を確認し、特定の相互作用トレースを調査し、エージェントが必要なすべての基準を満たしていることを確認できます。承認後、SageMaker AI with MLflow に登録されたエージェントは、評価に合格したエージェントの正確なバージョンが本番環境に移行されることを確実にしながら、スムーズなデプロイメントへの移行を促進します。この協調的でトレース可能なアプローチは、開発サイクルを加速するだけでなく、本番環境での生成 AI エージェントの信頼性と有効性に対する信頼を醸成します。

## クリーンアップ

不要な料金が発生しないようにするには、以下の手順を使用してこの記事で使用したリソースをクリーンアップしてください：

1. SageMaker AI with MLflow トラッキングサーバーの削除：  
   * SageMaker Studio で、実行中の MLflow トラッキングサーバーインスタンスを停止して削除します
2. Amazon Bedrock モデルアクセスの取り消し：  
   * Amazon Bedrock コンソールに移動します。  
   * モデルアクセスに移動し、このプロジェクト用に有効にしたモデルへのアクセスを削除します。
3. SageMaker ドメインの削除（必要ない場合）：  
   * SageMaker コンソールを開きます。  
   * ドメインセクションに移動します。  
   * このプロジェクト用に作成したドメインを選択します。  
   * ドメインの削除を選択し、アクションを確認します。  
   * 関連する S3 バケットと IAM ロールも削除します。

## 結論

この記事では、LangChain の LangGraph、Amazon SageMaker AI、および MLflow を組み合わせて、高度な生成 AI エージェントの開発、評価、デプロイのための強力なワークフローを示しました。この統合により、生成 AI エージェントのパフォーマンスについて深い洞察を得て、迅速に反復し、開発プロセス全体を通じてバージョン管理を維持するために必要なツールが提供されます。

AI 分野が進化し続けるにつれて、以下の考慮事項を含む、生成 AI エージェントの増大する複雑さを管理し、その有効性を確保するためにこのようなツールが不可欠になります。

1. トレーサビリティが最重要：SageMaker MLflow を使用したエージェント実行パスの効果的なトレースは、複雑な生成 AI ワークフローでのデバッグ、最適化、一貫したパフォーマンスの確保に不可欠です。問題を特定し、意思決定を理解し、相互作用トレースを調査し、エージェントプロセスの詳細な記録分析を通じてシステム全体の信頼性を向上させます。
2. 評価が改善を促進：MLflow の `evaluate()` 関数と RAGAS などの外部ライブラリとの統合を使用した標準化およびカスタマイズされた評価指標は、エージェントのパフォーマンスに関する定量化可能な洞察を提供し、反復的な改良と情報に基づいたデプロイメントの決定を導きます。
3. コラボレーションとガバナンスが不可欠：MLflow を搭載した SageMaker AI によって促進される統一されたガバナンスにより、データサイエンティストからコンプライアンス担当者まで、チーム間のシームレスなコラボレーションが可能になり、本番環境での生成 AI エージェントの責任ある信頼性の高いデプロイメントを確保します。

これらの原則を受け入れ、この記事で概説されたツールを使用することで、開発者と ML 実践者は生成 AI エージェントの開発とデプロイメントの複雑さに自信を持って対処し、実際のビジネス価値を提供する堅牢で信頼性の高いアプリケーションを構築できます。今こそ、エージェントワークフローにおける高度なトレース、評価、コラボレーションの可能性を解き放つ番です！[aws-samples GitHub リポジトリ](https://github.com/aws-samples/genai-ml-platform-examples/tree/main/operations/sagemaker-mlflow-trace-evaluate-langgraph-agent)を活用して、LangChain の LangGraph、Amazon SageMaker AI、および MLflow のパワーを生成 AI プロジェクトに活用し始めましょう。

---

### 著者について

![](https://d2908q01vomqb2.cloudfront.net/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59/2025/04/02/sandeep-raveesh.jpg)**Sandeep Raveesh** は AWS の生成 AI スペシャリストソリューションアーキテクトです。彼は、モデルトレーニング、検索拡張生成（RAG）、生成 AI エージェント、生成 AI ユースケースのスケーリングなど、AIOps の旅を通じて顧客と協力しています。また、生成 AI 分野の業界課題を解決するための製品構築と調整を支援する AWS の市場投入戦略にも焦点を当てています。Sandeep は [LinkedIn](https://www.linkedin.com/in/sandeep-raveesh-750aa630?utm%5Fsource=share&utm%5Fcampaign=share%5Fvia&utm%5Fcontent=profile&utm%5Fmedium=ios%5Fapp) で見つけることができます。

