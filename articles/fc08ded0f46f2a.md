---
title: "Megatron-LM ã®ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—åŒ–ã‚’ CPU ã§å†ç¾å®Ÿè£…ã—ã¦ç†è§£ã‚’æ·±ã‚ã‚‹"
emoji: "ğŸš€"
type: "tech"
topics: ["æ©Ÿæ¢°å­¦ç¿’", "æ·±å±¤å­¦ç¿’", "PyTorch", "åˆ†æ•£å­¦ç¿’", "ä¸¦åˆ—åŒ–"]
published: false
---

## ã¯ã˜ã‚ã«

å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã«ãŠã„ã¦ã€ãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—åŒ–ã¯å¿…è¦ä¸å¯æ¬ ãªæŠ€è¡“ã¨ãªã£ã¦ã„ã¾ã™ã€‚æœ¬è¨˜äº‹ã§ã¯ NVIDIA ãŒç™ºè¡¨ã—ãŸ [Megatron-LM](https://arxiv.org/abs/1909.08053) ã®ã‚³ã‚¢ã‚³ãƒ³ã‚»ãƒ—ãƒˆã§ã‚ã‚‹**ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—åŒ–**ã‚’ã€CPU ä¸Šã§å†ç¾å®Ÿè£…ã™ã‚‹ã“ã¨ã§ã€ãã®ä»•çµ„ã¿ã‚’è©³ã—ãè§£èª¬ã—ã¾ã™ã€‚

å®Ÿè£…ã—ãŸã‚³ãƒ¼ãƒ‰ã¯ä»¥ä¸‹ã®ãƒªãƒã‚¸ãƒˆãƒªã§å…¬é–‹ã—ã¦ã„ã¾ã™ã€‚

https://github.com/littlemex/samples/tree/main/ml_distributed_experiment_collection/megatron-simple

:::message
æœ¬å®Ÿè£…ã¯æ•™è‚²ç›®çš„ã§ã‚ã‚Šã€å®Ÿéš›ã®å¤§è¦æ¨¡è¨“ç·´ã«ã¯é©ã—ã¦ã„ã¾ã›ã‚“ã€‚å®Ÿé‹ç”¨ã§ã¯ NVIDIA ã®å…¬å¼å®Ÿè£…ã‚„ PyTorch ã®åˆ†æ•£å­¦ç¿’æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
:::

## Megatron-LM ã¨ã¯

Megatron-LM ã¯ NVIDIA ã® Mohammad Shoeybi æ°ã‚‰ã«ã‚ˆã£ã¦ 2019 å¹´ã«ç™ºè¡¨ã•ã‚ŒãŸã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’åŠ¹ç‡çš„ã«è¨“ç·´ã™ã‚‹ãŸã‚ã®ãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—åŒ–æ‰‹æ³•ã§ã™ã€‚ã“ã®æ‰‹æ³•ã«ã‚ˆã‚Šã€512 å€‹ã® NVIDIA V100 GPU ã‚’ä½¿ç”¨ã—ã¦ 83 å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ¢ãƒ‡ãƒ«ã‚’ 76% ã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°åŠ¹ç‡ã§è¨“ç·´ã™ã‚‹ã“ã¨ã«æˆåŠŸã—ã¾ã—ãŸã€‚

### å¾“æ¥æ‰‹æ³•ã¨ã®é•ã„

å¾“æ¥ã®**ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—åŒ–**ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã‚’å„ GPU ã«ã‚³ãƒ”ãƒ¼ã—ã€ãƒãƒƒãƒã‚’åˆ†å‰²ã—ã¦å‡¦ç†ã—ã¾ã™ã€‚ã“ã®æ–¹å¼ã§ã¯å˜ä¸€ GPU ã®ãƒ¡ãƒ¢ãƒªã«åã¾ã‚‰ãªã„å¤§ããªãƒ¢ãƒ‡ãƒ«ã¯è¨“ç·´ã§ãã¾ã›ã‚“ã€‚

```mermaid
%%{init: {'theme':'dark'}}%%
graph TB
    subgraph ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—åŒ–
        A[ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿] --> B[GPU 0: ãƒ¢ãƒ‡ãƒ«å…¨ä½“]
        A --> C[GPU 1: ãƒ¢ãƒ‡ãƒ«å…¨ä½“]
        B --> D[å‹¾é…ã®é›†ç´„]
        C --> D
    end

    subgraph ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—åŒ–
        E[å…¥åŠ›ãƒ‡ãƒ¼ã‚¿] --> F[GPU 0: ãƒ¢ãƒ‡ãƒ«ã®ä¸€éƒ¨]
        E --> G[GPU 1: ãƒ¢ãƒ‡ãƒ«ã®ä¸€éƒ¨]
        F --> H[å‡ºåŠ›ã®é›†ç´„]
        G --> H
    end

    style A fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style B fill:#0f3460,stroke:#16213e,stroke-width:2px,color:#fff
    style C fill:#0f3460,stroke:#16213e,stroke-width:2px,color:#fff
    style D fill:#16213e,stroke:#0f3460,stroke-width:2px,color:#fff
    style E fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style F fill:#e94560,stroke:#16213e,stroke-width:2px,color:#fff
    style G fill:#e94560,stroke:#16213e,stroke-width:2px,color:#fff
    style H fill:#16213e,stroke:#0f3460,stroke-width:2px,color:#fff
```

ä¸€æ–¹ã€**ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—åŒ–**ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«è‡ªä½“ã‚’åˆ†å‰²ã—ã¦è¤‡æ•°ã® GPU ã«é…ç½®ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šå˜ä¸€ GPU ã®ãƒ¡ãƒ¢ãƒªåˆ¶ç´„ã‚’è¶…ãˆãŸãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

### ãªãœãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—åŒ–ãŒé‡è¦ãªã®ã‹

GPT-3 (175B ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿) ã‚„ GPT-4 ã®ã‚ˆã†ãªè¶…å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿è¡Œåˆ—ã ã‘ã§æ•°ç™¾ GB ã®ãƒ¡ãƒ¢ãƒªã‚’æ¶ˆè²»ã—ã¾ã™ã€‚å˜ä¸€ GPU (ä¾‹ãˆã° NVIDIA A100 ã® 80GB) ã«ã¯åˆ°åº•åã¾ã‚Šã¾ã›ã‚“ã€‚ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—åŒ–ã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã‚’è¤‡æ•° GPU ã«åˆ†æ•£é…ç½®ã—ã€è¨“ç·´ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

## Megatron-LM ã®æ ¸å¿ƒï¼šf æ¼”ç®—å­ã¨ g æ¼”ç®—å­

Megatron-LM ã®æœ€å¤§ã®è²¢çŒ®ã¯ã€**é€šä¿¡å›æ•°ã‚’æœ€å°åŒ–**ã—ãªãŒã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ä¸¦åˆ—åŒ–ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚ãã®éµã¨ãªã‚‹ã®ãŒã€ã‚«ã‚¹ã‚¿ãƒ  autograd é–¢æ•°ã¨ã—ã¦å®Ÿè£…ã•ã‚ŒãŸ f æ¼”ç®—å­ã¨ g æ¼”ç®—å­ã§ã™ã€‚

### f æ¼”ç®—å­ï¼šIdentity Forward, All-Reduce Backward

f æ¼”ç®—å­ã¯é †æ–¹å‘ãƒ‘ã‚¹ã§ã¯æ’ç­‰å†™åƒï¼ˆä½•ã‚‚ã—ãªã„ï¼‰ã€é€†æ–¹å‘ãƒ‘ã‚¹ã§ã¯å‹¾é…ã‚’å…¨ GPU ã§å¹³å‡åŒ–ã—ã¾ã™ã€‚

```mermaid
%%{init: {'theme':'dark'}}%%
sequenceDiagram
    participant I as å…¥åŠ›
    participant F as f æ¼”ç®—å­
    participant O as å‡ºåŠ›

    Note over I,O: é †æ–¹å‘ãƒ‘ã‚¹
    I->>F: x
    F->>O: x (æ’ç­‰å†™åƒ)

    Note over I,O: é€†æ–¹å‘ãƒ‘ã‚¹
    O->>F: âˆ‚L/âˆ‚x
    F->>F: All-Reduce(å¹³å‡åŒ–)
    F->>I: avg(âˆ‚L/âˆ‚x)

    style I fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style F fill:#e94560,stroke:#16213e,stroke-width:2px,color:#fff
    style O fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
```

å®Ÿè£…ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

```python:src/parallel_ops.py
class IdentityForward_AllReduceBackward(torch.autograd.Function):
    """f æ¼”ç®—å­: é †æ–¹å‘ã¯æ’ç­‰ã€é€†æ–¹å‘ã§ All-Reduce"""

    @staticmethod
    def forward(ctx, input_tensor, tensors_for_all_reduce):
        ctx.tensors_for_all_reduce = tensors_for_all_reduce
        return input_tensor  # é †æ–¹å‘ã¯æ’ç­‰å†™åƒ

    @staticmethod
    def backward(ctx, grad_output):
        # é€†æ–¹å‘ã§å‹¾é…ã‚’å¹³å‡åŒ–
        tensors = ctx.tensors_for_all_reduce
        if len(tensors) > 0:
            stacked = torch.stack(tensors, dim=0)
            reduced_grad = stacked.mean(dim=0)
        else:
            reduced_grad = grad_output
        return reduced_grad, None
```

### g æ¼”ç®—å­ï¼šAll-Reduce Forward, Identity Backward

g æ¼”ç®—å­ã¯ f æ¼”ç®—å­ã®é€†ã§ã€é †æ–¹å‘ãƒ‘ã‚¹ã§å…¨ GPU ã®å‡ºåŠ›ã‚’åˆè¨ˆã—ã€é€†æ–¹å‘ãƒ‘ã‚¹ã§ã¯æ’ç­‰å†™åƒã‚’è¡Œã„ã¾ã™ã€‚

```mermaid
%%{init: {'theme':'dark'}}%%
sequenceDiagram
    participant I as å…¥åŠ›
    participant G as g æ¼”ç®—å­
    participant O as å‡ºåŠ›

    Note over I,O: é †æ–¹å‘ãƒ‘ã‚¹
    I->>G: x
    G->>G: All-Reduce(åˆè¨ˆ)
    G->>O: sum(x)

    Note over I,O: é€†æ–¹å‘ãƒ‘ã‚¹
    O->>G: âˆ‚L/âˆ‚x
    G->>I: âˆ‚L/âˆ‚x (æ’ç­‰å†™åƒ)

    style I fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style G fill:#0f3460,stroke:#16213e,stroke-width:2px,color:#fff
    style O fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
```

```python:src/parallel_ops.py
class AllReduceForward_IdentityBackward(torch.autograd.Function):
    """g æ¼”ç®—å­: é †æ–¹å‘ã§ All-Reduceã€é€†æ–¹å‘ã¯æ’ç­‰"""

    @staticmethod
    def forward(ctx, input_tensor, tensors_for_all_reduce):
        if len(tensors_for_all_reduce) > 1:
            # é †æ–¹å‘ã§å…¨ GPU ã®å‡ºåŠ›ã‚’åˆè¨ˆ
            stacked = torch.stack(tensors_for_all_reduce, dim=0)
            return stacked.sum(dim=0)
        else:
            return input_tensor

    @staticmethod
    def backward(ctx, grad_output):
        # é€†æ–¹å‘ã¯æ’ç­‰å†™åƒ
        return grad_output, None
```

## åˆ—ä¸¦åˆ—åŒ–ã¨è¡Œä¸¦åˆ—åŒ–

ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—åŒ–ã®å®Ÿè£…ã§ã¯ã€é‡ã¿è¡Œåˆ—ã‚’ 2 ã¤ã®æ–¹æ³•ã§åˆ†å‰²ã—ã¾ã™ã€‚

### åˆ—ä¸¦åˆ—åŒ– (Column Parallel)

é‡ã¿è¡Œåˆ—ã‚’**å‡ºåŠ›æ¬¡å…ƒæ–¹å‘**ã«åˆ†å‰²ã—ã¾ã™ã€‚å„ GPU ã¯è¡Œåˆ—ã®åˆ—ã®ä¸€éƒ¨ã‚’ä¿æŒã—ã¾ã™ã€‚

```mermaid
%%{init: {'theme':'dark'}}%%
graph LR
    subgraph å…¥åŠ›
        X[X<br/>å…¥åŠ›è¡Œåˆ—<br/>å…¨ä½“ã‚’è¤‡è£½]
    end

    subgraph GPU 0
        W0[Wâ‚€<br/>é‡ã¿è¡Œåˆ—ã®<br/>å‰åŠåˆ—]
        Y0[Yâ‚€<br/>å‡ºåŠ›ã®å‰åŠ]
    end

    subgraph GPU 1
        W1[Wâ‚<br/>é‡ã¿è¡Œåˆ—ã®<br/>å¾ŒåŠåˆ—]
        Y1[Yâ‚<br/>å‡ºåŠ›ã®å¾ŒåŠ]
    end

    X -->|f æ¼”ç®—å­| W0
    X -->|f æ¼”ç®—å­| W1
    W0 --> Y0
    W1 --> Y1
    Y0 -->|é€£çµ| OUT[Y = XW]
    Y1 -->|é€£çµ| OUT

    style X fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style W0 fill:#e94560,stroke:#16213e,stroke-width:2px,color:#fff
    style W1 fill:#e94560,stroke:#16213e,stroke-width:2px,color:#fff
    style Y0 fill:#0f3460,stroke:#16213e,stroke-width:2px,color:#fff
    style Y1 fill:#0f3460,stroke:#16213e,stroke-width:2px,color:#fff
    style OUT fill:#16213e,stroke:#0f3460,stroke-width:2px,color:#fff
```

å®Ÿè£…ä¾‹ï¼š

```python:src/parallel_layers.py
class ColumnParallelLinear(nn.Module):
    """åˆ—ä¸¦åˆ—åŒ–ç·šå½¢å±¤"""

    def __init__(self, in_features, out_features, bias=True, gather_output=False):
        super().__init__()
        world_size = get_tensor_parallel_world_size()

        # å‡ºåŠ›æ¬¡å…ƒã‚’ world_size ã§åˆ†å‰²
        self.out_features_per_partition = out_features // world_size

        # é‡ã¿è¡Œåˆ—ã®ä¸€éƒ¨ã®ã¿ã‚’ä¿æŒ
        self.weight = nn.Parameter(
            torch.empty(self.out_features_per_partition, in_features)
        )

        if bias:
            self.bias = nn.Parameter(
                torch.empty(self.out_features_per_partition)
            )

    def forward(self, input_tensor, all_input_tensors=None, all_output_tensors=None):
        # f æ¼”ç®—å­ã‚’é©ç”¨ï¼ˆå‹¾é…åŒæœŸã®ãŸã‚ï¼‰
        if all_input_tensors is not None:
            input_parallel = copy_to_tensor_parallel_region(
                input_tensor, all_input_tensors
            )
        else:
            input_parallel = input_tensor

        # ãƒ­ãƒ¼ã‚«ãƒ«ãªè¡Œåˆ—ç©
        output = torch.matmul(input_parallel, self.weight.t())

        if self.bias is not None:
            output = output + self.bias

        return output
```

### è¡Œä¸¦åˆ—åŒ– (Row Parallel)

é‡ã¿è¡Œåˆ—ã‚’**å…¥åŠ›æ¬¡å…ƒæ–¹å‘**ã«åˆ†å‰²ã—ã¾ã™ã€‚å…¥åŠ›ã‚‚åŒæ§˜ã«åˆ†å‰²ã•ã‚Œã€å„ GPU ã¯éƒ¨åˆ†çš„ãªè¨ˆç®—ã‚’è¡Œã„ã¾ã™ã€‚

```mermaid
%%{init: {'theme':'dark'}}%%
graph LR
    subgraph å…¥åŠ›åˆ†å‰²æ¸ˆã¿
        X0[Xâ‚€<br/>å…¥åŠ›ã®å‰åŠ]
        X1[Xâ‚<br/>å…¥åŠ›ã®å¾ŒåŠ]
    end

    subgraph GPU 0
        W0[Wâ‚€<br/>é‡ã¿è¡Œåˆ—ã®<br/>å‰åŠè¡Œ]
        Y0[Yâ‚€<br/>éƒ¨åˆ†å‡ºåŠ›]
    end

    subgraph GPU 1
        W1[Wâ‚<br/>é‡ã¿è¡Œåˆ—ã®<br/>å¾ŒåŠè¡Œ]
        Y1[Yâ‚<br/>éƒ¨åˆ†å‡ºåŠ›]
    end

    X0 --> W0
    X1 --> W1
    W0 --> Y0
    W1 --> Y1
    Y0 -->|g æ¼”ç®—å­<br/>åˆè¨ˆ| OUT[Y = XW]
    Y1 -->|g æ¼”ç®—å­<br/>åˆè¨ˆ| OUT

    style X0 fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style X1 fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style W0 fill:#e94560,stroke:#16213e,stroke-width:2px,color:#fff
    style W1 fill:#e94560,stroke:#16213e,stroke-width:2px,color:#fff
    style Y0 fill:#0f3460,stroke:#16213e,stroke-width:2px,color:#fff
    style Y1 fill:#0f3460,stroke:#16213e,stroke-width:2px,color:#fff
    style OUT fill:#16213e,stroke:#0f3460,stroke-width:2px,color:#fff
```

```python:src/parallel_layers.py
class RowParallelLinear(nn.Module):
    """è¡Œä¸¦åˆ—åŒ–ç·šå½¢å±¤"""

    def __init__(self, in_features, out_features, bias=True, reduce_results=True):
        super().__init__()
        world_size = get_tensor_parallel_world_size()
        rank = get_tensor_parallel_rank()

        # å…¥åŠ›æ¬¡å…ƒã‚’ world_size ã§åˆ†å‰²
        self.in_features_per_partition = in_features // world_size

        # é‡ã¿è¡Œåˆ—ã®ä¸€éƒ¨ã®ã¿ã‚’ä¿æŒ
        self.weight = nn.Parameter(
            torch.empty(out_features, self.in_features_per_partition)
        )

        # ãƒã‚¤ã‚¢ã‚¹ã¯ rank 0 ã®ã¿ãŒä¿æŒï¼ˆé‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚ï¼‰
        if bias and rank == 0:
            self.bias = nn.Parameter(torch.empty(out_features))
        else:
            self.register_parameter('bias', None)

    def forward(self, input_tensor, all_output_tensors=None):
        # ãƒ­ãƒ¼ã‚«ãƒ«ãªè¡Œåˆ—ç©
        output = torch.matmul(input_tensor, self.weight.t())

        # g æ¼”ç®—å­ã‚’é©ç”¨ï¼ˆå‡ºåŠ›ã‚’åˆè¨ˆï¼‰
        if self.reduce_results and all_output_tensors is not None:
            output = reduce_from_tensor_parallel_region(output, all_output_tensors)

        # ãƒã‚¤ã‚¢ã‚¹ã‚’åŠ ç®—ï¼ˆrank 0 ã®ã¿ï¼‰
        if self.bias is not None:
            output = output + self.bias

        return output
```

## Transformer ã§ã®ä¸¦åˆ—åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³

Megatron-LM ã§ã¯ Transformer ã®å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’åŠ¹ç‡çš„ã«ä¸¦åˆ—åŒ–ã—ã¾ã™ã€‚

### MLP ãƒ–ãƒ­ãƒƒã‚¯ã®ä¸¦åˆ—åŒ–

MLPï¼ˆå¤šå±¤ãƒ‘ãƒ¼ã‚»ãƒ—ãƒˆãƒ­ãƒ³ï¼‰ã¯ 2 ã¤ã®ç·šå½¢å±¤ã‹ã‚‰æ§‹æˆã•ã‚Œã¾ã™ã€‚ç¬¬ 1 å±¤ã‚’åˆ—ä¸¦åˆ—åŒ–ã€ç¬¬ 2 å±¤ã‚’è¡Œä¸¦åˆ—åŒ–ã™ã‚‹ã“ã¨ã§ã€**é€šä¿¡å›æ•°ã‚’æœ€å°åŒ–**ã—ã¾ã™ã€‚

```mermaid
%%{init: {'theme':'dark'}}%%
graph TB
    IN[å…¥åŠ›<br/>hidden_size] --> F[f æ¼”ç®—å­]
    F --> L1[åˆ—ä¸¦åˆ—åŒ–ç·šå½¢å±¤<br/>hidden_size â†’ intermediate_size/world_size]
    L1 --> GELU[GeLU æ´»æ€§åŒ–<br/>ãƒ­ãƒ¼ã‚«ãƒ«å‡¦ç†]
    GELU --> L2[è¡Œä¸¦åˆ—åŒ–ç·šå½¢å±¤<br/>intermediate_size/world_size â†’ hidden_size]
    L2 --> G[g æ¼”ç®—å­]
    G --> OUT[å‡ºåŠ›<br/>hidden_size]

    style IN fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style F fill:#e94560,stroke:#16213e,stroke-width:2px,color:#fff
    style L1 fill:#0f3460,stroke:#16213e,stroke-width:2px,color:#fff
    style GELU fill:#16213e,stroke:#0f3460,stroke-width:2px,color:#fff
    style L2 fill:#0f3460,stroke:#16213e,stroke-width:2px,color:#fff
    style G fill:#e94560,stroke:#16213e,stroke-width:2px,color:#fff
    style OUT fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
```

é‡è¦ãªã®ã¯ã€**GeLU æ´»æ€§åŒ–é–¢æ•°ã¯å„ GPU ã§ãƒ­ãƒ¼ã‚«ãƒ«ã«å®Ÿè¡Œã•ã‚Œã‚‹**ç‚¹ã§ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šä¸­é–“çŠ¶æ…‹ã®é€šä¿¡ãŒä¸è¦ã«ãªã‚Šã¾ã™ã€‚

```python:src/transformer.py
class ParallelMLP(nn.Module):
    """ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—åŒ–ã•ã‚ŒãŸ MLP"""

    def __init__(self, hidden_size, intermediate_size, dropout=0.1):
        super().__init__()

        # ç¬¬ 1 å±¤ï¼šåˆ—ä¸¦åˆ—åŒ–ï¼ˆæ‹¡å¼µï¼‰
        self.dense_h_to_4h = ColumnParallelLinear(
            in_features=hidden_size,
            out_features=intermediate_size,
            bias=True,
            gather_output=False,  # å‡ºåŠ›ã‚’é›†ç´„ã—ãªã„
        )

        # ç¬¬ 2 å±¤ï¼šè¡Œä¸¦åˆ—åŒ–ï¼ˆåç¸®ï¼‰
        self.dense_4h_to_h = RowParallelLinear(
            in_features=intermediate_size,
            out_features=hidden_size,
            bias=True,
            input_is_parallel=True,  # å…¥åŠ›ã¯æ—¢ã«åˆ†å‰²æ¸ˆã¿
            reduce_results=True,  # å‡ºåŠ›ã‚’åˆè¨ˆ
        )

        self.dropout = nn.Dropout(dropout)

    def forward(self, hidden_states, all_input_tensors_1=None,
                all_intermediate_tensors=None, all_output_tensors=None):
        # ç¬¬ 1 å±¤ï¼ˆåˆ—ä¸¦åˆ—åŒ–ï¼‰
        intermediate = self.dense_h_to_4h(
            hidden_states,
            all_input_tensors=all_input_tensors_1,
            all_output_tensors=all_intermediate_tensors,
        )

        # GeLU æ´»æ€§åŒ–ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ã€é€šä¿¡ãªã—ï¼‰
        intermediate = F.gelu(intermediate)

        # ç¬¬ 2 å±¤ï¼ˆè¡Œä¸¦åˆ—åŒ–ã€g æ¼”ç®—å­ã§å‡ºåŠ›ã‚’åˆè¨ˆï¼‰
        output = self.dense_4h_to_h(
            intermediate,
            all_output_tensors=all_output_tensors,
        )

        output = self.dropout(output)
        return output
```

### Self-Attention ã®ä¸¦åˆ—åŒ–

Self-Attention ã§ã¯ã€ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰ã‚’è¤‡æ•° GPU ã«åˆ†æ•£ã—ã¾ã™ã€‚å„ GPU ã¯å…¨ä½“ã®ãƒ˜ãƒƒãƒ‰ã®ä¸€éƒ¨ã‚’æ‹…å½“ã—ã¾ã™ã€‚

```mermaid
%%{init: {'theme':'dark'}}%%
graph TB
    IN[å…¥åŠ›] --> QKV[Q, K, V æŠ•å½±<br/>åˆ—ä¸¦åˆ—åŒ–]

    subgraph GPU 0
        QKV --> H0[ãƒ˜ãƒƒãƒ‰ 0, 1]
        H0 --> A0[Attention è¨ˆç®—]
    end

    subgraph GPU 1
        QKV --> H1[ãƒ˜ãƒƒãƒ‰ 2, 3]
        H1 --> A1[Attention è¨ˆç®—]
    end

    A0 --> CONCAT[é€£çµ]
    A1 --> CONCAT
    CONCAT --> OUT_PROJ[å‡ºåŠ›æŠ•å½±<br/>è¡Œä¸¦åˆ—åŒ–]
    OUT_PROJ --> OUT[å‡ºåŠ›]

    style IN fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style QKV fill:#e94560,stroke:#16213e,stroke-width:2px,color:#fff
    style H0 fill:#0f3460,stroke:#16213e,stroke-width:2px,color:#fff
    style H1 fill:#0f3460,stroke:#16213e,stroke-width:2px,color:#fff
    style A0 fill:#16213e,stroke:#0f3460,stroke-width:2px,color:#fff
    style A1 fill:#16213e,stroke:#0f3460,stroke-width:2px,color:#fff
    style CONCAT fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style OUT_PROJ fill:#e94560,stroke:#16213e,stroke-width:2px,color:#fff
    style OUT fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
```

```python:src/transformer.py
class ParallelSelfAttention(nn.Module):
    """ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—åŒ–ã•ã‚ŒãŸ Self-Attention"""

    def __init__(self, hidden_size, num_attention_heads, dropout=0.1):
        super().__init__()

        world_size = get_tensor_parallel_world_size()

        # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰ã‚’åˆ†å‰²
        assert num_attention_heads % world_size == 0
        self.num_attention_heads_per_partition = num_attention_heads // world_size
        self.head_dim = hidden_size // num_attention_heads
        self.hidden_size_per_partition = (
            self.num_attention_heads_per_partition * self.head_dim
        )

        # Q, K, V æŠ•å½±ï¼ˆåˆ—ä¸¦åˆ—åŒ–ï¼‰
        self.query_key_value = ColumnParallelLinear(
            in_features=hidden_size,
            out_features=3 * hidden_size,  # Q, K, V ã‚’é€£çµ
            bias=True,
            gather_output=False,
        )

        # å‡ºåŠ›æŠ•å½±ï¼ˆè¡Œä¸¦åˆ—åŒ–ï¼‰
        self.dense = RowParallelLinear(
            in_features=hidden_size,
            out_features=hidden_size,
            bias=True,
            input_is_parallel=True,
            reduce_results=True,
        )

        self.dropout = nn.Dropout(dropout)
        self.scale = 1.0 / math.sqrt(self.head_dim)

    def forward(self, hidden_states, attention_mask=None,
                all_input_tensors_qkv=None, all_qkv_tensors=None,
                all_output_tensors=None):
        batch_size, seq_len, _ = hidden_states.shape

        # Q, K, V æŠ•å½±ï¼ˆåˆ—ä¸¦åˆ—åŒ–ï¼‰
        mixed_qkv = self.query_key_value(
            hidden_states,
            all_input_tensors=all_input_tensors_qkv,
            all_output_tensors=all_qkv_tensors,
        )

        # Q, K, V ã«åˆ†å‰²
        qkv_size = self.hidden_size_per_partition
        query, key, value = torch.split(mixed_qkv, qkv_size, dim=-1)

        # ãƒãƒ«ãƒãƒ˜ãƒƒãƒ‰ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ç”¨ã« reshape
        query = query.view(
            batch_size, seq_len,
            self.num_attention_heads_per_partition, self.head_dim
        ).transpose(1, 2)
        key = key.view(
            batch_size, seq_len,
            self.num_attention_heads_per_partition, self.head_dim
        ).transpose(1, 2)
        value = value.view(
            batch_size, seq_len,
            self.num_attention_heads_per_partition, self.head_dim
        ).transpose(1, 2)

        # Attention ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        attention_scores = torch.matmul(query, key.transpose(-2, -1)) * self.scale

        if attention_mask is not None:
            attention_scores = attention_scores + attention_mask

        attention_probs = F.softmax(attention_scores, dim=-1)
        attention_probs = self.dropout(attention_probs)

        # å€¤ã¨ã®é‡ã¿ä»˜ãå’Œ
        context = torch.matmul(attention_probs, value)

        # Reshape back
        context = context.transpose(1, 2).contiguous().view(
            batch_size, seq_len, self.hidden_size_per_partition
        )

        # å‡ºåŠ›æŠ•å½±ï¼ˆè¡Œä¸¦åˆ—åŒ–ã€g æ¼”ç®—å­ã§åˆè¨ˆï¼‰
        output = self.dense(context, all_output_tensors=all_output_tensors)

        return output
```

## é€šä¿¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ

Megatron-LM ã®å„ªã‚ŒãŸç‚¹ã¯ã€**Transformer 1 å±¤ã‚ãŸã‚Šã®é€šä¿¡å›æ•°ãŒéå¸¸ã«å°‘ãªã„**ã“ã¨ã§ã™ã€‚

### 1 å±¤ã‚ãŸã‚Šã®é€šä¿¡å›æ•°

```mermaid
%%{init: {'theme':'dark'}}%%
graph TB
    subgraph Self-Attention
        A1[é †æ–¹å‘: ãªã—] --> A2[é€†æ–¹å‘: All-Reduce Ã— 1<br/>Q/K/V æŠ•å½±å¾Œã®å‹¾é…]
        A3[é †æ–¹å‘: All-Reduce Ã— 1<br/>å‡ºåŠ›æŠ•å½±å¾Œ] --> A4[é€†æ–¹å‘: ãªã—]
    end

    subgraph MLP
        M1[é †æ–¹å‘: ãªã—] --> M2[é€†æ–¹å‘: All-Reduce Ã— 1<br/>ç¬¬ 1 å±¤å¾Œã®å‹¾é…]
        M3[é †æ–¹å‘: All-Reduce Ã— 1<br/>ç¬¬ 2 å±¤å¾Œ] --> M4[é€†æ–¹å‘: ãªã—]
    end

    A2 --> TOTAL[åˆè¨ˆ: é †æ–¹å‘ 2 å›<br/>é€†æ–¹å‘ 2 å›]
    A3 --> TOTAL
    M2 --> TOTAL
    M3 --> TOTAL

    style A1 fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style A2 fill:#e94560,stroke:#16213e,stroke-width:2px,color:#fff
    style A3 fill:#0f3460,stroke:#16213e,stroke-width:2px,color:#fff
    style A4 fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style M1 fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style M2 fill:#e94560,stroke:#16213e,stroke-width:2px,color:#fff
    style M3 fill:#0f3460,stroke:#16213e,stroke-width:2px,color:#fff
    style M4 fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style TOTAL fill:#16213e,stroke:#0f3460,stroke-width:2px,color:#fff
```

å®Œå…¨ãª Transformer 1 å±¤ï¼ˆSelf-Attention + MLPï¼‰ã§å¿…è¦ãª All-Reduce æ“ä½œã¯ï¼š
- **é †æ–¹å‘ãƒ‘ã‚¹**: 2 å›ï¼ˆSelf-Attention å‡ºåŠ›æŠ•å½±å¾Œã€MLP ç¬¬ 2 å±¤å¾Œï¼‰
- **é€†æ–¹å‘ãƒ‘ã‚¹**: 2 å›ï¼ˆMLP ç¬¬ 1 å±¤å¾Œã®å‹¾é…ã€Self-Attention Q/K/V æŠ•å½±å¾Œã®å‹¾é…ï¼‰

ã“ã®é€šä¿¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚ˆã‚Šã€**è¨ˆç®—ã¨é€šä¿¡ã‚’åŠ¹ç‡çš„ã«ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—**ã•ã›ã‚‹ã“ã¨ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

## CPU ã§ã®å®Ÿè£…ã¨ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³

æœ¬å®Ÿè£…ã§ã¯ã€å®Ÿéš›ã® GPU åˆ†æ•£ç’°å¢ƒã‚’ä½¿ã‚ãšã€CPU ä¸Šã§è¤‡æ•°ã®ã€Œä»®æƒ³ GPUã€ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã—ã¾ã™ã€‚

### TensorParallelContext ã®å®Ÿè£…

```python:src/parallel_context.py
@dataclass
class TensorParallelConfig:
    """ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—åŒ–ã®è¨­å®š"""
    world_size: int = 1  # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã™ã‚‹ GPU æ•°
    rank: int = 0  # ç¾åœ¨ã®ãƒ©ãƒ³ã‚¯ï¼ˆ0 ã‹ã‚‰ world_size-1ï¼‰

# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
_TP_CONTEXT: Optional[TensorParallelConfig] = None

def initialize_tensor_parallel(world_size: int, rank: int) -> None:
    """ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—åŒ–ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆæœŸåŒ–"""
    global _TP_CONTEXT
    assert 0 <= rank < world_size
    _TP_CONTEXT = TensorParallelConfig(world_size=world_size, rank=rank)

def get_tensor_parallel_world_size() -> int:
    """world_size ã‚’å–å¾—"""
    return get_tensor_parallel_context().world_size

def get_tensor_parallel_rank() -> int:
    """ç¾åœ¨ã®ãƒ©ãƒ³ã‚¯ã‚’å–å¾—"""
    return get_tensor_parallel_context().rank

class TensorParallelContext:
    """ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—åŒ–ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£"""

    def __init__(self, world_size: int, rank: int):
        self.world_size = world_size
        self.rank = rank
        self.prev_context = None

    def __enter__(self):
        self.prev_context = _TP_CONTEXT
        initialize_tensor_parallel(self.world_size, self.rank)
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        global _TP_CONTEXT
        _TP_CONTEXT = self.prev_context
```

ä½¿ç”¨ä¾‹ï¼š

```python
# Rank 0 ã¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
with TensorParallelContext(world_size=2, rank=0):
    model_rank0 = ParallelGPTModel(config)
    # ã“ã®ã‚¹ã‚³ãƒ¼ãƒ—å†…ã§ã¯ rank=0 ã¨ã—ã¦å‹•ä½œ

# Rank 1 ã¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–
with TensorParallelContext(world_size=2, rank=1):
    model_rank1 = ParallelGPTModel(config)
    # ã“ã®ã‚¹ã‚³ãƒ¼ãƒ—å†…ã§ã¯ rank=1 ã¨ã—ã¦å‹•ä½œ
```

## å®Ÿé¨“çµæœ

å®Ÿè£…ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã£ã¦ã€å˜ä¸€ GPU ã¨ä¸¦åˆ—åŒ–ã®æ¯”è¼ƒå®Ÿé¨“ã‚’è¡Œã„ã¾ã—ãŸã€‚

### ãƒ¢ãƒ‡ãƒ«æ§‹æˆ

- èªå½™ã‚µã‚¤ã‚º: 1,000 ãƒˆãƒ¼ã‚¯ãƒ³
- éš ã‚Œå±¤æ¬¡å…ƒ: 128
- ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: 2
- ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰æ•°: 4
- ä¸­é–“å±¤æ¬¡å…ƒ: 512
- æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·: 64

### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã®æ¯”è¼ƒ

```python
# è¨“ç·´ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè¡Œçµæœ
Single GPU params: 660,992
Parallel (per rank): 364,480
Memory reduction: ~1.8x
```

2 ã¤ã® GPU ã«åˆ†æ•£ã™ã‚‹ã“ã¨ã§ã€å„ GPU ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒç´„ 1.8 å€å‰Šæ¸›ã•ã‚Œã¾ã—ãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å˜ä¸€ GPU ã«ã¯åã¾ã‚‰ãªã„å¤§ããªãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

### è¨“ç·´ã®å‹•ä½œç¢ºèª

```bash
$ python examples/simple_train.py --mode compare

[1] Training on single 'GPU' (world_size=1)...
Rank 0 - Epoch 1/2, Batch 5/5, Loss: 6.9342
Rank 0 - Epoch 2/2, Batch 5/5, Loss: 6.7745

[2] Training on 2 'GPUs' (world_size=2)...
--- Rank 0 ---
Rank 0 - Epoch 1/2, Batch 5/5, Loss: 6.9479
Rank 0 - Epoch 2/2, Batch 5/5, Loss: 6.7956

--- Rank 1 ---
Rank 1 - Epoch 1/2, Batch 5/5, Loss: 6.9324
Rank 1 - Epoch 2/2, Batch 5/5, Loss: 6.7686
```

ä¸¡æ–¹ã®ãƒ©ãƒ³ã‚¯ã§æ­£å¸¸ã«è¨“ç·´ãŒé€²è¡Œã—ã€æå¤±ãŒæ¸›å°‘ã—ã¦ã„ã‚‹ã“ã¨ãŒç¢ºèªã§ãã¾ã™ã€‚

## å®Ÿè£…ã®æ¤œè¨¼

å®Ÿè£…ã®æ­£ç¢ºæ€§ã‚’ç¢ºèªã™ã‚‹ãŸã‚ã«ã€è¤‡æ•°ã®ãƒ†ã‚¹ãƒˆã‚’ä½œæˆã—ã¾ã—ãŸã€‚

### f æ¼”ç®—å­ã¨ g æ¼”ç®—å­ã®ãƒ†ã‚¹ãƒˆ

```python:tests/test_parallel_ops.py
def test_f_operator():
    """f æ¼”ç®—å­ã®ãƒ†ã‚¹ãƒˆ"""
    x1 = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)
    x2 = torch.tensor([[5.0, 6.0], [7.0, 8.0]], requires_grad=True)

    grad_tensors = []
    y1 = copy_to_tensor_parallel_region(x1, grad_tensors)

    # é †æ–¹å‘ã¯æ’ç­‰å†™åƒ
    assert torch.allclose(y1, x1)

    # é€†æ–¹å‘ã§å‹¾é…ã‚’å¹³å‡åŒ–
    grad_tensors.append(torch.ones_like(x1))
    grad_tensors.append(torch.ones_like(x2))

    y1_new = copy_to_tensor_parallel_region(x1, grad_tensors)
    loss = y1_new.sum()
    loss.backward()

    # å‹¾é…ãŒå¹³å‡åŒ–ã•ã‚Œã¦ã„ã‚‹
    assert torch.allclose(x1.grad, torch.ones_like(x1))
```

### ä¸¦åˆ—å±¤ã®ç­‰ä¾¡æ€§ãƒ†ã‚¹ãƒˆ

```python:tests/test_parallel_layers.py
def test_column_parallel_linear():
    """åˆ—ä¸¦åˆ—åŒ–å±¤ãŒæ¨™æº–å±¤ã¨ç­‰ä¾¡ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª"""
    in_features, out_features = 8, 16
    world_size = 2

    # æ¨™æº–å±¤
    torch.manual_seed(42)
    standard_linear = nn.Linear(in_features, out_features)
    x = torch.randn(2, 4, in_features)
    standard_output = standard_linear(x)

    # ä¸¦åˆ—å±¤ï¼ˆå„ãƒ©ãƒ³ã‚¯ï¼‰
    parallel_outputs = []
    for rank in range(world_size):
        with TensorParallelContext(world_size=world_size, rank=rank):
            parallel_linear = ColumnParallelLinear(
                in_features=in_features,
                out_features=out_features,
            )
            # é‡ã¿ã‚’ã‚³ãƒ”ãƒ¼
            start_idx = rank * (out_features // world_size)
            end_idx = (rank + 1) * (out_features // world_size)
            parallel_linear.weight.copy_(
                standard_linear.weight[start_idx:end_idx, :]
            )

            output = parallel_linear(x)
            parallel_outputs.append(output)

    # é€£çµã—ãŸå‡ºåŠ›ãŒæ¨™æº–å±¤ã¨ä¸€è‡´
    gathered_output = torch.cat(parallel_outputs, dim=-1)
    assert torch.allclose(gathered_output, standard_output, rtol=1e-4)
```

ã™ã¹ã¦ã®ãƒ†ã‚¹ãƒˆãŒæˆåŠŸã—ã€å®Ÿè£…ã®æ­£ç¢ºæ€§ãŒç¢ºèªã•ã‚Œã¾ã—ãŸã€‚

## å®Ÿé‹ç”¨ã¸ã®æ‹¡å¼µ

æœ¬å®Ÿè£…ã¯æ•™è‚²ç›®çš„ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ã™ãŒã€å®Ÿé‹ç”¨ã§ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«æ‹¡å¼µã§ãã¾ã™ã€‚

### PyTorch Distributed ã¸ã®æ‹¡å¼µ

```python
import torch.distributed as dist

def all_reduce_real(tensor):
    """å®Ÿéš›ã®åˆ†æ•£ç’°å¢ƒã§ã® All-Reduce"""
    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)
    return tensor / dist.get_world_size()

class IdentityForward_AllReduceBackward_Real(torch.autograd.Function):
    """å®Ÿéš›ã® GPU ç’°å¢ƒç”¨ã® f æ¼”ç®—å­"""

    @staticmethod
    def forward(ctx, input_tensor):
        return input_tensor

    @staticmethod
    def backward(ctx, grad_output):
        # å®Ÿéš›ã® NCCL é€šä¿¡ã‚’ä½¿ç”¨
        return all_reduce_real(grad_output)
```

### Multi-Process ã§ã®å®Ÿè¡Œ

```python
import torch.multiprocessing as mp

def run_rank(rank, world_size, config):
    """å„ãƒ©ãƒ³ã‚¯ã‚’åˆ¥ãƒ—ãƒ­ã‚»ã‚¹ã§å®Ÿè¡Œ"""
    # åˆ†æ•£ç’°å¢ƒã®åˆæœŸåŒ–
    dist.init_process_group(
        backend='nccl',
        init_method='env://',
        world_size=world_size,
        rank=rank
    )

    # GPU ãƒ‡ãƒã‚¤ã‚¹ã®è¨­å®š
    torch.cuda.set_device(rank)

    # ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆã¨è¨“ç·´
    model = ParallelGPTModel(config).cuda(rank)
    # ... è¨“ç·´ãƒ«ãƒ¼ãƒ—

if __name__ == "__main__":
    world_size = 8
    mp.spawn(run_rank, args=(world_size, config), nprocs=world_size)
```

## ã¾ã¨ã‚

æœ¬è¨˜äº‹ã§ã¯ Megatron-LM ã®ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—åŒ–ã‚’ CPU ä¸Šã§å†ç¾å®Ÿè£…ã—ã€ãã®ä»•çµ„ã¿ã‚’è©³ã—ãè§£èª¬ã—ã¾ã—ãŸã€‚

### é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ

1. **f æ¼”ç®—å­ã¨ g æ¼”ç®—å­**: é †æ–¹å‘ã¨é€†æ–¹å‘ã§é€šä¿¡ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‚’åˆ¶å¾¡ã™ã‚‹
2. **åˆ—ä¸¦åˆ—åŒ–ã¨è¡Œä¸¦åˆ—åŒ–**: é‡ã¿è¡Œåˆ—ã‚’åŠ¹ç‡çš„ã«åˆ†å‰²ã™ã‚‹
3. **é€šä¿¡æœ€å°åŒ–**: Transformer 1 å±¤ã‚ãŸã‚Šé †æ–¹å‘ 2 å›ã€é€†æ–¹å‘ 2 å›ã® All-Reduce ã®ã¿
4. **ãƒ­ãƒ¼ã‚«ãƒ«å‡¦ç†**: GeLU ãªã©ã®æ´»æ€§åŒ–é–¢æ•°ã¯å„ GPU ã§ç‹¬ç«‹ã«å®Ÿè¡Œ

### ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—åŒ–ã®åˆ©ç‚¹

- å˜ä¸€ GPU ã®ãƒ¡ãƒ¢ãƒªåˆ¶ç´„ã‚’è¶…ãˆãŸãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ãŒå¯èƒ½
- ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—åŒ–ã¨çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ã•ã‚‰ãªã‚‹å¤§è¦æ¨¡åŒ–ãŒå¯èƒ½
- é€šä¿¡å›æ•°ãŒå°‘ãªãã€é«˜ã„ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°åŠ¹ç‡ã‚’å®Ÿç¾

### ä»Šå¾Œã®ç™ºå±•

Megatron-LM ã®æ¦‚å¿µã¯ä»¥ä¸‹ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§å®Ÿç”¨åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚

- [NVIDIA Megatron-LM](https://github.com/NVIDIA/Megatron-LM): å…¬å¼å®Ÿè£…
- [Microsoft DeepSpeed](https://github.com/microsoft/DeepSpeed): ZeRO ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã¨çµ„ã¿åˆã‚ã›ãŸå®Ÿè£…
- [Colossal-AI](https://github.com/hpcaitech/ColossalAI): è¤‡æ•°ã®ä¸¦åˆ—åŒ–æ‰‹æ³•ã‚’çµ±åˆã—ãŸãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã«ãŠã„ã¦ã€ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—åŒ–ã¯å¿…è¦ä¸å¯æ¬ ãªæŠ€è¡“ã¨ãªã£ã¦ã„ã¾ã™ã€‚æœ¬è¨˜äº‹ã®å®Ÿè£…ã‚’é€šã˜ã¦ã€ãã®ä»•çµ„ã¿ã‚’ç†è§£ã—ã¦ã„ãŸã ã‘ã‚Œã°å¹¸ã„ã§ã™ã€‚

## å‚è€ƒæ–‡çŒ®

- [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053) - Mohammad Shoeybi et al., 2019
- [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) - Deepak Narayanan et al., 2021
- [PyTorch Distributed Overview](https://pytorch.org/tutorials/beginner/dist_overview.html)
- [NVIDIA Megatron-LM GitHub Repository](https://github.com/NVIDIA/Megatron-LM)

---

å®Ÿè£…ã‚³ãƒ¼ãƒ‰ã®å…¨ä½“ã¯ä»¥ä¸‹ã®ãƒªãƒã‚¸ãƒˆãƒªã§å…¬é–‹ã—ã¦ã„ã¾ã™ã€‚

https://github.com/littlemex/samples/tree/main/ml_distributed_experiment_collection/megatron-simple
