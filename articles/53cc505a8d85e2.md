---
title: "XTTS v2 ã‚’ AWS Neuron ã§å‹•ã‹ã™"
emoji: "ğŸ§™â€â™‚ï¸"
type: "tech"
topics: ["AWSNeuron", "tts", "xtts", "inferentia", "éŸ³å£°åˆæˆ"]
published: false
---

æœ¬è¨˜äº‹ã¯ã€AWS Trainium/Inferentia2 ãƒãƒƒãƒ—ã§éŸ³å£°åˆæˆ (TTS) ã‚’å®Ÿè£…ã—ãŸã„ä¸­ç´šè€…ã‚’å¯¾è±¡ã¨ã—ã¦ã„ã¾ã™ã€‚Python åŸºç¤ãŠã‚ˆã³ PyTorch ã®åŸºæœ¬çš„ãªä½¿ã„æ–¹ã‚’å‰æçŸ¥è­˜ã¨ã—ã¾ã™ã€‚

## ã¯ã˜ã‚ã«

https://huggingface.co/coqui/XTTS-v2

### èƒŒæ™¯ã¨ç›®çš„

**XTTS v2** (eXtended Text-to-Speech v2, ç´„ 396M ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿) ã¯ã€Coqui ãŒé–‹ç™ºã—ãŸã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®éŸ³å£°åˆæˆãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚æœ€è¿‘ã®å¤§è¦æ¨¡ TTS ãƒ¢ãƒ‡ãƒ«ï¼ˆ1B+ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰ã¨æ¯”è¼ƒã™ã‚‹ã¨ã€**ã©ã¡ã‚‰ã‹ã¨ã„ã†ã¨è»½é‡ãªãƒ¢ãƒ‡ãƒ«ã®éƒ¨é¡**ã«å…¥ã‚Šã¾ã™ã€‚

https://zenn.dev/tosshi/articles/f6c49165c90e6d

æœ¬è¨˜äº‹ã§ã¯ä¸Šè¨˜ã§ç´¹ä»‹ã—ãŸ Amazon EC2 Inf2 / Trn2 ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§ NxD Inference ã‚’ç”¨ã„ã¦ Whisper ã¨ XTTS v2 ã‚’ä¸¡æ–¹æ··åœ¨ã§å‹•ã‹ã™ãŸã‚ã€AWS Neuron ã§ XTTS v2 ã‚’å‹•ã‹ã™å®Ÿé¨“ã‚’è©¦ã¿ã¾ã™ã€‚ãªãœæ··åœ¨ã•ã›ãŸã„ã‹ã¨ã„ã†ã¨ã€ã‚«ã‚¹ã‚¿ãƒ ãƒãƒƒãƒ—ã§è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’å‹•ã‹ã™ã“ã¨ã®å¯èƒ½æ€§ã‚’æ¢ã‚ŠãŸã„ã‹ã‚‰ã§ã™ã€‚

XTTS v2 ã¯å˜ç´”ãª end-to-end ãƒ¢ãƒ‡ãƒ«ã§ã¯ãªãã€**è¤‡æ•°ã®ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ï¼ˆGPT + HifiDecoderï¼‰ãŒæ··åœ¨**ã—ã¦ã„ã¾ã™ã€‚ã“ã®ã‚ˆã†ãªè¤‡é›‘ãªã‚±ãƒ¼ã‚¹ã§ AWS Neuron ã‚’ã©ã®ã‚ˆã†ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¦å‹•ã‹ã›ã°ã‚ˆã„ã®ã‹ã‚’æ¤œè¨¼ã—ã¦ã¿ã¾ã™ã€‚

### æŠ€è¡“ãƒ†ãƒ¼ãƒ

ã“ã®è¨˜äº‹ã§ã¯ã€**è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ãŒæ··åœ¨ã™ã‚‹ TTS ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**ã‚’ AWS Neuron ã§å‹•ã‹ã™ãŸã‚ã«å¿…è¦ã ã£ãŸæŠ€è¡“çš„å·¥å¤«ã‚’è§£èª¬ã—ã¾ã™ã€‚

ç¬¬ä¸€ã«ã€ã©ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ Neuron åŒ–ã™ã¹ãã‹ã¨ã„ã†ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æˆ¦ç•¥ã‚’æ¤œè¨ã—ã¾ã™ã€‚æ¬¡ã«ã€ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’æ—¢å­˜ã® PyTorch ã‚³ãƒ¼ãƒ‰ã«çµ±åˆã™ã‚‹ Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚ã•ã‚‰ã«ã€`torch_neuronx.trace()` ã®åˆ¶ç´„ã¨å®Ÿè£…ä¸Šã®å·¥å¤«ã¨ã—ã¦å›ºå®šé•·å…¥åŠ›ã¸ã®å¯¾å¿œã‚’è§£èª¬ã—ã¾ã™ã€‚æœ€å¾Œã«ã€ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’çµ„ã¿åˆã‚ã›ã¦ TTS æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã™ã‚‹æ–¹æ³•ã‚’ç¤ºã—ã¾ã™ã€‚

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ç´¹ä»‹ã™ã‚‹å®Ÿè£…ã¯ã€**NxD Inference ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã‚ãš**ã€`torch_neuronx.trace()` ã‚’ä½¿ã£ãŸãƒ—ãƒªãƒŸãƒ†ã‚£ãƒ–ãªæ–¹æ³•ã§ã™ã€‚PyTorch ã® `torch.jit.trace()` ã¨åŒã˜æ„Ÿè¦šã§ä½¿ãˆã‚‹ãŸã‚ã€æ—¢å­˜ã® PyTorch ã‚³ãƒ¼ãƒ‰ã«æœ€å°é™ã®å¤‰æ›´ã§çµ±åˆã§ãã¾ã™ã€‚NxD Inference ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¸ã®çµ±åˆã¯ç¾åœ¨æ¤œè¨¼ä¸­ã§ã‚ã‚Šä»Šå¾Œç´¹ä»‹ã—ãŸã„ã¨æ€ã„ã¾ã™ã€‚

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦

XTTS v2 ã¯ã€**2 ã¤ã®ä¸»è¦ãƒ¢ãƒ‡ãƒ«ã‚’çµ„ã¿åˆã‚ã›ãŸ TTS ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**ã§ã™ã€‚

::::details å‚è€ƒ
- [Coqui TTS - XTTS v2](https://github.com/coqui-ai/TTS/tree/main/TTS/tts/models)
- [Xtts ã‚¯ãƒ©ã‚¹](https://github.com/coqui-ai/TTS/blob/eef419b37393b11cc741662d041d8d793e011f2d/TTS/tts/models/xtts.py#L191)
- [GPT ã‚¯ãƒ©ã‚¹](https://github.com/coqui-ai/TTS/blob/eef419b37393b11cc741662d041d8d793e011f2d/TTS/tts/layers/xtts/gpt.py#L88)
- [HifiDecoder ã‚¯ãƒ©ã‚¹](https://github.com/coqui-ai/TTS/blob/eef419b37393b11cc741662d041d8d793e011f2d/TTS/tts/layers/xtts/hifigan_decoder.py#L615)
::::

### æ¨è«–æ™‚ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“å›³

ä»¥ä¸‹ã®å›³ã¯ã€XTTS v2 ã®æ¨è«–æ™‚ã«ãŠã‘ã‚‹å®Ÿéš›ã®å‡¦ç†ãƒ•ãƒ­ãƒ¼ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

```mermaid
sequenceDiagram
    participant User as ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›
    participant Tokenizer as Tokenizer
    participant GPT as GPT Model<br/>(395.8M params)
    participant HifiDecoder as HifiDecoder<br/>(11.8M params)
    participant Output as éŸ³å£°å‡ºåŠ›

    User->>Tokenizer: ãƒ†ã‚­ã‚¹ãƒˆ
    Tokenizer->>GPT: text_tokens

    Note over GPT: GPT.generate()
    GPT->>GPT: audio_codes ç”Ÿæˆ

    Note over GPT: GPT.forward()
    GPT->>GPT: latents æŠ½å‡º<br/>(1024-dim)

    GPT->>HifiDecoder: latents
    Note over HifiDecoder: hifigan_decoder()
    HifiDecoder->>Output: waveform (24kHz)
```

latents ã¯ GPT ãŒç”Ÿæˆã™ã‚‹éŸ³å£°ã®æŠ½è±¡çš„ãªç‰¹å¾´ã‚’è¡¨ç¾ã—ãŸä¸­é–“ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚ãã‚Œã‚’ HifiDecoder ã§éŸ³å£°æ³¢å½¢ã«ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¾ã™ã€‚end-to-end ã§ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ä¸€æ°—ã«éŸ³å£°æ³¢å½¢ã‚’ç”Ÿæˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚‚ã‚ã‚‹ã‚ˆã†ã§ã™ãŒä»Šå›ã¯äºŒã¤ã®ç‹¬ç«‹ã—ãŸãƒ¢ãƒ‡ãƒ«ãŒé€£æºï¼ˆãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼ã¨å‘¼ç§°ï¼‰ã—ã¦ã„ã¾ã™ã€‚

:::message alert
ä»Šå›ã®ç›®çš„ã¯ç‹¬ç«‹ã—ãŸäºŒã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãã‚Œãã‚Œ AWS Neuron ã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã€XTTS v2 ã®å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã‚³ãƒ¼ãƒ‰ã«æ‰‹ã‚’åŠ ãˆã‚‹ã“ã¨ãªã Inf2 ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§æ¨è«–å‡¦ç†ã‚’å®Ÿç¾ã™ã‚‹ã“ã¨ã§ã™ã€‚
:::

### æ¨è«–å‡¦ç†ã®ã‚³ãƒ¼ãƒ«ãƒ•ãƒ­ãƒ¼

ä»¥ä¸‹ã« XTTS v2 ã®æ¨è«–å‡¦ç†ãŒã©ã“ã‹ã‚‰å§‹ã¾ã‚Šã€ã©ã®ã‚ˆã†ãªçµŒè·¯ã§å‡¦ç†ãŒé€²ã‚€ã®ã‹ã‚’ GitHub URL ã§ç¤ºã—ã¾ã™ã€‚

#### 1. ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ -- full_inference

https://github.com/coqui-ai/TTS/blob/eef419b37393b11cc741662d041d8d793e011f2d/TTS/tts/models/xtts.py#L421-L500

ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå‘¼ã³å‡ºã™é«˜ãƒ¬ãƒ™ãƒ« API ã§ã™ã€‚ãƒ†ã‚­ã‚¹ãƒˆã¨å‚ç…§éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å—ã‘å–ã‚Šã¾ã™ã€‚å‚ç…§éŸ³å£°ã‹ã‚‰ latents ã‚’æŠ½å‡ºã™ã‚‹ `get_conditioning_latents()` ã‚’å‘¼ã³å‡ºã—ã€`self.inference()` ã‚’å‘¼ã³å‡ºã—ï¼ˆå®Ÿéš›ã®æ¨è«–å‡¦ç†ï¼‰ã—ã¾ã™ã€‚

#### 2. å‚ç…§éŸ³å£°ã®å‰å‡¦ç† -- get_conditioning_latents

https://github.com/coqui-ai/TTS/blob/eef419b37393b11cc741662d041d8d793e011f2d/TTS/tts/models/xtts.py#L326-L380

å‚ç…§éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ GPT ç”¨ã®æ¡ä»¶ä»˜ã‘ latents ã‚’æŠ½å‡ºã—ã¾ã™ã€‚`load_audio()` ã§éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€`get_gpt_cond_latents()` ã‚’å‘¼ã³å‡ºã—ã¾ã™ã€‚

#### 3. å®Ÿéš›ã®æ¨è«–å‡¦ç† -- inference

https://github.com/coqui-ai/TTS/blob/eef419b37393b11cc741662d041d8d793e011f2d/TTS/tts/models/xtts.py#L503-L583

å‰å‡¦ç†æ¸ˆã¿ã®æ¡ä»¶ä»˜ã‘ latents ã‚’å—ã‘å–ã‚Šã€GPT ã¨ HifiDecoder ã®ä¸¡æ–¹ã‚’é †æ¬¡å‘¼ã³å‡ºã—ã¾ã™ã€‚

##### 3-1. GPT.generate() ã§ audio_codes ç”Ÿæˆ

https://github.com/coqui-ai/TTS/blob/eef419b37393b11cc741662d041d8d793e011f2d/TTS/tts/models/xtts.py#L541

è‡ªå·±å›å¸°çš„ã«é›¢æ•£çš„ãª audio_codes ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

##### 3-2. GPT.forward() ã§ latents æŠ½å‡º

https://github.com/coqui-ai/TTS/blob/eef419b37393b11cc741662d041d8d793e011f2d/TTS/tts/models/xtts.py#L542

ç”Ÿæˆã•ã‚ŒãŸ audio_codes ã‚’é€£ç¶šçš„ãª latentsï¼ˆ1024-dimï¼‰ã«å¤‰æ›ã—ã¾ã™ã€‚

##### 3-3. HifiDecoder ã§æ³¢å½¢ç”Ÿæˆ

https://github.com/coqui-ai/TTS/blob/eef419b37393b11cc741662d041d8d793e011f2d/TTS/tts/models/xtts.py#L576-L583

latents ã‚’éŸ³å£°æ³¢å½¢ï¼ˆ24kHzï¼‰ã«ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

ã“ã®ã‚ˆã†ã«ã€æ¨è«–å‡¦ç†ã¯ `full_inference` ã‹ã‚‰å§‹ã¾ã‚Šã€`get_conditioning_latents` â†’ `inference` â†’ `GPT.generate()` â†’ `GPT.forward()` â†’ `HifiDecoder` ã®é †ã«å‡¦ç†ãŒé€²ã¿ã¾ã™ã€‚

## GPT ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ†ã‚­ã‚¹ãƒˆ â†’ latentsï¼‰

https://github.com/coqui-ai/TTS/blob/eef419b37393b11cc741662d041d8d793e011f2d/TTS/tts/layers/xtts/gpt.py#L88-L111

:::message
GPT ãƒ¢ãƒ‡ãƒ«ã¯ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰ latents ã‚’ç”Ÿæˆã™ã‚‹å½¹å‰²ã‚’æ‹…ã„ã¾ã™ã€‚ä¸»è¦ãªãƒ¡ã‚½ãƒƒãƒ‰ã¨ã—ã¦ã€`generate()` ãŒéŸ³å£°ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã—ã€`forward()` ãŒæ½œåœ¨å¤‰æ•°ã‚’è¨ˆç®—ã—ã¾ã™ã€‚å…¥åŠ›ã¨ã—ã¦ `text_tokens`ï¼ˆãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³åˆ— `(batch, text_len)`ï¼‰ã¨ `cond_latents`ï¼ˆéŸ³å£°æ¡ä»¶ä»˜ã‘æ½œåœ¨å¤‰æ•°ã€speaker embedding ãªã©ï¼‰ã‚’å—ã‘å–ã‚Šã¾ã™ã€‚å‡ºåŠ›ã¯ `generate()` ãƒ¡ã‚½ãƒƒãƒ‰ã§ã¯ `audio_codes`ï¼ˆç”Ÿæˆã•ã‚ŒãŸéŸ³å£°ã‚³ãƒ¼ãƒ‰ `(batch, audio_len)`ï¼‰ã€`forward()` ãƒ¡ã‚½ãƒƒãƒ‰ã§ã¯ `latents`ï¼ˆæ½œåœ¨è¡¨ç¾ `(batch, latent_len, latent_dim)` = `(batch, N, 1024)`ï¼‰ã‚’è¿”ã—ã¾ã™ã€‚
:::

`generate()` ã§ autoregressive ç”Ÿæˆã—ã€`forward()` ã§ç”Ÿæˆã•ã‚ŒãŸé›¢æ•£çš„ãª audio_codes ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ latents ã«å¤‰æ›ã—ã¾ã™ã€‚

ã“ã®å‡¦ç†ã¯ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã§é›¢æ•£çš„ãƒˆãƒ¼ã‚¯ãƒ³ ID ã‚’ Embedding å±¤ã«ã‚ˆã£ã¦é€£ç¶šçš„ãª Hidden States ã«å¤‰æ›ã™ã‚‹ã®ã¨é¡ä¼¼ã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã‚ã‚Šã€GPT.forward() ãŒå®Ÿè³ªçš„ã« Audio Code Embedding ã®å½¹å‰²ã‚’æœãŸã—ã¦ã„ã¾ã™ã€‚

## HifiDecoderï¼ˆlatents â†’ éŸ³å£°æ³¢å½¢ï¼‰

https://github.com/coqui-ai/TTS/blob/eef419b37393b11cc741662d041d8d793e011f2d/TTS/tts/layers/xtts/hifigan_decoder.py#L615-L639

:::message
HifiDecoder ã¯ latents ã‚’éŸ³å£°æ³¢å½¢ã«å¤‰æ›ã™ã‚‹å½¹å‰²ã‚’æ‹…ã„ã¾ã™ã€‚ä¸»è¦ãªãƒ¡ã‚½ãƒƒãƒ‰ã¨ã—ã¦ã€`forward()` ãŒè¨“ç·´æ™‚ã®é †ä¼æ’­å‡¦ç†ï¼ˆå‹¾é…è¨ˆç®—ã‚ã‚Šï¼‰ã‚’å®Ÿè¡Œã—ã€`inference()` ãŒæ¨è«–æ™‚ã®å‡¦ç†ï¼ˆ`@torch.inference_mode()` ã§å‹¾é…è¨ˆç®—ãªã—ï¼‰ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚å…¥åŠ›ã¨ã—ã¦ `latents`ï¼ˆGPT ãŒç”Ÿæˆã—ãŸæ½œåœ¨è¡¨ç¾ `(batch, latent_len, latent_dim)` = `(batch, N, 1024)`ã€å†…éƒ¨ã§ transpose ã•ã‚Œã‚‹ï¼‰ã¨ `g`ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®æ¡ä»¶ä»˜ã‘ãƒ†ãƒ³ã‚½ãƒ«ã€speaker embedding ãªã©ï¼‰ã‚’å—ã‘å–ã‚Šã¾ã™ã€‚å‡ºåŠ›ã¯ `waveform`ï¼ˆéŸ³å£°æ³¢å½¢ `(batch, 1, sample_len)`ï¼‰ã‚’è¿”ã—ã¾ã™ã€‚
:::

HifiDecoder ã¯ GPT ã‹ã‚‰å—ã‘å–ã£ãŸé€£ç¶šçš„ãª latents ã‚’éŸ³å£°æ³¢å½¢ã«ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¾ã™ã€‚`forward()` ã¨ `inference()` ã¯æœ¬è³ªçš„ã«åŒã˜å‡¦ç†ã‚’è¡Œã„ã¾ã™ãŒã€å¾Œè€…ã¯ `@torch.inference_mode()` ã§å‹¾é…è¨ˆç®—ã‚’çœç•¥ã—ã¾ã™ã€‚

## Neuron ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«

end-to-end ã®ãƒ¢ãƒ‡ãƒ«ã¨ã¯ç•°ãªã‚Šä¸Šè¿°ã—ãŸ GPTã€HifiDecoder ã¯ç‹¬ç«‹ã—ã¦ã„ã‚‹ãŸã‚ã€**å€‹åˆ¥ã« Neuron ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«**ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

### Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å®Ÿè£…

XTTS v2 ã®ã‚ˆã†ãªå…¬é–‹ OSS ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ AWS Neuron ã«å¯¾å¿œã•ã›ã‚‹å ´åˆã€åŸºæœ¬çš„ã«ã¯ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ãŒé€šã‚Œã°è‰¯ã„ã®ã§å…ƒã® OSS ã®ä¸Šè¨˜ã§ç´¹ä»‹ã—ãŸã‚³ãƒ¼ãƒ‰ã«ç›´æ¥æ‰‹ã‚’åŠ ãˆã¦ AWS Neuron ã«ç‰¹åŒ–ã•ã›ã‚‹å½¢ã§ä¿®æ­£ã—ã¦ã‚‚è‰¯ã„ã§ã™ãŒ OSS å´ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚¢ãƒƒãƒ—ã”ã¨ã« AWS Neuron ã®ä¿®æ­£ã‚’æ‰‹å‹•ã§ãƒãƒ¼ã‚¸ã™ã‚‹å¿…è¦æ€§ãŒã‚ã‚Šã€ãƒ•ã‚©ãƒ¼ã‚¯ã—ã¦ç‹¬è‡ªãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ãŒç™ºç”Ÿã—ã¾ã™ã€‚å€‹äººçš„ã«ã¯ç’°å¢ƒäº’æ›æ€§ã‚’é‡è¦–ã™ã‚‹ãŸã‚ã§ãã‚Œã°ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä¾å­˜ã‚„ç’°å¢ƒä¾å­˜ã‚’å…¥ã‚ŒãŸãã‚ã‚Šã¾ã›ã‚“ã€‚

ãã“ã§ä»¥ä¸‹ã®ã‚ˆã†ã«å…ƒã® OSS ã®ã‚³ãƒ¼ãƒ‰ã‚’å¤‰æ›´ã›ãšã«ã€pip ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã€ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«å¾Œã«å¤–ã‹ã‚‰ `forward()` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ Override ã™ã‚‹æ–¹æ³•ãŒè‰¯ã„ã®ã§ã¯ãªã„ã‹ã¨æ€ã£ã¦è©¦ã—ã¦ã¿ã¾ã—ãŸã€‚OSS ã®å®Ÿè£…æ–¹æ³•ã«ã‚ˆã£ã¦ã¯ä¸è¦ã ã£ãŸã‚Šã€å¥½ã¿ã®å•é¡Œã¯ã‚ã‚‹ã®ã§ã“ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ã†ã‹ã©ã†ã‹ã¯å®Ÿè£…è€…ãŒæ±ºã‚ã‚Œã°è‰¯ã„ã¨æ€ã„ã¾ã™ã€‚ä»¥é™ã“ã®æ–¹æ³•ã‚’ Forward Override ã¨å‘¼ç§°ã—ã¾ã™ã€‚

```python
from TTS.api import TTS
import types
import torch
import torch_neuronx

# XTTS v2 ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
tts = TTS("tts_models/multilingual/multi-dataset/xtts_v2").to("cpu")
model = tts.synthesizer.tts_model

# 1. ã‚ªãƒªã‚¸ãƒŠãƒ«ã® forward ã‚’ä¿å­˜ï¼ˆã‚¯ãƒ©ã‚¹å®šç¾©ã‹ã‚‰å–å¾—ã—ã€å†å®Ÿè¡Œã§ã‚‚å®‰å…¨ï¼‰
model.gpt.forward_original = types.MethodType(type(model.gpt).forward, model.gpt)

# 2. ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ç”¨ forward ã«ä¸Šæ›¸ãï¼ˆkwargs ã‚’ positional args åŒ–ã—å›ºå®šï¼‰
def compile_forward(self, text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents):
    return self.forward_original(
        text_inputs=text_inputs, text_lengths=text_lengths,
        audio_codes=audio_codes, wav_lengths=wav_lengths,
        cond_latents=cond_latents, return_attentions=False, return_latent=True
    )
model.gpt.forward = types.MethodType(compile_forward, model.gpt)

# 3. Neuron ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ï¼ˆModule ã‚’ç›´æ¥æ¸¡ã™ï¼‰
neuron_gpt = torch_neuronx.trace(
    model.gpt,  # torch.nn.Module ã‚’æ¸¡ã™ï¼ˆwrapper é–¢æ•°ã§ã¯ãªã„ï¼‰
    (text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents),
    compiler_workdir='/tmp/neuron_cache_gpt',
    compiler_args='--model-type=transformer --auto-cast=all --auto-cast-type=bf16'
)

# 4. Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åˆ‡ã‚Šæ›¿ãˆ
def forward_wrapper(self, text_inputs, text_lengths, audio_codes, wav_lengths,
                    cond_latents=None, return_attentions=False, return_latent=False):
    if hasattr(self, 'forward_neuron'):
        return self.forward_neuron(text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents)
    else:
        return self.forward_original(text_inputs, text_lengths, audio_codes, wav_lengths,
            cond_latents=cond_latents, return_attentions=return_attentions, return_latent=return_latent)

model.gpt.forward = types.MethodType(forward_wrapper, model.gpt)
model.gpt.forward_neuron = neuron_gpt
```

ã“ã®æ–¹æ³•ã«ã‚ˆã£ã¦ä»Šå›ã®ã‚±ãƒ¼ã‚¹ã ã¨ **XTTS v2 ã®ã‚³ãƒ¼ãƒ‰ã‚’ä¸€åˆ‡å¤‰æ›´ã—ãªã„**ã§ AWS Neuron ã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’åˆ©ç”¨å¯èƒ½ã§ã™ã€‚ä»Šå¾Œã“ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã‚ˆã‚Šæ±ç”¨åŒ–ã•ã›ã¦ end-to-end ã§ã¯ãªã„ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã™ã‚‹ AWS Neuron ä½¿ç”¨ã®ä½“é¨“ã‚’æ”¹å–„ã—ã¦ã„ã‘ãªã„ã‹è€ƒãˆã¦ã¿ã¾ã™ã€‚


ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã¯ Python ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã‚·ã‚§ãƒ«ã§ 1 è¡Œãšã¤å®Ÿè¡Œã§ãã¾ã™ã€‚

::::details æº–å‚™

```bash
source /opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/bin/activate
```

:::message
- **coqui-tts 0.27.5**ï¼ˆidiap ãƒ•ã‚©ãƒ¼ã‚¯ç‰ˆã€Python 3.12 å¯¾å¿œï¼‰
- **torch 2.9.0**
- **torch_neuronx 2.9.0.2.11.19912**
- **transformers 4.57.6**
:::

```python
# Python REPL ã‚’èµ·å‹•
# $ python3

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import os
import torch
import types
os.environ['COQUI_TOS_AGREED'] = '1'

# XTTS v2 ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
from TTS.api import TTS
tts = TTS("tts_models/multilingual/multi-dataset/xtts_v2").to("cpu")
model = tts.synthesizer.tts_model

# ãƒ¢ãƒ‡ãƒ«ã®ç¢ºèª
print(f"GPT: {type(model.gpt)}")
print(f"HifiDecoder: {type(model.hifigan_decoder)}")
```

å®Ÿè¡Œçµæœ

```python
>>> # ãƒ¢ãƒ‡ãƒ«ã®ç¢ºèª
>>> print(f"GPT: {type(model.gpt)}")
GPT: <class 'TTS.tts.layers.xtts.gpt.GPT'>
>>> print(f"HifiDecoder: {type(model.hifigan_decoder)}")
HifiDecoder: <class 'TTS.tts.layers.xtts.hifigan_decoder.HifiDecoder'>
```
::::

::::details Forward Override + Neuron ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«

```python
# ============================================================================
# Step 1: forward_original ã‚’ä¿å­˜ï¼ˆã‚¯ãƒ©ã‚¹å®šç¾©ã® forward ã‚’å–å¾—ï¼‰
# ============================================================================
# é‡è¦: model.gpt.forward ã§ã¯ãªã type(model.gpt).forward ã‚’ä½¿ã†ã€‚
# model.gpt.forward ã¯ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å±æ€§ï¼ˆå‰å›ã® forward_wrapper ç­‰ï¼‰ã‚’è¿”ã™å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
# type(...).forward ã¯ã‚¯ãƒ©ã‚¹å®šç¾©ã®ã‚ªãƒªã‚¸ãƒŠãƒ« forward ã‚’å¸¸ã«è¿”ã™ã€‚
model.gpt.forward_original = types.MethodType(type(model.gpt).forward, model.gpt)
print(f"[OK] forward_original ä¿å­˜: {type(model.gpt.forward_original)}")

# ============================================================================
# Step 2: ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ç”¨ forward ã«ä¸Šæ›¸ãï¼ˆkwargs ã‚’å›ºå®šï¼‰
# ============================================================================
def compile_forward(self, text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents):
    """kwargs ã‚’ positional args åŒ–ã—ã€return_latent=True ã‚’å›ºå®š"""
    return self.forward_original(
        text_inputs=text_inputs, text_lengths=text_lengths,
        audio_codes=audio_codes, wav_lengths=wav_lengths,
        cond_latents=cond_latents, return_attentions=False, return_latent=True
    )

model.gpt.forward = types.MethodType(compile_forward, model.gpt)
print(f"[OK] compile_forward é©ç”¨: {type(model.gpt.forward)}")

# ============================================================================
# Step 3: ãƒ€ãƒŸãƒ¼å…¥åŠ›ã‚’ä½œæˆ
# ============================================================================
text_inputs = torch.randint(0, 256, (1, 50))
text_lengths = torch.tensor([50])
audio_codes = torch.randint(0, 1024, (1, 100))
wav_lengths = torch.tensor([100])
cond_latents = torch.randn(1, 32, 1024)

print(f"[OK] ãƒ€ãƒŸãƒ¼å…¥åŠ›ä½œæˆ")
print(f"  text_inputs:   {text_inputs.shape}")
print(f"  text_lengths:  {text_lengths.shape}")
print(f"  audio_codes:   {audio_codes.shape}")
print(f"  wav_lengths:   {wav_lengths.shape}")
print(f"  cond_latents:  {cond_latents.shape}")

# ============================================================================
# Step 4: CPU æ¨è«–ãƒ†ã‚¹ãƒˆï¼ˆcompile_forward ãŒæ­£ã—ãå‹•ä½œã™ã‚‹ã‹ç¢ºèªï¼‰
# ============================================================================
with torch.no_grad():
    result = model.gpt(text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents)
    print(f"[OK] CPU æ¨è«–æˆåŠŸ: {result.shape}")

# ============================================================================
# Step 5: Neuron ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ï¼ˆModule ã‚’ç›´æ¥æ¸¡ã™ï¼‰
# ============================================================================
import torch_neuronx

neuron_gpt = torch_neuronx.trace(
    model.gpt,  # torch.nn.Module ã‚’ç›´æ¥æ¸¡ã™
    (text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents),
    compiler_workdir='/tmp/neuron_cache_gpt',
    compiler_args='--model-type=transformer --auto-cast=all --auto-cast-type=bf16'
)
print(f"[OK] Neuron ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æˆåŠŸ: {type(neuron_gpt).__name__}")

# ============================================================================
# Step 6: Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åˆ‡ã‚Šæ›¿ãˆ + forward_neuron ç™»éŒ²
# ============================================================================
def forward_wrapper(self, text_inputs, text_lengths, audio_codes, wav_lengths,
                    cond_latents=None, return_attentions=False, return_latent=False):
    if hasattr(self, 'forward_neuron'):
        return self.forward_neuron(text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents)
    else:
        return self.forward_original(text_inputs, text_lengths, audio_codes, wav_lengths,
            cond_latents=cond_latents, return_attentions=return_attentions, return_latent=return_latent)

model.gpt.forward = types.MethodType(forward_wrapper, model.gpt)
model.gpt.forward_neuron = neuron_gpt
print(f"[OK] forward_neuron ç™»éŒ²å®Œäº†")

# ============================================================================
# Step 7: Neuron æ¨è«–ãƒ†ã‚¹ãƒˆ
# ============================================================================
with torch.no_grad():
    result2 = model.gpt(text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents=cond_latents)
    print(f"[OK] Neuron æ¨è«–æˆåŠŸ: {result2.shape}")
```

å®Ÿè¡Œçµæœ

```python
>>> model.gpt.forward_original = types.MethodType(type(model.gpt).forward, model.gpt)
>>> print(f"[OK] forward_original ä¿å­˜: {type(model.gpt.forward_original)}")
[OK] forward_original ä¿å­˜: <class 'method'>
>>> def compile_forward(self, text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents):
...     """kwargs ã‚’ positional args åŒ–ã—ã€return_latent=True ã‚’å›ºå®š"""
...     return self.forward_original(
...         text_inputs=text_inputs, text_lengths=text_lengths,
...         audio_codes=audio_codes, wav_lengths=wav_lengths,
...         cond_latents=cond_latents, return_attentions=False, return_latent=True
...     )
... 
>>> model.gpt.forward = types.MethodType(compile_forward, model.gpt)
>>> print(f"[OK] compile_forward é©ç”¨: {type(model.gpt.forward)}")
[OK] compile_forward é©ç”¨: <class 'method'>
>>> 
>>> 
>>> text_inputs = torch.randint(0, 256, (1, 50))
>>> text_lengths = torch.tensor([50])
>>> audio_codes = torch.randint(0, 1024, (1, 100))
>>> wav_lengths = torch.tensor([100])
>>> cond_latents = torch.randn(1, 32, 1024)
>>> 
>>> print(f"[OK] ãƒ€ãƒŸãƒ¼å…¥åŠ›ä½œæˆ")
[OK] ãƒ€ãƒŸãƒ¼å…¥åŠ›ä½œæˆ
>>> print(f"  text_inputs:   {text_inputs.shape}")
  text_inputs:   torch.Size([1, 50])
>>> print(f"  text_lengths:  {text_lengths.shape}")
  text_lengths:  torch.Size([1])
>>> print(f"  audio_codes:   {audio_codes.shape}")
  audio_codes:   torch.Size([1, 100])
>>> print(f"  wav_lengths:   {wav_lengths.shape}")
  wav_lengths:   torch.Size([1])
>>> print(f"  cond_latents:  {cond_latents.shape}")
  cond_latents:  torch.Size([1, 32, 1024])
>>> with torch.no_grad():
...     result = model.gpt(text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents)
...     print(f"[OK] CPU æ¨è«–æˆåŠŸ: {result.shape}")
... 
[OK] CPU æ¨è«–æˆåŠŸ: torch.Size([1, 1, 1024])
>>> 
>>> import torch_neuronx
>>> 
>>> neuron_gpt = torch_neuronx.trace(
...     model.gpt,  # torch.nn.Module ã‚’ç›´æ¥æ¸¡ã™
...     (text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents),
...     compiler_workdir='/tmp/neuron_cache_gpt',
...     compiler_args='--model-type=transformer --auto-cast=all --auto-cast-type=bf16'
... )
/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=1, shape=torch.Size([1]), dtype=torch.int64). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
  warnings.warn(
/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=3, shape=torch.Size([1]), dtype=torch.int64). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
  warnings.warn(

......Completed run_backend_driver.

Compiler status PASS
>>> print(f"[OK] Neuron ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æˆåŠŸ: {type(neuron_gpt).__name__}")
[OK] Neuron ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æˆåŠŸ: TopLevelTracedModule

[OK] forward_neuron ç™»éŒ²å®Œäº†

[OK] Neuron æ¨è«–æˆåŠŸ: torch.Size([1, 1, 1024])
```

:::message alert
[é‡è¦] `torch_neuronx.trace()` ã«ã¯ **`torch.nn.Module` ã‚’ç›´æ¥æ¸¡ã™**å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚wrapper é–¢æ•°ã‚’æ¸¡ã™ã¨ `RuntimeError: Expected XLA tensor` ãŒç™ºç”Ÿã—ã¾ã™ã€‚ç†ç”±ã¯å†…éƒ¨ã§ `isinstance(func, torch.nn.Module)` ã®ãƒã‚§ãƒƒã‚¯ã«ã‚ˆã‚Šã€Module ã®å ´åˆã®ã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒ XLA ãƒ‡ãƒã‚¤ã‚¹ã«å¤‰æ›ã•ã‚Œã‚‹ãŸã‚ã§ã™ã€‚Module ã® `forward` ã‚’ä¸€æ™‚çš„ã«ä¸Šæ›¸ãã—ã¦ã‹ã‚‰ `trace(model.gpt, ...)` ã‚’å‘¼ã³ã€ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«å¾Œã« Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åˆ‡ã‚Šæ›¿ãˆã¦ã„ã¾ã™ã€‚
:::

**forward_neuron**

```
>>> model.gpt.forward_neuron
NeuronModule(
  original_name=NeuronModule
  (states): ParameterList(original_name=ParameterList)
  (weights): ParameterDict(original_name=ParameterDict)
)
```

`torch_neuronx.trace()` ãŒ PyTorch ãƒ¢ãƒ‡ãƒ«ã‚’ NeuronCore ç”¨ã®å‘½ä»¤åˆ—ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã€ãã®çµæœãŒ callable ãªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã—ã¦è¿”ã•ã‚Œã¾ã™ã€‚ã“ã‚Œã‚’ `forward_neuron` å±æ€§ã¨ã—ã¦ç™»éŒ²ã™ã‚‹ã“ã¨ã§ã€å…ƒã® GPT ã‚¯ãƒ©ã‚¹ã®ã‚³ãƒ¼ãƒ‰ã‚’ä¸€åˆ‡å¤‰æ›´ã›ãšã« Neuron å¯¾å¿œã§ãã¾ã™ã€‚
::::

### å®Ÿè£…ã®è§£èª¬

Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ `model.gpt` ãŒã©ã®ã‚ˆã†ã«å¤‰åŒ–ã™ã‚‹ã‹ã‚’å›³ã§ç¤ºã—ã¾ã™ã€‚

```mermaid
graph TB
    subgraph "model.gpt ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹"
        A[forward]
        B[forward_original]
        C[forward_neuron]
    end

    A -->|"æŒ¯ã‚Šåˆ†ã‘ãƒ­ã‚¸ãƒƒã‚¯<br/>(forward_wrapper)"| D{forward_neuron<br/>å­˜åœ¨ï¼Ÿ}
    D -->|YES| C
    D -->|NO| B

    B -->|"å…ƒã® forward ãƒ¡ã‚½ãƒƒãƒ‰<br/>(CPU/GPU å®Ÿè¡Œ)"| E[GPT ã‚ªãƒªã‚¸ãƒŠãƒ«ã® forward]
    C -->|"Neuron ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿<br/>(NeuronCore å®Ÿè¡Œ)"| F[TorchScript ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«]

    style A fill: #e1f5ff
    style B fill: #f3e5f5
    style C fill: #fff3e0
    style D fill: #ffe0b2
    style E fill: #f3e5f5
    style F fill: #fff3e0
```

::::details åˆå­¦è€…å‘ã‘ -- Python ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®åŸºç¤çŸ¥è­˜

Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«ã€Python ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ãƒ¡ã‚½ãƒƒãƒ‰ã®ä»•çµ„ã¿ã‚’æŒ¯ã‚Šè¿”ã‚Šã¾ã™ã€‚

## model.gpt ã¯ä½•ã‹ï¼Ÿ

`model.gpt` ã¯ `torch.nn.Module` ã‚’ç¶™æ‰¿ã—ãŸ Python ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ã™ã€‚

```python
type(model.gpt)
# <class 'TTS.tts.layers.xtts.gpt.GPT'>

isinstance(model.gpt, torch.nn.Module)
# True
```

Python ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«ã¯ã€**å±æ€§ã‚’å‹•çš„ã«è¿½åŠ ã§ãã‚‹**ã¨ã„ã†ç‰¹å¾´ãŒã‚ã‚Šã¾ã™ã€‚

```python
# é€šå¸¸ã®å±æ€§è¿½åŠ 
model.gpt.my_custom_value = 42

# ãƒ¡ã‚½ãƒƒãƒ‰ã‚‚å±æ€§ã®ä¸€ç¨®
model.gpt.forward  # ã“ã‚Œã‚‚å±æ€§
```

`gpt.forward` ã¯ `gpt` ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«ãƒã‚¤ãƒ³ãƒ‰ã•ã‚ŒãŸãƒ¡ã‚½ãƒƒãƒ‰ã§ã™ã€‚

```python
>>> type(model.gpt.forward)
<class 'method'>
```
::::

### types.MethodType ã®å½¹å‰²

`types.MethodType(func, instance)` ã¯ã€**é€šå¸¸ã®é–¢æ•°ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãƒ¡ã‚½ãƒƒãƒ‰ã«å¤‰æ›**ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ä»¥ä¸‹ã®ã‚ˆã†ã« `compile_forward` ã‚’ model.gpt ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãƒ¡ã‚½ãƒƒãƒ‰ã¨ã—ã¦ç™»éŒ²ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

```python
# compile_forward ã‚’å®šç¾©
def compile_forward(self, text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents):
    return self.forward_original(...)

# model.gpt ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¨ã—ã¦ç™»éŒ²
model.gpt.forward = types.MethodType(compile_forward, model.gpt)

# å‘¼ã³å‡ºã—æ™‚ã€self=model.gpt ãŒè‡ªå‹•çš„ã«æ¸¡ã•ã‚Œã‚‹
model.gpt(text_inputs, ...)  # compile_forward(model.gpt, text_inputs, ...) ãŒå‘¼ã°ã‚Œã‚‹
```

### torch_neuronx.trace() ãŒè¿”ã™ã‚‚ã®

`torch_neuronx.trace()` ã¯ **TorchScript ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«**ï¼ˆ`torch.jit.ScriptModule`ï¼‰ã‚’è¿”ã—ã¾ã™ã€‚

```python
neuron_gpt = torch_neuronx.trace(model.gpt, example_inputs, ...)

type(neuron_gpt)
# <class 'torch.jit._trace.TopLevelTracedModule'>

# ã“ã‚Œã¯ callable
neuron_gpt(text_inputs, ...)  # NeuronCore ã§å®Ÿè¡Œã•ã‚Œã‚‹
```

TorchScript ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ PyTorch ãƒ¢ãƒ‡ãƒ«ã®æœ€é©åŒ–ã•ã‚ŒãŸå®Ÿè¡Œå¯èƒ½å½¢å¼ã§ã€NeuronCore å®Ÿè¡Œç”¨ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ã§ã‚ã‚Šã€Python ã‚¤ãƒ³ã‚¿ãƒ—ãƒªã‚¿ã‚’çµŒç”±ã—ãªã„é«˜é€Ÿå®Ÿè¡Œã€`torch.jit.load()` ã§ä¿å­˜ãƒ»å¾©å…ƒå¯èƒ½ã€ãªã©ã®ç‰¹å¾´ãŒã‚ã‚Šã¾ã™ã€‚

### Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ä»•çµ„ã¿ï¼ˆå†æ²ï¼‰

ä»¥ä¸Šã®çŸ¥è­˜ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã¨ã€Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«å‹•ä½œã—ã¾ã™ã€‚

```python
# Step 1: ã‚ªãƒªã‚¸ãƒŠãƒ«ã® forward ã‚’ä¿å­˜
model.gpt.forward_original = types.MethodType(type(model.gpt).forward, model.gpt)
# â†’ GPT ã‚¯ãƒ©ã‚¹ã® forward ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ model.gpt ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«ãƒã‚¤ãƒ³ãƒ‰

# Step 2: æŒ¯ã‚Šåˆ†ã‘ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®šç¾©
def forward_wrapper(self, text_inputs, text_lengths, audio_codes, wav_lengths,
                    cond_latents=None, return_attentions=False, return_latent=False):
    if hasattr(self, 'forward_neuron'):
        # Neuron ç‰ˆãŒå­˜åœ¨ã™ã‚Œã°å‘¼ã³å‡ºã™
        return self.forward_neuron(text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents)
    else:
        # ãªã‘ã‚Œã°ã‚ªãƒªã‚¸ãƒŠãƒ«ç‰ˆã‚’å‘¼ã³å‡ºã™
        return self.forward_original(text_inputs, text_lengths, audio_codes, wav_lengths,
            cond_latents=cond_latents, return_attentions=return_attentions, return_latent=return_latent)

# Step 3: forward ã‚’ç½®ãæ›ãˆ
model.gpt.forward = types.MethodType(forward_wrapper, model.gpt)
# â†’ model.gpt.forward ãŒ forward_wrapper ã«ãªã‚‹

# Step 4: Neuron ç‰ˆã‚’ç™»éŒ²
model.gpt.forward_neuron = neuron_gpt
# â†’ model.gpt.forward_neuron ã¨ã„ã†æ–°ã—ã„å±æ€§ã‚’è¿½åŠ 

# å®Ÿè¡Œæ™‚ã®å‹•ä½œ
model.gpt(inputs)
# â†“
# model.gpt.forward(inputs) ãŒå‘¼ã°ã‚Œã‚‹
# â†“
# forward_wrapper(model.gpt, inputs) ãŒå®Ÿè¡Œã•ã‚Œã‚‹
# â†“
# hasattr(model.gpt, 'forward_neuron') â†’ True
# â†“
# model.gpt.forward_neuron(inputs) ãŒå‘¼ã°ã‚Œã‚‹ï¼ˆNeuronCore å®Ÿè¡Œï¼‰
```

**é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ**ã¯ã€ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’ä¸€åˆ‡å¤‰æ›´ã›ãšã«å…ƒã® CPU/GPU å‘ã‘å¯¾å¿œã«åŠ ãˆã¦ Neuron å¯¾å¿œã‚’è¿½åŠ ã—ãŸã“ã¨ã§ã™ã€‚

## torch_neuronx ã«ã¤ã„ã¦

### torch_neuronx.trace()

`torch_neuronx.trace()` ã¯ PyTorch ã® `torch.jit.trace()` ã¨åŒã˜æ„Ÿè¦šã§ä½¿ãˆã¾ã™ã€‚

**åŸºæœ¬çš„ãªä½¿ã„æ–¹**ã‚’ç¤ºã—ã¾ã™ã€‚

```python
import torch
import torch_neuronx

# ãƒ¢ãƒ‡ãƒ«ã¨å…¥åŠ›ä¾‹ã‚’ç”¨æ„
model = MyModel()
example_input = torch.randn(1, 100, 1024)

# å˜ä¸€å¼•æ•°ã®å ´åˆ
neuron_model = torch_neuronx.trace(
    model,
    example_input,
    compiler_args='--model-type=transformer --auto-cast=all'
)

# è¤‡æ•°å¼•æ•°ã®å ´åˆï¼ˆã‚¿ãƒ—ãƒ«ã§æ¸¡ã™ï¼‰
input1 = torch.randint(0, 256, (1, 50))
input2 = torch.tensor([50])
neuron_model = torch_neuronx.trace(
    model,
    (input1, input2),
    compiler_args='--model-type=transformer --auto-cast=all'
)

# ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
neuron_model.save('model_neuron.pt')

# ãƒ­ãƒ¼ãƒ‰
loaded_model = torch.jit.load('model_neuron.pt')
```

:::message alert
**é‡è¦ 1:** `torch_neuronx.trace()` ã®ç¬¬ 1 å¼•æ•°ã«ã¯ **`torch.nn.Module` ã‚’æ¸¡ã™**å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

**é‡è¦ 2:** `torch_neuronx.trace` ã¯ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ã®å…¥åŠ›å½¢çŠ¶ãŒå›ºå®šã•ã‚Œã¾ã™ã€‚å¯å¤‰é•·å…¥åŠ›ã«ã¯ç¾çŠ¶å¯¾å¿œã—ã¦ã„ãªã„ãŸã‚ã€å®Ÿé‹ç”¨ã§ã¯ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç†ãŒå¿…è¦ã§ã™ã€‚æ¨è«–æ™‚ã¯å¿…ãšã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ã¨åŒã˜ãƒ‡ãƒ¼ã‚¿å‹ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
:::


ã‚‚ã—ãƒ¢ãƒ‡ãƒ«ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³å¼•æ•°ï¼ˆ`return_attentions`, `return_latent` ãªã©ï¼‰ã®å€¤ã‚’æŒ‡å®šã—ã¦ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ãŸã„å ´åˆã€Module ã® `forward` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä¸€æ™‚çš„ã«ç½®ãæ›ãˆã¾ã™ã€‚

**ä¾‹**: GPT ã® `forward` ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ `return_latent=False` ã®æ™‚ã«ã€`return_latent=True` ã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ãŸã„å ´åˆ

```python
import types

# Step 1: ã‚ªãƒªã‚¸ãƒŠãƒ«ã® forward ã‚’ä¿å­˜
model.gpt.forward_original = types.MethodType(type(model.gpt).forward, model.gpt)

# Step 2: å¼•æ•°ã‚’æ±ºã‚æ‰“ã¡ã—ãŸ forward ã«ä¸€æ™‚çš„ã«ç½®ãæ›ãˆ
def compile_forward(self, text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents):
    # return_latent=True ã‚’æŒ‡å®šã—ã¦ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
    return self.forward_original(
        text_inputs, text_lengths, audio_codes, wav_lengths,
        cond_latents=cond_latents,
        return_attentions=False,  # â† ã“ã®å€¤ã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
        return_latent=True        # â† ã“ã®å€¤ã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
    )

model.gpt.forward = types.MethodType(compile_forward, model.gpt)

# Step 3: Module ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ trace ã«æ¸¡ã™
neuron_gpt = torch_neuronx.trace(
    model.gpt,  # â† Module ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ¸¡ã™
    example_inputs
)
```

:::message
ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«å¾Œã¯ã€`return_latent=True` ã§å›ºå®šã•ã‚Œã‚‹ã®ã§æ¨è«–æ™‚ã« `False` ã‚’æŒ‡å®šã—ã¦ã‚‚ç„¡è¦–ã•ã‚Œã¾ã™ã€‚
:::

`torch_neuronx.trace()` ã® [`compiler_args`](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuron/api-compilation-python-api.html#torch_neuron.trace) ã§æœ€é©åŒ–ã‚’åˆ¶å¾¡ã§ãã¾ã™ã€‚è©³ç´°ã¯ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

## å†ç¾æ‰‹é †

ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€inf2.xlarge ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä¸Šã§ XTTS v2 ã‚’å‹•ä½œã•ã›ã‚‹æ‰‹é †ã‚’ç¤ºã—ã¾ã™ã€‚å…¨ã¦ã®æ‰‹é †ã¯ heredoc å½¢å¼ã§è¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ã‚³ãƒ”ãƒ¼&ãƒšãƒ¼ã‚¹ãƒˆã§å®Ÿè¡Œã§ãã¾ã™ã€‚

https://zenn.dev/tosshi/articles/a18dce7d66424d

ã“ã®è¨˜äº‹ã‚’è¦‹ã‚Œã° Inf2 ã‚„ Trn2 ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç«‹ã¡ä¸Šã’ã¦åˆ©ç”¨ã™ã‚‹æ¤œè¨¼ç’°å¢ƒã‚’ç°¡å˜ã«æ§‹ç¯‰ã§ãã¾ã™ã€‚

### æ¨å¥¨ç’°å¢ƒ

- ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—: inf2.xlargeã€trn1.2xlargeã€trn2.3xlarge
- Neuron SDK: 2.28+
- neuronxcc: 2.22+
- Python: 3.10-3.12
- coqui-tts: 0.27+ã€idiap ãƒ•ã‚©ãƒ¼ã‚¯ç‰ˆï¼ˆPython 3.12 å¯¾å¿œï¼‰
- torchaudio: 2.9+
- transformers: <5.0
- soundfile: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ç”¨

:::message
æœ¬ã‚¬ã‚¤ãƒ‰ã§ã¯ **idiap/coqui-tts** ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ã‚ªãƒªã‚¸ãƒŠãƒ«ã® coqui-ai/TTS ã¯ Python <3.12 ã‚’è¦æ±‚ã—ã¾ã™ãŒã€idiap ãƒ•ã‚©ãƒ¼ã‚¯ã¯ Python 3.12 ã«å®Œå…¨å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚Neuron ç’°å¢ƒã® Python ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨ã®æ•´åˆã®ãŸã‚ã®å¯¾å¿œã§ã™ãŒç’°å¢ƒæ§‹ç¯‰ã«ã¤ã„ã¦ã¯ä¸€ä¾‹ã‚’ç¤ºã—ã¾ã™ãŒè‡ªåˆ†ãŸã¡ã®ç’°å¢ƒã«åˆã‚ã›ã¦é ‘å¼µã£ã¦ãã ã•ã„ã€‚
:::

### Step 0: äº‹å‰æº–å‚™

```bash
source /opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/bin/activate

/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/bin/pip install coqui-tts soundfile

mkdir -p ~/xtts-test && cd ~/xtts-test
```

### Step 1: GPT ãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«

Step 1 ã§ Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é©ç”¨ã—ãŸ XTTS v2 ãƒ¢ãƒ‡ãƒ«ã® GPT ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ Neuron ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¾ã™ã€‚

::::details ã‚¹ãƒ†ãƒƒãƒ— 1 ã®ã‚³ãƒ¼ãƒ‰
```python
cat > 01_compile_gpt_bucket128.py << 'EOF'
"""
GPT ãƒ¢ãƒ‡ãƒ«ã‚’ torch_neuronx.trace() ã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¦ä¿å­˜ï¼ˆBUCKET_SIZE=128ï¼‰
"""
import sys
import os
os.environ['PATH'] = '/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/bin: ' + os.environ.get('PATH', '')
os.environ['COQUI_TOS_AGREED'] = '1'

import torch
import torch_neuronx
import types
from pathlib import Path

print("=" * 80)
print("Step 1: GPT ãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ï¼ˆBUCKET_SIZE=128ï¼‰")
print("=" * 80)

# XTTS v2 ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
print("\n[1/5] XTTS v2 ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰...")
from TTS.api import TTS
tts = TTS("tts_models/multilingual/multi-dataset/xtts_v2").to("cpu")
model = tts.synthesizer.tts_model

print(f"  [OK] XTTS v2 loaded")
print(f"  GPT: {type(model.gpt)}")

# Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é©ç”¨
print("\n[2/5] Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é©ç”¨...")
model.gpt.forward_original = types.MethodType(type(model.gpt).forward, model.gpt)
print(f"  [OK] forward_original ä¿å­˜å®Œäº†")

# ãƒ€ãƒŸãƒ¼å…¥åŠ›ã‚’ä½œæˆ
print("\n[3/5] ãƒ€ãƒŸãƒ¼å…¥åŠ›ã‚’ä½œæˆ...")

# XTTS v2 GPT ã®å…¥åŠ›å½¢çŠ¶
batch_size = 1
text_seq_len = 50
audio_seq_len = 128

# GPT.forward() ã®å¼•æ•°
text_inputs = torch.randint(0, 256, (batch_size, text_seq_len))  # ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³
text_lengths = torch.tensor([text_seq_len])  # ãƒ†ã‚­ã‚¹ãƒˆé•·
audio_codes = torch.randint(0, 1024, (batch_size, audio_seq_len))  # éŸ³å£°ã‚³ãƒ¼ãƒ‰
wav_lengths = torch.tensor([audio_seq_len * 1024])  # éŸ³å£°é•·ï¼ˆæ³¢å½¢ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼‰
cond_latents = torch.randn(batch_size, 32, 1024)  # conditioning latents (batch, 32, 1024)

# æ³¨æ„: cond_latents ã®å½¢çŠ¶ã¯ (batch, 32, 1024)
# ã“ã‚Œã¯ get_style_emb() ã®å‡ºåŠ› (batch, 1024, 32) ã‚’ .transpose(1, 2) ã—ãŸã‚‚ã®
# å‚è€ƒ: TTS/tts/layers/xtts/gpt.py ã® forward() å†…ã®å‡¦ç†

# é‡è¦: wav_lengths ã¯ã€Œæ³¢å½¢ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ã€ã‚’è¡¨ã™
# XTTS v2 ã§ã¯ hop_length = 1024 ãªã®ã§ã€audio_seq_len * 1024 ãŒæ­£ã—ã„å€¤
# ä¾‹: audio_seq_len=100 â†’ wav_lengths=102400 (ç´„ 4.27 ç§’ @ 24kHz)

print(f"  text_inputs: {text_inputs.shape}")
print(f"  text_lengths: {text_lengths.shape}")
print(f"  audio_codes: {audio_codes.shape}")
print(f"  wav_lengths: {wav_lengths.shape}")
print(f"  cond_latents: {cond_latents.shape}  # (batch, 32, 1024)")

# CPU ã§å‹•ä½œç¢ºèª
print("\n[4/5] CPU ã§å‹•ä½œç¢ºèª...")
model.gpt.eval()
with torch.no_grad():
    try:
        # XTTS v2 GPT ã® forward ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã—
        output = model.gpt.forward_original(
            text_inputs=text_inputs,
            text_lengths=text_lengths,
            audio_codes=audio_codes,
            wav_lengths=wav_lengths,
            cond_latents=cond_latents,
            return_attentions=False,
            return_latent=True
        )
        print(f"  [OK] CPU inference successful")
        print(f"  Output shape: {output.shape}")
    except Exception as e:
        print(f"  [ERROR] CPU inference failed: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

# Neuron ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
print("\n[5/5] Neuron ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«...")
print(f"  [NOTE] ã“ã‚Œã«ã¯ç´„ 1-2 åˆ†ã‹ã‹ã‚Šã¾ã™...")
print(f"  [NOTE] ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ãƒ­ã‚°ã¯ /tmp/neuron_cache_gpt_bucket128/ ã«ä¿å­˜ã•ã‚Œã¾ã™")

try:
    # ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ç”¨ forward ã«ä¸Šæ›¸ãï¼ˆkwargs ã‚’å›ºå®šï¼‰
    def compile_forward(self, text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents):
        return self.forward_original(
            text_inputs=text_inputs, text_lengths=text_lengths,
            audio_codes=audio_codes, wav_lengths=wav_lengths,
            cond_latents=cond_latents, return_attentions=False, return_latent=True
        )
    model.gpt.forward = types.MethodType(compile_forward, model.gpt)

    # Module ã‚’ç›´æ¥æ¸¡ã—ã¦ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
    neuron_gpt = torch_neuronx.trace(
        model.gpt,  # torch.nn.Module ã‚’ç›´æ¥æ¸¡ã™
        (text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents),
        compiler_workdir='/tmp/neuron_cache_gpt_bucket128',
        compiler_args='--model-type=transformer --auto-cast=all --auto-cast-type=bf16'
    )

    print(f"  [OK] Neuron compilation successful")

    # ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
    output_dir = Path("models/compiled_gpt")
    output_dir.mkdir(parents=True, exist_ok=True)
    neuron_gpt.save(str(output_dir / "gpt_neuron_bucket128.pt"))

    print(f"  [OK] ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜: models/compiled_gpt/gpt_neuron_bucket128.pt")

except Exception as e:
    print(f"  [ERROR] Compilation failed: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

print("\n" + "=" * 80)
print("Step 1 å®Œäº†")
print("=" * 80)
print(f"  ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«: models/compiled_gpt/gpt_neuron_bucket128.pt")
print(f"  BUCKET_SIZE: text=50, audio=128")
print(f"  æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: æ¨è«–å®Ÿè¡Œ (03_inference_with_trim.py)")
print("=" * 80)
EOF

# å®Ÿè¡Œï¼ˆNeuron ç’°å¢ƒã§ã®ã¿å‹•ä½œï¼‰
python 01_compile_gpt_bucket128.py
```
::::

::::details å‡ºåŠ›
```bash
================================================================================
Step 1: GPT ãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ï¼ˆBUCKET_SIZE=128ï¼‰
================================================================================

[1/5] XTTS v2 ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰...
  [OK] XTTS v2 loaded
  GPT: <class 'TTS.tts.layers.xtts.gpt.GPT'>

[2/5] Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é©ç”¨...
  [OK] forward_original ä¿å­˜å®Œäº†

[3/5] ãƒ€ãƒŸãƒ¼å…¥åŠ›ã‚’ä½œæˆ...
  text_inputs: torch.Size([1, 50])
  text_lengths: torch.Size([1])
  audio_codes: torch.Size([1, 128])
  wav_lengths: torch.Size([1])
  cond_latents: torch.Size([1, 32, 1024])  # (batch, 32, 1024)

[4/5] CPU ã§å‹•ä½œç¢ºèª...
  [OK] CPU inference successful
  Output shape: torch.Size([1, 128, 1024])

[5/5] Neuron ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«...
  [NOTE] ã“ã‚Œã«ã¯ç´„ 1-2 åˆ†ã‹ã‹ã‚Šã¾ã™...
  [NOTE] ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ãƒ­ã‚°ã¯ /tmp/neuron_cache_gpt_bucket128/ ã«ä¿å­˜ã•ã‚Œã¾ã™
/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=1, shape=torch.Size([1]), dtype=torch.int64). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
  warnings.warn(
/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=3, shape=torch.Size([1]), dtype=torch.int64). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
  warnings.warn(
.......Completed run_backend_driver.

Compiler status PASS
  [OK] Neuron compilation successful
  [OK] ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜: models/compiled_gpt/gpt_neuron_bucket128.pt

================================================================================
Step 1 å®Œäº†
================================================================================
  ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«: models/compiled_gpt/gpt_neuron_bucket128.pt
  BUCKET_SIZE: text=50, audio=128
  æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: æ¨è«–å®Ÿè¡Œ (03_inference_with_trim.py)
================================================================================
```
::::

### Step 2: HifiDecoder ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«

:::message alert
HifiDecoder ã® Neuron ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã¯ç¾çŠ¶ã§ã¯å¯¾å¿œã—ã¦ã„ã¾ã›ã‚“ã§ã—ãŸã€‚HifiDecoder ã§ä½¿ç”¨ã•ã‚Œã‚‹ `torch.nn.functional.interpolate(mode="linear")` ãŒ AWS Neuron ã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ãªã„ã‚ˆã†ã§ã™ã€‚ãã®ãŸã‚ã€HifiDecoder ã¯ CPU ã§å®Ÿè¡Œã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚HifiDecoder ã¯è»½é‡ãªãƒ¢ãƒ‡ãƒ«ï¼ˆç´„ 12M ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰ã®ãŸã‚ã€CPU ã§ã‚‚ååˆ†é«˜é€Ÿã«å‹•ä½œã—ã¾ã™ã€‚
:::

ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã€Step 3 ã«é€²ã¿ã¾ã—ã‚‡ã†ã€‚

### Step 3: æ¨è«–å®Ÿè¡Œ

Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚ˆã‚Šã€ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ GPT ãƒ¢ãƒ‡ãƒ«ãŒè‡ªå‹•çš„ã«ä½¿ç”¨ã•ã‚Œã€HifiDecoder ã¯ CPU å®Ÿè¡Œã—ã¾ã™ã€‚

:::message
ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã«ã¯ã„ãã¤ã‹ã®é‡è¦ãªç‚¹ãŒã‚ã‚Šã¾ã™ã€‚ç¬¬ä¸€ã«ã€wav_lengths ã¯ã€Œæ³¢å½¢ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ã€ã‚’è¡¨ã™ãŸã‚ã€`audio_seq_len * 1024` ã®å€¤ã‚’æ­£ã—ãè¨­å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ç¬¬äºŒã«ã€Trim Strategy ã¨ã—ã¦å…¥åŠ›ã•ã‚Œã‚‹å®Ÿãƒ‡ãƒ¼ã‚¿é•·ã‚’è¨˜éŒ²ã—ã€Neuron å‡ºåŠ›ã‚’å®Ÿãƒ‡ãƒ¼ã‚¿æ¯”ç‡ã§ãƒˆãƒªãƒŸãƒ³ã‚°ã—ã¦ã„ã¾ã™ã€‚ç¬¬ä¸‰ã«ã€forward_wrapper_with_trim ã§ã® padding å‡¦ç†ã«ã‚ˆã‚Šã€Neuron ã®å›ºå®šå½¢çŠ¶åˆ¶ç´„ã«å¯¾å¿œã™ã‚‹ãŸã‚å…¥åŠ›ã‚’å›ºå®šé•· (text_len=50, audio_len=128) ã« padding/truncate ã—ã€å‡ºåŠ›ã‚’ãƒˆãƒªãƒŸãƒ³ã‚°ã—ã¦ã„ã¾ã™ã€‚è©³ç´°ã¯ä»¥ä¸‹ã®ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚
:::

::::details ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°è©³ç´°
ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã§é­é‡ã—ãŸå•é¡Œã¨è§£æ±ºç­–ã¯ä»¥ä¸‹ã§ã™ã€‚

## å•é¡Œ 1: FFmpeg/torchaudio ã®ã‚¨ãƒ©ãƒ¼
```
RuntimeError: Could not load libtorchcodec
```

ã“ã®å•é¡Œã¯ load_audio é–¢æ•°ã‚’ soundfile ãƒ™ãƒ¼ã‚¹ã«ç½®ãæ›ãˆã‚‹ã“ã¨ã§è§£æ±ºã—ã¾ã—ãŸã€‚ç§ã®éŸ³å£°ç³»ç†è§£ãŒæµ…ã„ã®ã§é–“é•ã£ã¦ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚torchcodec ã¯ torchaudio ã®æ–°ã—ã„éŸ³å£°èª­ã¿è¾¼ã¿ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã§ã™ã€‚ã©ã†ã‚„ã‚‰ torchcodec ãŒå¿…è¦ã¨ã™ã‚‹ C++ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãŒã€ã‚·ã‚¹ãƒ†ãƒ ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„ã¨ã„ã†ã“ã¨ã®ã‚ˆã†ã§ã™ã€‚PyTorch 2.9 ã¨ã® C++ ãƒ¬ãƒ™ãƒ«ã®äº’æ›æ€§ã®å•é¡Œã®ã‚ˆã†ã§ã™ãŒã‚ã¾ã‚Šã“ã“ã®ä¾å­˜é–¢ä¿‚å¯¾å¿œãŒä»Šå›ã®è¨˜äº‹ã®ä¸»çœ¼ã§ã¯ãªã„ã®ã§ soundfile ã¨ã„ã† Python ã‹ã‚‰ç°¡å˜ã«ä½¿ãˆã‚‹éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«ç½®æ›ã—ã¦ã€Python ãƒ¬ãƒ™ãƒ«ã§ã®ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ‡ãƒ¼ã‚¿å¤‰æ›ï¼ˆNumPy â†’ PyTorchï¼‰ã§åŒã˜çµæœã‚’å¾—ã‚‰ã‚Œã‚‹ã‚ˆã†ã«ã—ã¾ã—ãŸã€‚å¯¾å‡¦æ–¹æ³•ã¨ã—ã¦ã®æœ€é©è§£ã§ã¯ãªã„å¯èƒ½æ€§ãŒé«˜ã„ã§ã™ãŒä¸€æ—¦ã“ã‚Œã§å¯¾å¿œã—ã¾ã™ã€‚

## å•é¡Œ 2: GPT Neuron ãƒ¢ãƒ‡ãƒ«ã®å›ºå®šå½¢çŠ¶åˆ¶ç´„
```
RuntimeError: Incorrect tensor shape at input tensor #0: received 1 38, expected 1 50
```

ã“ã®å•é¡Œã¯ forward_wrapper ã§å…¥åŠ›ã‚’å›ºå®šé•·ã« padding/truncate ã™ã‚‹ã“ã¨ã§è§£æ±ºã—ã¾ã—ãŸã€‚Neuron ã¯å›ºå®šå½¢çŠ¶ã®ãƒã‚±ãƒƒãƒˆã‚’æœŸå¾…ã—ã¦ã„ã‚‹ã®ã§å¯å¤‰é•·ã¯ã‚¨ãƒ©ãƒ¼ã—ã¾ã™ã€‚NxD Inference ãªã©ã§ã¯ Bucketing ã‚’ä½¿ç”¨ã—ã¦è¤‡æ•°ã®å›ºå®šå½¢çŠ¶ï¼ˆä¾‹: 32, 64, 128, 256ï¼‰ã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã€å®Ÿè¡Œæ™‚ã«æœ€ã‚‚è¿‘ã„ãƒã‚±ãƒƒãƒˆã‚’è‡ªå‹•é¸æŠã•ã›ã‚‹ã“ã¨ã§ã€padding ã®ç„¡é§„ã‚’æœ€å°åŒ–ã—ã¤ã¤è¤‡æ•°ã®å…¥åŠ›é•·ã«æŸ”è»Ÿã«å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚ãã®ãŸã‚ä»Šå¾Œ NxD Inference å´ã® Bucketing ã®æ©æµã‚’å—ã‘ã‚‰ã‚Œã‚‹å®Ÿè£…ã«ä¿®æ­£ã™ã‚‹ã“ã¨ã‚’èª¿æŸ»ã—ã¦ã¿ã¾ã™ã€‚

## å•é¡Œ 3: wav_lengths ã®å€¤ãŒå‡ºåŠ› shape ã«ä¸ãˆã‚‹å½±éŸ¿

åˆæœŸå®Ÿè£…ã§ã¯ã€éŸ³å£°ç”ŸæˆãŒå®Œå…¨ã«å¤±æ•—ã—ã¦ã„ã¾ã—ãŸã€‚

```bash
[5/6] TTS å®Ÿè¡Œ...
  [OK] TTS execution successful
  Duration: 0.46 seconds  # ç•°å¸¸ã«çŸ­ã„
  Size: 21.6 KB
```

ç”Ÿæˆã•ã‚ŒãŸéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€ã»ã¼ç„¡éŸ³ï¼ˆRMS: 0.005ã€ã‚¼ãƒ­ã‚µãƒ³ãƒ—ãƒ«å‰²åˆ: 90.7%ï¼‰ã§ã€STT èªè­˜ç‡ã‚‚ 0% ã§ã—ãŸã€‚

**æ ¹æœ¬åŸå› ã®ç‰¹å®š**

ãƒ‡ãƒãƒƒã‚°ã®çµæœã€ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ã® `wav_lengths` ã®å€¤ãŒé–“é•ã£ã¦ã„ã‚‹ã“ã¨ãŒåˆ¤æ˜ã—ã¾ã—ãŸã€‚

```python
# [NG] åˆæœŸå®Ÿè£…ï¼ˆStep 1 ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ï¼‰
wav_lengths = torch.tensor([audio_seq_len])  # tensor([100])

# CPU ã§ã®æ¤œè¨¼
output = model.gpt.forward(text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents)
print(output.shape)  # torch.Size([1, 1, 1024]) â† 1 ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã¿ï¼
```

**wav_lengths ã¯ã€Œæ³¢å½¢ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ã€ã‚’è¡¨ã™**ãŸã‚ã€`audio_seq_len * hop_length` ã®å€¤ãŒå¿…è¦ã§ã™ã€‚XTTS v2 ã§ã¯ hop_length = 1024 ãªã®ã§ã€100 ãƒ•ãƒ¬ãƒ¼ãƒ ã®éŸ³å£°ã‚³ãƒ¼ãƒ‰ã«å¯¾ã—ã¦ `100 * 1024 = 102400` ã‚µãƒ³ãƒ—ãƒ«ãŒæ­£ã—ã„å€¤ã§ã™ã€‚

```python
# [OK] ä¿®æ­£ç‰ˆ
wav_lengths = torch.tensor([audio_seq_len * 1024])  # tensor([102400])

# CPU ã§ã®æ¤œè¨¼
output = model.gpt.forward(text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents)
print(output.shape)  # torch.Size([1, 100, 1024]) â† æ­£ã—ã„ shape
```

**CPU ã§ã®æ¤œè¨¼å®Ÿé¨“**

wav_lengths ã®å€¤ãŒå‡ºåŠ› shape ã«ç›´æ¥å½±éŸ¿ã™ã‚‹ã“ã¨ã‚’ CPU ç’°å¢ƒã§ç¢ºèªã—ã¾ã—ãŸã€‚

| wav_lengths ã®å€¤ | å‡ºåŠ› shape | è©•ä¾¡ |
|-----------------|------------|------|
| 100 | `[1, 1, 1024]` | [NG] 1 ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã¿ |
| 102400 (100 * 1024) | `[1, 100, 1024]` | [OK] æ­£ã—ã„ shape |

ã“ã®å•é¡Œã«ã‚ˆã‚Šã€HifiDecoder ã«ã¯ 1 ãƒ•ãƒ¬ãƒ¼ãƒ åˆ†ã® latent ã—ã‹æ¸¡ã•ã‚Œãšã€0.46 ç§’ã®éå¸¸ã«çŸ­ã„ï¼ˆå®Ÿè³ªç„¡éŸ³ã®ï¼‰éŸ³å£°ã—ã‹ç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚

## å•é¡Œ 4: å®Ÿãƒ‡ãƒ¼ã‚¿é•·ã¨ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿å›ºå®šé•·ã®ãƒŸã‚¹ãƒãƒƒãƒ

wav_lengths ã‚’ä¿®æ­£ã—ãŸå¾Œã‚‚ã€åˆ¥ã®èª²é¡ŒãŒæ®‹ã‚Šã¾ã—ãŸã€‚å®Ÿéš›ã®æ¨è«–æ™‚ã«ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã‚„éŸ³å£°ã®é•·ã•ã¯å¯å¤‰ã§ã™ã€‚

- ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚: `text=50, audio=128`
- å®Ÿéš›ã®æ¨è«–: `text=48, audio=110`ï¼ˆä¾‹ï¼‰

å˜ç´”ã« padding ã™ã‚‹ã ã‘ã§ã¯å•é¡ŒãŒç™ºç”Ÿã—ã¾ã™ã€‚ç¬¬ä¸€ã«ã€truncate ã«ã‚ˆã‚‹ãƒ‡ãƒ¼ã‚¿å–ªå¤±ã§ã™ã€‚audio_codes=129 ã‚’ 128 ã«åˆ‡ã‚Šè©°ã‚ã‚‹ã¨ã€1 ãƒˆãƒ¼ã‚¯ãƒ³åˆ†ã®ãƒ‡ãƒ¼ã‚¿ãŒå¤±ã‚ã‚Œã¾ã™ã€‚ç¬¬äºŒã«ã€padding ã«ã‚ˆã‚‹è¨ˆç®—ã®æ­ªã¿ã§ã™ã€‚GPT å†…éƒ¨ã® attention mask ã‚„ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ãŒå®Ÿãƒ‡ãƒ¼ã‚¿é•·ã¨åˆã‚ãªããªã‚Šã¾ã™ã€‚

**è§£æ±ºç­–: Bucket Size ã®æ‹¡å¤§ + Trim æˆ¦ç•¥**

ã“ã®å•é¡Œã«å¯¾å‡¦ã™ã‚‹ãŸã‚ã€ä»¥ä¸‹ã® 2 ã¤ã®æˆ¦ç•¥ã‚’çµ„ã¿åˆã‚ã›ã¾ã—ãŸã€‚

ç¬¬ä¸€ã«ã€Bucket Size ã‚’æ‹¡å¤§ã—ã¦ audio_seq_len ã‚’ 100 ã‹ã‚‰ 128 ã«å¤‰æ›´ã—ã€truncate ã®é »åº¦ã‚’æ¸›ã‚‰ã—ã¾ã—ãŸã€‚ç¬¬äºŒã«ã€Trim æˆ¦ç•¥ã¨ã—ã¦å®Ÿãƒ‡ãƒ¼ã‚¿é•·ã‚’è¨˜éŒ²ã—ã€Neuron å‡ºåŠ›ã‚’å®Ÿãƒ‡ãƒ¼ã‚¿æ¯”ç‡ã§ãƒˆãƒªãƒŸãƒ³ã‚°ã™ã‚‹ã‚ˆã†ã«ã—ã¾ã—ãŸã€‚æ ¹æœ¬çš„ãªè§£æ±ºç­–ã§ã¯ãªã„ã®ã§ä»Šåº¦ä¸Šè¿°ã—ãŸã‚ˆã†ã« Bucketing ç­‰ã‚’è©¦ãã†ã¨æ€ã£ã¦ã„ã¾ã™ã€‚

```python
# [1] å®Ÿéš›ã®é•·ã•ã‚’è¨˜éŒ²ï¼ˆpadding å‰ï¼‰
actual_audio_len = audio_codes.size(1)  # ä¾‹: 110

# [2] Bucket ã‚µã‚¤ã‚ºã« padding
COMPILED_AUDIO_LEN = 128
audio_codes_padded = pad_to_size(audio_codes, COMPILED_AUDIO_LEN)

# [3] Neuron ãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œï¼ˆå›ºå®šé•·ã§å®Ÿè¡Œï¼‰
wav_lengths_fixed = torch.tensor([COMPILED_AUDIO_LEN * 1024])  # é‡è¦: 1024 ã‚’æ›ã‘ã‚‹
output = self.forward_neuron(...)  # [1, 128, 1024]

# [4] å‡ºåŠ›ã‚’ãƒˆãƒªãƒŸãƒ³ã‚°ï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿æ¯”ç‡ã«åŸºã¥ãï¼‰
trim_ratio = actual_audio_len / COMPILED_AUDIO_LEN  # 110/128 = 0.859
expected_output_len = int(output.size(1) * trim_ratio)  # 128 * 0.859 = 109
output_trimmed = output[: , : expected_output_len, : ]  # [1, 109, 1024]
```

ã“ã®æˆ¦ç•¥ã«ã‚ˆã‚Šã€padding ã®å½±éŸ¿ã‚’æœ€å°åŒ–ã—ã¤ã¤ã€Neuron ã®å›ºå®šå½¢çŠ¶åˆ¶ç´„ã«å¯¾å¿œã§ãã¾ã™ã€‚

::::

::::details ã‚¹ãƒ†ãƒƒãƒ— 3 ã®ã‚³ãƒ¼ãƒ‰
```python
cat > 03_inference_with_trim.py << 'EOF'
"""
Step 3: XTTS v2 ã§éŸ³å£°åˆæˆã‚’å®Ÿè¡Œï¼ˆNeuron + trim æˆ¦ç•¥ï¼‰
"""
import sys
import os
os.environ['COQUI_TOS_AGREED'] = '1'
# torchaudio ãŒ soundfile ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«è¨­å®šï¼ˆtorchcodec äº’æ›æ€§å•é¡Œã‚’å›é¿ï¼‰
os.environ['TORCHAUDIO_USE_BACKEND_DISPATCHER'] = '0'

import argparse
import torch
import torch_neuronx  # Neuron ã‚¯ãƒ©ã‚¹ã®ç™»éŒ²ã®ãŸã‚å¿…è¦
import types
import soundfile as sf
from pathlib import Path
import torchaudio
import numpy as np

# load_audio é–¢æ•°ã‚’äº‹å‰ã« patchï¼ˆTTS.api ã® import å‰ã«å®Ÿè¡Œï¼‰
import TTS.tts.models.xtts

def load_audio_fixed(audiopath, sampling_rate):
    """soundfile ã‚’ä½¿ç”¨ã—ã¦éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆtorchcodec äº’æ›æ€§å•é¡Œã‚’å›é¿ï¼‰"""
    audio_np, sr = sf.read(audiopath)
    # ãƒ¢ãƒãƒ©ãƒ«ã«å¤‰æ›
    if len(audio_np.shape) > 1:
        audio_np = audio_np.mean(axis=1)
    # torch.Tensor ã«å¤‰æ› (1, samples)
    audio = torch.from_numpy(audio_np).float().unsqueeze(0)
    # ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
    if sr != sampling_rate:
        audio = torchaudio.functional.resample(audio, sr, sampling_rate)
    # clip audio invalid values
    audio.clip_(-1, 1)
    return audio

# XTTS ã® load_audio é–¢æ•°ã‚’ç½®ãæ›ãˆ
TTS.tts.models.xtts.load_audio = load_audio_fixed

# ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°
parser = argparse.ArgumentParser(description='XTTS v2 æ¨è«–å®Ÿè¡Œ')
parser.add_argument('--hifidecoder-device', choices=['cpu', 'neuron'], default='cpu',
                    help='HifiDecoder ã®å®Ÿè¡Œãƒ‡ãƒã‚¤ã‚¹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: cpuã€æ¨å¥¨ï¼‰')
args = parser.parse_args()

print("=" * 80)
print(f"Step 3: XTTS v2 æ¨è«–å®Ÿè¡Œ")
print(f"  GPT: Neuron")
print(f"  HifiDecoder: {args.hifidecoder_device.upper()}")
print("=" * 80)

# XTTS v2 ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
print("\n[1/6] XTTS v2 ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰...")
from TTS.api import TTS
tts = TTS("tts_models/multilingual/multi-dataset/xtts_v2").to("cpu")
model = tts.synthesizer.tts_model

print(f"  [OK] XTTS v2 loaded")

# GPT Neuron ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
print("\n[2/6] GPT Neuron ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆbucket=128ï¼‰...")

gpt_neuron_path = Path("models/compiled_gpt/gpt_neuron_bucket128.pt")
if gpt_neuron_path.exists():
    neuron_gpt = torch.jit.load(str(gpt_neuron_path))

    # Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é©ç”¨
    model.gpt.forward_original = types.MethodType(type(model.gpt).forward, model.gpt)

    # ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ã®å›ºå®šã‚µã‚¤ã‚ºï¼ˆBucket Strategyï¼‰
    COMPILED_TEXT_LEN = 50
    COMPILED_AUDIO_LEN = 128  # 100 â†’ 128 ã«æ‹¡å¤§ï¼ˆtruncate é »åº¦ã‚’å‰Šæ¸›ï¼‰

    def forward_wrapper_with_trim(self, text_inputs, text_lengths, audio_codes, wav_lengths,
                                   cond_latents=None, return_attentions=False, return_latent=False):
        if hasattr(self, 'forward_neuron'):
            # [1] å®Ÿéš›ã®é•·ã•ã‚’è¨˜éŒ²ï¼ˆpadding å‰ï¼‰
            actual_text_len = text_inputs.size(1)
            actual_audio_len = audio_codes.size(1)

            # [2] text_inputs ã‚’å›ºå®šé•·ã« padding/truncateï¼ˆãƒ‡ãƒ¼ã‚¿å‹ã‚‚ torch.int64 ã«å¤‰æ›ï¼‰
            text_inputs = text_inputs.to(torch.int64)
            if actual_text_len < COMPILED_TEXT_LEN:
                pad_size = COMPILED_TEXT_LEN - actual_text_len
                text_inputs_padded = torch.nn.functional.pad(text_inputs, (0, pad_size), value=0)
            elif actual_text_len > COMPILED_TEXT_LEN:
                text_inputs_padded = text_inputs[: , : COMPILED_TEXT_LEN]
                actual_text_len = COMPILED_TEXT_LEN  # truncate ã—ãŸå ´åˆã¯å®Ÿé•·ã‚’æ›´æ–°
            else:
                text_inputs_padded = text_inputs

            # [3] audio_codes ã‚’å›ºå®šé•·ã« padding/truncateï¼ˆãƒ‡ãƒ¼ã‚¿å‹ã‚‚ torch.int64 ã«å¤‰æ›ï¼‰
            audio_codes = audio_codes.to(torch.int64)
            if actual_audio_len < COMPILED_AUDIO_LEN:
                pad_size = COMPILED_AUDIO_LEN - actual_audio_len
                audio_codes_padded = torch.nn.functional.pad(audio_codes, (0, pad_size), value=0)
            elif actual_audio_len > COMPILED_AUDIO_LEN:
                audio_codes_padded = audio_codes[: , : COMPILED_AUDIO_LEN]
                actual_audio_len = COMPILED_AUDIO_LEN  # truncate ã—ãŸå ´åˆã¯å®Ÿé•·ã‚’æ›´æ–°
            else:
                audio_codes_padded = audio_codes

            # [4] lengths ã‚’å›ºå®šå€¤ã«è¨­å®šï¼ˆé‡è¦: wav_lengths ã¯ ã‚µãƒ³ãƒ—ãƒ«æ•° = audio_len * 1024ï¼‰
            text_lengths_fixed = torch.tensor([COMPILED_TEXT_LEN], dtype=text_lengths.dtype, device=text_lengths.device)
            wav_lengths_fixed = torch.tensor([COMPILED_AUDIO_LEN * 1024], dtype=wav_lengths.dtype, device=wav_lengths.device)

            # [5] Neuron ãƒ¢ãƒ‡ãƒ«å®Ÿè¡Œ
            output = self.forward_neuron(text_inputs_padded, text_lengths_fixed, audio_codes_padded, wav_lengths_fixed, cond_latents)

            # [6] å‡ºåŠ›ã‚’ãƒˆãƒªãƒŸãƒ³ã‚°ï¼ˆTrim Strategyï¼‰
            if actual_audio_len < COMPILED_AUDIO_LEN:
                trim_ratio = actual_audio_len / COMPILED_AUDIO_LEN
                expected_output_len = max(1, int(output.size(1) * trim_ratio))
                output_trimmed = output[: , : expected_output_len, : ]
            else:
                # truncate ã•ã‚ŒãŸå ´åˆã¯ãã®ã¾ã¾ï¼ˆå®Ÿé•· = bucket ã‚µã‚¤ã‚ºï¼‰
                output_trimmed = output

            return output_trimmed
        else:
            return self.forward_original(text_inputs, text_lengths, audio_codes, wav_lengths,
                cond_latents=cond_latents, return_attentions=return_attentions, return_latent=return_latent)

    model.gpt.forward = types.MethodType(forward_wrapper_with_trim, model.gpt)
    model.gpt.forward_neuron = neuron_gpt
    print(f"  [OK] GPT Neuron ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼ˆbucket=128, trim æˆ¦ç•¥ï¼‰")
else:
    print(f"  [ERROR] GPT Neuron ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {gpt_neuron_path}")
    sys.exit(1)

# HifiDecoder Neuron ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆå¼•æ•°ã«å¿œã˜ã¦ï¼‰
print(f"\n[3/6] HifiDecoder {args.hifidecoder_device.upper()} ãƒ¢ãƒ¼ãƒ‰...")

if args.hifidecoder_device == 'neuron':
    hifidecoder_neuron_path = Path("models/compiled_hifidecoder/hifidecoder_neuron.pt")
    if hifidecoder_neuron_path.exists():
        neuron_hifidecoder = torch.jit.load(str(hifidecoder_neuron_path))

        # Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é©ç”¨
        model.hifigan_decoder.forward_original = model.hifigan_decoder.forward

        def hifidecoder_forward_wrapper(self, latents, g=None):
            if hasattr(self, 'forward_neuron'):
                return self.forward_neuron(latents, g)
            else:
                return self.forward_original(latents, g=None)

        model.hifigan_decoder.forward = types.MethodType(hifidecoder_forward_wrapper, model.hifigan_decoder)
        model.hifigan_decoder.forward_neuron = neuron_hifidecoder
        print(f"  [OK] HifiDecoder Neuron ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†")
    else:
        print(f"  [ERROR] HifiDecoder Neuron ãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {hifidecoder_neuron_path}")
        sys.exit(1)
else:
    print(f"  [OK] HifiDecoder ã¯ CPU ã§å®Ÿè¡Œã•ã‚Œã¾ã™ï¼ˆæ¨å¥¨ï¼‰")

print(f"  GPT has forward_neuron: {hasattr(model.gpt, 'forward_neuron')}")
print(f"  HifiDecoder has forward_neuron: {hasattr(model.hifigan_decoder, 'forward_neuron')}")

# å‚ç…§éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æº–å‚™
print("\n[4/6] å‚ç…§éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æº–å‚™...")

sample_rate = 24000
duration = 3  # 3 seconds
reference_wav = torch.randn(1, sample_rate * duration)

outputs_dir = Path("outputs")
outputs_dir.mkdir(exist_ok=True)

reference_path = outputs_dir / "reference_speaker.wav"
sf.write(str(reference_path), reference_wav.squeeze().numpy(), sample_rate)

print(f"  [OK] Reference audio created: {reference_path}")

# TTS å®Ÿè¡Œ
print("\n[5/6] TTS å®Ÿè¡Œ...")

text = "Hello, this is a test of XTTS v2 running on AWS Neuron."
language = "en"

print(f"  Text: {text}")
print(f"  Language: {language}")
print(f"  Reference: {reference_path}")

try:
    wav = tts.tts(
        text=text,
        speaker_wav=str(reference_path),
        language=language
    )

    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
    output_filename = f"xtts_neuron_gpt_{args.hifidecoder_device}.wav"
    output_path = outputs_dir / output_filename
    sf.write(str(output_path), wav, sample_rate)

    print(f"\n  [OK] TTS execution successful")
    print(f"  Output: {output_path}")
    print(f"  Duration: {len(wav) / sample_rate: .2f} seconds")
    print(f"  Size: {output_path.stat().st_size / 1024: .1f} KB")

except Exception as e:
    print(f"\n  [ERROR] TTS execution failed: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# å‹•ä½œç¢ºèª
print("\n[6/6] å‹•ä½œç¢ºèª...")
print(f"  GPT forward_neuron ãŒå‘¼ã°ã‚ŒãŸ: {hasattr(model.gpt, 'forward_neuron')}")
if args.hifidecoder_device == 'neuron':
    print(f"  HifiDecoder forward_neuron ãŒå‘¼ã°ã‚ŒãŸ: {hasattr(model.hifigan_decoder, 'forward_neuron')}")
else:
    print(f"  HifiDecoder ã¯ CPU ã§å®Ÿè¡Œã•ã‚ŒãŸ")

print("\n" + "=" * 80)
print("Step 3 å®Œäº†ï¼ˆNeuron + trim æˆ¦ç•¥ï¼‰")
print("=" * 80)
print(f"  éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«: {output_path.absolute()}")
print(f"  GPT: Neuron (bucket=128, trim æˆ¦ç•¥)")
print(f"  HifiDecoder: {args.hifidecoder_device.upper()}")
print("=" * 80)
EOF

python 03_inference_with_trim.py
```
::::

::::details å‡ºåŠ›
```bash
================================================================================
Step 3: XTTS v2 æ¨è«–å®Ÿè¡Œ
  GPT: Neuron
  HifiDecoder: CPU
================================================================================

[1/6] XTTS v2 ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰...
  [OK] XTTS v2 loaded

[2/6] GPT Neuron ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆbucket=128ï¼‰...
  [OK] GPT Neuron ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†ï¼ˆbucket=128, trim æˆ¦ç•¥ï¼‰

[3/6] HifiDecoder CPU ãƒ¢ãƒ¼ãƒ‰...
  [OK] HifiDecoder ã¯ CPU ã§å®Ÿè¡Œã•ã‚Œã¾ã™ï¼ˆæ¨å¥¨ï¼‰
  GPT has forward_neuron: True
  HifiDecoder has forward_neuron: False

[4/6] å‚ç…§éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æº–å‚™...
  [OK] Reference audio created: outputs/reference_speaker.wav

[5/6] TTS å®Ÿè¡Œ...
  Text: Hello, this is a test of XTTS v2 running on AWS Neuron.
  Language: en
  Reference: outputs/reference_speaker.wav

  [OK] TTS execution successful
  Output: outputs/xtts_neuron_gpt_cpu.wav
  Duration:  6.36 seconds
  Size:  298.1 KB

[6/6] å‹•ä½œç¢ºèª...
  GPT forward_neuron ãŒå‘¼ã°ã‚ŒãŸ: True
  HifiDecoder ã¯ CPU ã§å®Ÿè¡Œã•ã‚ŒãŸ

================================================================================
Step 3 å®Œäº†ï¼ˆNeuron + trim æˆ¦ç•¥ï¼‰
================================================================================
  éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«: /home/coder/xtts-test/outputs/xtts_neuron_gpt_cpu.wav
  GPT: Neuron (bucket=128, trim æˆ¦ç•¥)
  HifiDecoder: CPU
================================================================================
```

**å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰åˆ¥ã®æ€§èƒ½æ¯”è¼ƒ**

wav_lengths ä¿®æ­£ã¨ trim æˆ¦ç•¥ã®åŠ¹æœã‚’ç¢ºèªã™ã‚‹ãŸã‚ã€2 ã¤ã®å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã§æ¯”è¼ƒã—ã¾ã—ãŸã€‚

| é …ç›® | Neuron<br/>(wav_lengths=128*1024, trim) | CPU ã®ã¿ | è©•ä¾¡ |
|------|--------------------------------|---------|------|
| **Duration** |  **5.52 ç§’** | 4.17 ç§’ | [OK] é©åˆ‡ãªé•·ã• |
| **RMS** | **0.139** | 0.132 | [OK] ã»ã¼åŒç­‰ |
| **Peak** |  **0.712** | 0.786 | [OK] ã»ã¼åŒç­‰ |
| **ã‚¼ãƒ­ã‚µãƒ³ãƒ—ãƒ«** | **8.7%** | 10.5% | [OK] æ”¹å–„ |
| **STT èªè­˜ç‡** | **80.0%** | 81.8% | [OK] ã»ã¼åŒç­‰ |
| **Neuron å‡ºåŠ›** | **[1,128,1024]** | N/A | [OK] æ­£ã—ã„ shape |
| **GPT å®Ÿè¡Œç’°å¢ƒ** | Neuron | CPU | - |
| **HifiDecoder å®Ÿè¡Œç’°å¢ƒ** | CPU | CPU | - |

wav_lengths ã®ä¿®æ­£ã¨ trim æˆ¦ç•¥ã«ã‚ˆã‚Šã€Neuron å®Ÿè£…ãŒ CPU ã®ã¿ã®å®Ÿè£…ã¨ã»ã¼åŒç­‰ã®å“è³ªã‚’é”æˆã—ã¾ã—ãŸã€‚Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯æ­£ã—ãæ©Ÿèƒ½ã—ã€padding ã®å½±éŸ¿ã‚‚æœ€å°åŒ–ã§ãã¦ã„ã¾ã™ã€‚ã—ã‹ã— CPU ã‚ˆã‚Šé…ã„ã®ã§å®Ÿè³ªçš„ã«ã¯ Neuron ã‚’åˆ©ç”¨ã™ã‚‹æ„ç¾©ã¯è–„ã„ã§ã™ãŒä»Šå¾Œã“ã“ã®æ€§èƒ½æ”¹å–„ã«å–ã‚Šçµ„ã‚“ã§ã¿ã¾ã™ï¼
::::

### Step 4: éŸ³å£°å“è³ªã®è‡ªå‹•æ¤œè¨¼ï¼ˆSTTï¼‰

ç”Ÿæˆã•ã‚ŒãŸéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã„ã¦èã‹ãªãã¦ã‚‚ã€Speech-to-Textï¼ˆSTTï¼‰ã§è‡ªå‹•çš„ã«å“è³ªã‚’ç¢ºèªã§ãã¾ã™ã€‚

::::details ã‚¹ãƒ†ãƒƒãƒ— 4 ã®ã‚³ãƒ¼ãƒ‰

```bash
cat > 04_verify_audio.py <<'EOF'
"""
ç”ŸæˆéŸ³å£°ã®å“è³ªã‚’ STT ã§è‡ªå‹•æ¤œè¨¼
"""
import torch
import soundfile as sf
import numpy as np
from transformers import AutoProcessor, WhisperForConditionalGeneration

# ç”Ÿæˆã•ã‚ŒãŸéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
audio_path = "outputs/xtts_neuron_gpt_cpu.wav"
audio_np, sample_rate = sf.read(audio_path)

# ãƒ¢ãƒãƒ©ãƒ«ã«å¤‰æ›ï¼ˆå¿…è¦ãªå ´åˆï¼‰
if len(audio_np.shape) > 1:
    audio_np = audio_np.mean(axis=1)

waveform = torch.from_numpy(audio_np).float().unsqueeze(0)

# éŸ³å£°ä¿¡å·ã®åŸºæœ¬çµ±è¨ˆ
rms = torch.sqrt(torch.mean(waveform ** 2)).item()
peak = torch.max(torch.abs(waveform)).item()
zero_ratio = (torch.abs(waveform) < 0.01).float().mean().item() * 100

print(f"éŸ³å£°å“è³ªæŒ‡æ¨™: ")
print(f"  RMS: {rms: .6f}")
print(f"  Peak: {peak: .6f}")
print(f"  ã‚¼ãƒ­ã‚µãƒ³ãƒ—ãƒ«å‰²åˆ: {zero_ratio: .1f}%")
print(f"  åˆ¤å®š: {'[OK] æœ‰æ„ãªéŸ³å£°ä¿¡å·ã‚ã‚Š' if rms > 0.05 and zero_ratio < 50 else '[NG] éŸ³å£°ä¿¡å·ãŒä¸ååˆ†'}")

# Whisper ã§éŸ³å£°èªè­˜
print(f"\nSTT æ¤œè¨¼: ")
processor = AutoProcessor.from_pretrained("openai/whisper-tiny")
model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-tiny")
model.eval()

# 24kHz â†’ 16kHz ã«ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆWhisper ã®å…¥åŠ›å½¢å¼ï¼‰
if sample_rate != 16000:
    import torchaudio
    waveform = torchaudio.functional.resample(waveform, sample_rate, 16000)

# éŸ³å£°èªè­˜å®Ÿè¡Œ
input_features = processor(waveform.squeeze().numpy(), sampling_rate=16000, return_tensors="pt").input_features
with torch.no_grad():
    predicted_ids = model.generate(
        input_features,
        language="en",
        task="transcribe"
    )
transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]

# å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã¨æ¯”è¼ƒ
input_text = "Hello, this is a test of XTTS v2 running on AWS Neuron."
print(f"  å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ: \"{input_text}\"")
print(f"  èªè­˜ãƒ†ã‚­ã‚¹ãƒˆ: \"{transcription}\"")

# å˜èªå˜ä½ã§ã®ä¸€è‡´ç‡ã‚’è¨ˆç®—
input_words = set(input_text.lower().replace(".", "").replace(",", "").split())
recognized_words = set(transcription.lower().replace(".", "").replace(",", "").split())
word_accuracy = len(input_words & recognized_words) / len(input_words) * 100

print(f"  å˜èªå†ç¾ç‡: {word_accuracy: .1f}%")
print(f"  åˆ¤å®š: {'[OK] éŸ³å£°ã«æœ‰æ„ãªéŸ³å£°å†…å®¹ãŒå«ã¾ã‚Œã¦ã„ã‚‹' if word_accuracy >= 60 else '[NG] éŸ³å£°å†…å®¹ãŒä¸æ˜ç­'}")
EOF

python 04_verify_audio.py
```
::::

::::details å‡ºåŠ›

```bash
"""
ç”ŸæˆéŸ³å£°ã®å“è³ªã‚’ STT ã§è‡ªå‹•æ¤œè¨¼
"""
import torch
import soundfile as sf
import numpy as np
from transformers import AutoProcessor, WhisperForConditionalGeneration

# ç”Ÿæˆã•ã‚ŒãŸéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
audio_path = "outputs/xtts_neuron_gpt_cpu.wav"
audio_np, sample_rate = sf.read(audio_path)

# ãƒ¢ãƒãƒ©ãƒ«ã«å¤‰æ›ï¼ˆå¿…è¦ãªå ´åˆï¼‰
if len(audio_np.shape) > 1:
    audio_np = audio_np.mean(axis=1)

waveform = torch.from_numpy(audio_np).float().unsqueeze(0)

# éŸ³å£°ä¿¡å·ã®åŸºæœ¬çµ±è¨ˆ
rms = torch.sqrt(torch.mean(waveform ** 2)).item()
peak = torch.max(torch.abs(waveform)).item()
zero_ratio = (torch.abs(waveform) < 0.01).float().mean().item() * 100

print(f"éŸ³å£°å“è³ªæŒ‡æ¨™: ")
print(f"  RMS: {rms: .6f}")
print(f"  Peak: {peak: .6f}")
print(f"  ã‚¼ãƒ­ã‚µãƒ³ãƒ—ãƒ«å‰²åˆ: {zero_ratio: .1f}%")
print(f"  åˆ¤å®š: {'[OK] æœ‰æ„ãªéŸ³å£°ä¿¡å·ã‚ã‚Š' if rms > 0.05 and zero_ratio < 50 else '[NG] éŸ³å£°ä¿¡å·ãŒä¸ååˆ†'}")

# Whisper ã§éŸ³å£°èªè­˜
print(f"\nSTT æ¤œè¨¼: ")
processor = AutoProcessor.from_pretrained("openai/whisper-tiny")
model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-tiny")
model.eval()

# 24kHz â†’ 16kHz ã«ãƒªã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆWhisper ã®å…¥åŠ›å½¢å¼ï¼‰
if sample_rate != 16000:
    import torchaudio
    waveform = torchaudio.functional.resample(waveform, sample_rate, 16000)

# éŸ³å£°èªè­˜å®Ÿè¡Œ
EOFnt(f"  åˆ¤å®š: {'[OK] éŸ³å£°ã«æœ‰æ„ãªéŸ³å£°å†…å®¹ãŒå«ã¾ã‚Œã¦ã„ã‚‹' if word_accuracy >= 60 else '[NG] éŸ³å£°å†…å®¹ãŒä¸æ˜ç­'}")
(aws_neuronx_venv_pytorch_2_9_nxd_inference) coder:~/xtts-test$ python verify_audio.py
éŸ³å£°å“è³ªæŒ‡æ¨™: 
  RMS:  0.128229
  Peak:  0.697968
  ã‚¼ãƒ­ã‚µãƒ³ãƒ—ãƒ«å‰²åˆ:  27.7%
  åˆ¤å®š: [OK] æœ‰æ„ãªéŸ³å£°ä¿¡å·ã‚ã‚Š

STT æ¤œè¨¼: 
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
  å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ: "Hello, this is a test of XTTS v2 running on AWS Neuron."
  èªè­˜ãƒ†ã‚­ã‚¹ãƒˆ: " Hello, this is a test of XTT, running on AWS."
  å˜èªå†ç¾ç‡:  75.0%
  åˆ¤å®š: [OK] éŸ³å£°ã«æœ‰æ„ãªéŸ³å£°å†…å®¹ãŒå«ã¾ã‚Œã¦ã„ã‚‹
```

éŸ³å£°å“è³ªã¯æ¯”è¼ƒçš„è‰¯å¥½ã§ã™ãŒå¤šå°‘åŠ£åŒ–ã—ã¦ã„ã¾ã™ã­ã€‚

::::

## ã¾ã¨ã‚

ä»Šå›ã¯ XTTS v2 ã¨ã„ã† end-to-end ã§ã¯ãªã„ãƒ¢ãƒ‡ãƒ«ã‚’ AWS Neuron ã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã€æ¨è«–ã™ã‚‹ä»•çµ„ã¿ã‚’æ¤œè¨¼ã—ã¾ã—ãŸã€‚

### Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æœ‰åŠ¹æ€§

Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯å¿…é ˆã®æŠ€è¡“ã§ã¯ã‚ã‚Šã¾ã›ã‚“ãŒã€ä¸»è¦ã§ã¯ãªã„ãƒ¢ãƒ‡ãƒ«ã‚’é‹ç”¨ã‚³ã‚¹ãƒˆã‚’ä¸‹ã’ã€CPU/GPU ã¨ã®ç’°å¢ƒäº’æ›æ€§ã‚’ç¶­æŒã—ãªãŒã‚‰æ¨è«–ã‚µãƒ¼ãƒ“ãƒ³ã‚°ã™ã‚‹éš›ã«ã¯æœ‰ç”¨ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ä»Šå›ã®æ¤œè¨¼ã§ã¯ã€OSS ã®ã‚³ãƒ¼ãƒ‰ã‚’å¤‰æ›´ã›ãšã«ä¸€éƒ¨ãƒ¢ãƒ‡ãƒ«ã® Neuron å¯¾å¿œã‚’å®Ÿç¾ã§ãã¾ã—ãŸã€‚

### ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆ¥ã®å®Ÿè¡Œç’°å¢ƒ

GPT ã¯å•é¡Œãªã Neuron ã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ãƒ»æ¨è«–ãŒã§ãã¾ã—ãŸã€‚Forward Override + Trim æˆ¦ç•¥ã§ CPU ã¨ã»ã¼åŒç­‰ã®å“è³ªã‚’é”æˆã—ã¦ã„ã¾ã™ã€‚ä¸€æ–¹ã€HifiDecoder ã¯ Neuron ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æœªã‚µãƒãƒ¼ãƒˆã§ã™ãŒã€è»½é‡ãƒ¢ãƒ‡ãƒ«ã®ãŸã‚ CPU ã§å…¨ãå•é¡Œãªãå‹•ä½œã—ã¾ã™ã€‚

:::message alert
CPU ã«å‘ãæ¼”ç®—ã‚’ã‚ãˆã¦ã‚«ã‚¹ã‚¿ãƒ ãƒãƒƒãƒ—ã§å®Ÿæ–½ã™ã‚‹å¿…è¦æ€§ã¯è–„ã„ãŸã‚ã€é©æé©æ‰€ã§è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã‚’ä½¿ã†ã¨ã„ã†åˆ¤æ–­ãŒé‡è¦ã§ã™ã€‚å¼·åŒ–å­¦ç¿’ã®æ¨è«–ã¯ MIMD ã®ã‚ˆã†ãªã®ã§ CPU æ¨è«–ã®ãƒ‹ãƒ¼ã‚ºã‚‚å¢—ãˆã¦ã„ãã“ã¨ã‚’è€ƒãˆã‚‹ã¨ã€ä»Šå¾Œã¯ã‚ˆã‚Šè¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã®ç’°å¢ƒäº’æ›æ€§ã‚„é©æé©æ‰€ã§ã®åˆ©ç”¨ã€ãƒ˜ãƒ†ãƒ­ã‚¸ãƒ‹ã‚¢ã‚¹ç’°å¢ƒã§ã®åˆ©ç”¨ã¨ã„ã†ã®ãŒé‡è¦ã«ãªã£ã¦ãã‚‹ã¨æ€ã£ã¦ã„ã¾ã™ã€‚GPU ã«ã‚‚å½“ã¦ã¯ã¾ã‚Šã¾ã™ãŒãªã‚“ã§ã‚‚æ€§èƒ½ãŒä¸ŠãŒã‚‹ã¨éä¿¡ã—ã¦ä½¿ã†ã®ã§ã¯ãªãåœ§å€’çš„ã«åŠ¹æœã®å‡ºã‚‹ã¨ã“ã‚ã‹ã‚‰ä½¿ã£ã¦ã„ãã®ãŒè‰¯ã„ã§ã™ã­ï¼
:::

### generate() ã¨ forward() ã®å‡¦ç†è² è·

XTTS v2 ã® GPT ãƒ¢ãƒ‡ãƒ«ã«ã¯ 2 ã¤ã®ä¸»è¦ãªãƒ¡ã‚½ãƒƒãƒ‰ãŒã‚ã‚Šã¾ã™ã€‚

ç¬¬ä¸€ã«ã€**GPT.generate()** ã¯è‡ªå·±å›å¸°çš„ã« audio_codes ã‚’ç”Ÿæˆã™ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰ã§ã€è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„å‡¦ç†ã§ã™ã€‚1 ãƒˆãƒ¼ã‚¯ãƒ³ãšã¤é †æ¬¡ç”Ÿæˆã™ã‚‹ autoregressive å‡¦ç†ã§ã‚ã‚Šã€æ¨è«–å…¨ä½“ã®è¨ˆç®—æ™‚é–“ã®å¤§éƒ¨åˆ†ã‚’å ã‚ã¾ã™ã€‚ç¾åœ¨ã®å®Ÿè£…ã§ã¯ CPU ã§å®Ÿè¡Œã—ã¦ãŠã‚Šã€Neuron åŒ–ã—ã¦ã„ã¾ã›ã‚“ã€‚Whisper ã¯åŒæ§˜ã«å¯å¤‰é•· autoregressive å‡¦ç†ãŒã‚ã‚Šã¾ã™ãŒ NxD Inference å¯¾å¿œã§ãã¦ã„ã‚‹ãŸã‚ã€ä»Šå¾Œå®Ÿè£…ã‚’èª¿æŸ»ã—ãŸä¸Šã§å¯¾å¿œã—ã¦ã¿ã‚‹äºˆå®šã§ã™ã€‚

ç¬¬äºŒã«ã€**GPT.forward()** ã¯ç”Ÿæˆã•ã‚ŒãŸ audio_codes ã‚’ latents ã«å¤‰æ›ã™ã‚‹ãƒ¡ã‚½ãƒƒãƒ‰ã§ã€è¨ˆç®—ã‚³ã‚¹ãƒˆã¯ç›¸å¯¾çš„ã«ä½ã„å‡¦ç†ã§ã€å›ºå®šé•·ã®å…¥åŠ›ã‚’ä¸€åº¦ã«å‡¦ç†ã—ã¾ã™ã€‚ä»Šå›ã®å®Ÿè£…ã§ Forward Override + Trim æˆ¦ç•¥ã«ã‚ˆã‚Š Neuron åŒ–ã«æˆåŠŸã—ã¾ã—ãŸã€‚

ä»Šå›ã®å®Ÿè£…ã§ã¯ `forward()` ã®ã¿ã‚’ Neuron åŒ–ã—ã¾ã—ãŸãŒã€æœ¬æ¥ã§ã‚ã‚Œã° `generate()` ã‚’ Neuron åŒ–ã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šå¤§ããªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸ŠãŒæœŸå¾…ã§ãã¾ã™ã€‚ãŸã ã—ã€`generate()` ã¯å¯å¤‰é•·ã® autoregressive å‡¦ç†ã®ãŸã‚ã€Neuron ã®å›ºå®šå½¢çŠ¶åˆ¶ç´„ã¨ã®ç›¸æ€§ãŒæ‚ªãã€å®Ÿè£…é›£æ˜“åº¦ãŒé«˜ã„ã§ã™ã€‚

### ä»Šå¾Œã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„æ¡ˆ

#### 1. Multiple Bucket Strategy

ç¾åœ¨ã¯å˜ä¸€ã®å›ºå®šã‚µã‚¤ã‚ºï¼ˆtext=50, audio=128ï¼‰ã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¦ã„ã¾ã™ãŒã€è¤‡æ•°ã®ãƒã‚±ãƒƒãƒˆã‚µã‚¤ã‚ºã‚’ç”¨æ„ã™ã‚‹ã“ã¨ã§ã€padding ã®ç„¡é§„ã‚’å‰Šæ¸›ã§ãã¾ã™ã€‚

```python
# è¤‡æ•°ã®ãƒã‚±ãƒƒãƒˆã‚µã‚¤ã‚ºã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
BUCKETS = [
    (32, 64),    # çŸ­ã„ãƒ†ã‚­ã‚¹ãƒˆãƒ»éŸ³å£°ç”¨
    (50, 128),   # ä¸­ç¨‹åº¦ã®é•·ã•ç”¨ï¼ˆç¾åœ¨å®Ÿè£…ï¼‰
    (80, 256),   # é•·ã„ãƒ†ã‚­ã‚¹ãƒˆãƒ»éŸ³å£°ç”¨
    (120, 512),  # éå¸¸ã«é•·ã„ãƒ†ã‚­ã‚¹ãƒˆç”¨
]

# æ¨è«–æ™‚ã«æœ€é©ãªãƒã‚±ãƒƒãƒˆã‚’è‡ªå‹•é¸æŠ
def select_bucket(text_len, audio_len):
    for text_bucket, audio_bucket in BUCKETS:
        if text_len <= text_bucket and audio_len <= audio_bucket:
            return text_bucket, audio_bucket
    # å…¨ã¦ã®ãƒã‚±ãƒƒãƒˆã‚’è¶…ãˆã‚‹å ´åˆã¯æœ€å¤§ãƒã‚±ãƒƒãƒˆã§ truncate
    return BUCKETS[-1]
```

ã“ã®æˆ¦ç•¥ã«ã‚ˆã‚Šã€padding ã®ç„¡é§„ãŒæ¸›å°‘ã—ã¦è¨ˆç®—åŠ¹ç‡ãŒå‘ä¸Šã—ã€truncate ã®é »åº¦ãŒæ¸›å°‘ã—ã¦å“è³ªãŒå‘ä¸Šã—ã¾ã™ã€‚ã¾ãŸã€æ§˜ã€…ãªé•·ã•ã®å…¥åŠ›ã«æŸ”è»Ÿã«å¯¾å¿œã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚å½“ç„¶ãƒ¡ãƒ¢ãƒªã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãªã©ã®å½±éŸ¿ã‚’å—ã‘ã‚‹ãŸã‚å®Ÿãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã§ã®æœ€é©ãªè¨­å®šã®è¦‹æ¥µã‚ãŒã©ã®ã¿ã¡é‡è¦ã§ã™ã€‚NxD Inference ã§ã¯ Bucketing æˆ¦ç•¥ãŒçµ„ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã™ã€‚

#### 2. generate() ã® Neuron åŒ–

`generate()` ã‚’ Neuron åŒ–ã™ã‚‹ã“ã¨ã§ã€æ¨è«–å…¨ä½“ã®è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’å¤§å¹…ã«å‰Šæ¸›ã§ãã¾ã™ã€‚ãŸã ã—ã€autoregressive ç”Ÿæˆã¯å›ºå®šå½¢çŠ¶åˆ¶ç´„ã¨ã®ç›¸æ€§ãŒæ‚ªã„ãŸã‚ã€ã„ãã¤ã‹ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒè€ƒãˆã‚‰ã‚Œã¾ã™ã€‚ç¬¬ä¸€ã«ã€æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã‚’å›ºå®šã—ã¦ç”Ÿæˆã— Early stopping ã¯ CPU å´ã§åˆ¤å®šã™ã‚‹å›ºå®šé•·ç”Ÿæˆã§ã™ã€‚ç¬¬äºŒã«ã€è¤‡æ•°ã®æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆä¾‹: 64, 128, 256ï¼‰ã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã™ã‚‹ Bucket ãƒ™ãƒ¼ã‚¹ã®ç”Ÿæˆã§ã™ã€‚ç¬¬ä¸‰ã«ã€NxD Inference ã®å‹•çš„ãƒãƒƒãƒãƒ³ã‚°ã¨ Bucketing ã®ã‚µãƒãƒ¼ãƒˆã‚’æ´»ç”¨ã™ã‚‹æ–¹æ³•ã§ã™ã€‚

#### 3. NxD Inference ã«ã‚ˆã‚‹æœ¬æ ¼å¯¾å¿œ

ç¾åœ¨ã®å®Ÿè£…ã¯ `torch_neuronx.trace()` ã‚’ä½¿ã£ãŸãƒ—ãƒªãƒŸãƒ†ã‚£ãƒ–ãªæ–¹æ³•ã§ã™ãŒã€NxD Inference ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€è¤‡æ•°ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’è‡ªå‹•çš„ã«ãƒãƒƒãƒåŒ–ã™ã‚‹ Dynamic Batchingã€å…¥åŠ›é•·ã«å¿œã˜ã¦æœ€é©ãªãƒã‚±ãƒƒãƒˆã‚’è‡ªå‹•é¸æŠã™ã‚‹ Automatic Bucketingã€Transformer ã® KV-Cache ã‚’åŠ¹ç‡çš„ã«ç®¡ç†ã™ã‚‹æ©Ÿèƒ½ã€ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ»ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãƒ»ã‚³ã‚¹ãƒˆåŠ¹ç‡ãŒæœ€é©åŒ–ã•ã‚ŒãŸ Production-ready ãªå®Ÿè£…ã¨ã„ã£ãŸæ©æµã‚’å—ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

NxD Inference ã¸ã®çµ±åˆã¯ä»Šå¾Œã®èª²é¡Œã¨ã—ã¦æ¤œè¨¼ã—ã¦ã„ãã¾ã™ã€‚

:::message
**Let's try AWS Neuron!!!!!!**
:::

## å‚è€ƒè³‡æ–™

- [Coqui TTS GitHub Repository](https://github.com/coqui-ai/TTS)
- [AWS Neuron Documentation](https://awsdocs-neuron.readthedocs-hosted.com/)
- [torch_neuronx API Reference](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/index.html)
- [OpenAI Whisper ãƒ¢ãƒ‡ãƒ«ã‚’ AWS Neuron ã§å‹•ã‹ã™](../neuron-adapter/phase2-nxd-whisper/blog/nxd-inference-whisper-guide.md)
- [AWS Neuron GitHub Issues](https://github.com/aws-neuron/aws-neuron-sdk/issues)
- [Coqui TTS Discussions](https://github.com/coqui-ai/TTS/discussions)
