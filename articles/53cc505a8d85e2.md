---
title: "XTTS v2 ã‚’ AWS Neuron ã§å‹•ã‹ã™"
emoji: "ğŸ§™â€â™‚ï¸"
type: "tech"
topics: ["AWSNeuron", "tts", "xtts", "inferentia", "éŸ³å£°åˆæˆ"]
published: false
---

**å¯¾è±¡èª­è€…**: AWS Trainium/Inferentia2 ãƒãƒƒãƒ—ã§éŸ³å£°åˆæˆ (TTS) ã‚’å®Ÿè£…ã—ãŸã„ä¸­ç´šè€…
**å‰æçŸ¥è­˜**: Python åŸºç¤ã€PyTorch ã®åŸºæœ¬çš„ãªä½¿ã„æ–¹

## ã¯ã˜ã‚ã«

https://huggingface.co/coqui/XTTS-v2

### èƒŒæ™¯ã¨ç›®çš„

**XTTS v2** (eXtended Text-to-Speech v2, ç´„ 396M ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿) ã¯ã€Coqui ãŒé–‹ç™ºã—ãŸã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã®éŸ³å£°åˆæˆãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚æœ€è¿‘ã®å¤§è¦æ¨¡ TTS ãƒ¢ãƒ‡ãƒ«ï¼ˆ1B+ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰ã¨æ¯”è¼ƒã™ã‚‹ã¨ã€**ã©ã¡ã‚‰ã‹ã¨ã„ã†ã¨è»½é‡ãªãƒ¢ãƒ‡ãƒ«ã®éƒ¨é¡**ã«å…¥ã‚Šã¾ã™ã€‚

https://zenn.dev/tosshi/articles/f6c49165c90e6d

æœ¬è¨˜äº‹ã§ã¯ä¸Šè¨˜ã§ç´¹ä»‹ã—ãŸ Amazon EC2 Inf2 / Trn2 ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§ NxD Inference ã‚’ç”¨ã„ã¦ Whisper ã¨ XTTS v2 ã‚’ä¸¡æ–¹æ··åœ¨ã§å‹•ã‹ã™ãŸã‚ã€AWS Neuron ã§ XTTS v2 ã‚’å‹•ã‹ã™å®Ÿé¨“ã‚’è©¦ã¿ã¾ã™ã€‚ãªãœæ··åœ¨ã•ã›ãŸã„ã‹ã¨ã„ã†ã¨ã€ã‚«ã‚¹ã‚¿ãƒ ãƒãƒƒãƒ—ã§è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ã‚’å‹•ã‹ã™ã“ã¨ã®å¯èƒ½æ€§ã‚’æ¢ã‚ŠãŸã„ã‹ã‚‰ã§ã™ã€‚

XTTS v2 ã¯å˜ç´”ãª end-to-end ãƒ¢ãƒ‡ãƒ«ã§ã¯ãªãã€**è¤‡æ•°ã®ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ï¼ˆGPT + HifiDecoderï¼‰ãŒæ··åœ¨**ã—ã¦ã„ã¾ã™ã€‚ã“ã®ã‚ˆã†ãªè¤‡é›‘ãªã‚±ãƒ¼ã‚¹ã§ AWS Neuron ã‚’ã©ã®ã‚ˆã†ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¦å‹•ã‹ã›ã°ã‚ˆã„ã®ã‹ã‚’æ¤œè¨¼ã—ã¦ã¿ã¾ã™ã€‚

### æŠ€è¡“ãƒ†ãƒ¼ãƒ

ã“ã®è¨˜äº‹ã§ã¯ã€**è¤‡æ•°ã®ãƒ¢ãƒ‡ãƒ«ãŒæ··åœ¨ã™ã‚‹ TTS ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**ã‚’ AWS Neuron ã§å‹•ã‹ã™ãŸã‚ã«å¿…è¦ã ã£ãŸæŠ€è¡“çš„å·¥å¤«ã‚’è§£èª¬ã—ã¾ã™ã€‚

1. **ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æˆ¦ç•¥**: ã©ã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ Neuron åŒ–ã™ã¹ãã‹
2. **Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³**: ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’æ—¢å­˜ã® PyTorch ã‚³ãƒ¼ãƒ‰ã«çµ±åˆã™ã‚‹æ–¹æ³•
3. **å›ºå®šé•·å…¥åŠ›ã¸ã®å¯¾å¿œ**: `torch_neuronx.trace()` ã®åˆ¶ç´„ã¨å®Ÿè£…ä¸Šã®å·¥å¤«
4. **ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æœ€é©åŒ–**: bf16 auto-casting ã¨ Transformer æœ€é©åŒ–ã®åŠ¹æœ
5. **æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®æ§‹ç¯‰**: ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’çµ„ã¿åˆã‚ã›ã¦ TTS ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ç´¹ä»‹ã™ã‚‹å®Ÿè£…ã¯ã€**NxD Inference ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã‚ãš**ã€`torch_neuronx.trace()` ã‚’ä½¿ã£ãŸãƒ—ãƒªãƒŸãƒ†ã‚£ãƒ–ãªæ–¹æ³•ã§ã™ã€‚PyTorch ã® `torch.jit.trace()` ã¨åŒã˜æ„Ÿè¦šã§ä½¿ãˆã‚‹ãŸã‚ã€æ—¢å­˜ã® PyTorch ã‚³ãƒ¼ãƒ‰ã«æœ€å°é™ã®å¤‰æ›´ã§çµ±åˆã§ãã¾ã™ã€‚NxD Inference ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¸ã®çµ±åˆã¯ç¾åœ¨æ¤œè¨¼ä¸­ã§ã‚ã‚Šä»Šå¾Œç´¹ä»‹ã—ãŸã„ã¨æ€ã„ã¾ã™ã€‚

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦

XTTS v2 ã¯ã€**2 ã¤ã®ä¸»è¦ãƒ¢ãƒ‡ãƒ«ã‚’çµ„ã¿åˆã‚ã›ãŸ TTS ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**ã§ã™ã€‚

::::details å‚è€ƒ
- [Coqui TTS - XTTS v2](https://github.com/coqui-ai/TTS/tree/main/TTS/tts/models)
- [Xtts ã‚¯ãƒ©ã‚¹](https://github.com/coqui-ai/TTS/blob/eef419b37393b11cc741662d041d8d793e011f2d/TTS/tts/models/xtts.py#L191)
- [GPT ã‚¯ãƒ©ã‚¹](https://github.com/coqui-ai/TTS/blob/eef419b37393b11cc741662d041d8d793e011f2d/TTS/tts/layers/xtts/gpt.py#L88)
- [HifiDecoder ã‚¯ãƒ©ã‚¹](https://github.com/coqui-ai/TTS/blob/eef419b37393b11cc741662d041d8d793e011f2d/TTS/tts/layers/xtts/hifigan_decoder.py#L615)
::::

### æ¨è«–æ™‚ã®ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å…¨ä½“å›³

ä»¥ä¸‹ã®å›³ã¯ã€XTTS v2 ã®æ¨è«–æ™‚ã«ãŠã‘ã‚‹å®Ÿéš›ã®å‡¦ç†ãƒ•ãƒ­ãƒ¼ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚

```mermaid
sequenceDiagram
    participant User as ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›
    participant Tokenizer as Tokenizer
    participant GPT as GPT Model<br/>(395.8M params)
    participant HifiDecoder as HifiDecoder<br/>(11.8M params)
    participant Output as éŸ³å£°å‡ºåŠ›

    User->>Tokenizer: ãƒ†ã‚­ã‚¹ãƒˆ
    Tokenizer->>GPT: text_tokens

    Note over GPT: GPT.generate()
    GPT->>GPT: audio_codes ç”Ÿæˆ

    Note over GPT: GPT.forward()
    GPT->>GPT: latents æŠ½å‡º<br/>(1024-dim)

    GPT->>HifiDecoder: latents
    Note over HifiDecoder: hifigan_decoder()
    HifiDecoder->>Output: waveform (24kHz)
```

latents ã¯ GPT ãŒç”Ÿæˆã™ã‚‹éŸ³å£°ã®æŠ½è±¡çš„ãªç‰¹å¾´ã‚’è¡¨ç¾ã—ãŸä¸­é–“ãƒ‡ãƒ¼ã‚¿ã§ã™ã€‚ãã‚Œã‚’ HifiDecoder ã§éŸ³å£°æ³¢å½¢ã«ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¾ã™ã€‚end-to-end ã§ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ä¸€æ°—ã«éŸ³å£°æ³¢å½¢ã‚’ç”Ÿæˆã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚‚ã‚ã‚‹ã‚ˆã†ã§ã™ãŒä»Šå›ã¯äºŒã¤ã®ç‹¬ç«‹ã—ãŸãƒ¢ãƒ‡ãƒ«ãŒé€£æºï¼ˆãƒ¢ã‚¸ãƒ¥ãƒ©ãƒ¼ã¨å‘¼ç§°ï¼‰ã—ã¦ã„ã¾ã™ã€‚

:::message alert
ä»Šå›ã®ç›®çš„ã¯ç‹¬ç«‹ã—ãŸäºŒã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãã‚Œãã‚Œ AWS Neuron ã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã€XTTS v2 ã®å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ã‚³ãƒ¼ãƒ‰ã«æ‰‹ã‚’åŠ ãˆã‚‹ã“ã¨ãªã Inf2 ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§æ¨è«–å‡¦ç†ã‚’å®Ÿç¾ã™ã‚‹ã“ã¨ã§ã™ã€‚
:::

### æ¨è«–å‡¦ç†ã®ã‚³ãƒ¼ãƒ«ãƒ•ãƒ­ãƒ¼

ä»¥ä¸‹ã« XTTS v2 ã®æ¨è«–å‡¦ç†ãŒã©ã“ã‹ã‚‰å§‹ã¾ã‚Šã€ã©ã®ã‚ˆã†ãªçµŒè·¯ã§å‡¦ç†ãŒé€²ã‚€ã®ã‹ã‚’ GitHub URL ã§ç¤ºã—ã¾ã™ã€‚

#### 1. ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ: full_inference

https://github.com/coqui-ai/TTS/blob/eef419b37393b11cc741662d041d8d793e011f2d/TTS/tts/models/xtts.py#L421-L500

ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒå‘¼ã³å‡ºã™é«˜ãƒ¬ãƒ™ãƒ« API ã§ã™ã€‚ãƒ†ã‚­ã‚¹ãƒˆã¨å‚ç…§éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’å—ã‘å–ã‚Šã¾ã™ã€‚å‚ç…§éŸ³å£°ã‹ã‚‰ latents ã‚’æŠ½å‡ºã™ã‚‹ `get_conditioning_latents()` ã‚’å‘¼ã³å‡ºã—ã€`self.inference()` ã‚’å‘¼ã³å‡ºã—ï¼ˆå®Ÿéš›ã®æ¨è«–å‡¦ç†ï¼‰ã—ã¾ã™ã€‚

#### 2. å‚ç…§éŸ³å£°ã®å‰å‡¦ç†: get_conditioning_latents

https://github.com/coqui-ai/TTS/blob/eef419b37393b11cc741662d041d8d793e011f2d/TTS/tts/models/xtts.py#L326-L380

å‚ç…§éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ GPT ç”¨ã®æ¡ä»¶ä»˜ã‘ latents ã‚’æŠ½å‡ºã—ã¾ã™ã€‚`load_audio()` ã§éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€`get_gpt_cond_latents()` ã‚’å‘¼ã³å‡ºã—ã¾ã™ã€‚

#### 3. å®Ÿéš›ã®æ¨è«–å‡¦ç†: inference

https://github.com/coqui-ai/TTS/blob/eef419b37393b11cc741662d041d8d793e011f2d/TTS/tts/models/xtts.py#L503-L583

å‰å‡¦ç†æ¸ˆã¿ã®æ¡ä»¶ä»˜ã‘ latents ã‚’å—ã‘å–ã‚Šã€GPT ã¨ HifiDecoder ã®ä¸¡æ–¹ã‚’é †æ¬¡å‘¼ã³å‡ºã—ã¾ã™ã€‚

##### 3-1. GPT.generate() ã§ audio_codes ç”Ÿæˆ

https://github.com/coqui-ai/TTS/blob/eef419b37393b11cc741662d041d8d793e011f2d/TTS/tts/models/xtts.py#L541

è‡ªå·±å›å¸°çš„ã«é›¢æ•£çš„ãª audio_codes ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

##### 3-2. GPT.forward() ã§ latents æŠ½å‡º

https://github.com/coqui-ai/TTS/blob/eef419b37393b11cc741662d041d8d793e011f2d/TTS/tts/models/xtts.py#L365

ç”Ÿæˆã•ã‚ŒãŸ audio_codes ã‚’é€£ç¶šçš„ãª latentsï¼ˆ1024-dimï¼‰ã«å¤‰æ›ã—ã¾ã™ã€‚

##### 3-3. HifiDecoder ã§æ³¢å½¢ç”Ÿæˆ

https://github.com/coqui-ai/TTS/blob/eef419b37393b11cc741662d041d8d793e011f2d/TTS/tts/models/xtts.py#L576-L583

latents ã‚’éŸ³å£°æ³¢å½¢ï¼ˆ24kHzï¼‰ã«ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

ã“ã®ã‚ˆã†ã«ã€æ¨è«–å‡¦ç†ã¯ `full_inference` ã‹ã‚‰å§‹ã¾ã‚Šã€`get_conditioning_latents` â†’ `inference` â†’ `GPT.generate()` â†’ `GPT.forward()` â†’ `HifiDecoder` ã®é †ã«å‡¦ç†ãŒé€²ã¿ã¾ã™ã€‚

## GPT ãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ†ã‚­ã‚¹ãƒˆ â†’ latentsï¼‰

https://github.com/coqui-ai/TTS/blob/eef419b37393b11cc741662d041d8d793e011f2d/TTS/tts/layers/xtts/gpt.py#L88-L111

:::message
GPT ãƒ¢ãƒ‡ãƒ«ã¯ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³ã‹ã‚‰ latents ã‚’ç”Ÿæˆã™ã‚‹å½¹å‰²ã‚’æ‹…ã„ã¾ã™ã€‚ä¸»è¦ãªãƒ¡ã‚½ãƒƒãƒ‰ã¨ã—ã¦ã€`generate()` ãŒéŸ³å£°ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆã—ã€`forward()` ãŒæ½œåœ¨å¤‰æ•°ã‚’è¨ˆç®—ã—ã¾ã™ã€‚å…¥åŠ›ã¨ã—ã¦ `text_tokens`ï¼ˆãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³åˆ— `(batch, text_len)`ï¼‰ã¨ `cond_latents`ï¼ˆéŸ³å£°æ¡ä»¶ä»˜ã‘æ½œåœ¨å¤‰æ•°ã€speaker embedding ãªã©ï¼‰ã‚’å—ã‘å–ã‚Šã¾ã™ã€‚å‡ºåŠ›ã¯ `generate()` ãƒ¡ã‚½ãƒƒãƒ‰ã§ã¯ `audio_codes`ï¼ˆç”Ÿæˆã•ã‚ŒãŸéŸ³å£°ã‚³ãƒ¼ãƒ‰ `(batch, audio_len)`ï¼‰ã€`forward()` ãƒ¡ã‚½ãƒƒãƒ‰ã§ã¯ `latents`ï¼ˆæ½œåœ¨è¡¨ç¾ `(batch, latent_dim, latent_len)`ï¼‰ã‚’è¿”ã—ã¾ã™ã€‚
:::

`generate()` ã§ autoregressive ç”Ÿæˆã—ã€`forward()` ã§ç”Ÿæˆã•ã‚ŒãŸé›¢æ•£çš„ãª audio_codes ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ latents ã«å¤‰æ›ã—ã¾ã™ã€‚

ã“ã®å‡¦ç†ã¯ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼ã§é›¢æ•£çš„ãƒˆãƒ¼ã‚¯ãƒ³ ID ã‚’ Embedding å±¤ã«ã‚ˆã£ã¦é€£ç¶šçš„ãª Hidden States ã«å¤‰æ›ã™ã‚‹ã®ã¨é¡ä¼¼ã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã‚ã‚Šã€GPT.forward() ãŒå®Ÿè³ªçš„ã« Audio Code Embedding ã®å½¹å‰²ã‚’æœãŸã—ã¦ã„ã¾ã™ã€‚

## HifiDecoderï¼ˆlatents â†’ éŸ³å£°æ³¢å½¢ï¼‰

https://github.com/coqui-ai/TTS/blob/eef419b37393b11cc741662d041d8d793e011f2d/TTS/tts/layers/xtts/hifigan_decoder.py#L615-L639

:::message
HifiDecoder ã¯ latents ã‚’éŸ³å£°æ³¢å½¢ã«å¤‰æ›ã™ã‚‹å½¹å‰²ã‚’æ‹…ã„ã¾ã™ã€‚ä¸»è¦ãªãƒ¡ã‚½ãƒƒãƒ‰ã¨ã—ã¦ã€`forward()` ãŒè¨“ç·´æ™‚ã®é †ä¼æ’­å‡¦ç†ï¼ˆå‹¾é…è¨ˆç®—ã‚ã‚Šï¼‰ã‚’å®Ÿè¡Œã—ã€`inference()` ãŒæ¨è«–æ™‚ã®å‡¦ç†ï¼ˆ`@torch.no_grad()` ã§å‹¾é…è¨ˆç®—ãªã—ï¼‰ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚å…¥åŠ›ã¨ã—ã¦ `latents`ï¼ˆGPT ãŒç”Ÿæˆã—ãŸæ½œåœ¨è¡¨ç¾ `(batch, latent_dim, latent_len)`ï¼‰ã¨ `g`ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®æ¡ä»¶ä»˜ã‘ãƒ†ãƒ³ã‚½ãƒ«ã€speaker embedding ãªã©ï¼‰ã‚’å—ã‘å–ã‚Šã¾ã™ã€‚å‡ºåŠ›ã¯ `waveform`ï¼ˆéŸ³å£°æ³¢å½¢ `(batch, 1, sample_len)`ï¼‰ã‚’è¿”ã—ã¾ã™ã€‚
:::

HifiDecoder ã¯ GPT ã‹ã‚‰å—ã‘å–ã£ãŸé€£ç¶šçš„ãª latents ã‚’éŸ³å£°æ³¢å½¢ã«ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¾ã™ã€‚`forward()` ã¨ `inference()` ã¯æœ¬è³ªçš„ã«åŒã˜å‡¦ç†ã‚’è¡Œã„ã¾ã™ãŒã€å¾Œè€…ã¯ `@torch.no_grad()` ã§å‹¾é…è¨ˆç®—ã‚’çœç•¥ã—ã¾ã™ã€‚

## Neuron ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«

end-to-end ã®ãƒ¢ãƒ‡ãƒ«ã¨ã¯ç•°ãªã‚Šä¸Šè¿°ã—ãŸ GPTã€HifiDecoder ã¯ç‹¬ç«‹ã—ã¦ã„ã‚‹ãŸã‚ã€**å€‹åˆ¥ã« Neuron ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«**ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

### Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å®Ÿè£…

XTTS v2 ã®ã‚ˆã†ãªå…¬é–‹ OSS ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ AWS Neuron ã«å¯¾å¿œã•ã›ã‚‹å ´åˆã€åŸºæœ¬çš„ã«ã¯ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ãŒé€šã‚Œã°è‰¯ã„ã®ã§å…ƒã® OSS ã®ä¸Šè¨˜ã§ç´¹ä»‹ã—ãŸã‚³ãƒ¼ãƒ‰ã«ç›´æ¥æ‰‹ã‚’åŠ ãˆã¦ AWS Neuron ã«ç‰¹åŒ–ã•ã›ã‚‹å½¢ã§ä¿®æ­£ã—ã¦ã‚‚è‰¯ã„ã§ã™ãŒ OSS å´ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚¢ãƒƒãƒ—ã”ã¨ã« AWS Neuron ã®ä¿®æ­£ã‚’æ‰‹å‹•ã§ãƒãƒ¼ã‚¸ã™ã‚‹å¿…è¦æ€§ãŒã‚ã‚Šã€ãƒ•ã‚©ãƒ¼ã‚¯ã—ã¦ç‹¬è‡ªãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ãŒç™ºç”Ÿã—ã¾ã™ã€‚å€‹äººçš„ã«ã¯ç’°å¢ƒäº’æ›æ€§ã‚’é‡è¦–ã™ã‚‹ãŸã‚ã§ãã‚Œã°ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆä¾å­˜ã‚„ç’°å¢ƒä¾å­˜ã‚’å…¥ã‚ŒãŸãã‚ã‚Šã¾ã›ã‚“ã€‚

ãã“ã§ä»¥ä¸‹ã®ã‚ˆã†ã«å…ƒã® OSS ã®ã‚³ãƒ¼ãƒ‰ã‚’å¤‰æ›´ã›ãšã«ã€pip ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã€ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«å¾Œã«å¤–ã‹ã‚‰ `forward()` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ Override ã™ã‚‹æ–¹æ³•ãŒè‰¯ã„ã®ã§ã¯ãªã„ã‹ã¨æ€ã£ã¦è©¦ã—ã¦ã¿ã¾ã—ãŸã€‚OSS ã®å®Ÿè£…æ–¹æ³•ã«ã‚ˆã£ã¦ã¯ä¸è¦ã ã£ãŸã‚Šã€å¥½ã¿ã®å•é¡Œã¯ã‚ã‚‹ã®ã§ã“ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ä½¿ã†ã‹ã©ã†ã‹ã¯å®Ÿè£…è€…ãŒæ±ºã‚ã‚Œã°è‰¯ã„ã¨æ€ã„ã¾ã™ã€‚ä»¥é™ã“ã®æ–¹æ³•ã‚’ Forward Override ã¨å‘¼ç§°ã—ã¾ã™ã€‚

```python
from TTS.api import TTS
import types
import torch
import torch_neuronx

# XTTS v2 ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
tts = TTS("tts_models/multilingual/multi-dataset/xtts_v2").to("cpu")
model = tts.synthesizer.tts_model

# 1. ã‚ªãƒªã‚¸ãƒŠãƒ«ã® forward ã‚’ä¿å­˜ï¼ˆã‚¯ãƒ©ã‚¹å®šç¾©ã‹ã‚‰å–å¾—ã—ã€å†å®Ÿè¡Œã§ã‚‚å®‰å…¨ï¼‰
model.gpt.forward_original = types.MethodType(type(model.gpt).forward, model.gpt)

# 2. ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ç”¨ forward ã«ä¸Šæ›¸ãï¼ˆkwargs ã‚’ positional args åŒ–ã—å›ºå®šï¼‰
def compile_forward(self, text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents):
    return self.forward_original(
        text_inputs=text_inputs, text_lengths=text_lengths,
        audio_codes=audio_codes, wav_lengths=wav_lengths,
        cond_latents=cond_latents, return_attentions=False, return_latent=True
    )
model.gpt.forward = types.MethodType(compile_forward, model.gpt)

# 3. Neuron ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ï¼ˆModule ã‚’ç›´æ¥æ¸¡ã™ï¼‰
neuron_gpt = torch_neuronx.trace(
    model.gpt,  # torch.nn.Module ã‚’æ¸¡ã™ï¼ˆwrapper é–¢æ•°ã§ã¯ãªã„ï¼‰
    (text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents),
    compiler_workdir='/tmp/neuron_cache_gpt',
    compiler_args='--model-type=transformer --auto-cast=all --auto-cast-type=bf16'
)

# 4. Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åˆ‡ã‚Šæ›¿ãˆ
def forward_wrapper(self, text_inputs, text_lengths, audio_codes, wav_lengths,
                    cond_latents=None, return_attentions=False, return_latent=False):
    if hasattr(self, 'forward_neuron'):
        return self.forward_neuron(text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents)
    else:
        return self.forward_original(text_inputs, text_lengths, audio_codes, wav_lengths,
            cond_latents=cond_latents, return_attentions=return_attentions, return_latent=return_latent)

model.gpt.forward = types.MethodType(forward_wrapper, model.gpt)
model.gpt.forward_neuron = neuron_gpt
```

ã“ã®æ–¹æ³•ã«ã‚ˆã£ã¦ä»Šå›ã®ã‚±ãƒ¼ã‚¹ã ã¨ **XTTS v2 ã®ã‚³ãƒ¼ãƒ‰ã‚’ä¸€åˆ‡å¤‰æ›´ã—ãªã„**ã§ AWS Neuron ã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’åˆ©ç”¨å¯èƒ½ã§ã™ã€‚ä»Šå¾Œã“ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã‚ˆã‚Šæ±ç”¨åŒ–ã•ã›ã¦ end-to-end ã§ã¯ãªã„ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã™ã‚‹ AWS Neuron ä½¿ç”¨ã®ä½“é¨“ã‚’æ”¹å–„ã—ã¦ã„ã‘ãªã„ã‹è€ƒãˆã¦ã¿ã¾ã™ã€‚


ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã¯ Python ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ã‚·ã‚§ãƒ«ã§ 1 è¡Œãšã¤å®Ÿè¡Œã§ãã¾ã™ã€‚

::::details æº–å‚™

```bash
source /opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/bin/activate
```

:::message
- **coqui-tts 0.27.5**ï¼ˆidiap ãƒ•ã‚©ãƒ¼ã‚¯ç‰ˆã€Python 3.12 å¯¾å¿œï¼‰
- **torch 2.9.0**
- **torch_neuronx 2.9.0.2.11.19912**
- **transformers 4.57.6**
:::

```python
# Python REPL ã‚’èµ·å‹•
# $ python3

# å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import os
import torch
import types
os.environ['COQUI_TOS_AGREED'] = '1'

# XTTS v2 ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
from TTS.api import TTS
tts = TTS("tts_models/multilingual/multi-dataset/xtts_v2").to("cpu")
model = tts.synthesizer.tts_model

# ãƒ¢ãƒ‡ãƒ«ã®ç¢ºèª
print(f"GPT: {type(model.gpt)}")
print(f"HifiDecoder: {type(model.hifigan_decoder)}")
```

å®Ÿè¡Œçµæœ

```python
>>> # ãƒ¢ãƒ‡ãƒ«ã®ç¢ºèª
>>> print(f"GPT: {type(model.gpt)}")
GPT: <class 'TTS.tts.layers.xtts.gpt.GPT'>
>>> print(f"HifiDecoder: {type(model.hifigan_decoder)}")
HifiDecoder: <class 'TTS.tts.layers.xtts.hifigan_decoder.HifiDecoder'>
```
::::

::::details Forward Override + Neuron ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«

```python
# ============================================================================
# Step 1: forward_original ã‚’ä¿å­˜ï¼ˆã‚¯ãƒ©ã‚¹å®šç¾©ã® forward ã‚’å–å¾—ï¼‰
# ============================================================================
# é‡è¦: model.gpt.forward ã§ã¯ãªã type(model.gpt).forward ã‚’ä½¿ã†ã€‚
# model.gpt.forward ã¯ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å±æ€§ï¼ˆå‰å›ã® forward_wrapper ç­‰ï¼‰ã‚’è¿”ã™å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚
# type(...).forward ã¯ã‚¯ãƒ©ã‚¹å®šç¾©ã®ã‚ªãƒªã‚¸ãƒŠãƒ« forward ã‚’å¸¸ã«è¿”ã™ã€‚
model.gpt.forward_original = types.MethodType(type(model.gpt).forward, model.gpt)
print(f"[OK] forward_original ä¿å­˜: {type(model.gpt.forward_original)}")

# ============================================================================
# Step 2: ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ç”¨ forward ã«ä¸Šæ›¸ãï¼ˆkwargs ã‚’å›ºå®šï¼‰
# ============================================================================
def compile_forward(self, text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents):
    """kwargs ã‚’ positional args åŒ–ã—ã€return_latent=True ã‚’å›ºå®š"""
    return self.forward_original(
        text_inputs=text_inputs, text_lengths=text_lengths,
        audio_codes=audio_codes, wav_lengths=wav_lengths,
        cond_latents=cond_latents, return_attentions=False, return_latent=True
    )

model.gpt.forward = types.MethodType(compile_forward, model.gpt)
print(f"[OK] compile_forward é©ç”¨: {type(model.gpt.forward)}")

# ============================================================================
# Step 3: ãƒ€ãƒŸãƒ¼å…¥åŠ›ã‚’ä½œæˆ
# ============================================================================
text_inputs = torch.randint(0, 256, (1, 50))
text_lengths = torch.tensor([50])
audio_codes = torch.randint(0, 1024, (1, 100))
wav_lengths = torch.tensor([100])
cond_latents = torch.randn(1, 32, 1024)

print(f"[OK] ãƒ€ãƒŸãƒ¼å…¥åŠ›ä½œæˆ")
print(f"  text_inputs:   {text_inputs.shape}")
print(f"  text_lengths:  {text_lengths.shape}")
print(f"  audio_codes:   {audio_codes.shape}")
print(f"  wav_lengths:   {wav_lengths.shape}")
print(f"  cond_latents:  {cond_latents.shape}")

# ============================================================================
# Step 4: CPU æ¨è«–ãƒ†ã‚¹ãƒˆï¼ˆcompile_forward ãŒæ­£ã—ãå‹•ä½œã™ã‚‹ã‹ç¢ºèªï¼‰
# ============================================================================
with torch.no_grad():
    result = model.gpt(text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents)
    print(f"[OK] CPU æ¨è«–æˆåŠŸ: {result.shape}")

# ============================================================================
# Step 5: Neuron ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ï¼ˆModule ã‚’ç›´æ¥æ¸¡ã™ï¼‰
# ============================================================================
import torch_neuronx

neuron_gpt = torch_neuronx.trace(
    model.gpt,  # torch.nn.Module ã‚’ç›´æ¥æ¸¡ã™
    (text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents),
    compiler_workdir='/tmp/neuron_cache_gpt',
    compiler_args='--model-type=transformer --auto-cast=all --auto-cast-type=bf16'
)
print(f"[OK] Neuron ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æˆåŠŸ: {type(neuron_gpt).__name__}")

# ============================================================================
# Step 6: Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åˆ‡ã‚Šæ›¿ãˆ + forward_neuron ç™»éŒ²
# ============================================================================
def forward_wrapper(self, text_inputs, text_lengths, audio_codes, wav_lengths,
                    cond_latents=None, return_attentions=False, return_latent=False):
    if hasattr(self, 'forward_neuron'):
        return self.forward_neuron(text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents)
    else:
        return self.forward_original(text_inputs, text_lengths, audio_codes, wav_lengths,
            cond_latents=cond_latents, return_attentions=return_attentions, return_latent=return_latent)

model.gpt.forward = types.MethodType(forward_wrapper, model.gpt)
model.gpt.forward_neuron = neuron_gpt
print(f"[OK] forward_neuron ç™»éŒ²å®Œäº†")

# ============================================================================
# Step 7: Neuron æ¨è«–ãƒ†ã‚¹ãƒˆ
# ============================================================================
with torch.no_grad():
    result2 = model.gpt(text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents=cond_latents)
    print(f"[OK] Neuron æ¨è«–æˆåŠŸ: {result2.shape}")
```

å®Ÿè¡Œçµæœ

```python
>>> model.gpt.forward_original = types.MethodType(type(model.gpt).forward, model.gpt)
>>> print(f"[OK] forward_original ä¿å­˜: {type(model.gpt.forward_original)}")
[OK] forward_original ä¿å­˜: <class 'method'>
>>> def compile_forward(self, text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents):
...     """kwargs ã‚’ positional args åŒ–ã—ã€return_latent=True ã‚’å›ºå®š"""
...     return self.forward_original(
...         text_inputs=text_inputs, text_lengths=text_lengths,
...         audio_codes=audio_codes, wav_lengths=wav_lengths,
...         cond_latents=cond_latents, return_attentions=False, return_latent=True
...     )
... 
>>> model.gpt.forward = types.MethodType(compile_forward, model.gpt)
>>> print(f"[OK] compile_forward é©ç”¨: {type(model.gpt.forward)}")
[OK] compile_forward é©ç”¨: <class 'method'>
>>> 
>>> 
>>> text_inputs = torch.randint(0, 256, (1, 50))
>>> text_lengths = torch.tensor([50])
>>> audio_codes = torch.randint(0, 1024, (1, 100))
>>> wav_lengths = torch.tensor([100])
>>> cond_latents = torch.randn(1, 32, 1024)
>>> 
>>> print(f"[OK] ãƒ€ãƒŸãƒ¼å…¥åŠ›ä½œæˆ")
[OK] ãƒ€ãƒŸãƒ¼å…¥åŠ›ä½œæˆ
>>> print(f"  text_inputs:   {text_inputs.shape}")
  text_inputs:   torch.Size([1, 50])
>>> print(f"  text_lengths:  {text_lengths.shape}")
  text_lengths:  torch.Size([1])
>>> print(f"  audio_codes:   {audio_codes.shape}")
  audio_codes:   torch.Size([1, 100])
>>> print(f"  wav_lengths:   {wav_lengths.shape}")
  wav_lengths:   torch.Size([1])
>>> print(f"  cond_latents:  {cond_latents.shape}")
  cond_latents:  torch.Size([1, 32, 1024])
>>> with torch.no_grad():
...     result = model.gpt(text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents)
...     print(f"[OK] CPU æ¨è«–æˆåŠŸ: {result.shape}")
... 
[OK] CPU æ¨è«–æˆåŠŸ: torch.Size([1, 1, 1024])
>>> 
>>> import torch_neuronx
>>> 
>>> neuron_gpt = torch_neuronx.trace(
...     model.gpt,  # torch.nn.Module ã‚’ç›´æ¥æ¸¡ã™
...     (text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents),
...     compiler_workdir='/tmp/neuron_cache_gpt',
...     compiler_args='--model-type=transformer --auto-cast=all --auto-cast-type=bf16'
... )
/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=1, shape=torch.Size([1]), dtype=torch.int64). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
  warnings.warn(
/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=3, shape=torch.Size([1]), dtype=torch.int64). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
  warnings.warn(

......Completed run_backend_driver.

Compiler status PASS
>>> print(f"[OK] Neuron ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æˆåŠŸ: {type(neuron_gpt).__name__}")
[OK] Neuron ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æˆåŠŸ: TopLevelTracedModule

[OK] forward_neuron ç™»éŒ²å®Œäº†

[OK] Neuron æ¨è«–æˆåŠŸ: torch.Size([1, 1, 1024])
```

:::message alert
[é‡è¦] `torch_neuronx.trace()` ã«ã¯ **`torch.nn.Module` ã‚’ç›´æ¥æ¸¡ã™**å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚wrapper é–¢æ•°ã‚’æ¸¡ã™ã¨ `RuntimeError: Expected XLA tensor` ãŒç™ºç”Ÿã—ã¾ã™ã€‚ç†ç”±ã¯å†…éƒ¨ã§ `isinstance(func, torch.nn.Module)` ã®ãƒã‚§ãƒƒã‚¯ã«ã‚ˆã‚Šã€Module ã®å ´åˆã®ã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒ XLA ãƒ‡ãƒã‚¤ã‚¹ã«å¤‰æ›ã•ã‚Œã‚‹ãŸã‚ã§ã™ã€‚Module ã® `forward` ã‚’ä¸€æ™‚çš„ã«ä¸Šæ›¸ãã—ã¦ã‹ã‚‰ `trace(model.gpt, ...)` ã‚’å‘¼ã³ã€ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«å¾Œã« Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åˆ‡ã‚Šæ›¿ãˆã¦ã„ã¾ã™ã€‚
:::

**forward_neuron**

```
>>> model.gpt.forward_neuron
NeuronModule(
  original_name=NeuronModule
  (states): ParameterList(original_name=ParameterList)
  (weights): ParameterDict(original_name=ParameterDict)
)
```

`torch_neuronx.trace()` ãŒ PyTorch ãƒ¢ãƒ‡ãƒ«ã‚’ NeuronCore ç”¨ã®å‘½ä»¤åˆ—ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã€ãã®çµæœãŒ callable ãªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ã—ã¦è¿”ã•ã‚Œã¾ã™ã€‚ã“ã‚Œã‚’ `forward_neuron` å±æ€§ã¨ã—ã¦ç™»éŒ²ã™ã‚‹ã“ã¨ã§ã€å…ƒã® GPT ã‚¯ãƒ©ã‚¹ã®ã‚³ãƒ¼ãƒ‰ã‚’ä¸€åˆ‡å¤‰æ›´ã›ãšã« Neuron å¯¾å¿œã§ãã¾ã™ã€‚
::::

### å®Ÿè£…ã®è§£èª¬

Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ `model.gpt` ãŒã©ã®ã‚ˆã†ã«å¤‰åŒ–ã™ã‚‹ã‹ã‚’å›³ã§ç¤ºã—ã¾ã™ã€‚

```mermaid
graph TB
    subgraph "model.gpt ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹"
        A[forward]
        B[forward_original]
        C[forward_neuron]
    end

    A -->|"æŒ¯ã‚Šåˆ†ã‘ãƒ­ã‚¸ãƒƒã‚¯<br/>(forward_wrapper)"| D{forward_neuron<br/>å­˜åœ¨ï¼Ÿ}
    D -->|YES| C
    D -->|NO| B

    B -->|"å…ƒã® forward ãƒ¡ã‚½ãƒƒãƒ‰<br/>(CPU/GPU å®Ÿè¡Œ)"| E[GPT ã‚ªãƒªã‚¸ãƒŠãƒ«ã® forward]
    C -->|"Neuron ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿<br/>(NeuronCore å®Ÿè¡Œ)"| F[TorchScript ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«]

    style A fill: #e1f5ff
    style B fill: #f3e5f5
    style C fill: #fff3e0
    style D fill: #ffe0b2
    style E fill: #f3e5f5
    style F fill: #fff3e0
```

::::details åˆå­¦è€…å‘ã‘: Python ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®åŸºç¤çŸ¥è­˜

Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ç†è§£ã™ã‚‹ãŸã‚ã«ã€Python ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¨ãƒ¡ã‚½ãƒƒãƒ‰ã®ä»•çµ„ã¿ã‚’æŒ¯ã‚Šè¿”ã‚Šã¾ã™ã€‚

## model.gpt ã¯ä½•ã‹ï¼Ÿ

`model.gpt` ã¯ `torch.nn.Module` ã‚’ç¶™æ‰¿ã—ãŸ Python ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§ã™ã€‚

```python
type(model.gpt)
# <class 'TTS.tts.layers.xtts.gpt.GPT'>

isinstance(model.gpt, torch.nn.Module)
# True
```

Python ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«ã¯ã€**å±æ€§ã‚’å‹•çš„ã«è¿½åŠ ã§ãã‚‹**ã¨ã„ã†ç‰¹å¾´ãŒã‚ã‚Šã¾ã™ã€‚

```python
# é€šå¸¸ã®å±æ€§è¿½åŠ 
model.gpt.my_custom_value = 42

# ãƒ¡ã‚½ãƒƒãƒ‰ã‚‚å±æ€§ã®ä¸€ç¨®
model.gpt.forward  # ã“ã‚Œã‚‚å±æ€§
```

`gpt.forward` ã¯ `gpt` ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«ãƒã‚¤ãƒ³ãƒ‰ã•ã‚ŒãŸãƒ¡ã‚½ãƒƒãƒ‰ã§ã™ã€‚

```python
>>> type(model.gpt.forward)
<class 'method'>
```
::::

### types.MethodType ã®å½¹å‰²

`types.MethodType(func, instance)` ã¯ã€**é€šå¸¸ã®é–¢æ•°ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãƒ¡ã‚½ãƒƒãƒ‰ã«å¤‰æ›**ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ä»¥ä¸‹ã®ã‚ˆã†ã« `compile_forward` ã‚’ model.gpt ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãƒ¡ã‚½ãƒƒãƒ‰ã¨ã—ã¦ç™»éŒ²ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

```python
# compile_forward ã‚’å®šç¾©
def compile_forward(self, text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents):
    return self.forward_original(...)

# model.gpt ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¨ã—ã¦ç™»éŒ²
model.gpt.forward = types.MethodType(compile_forward, model.gpt)

# å‘¼ã³å‡ºã—æ™‚ã€self=model.gpt ãŒè‡ªå‹•çš„ã«æ¸¡ã•ã‚Œã‚‹
model.gpt(text_inputs, ...)  # compile_forward(model.gpt, text_inputs, ...) ãŒå‘¼ã°ã‚Œã‚‹
```

### torch_neuronx.trace() ãŒè¿”ã™ã‚‚ã®

`torch_neuronx.trace()` ã¯ **TorchScript ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«**ï¼ˆ`torch.jit.ScriptModule`ï¼‰ã‚’è¿”ã—ã¾ã™ã€‚

```python
neuron_gpt = torch_neuronx.trace(model.gpt, example_inputs, ...)

type(neuron_gpt)
# <class 'torch.jit._trace.TopLevelTracedModule'>

# ã“ã‚Œã¯ callable
neuron_gpt(text_inputs, ...)  # NeuronCore ã§å®Ÿè¡Œã•ã‚Œã‚‹
```

TorchScript ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ PyTorch ãƒ¢ãƒ‡ãƒ«ã®æœ€é©åŒ–ã•ã‚ŒãŸå®Ÿè¡Œå¯èƒ½å½¢å¼ã§ã€NeuronCore å®Ÿè¡Œã‚ˆã†ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ã§ã‚ã‚Šã€Python ã‚¤ãƒ³ã‚¿ãƒ—ãƒªã‚¿ã‚’çµŒç”±ã—ãªã„é«˜é€Ÿå®Ÿè¡Œã€`torch.jit.load()` ã§ä¿å­˜ãƒ»å¾©å…ƒå¯èƒ½ã€ãªã©ã®ç‰¹å¾´ãŒã‚ã‚Šã¾ã™ã€‚

### Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ä»•çµ„ã¿ï¼ˆå†æ²ï¼‰

ä»¥ä¸Šã®çŸ¥è­˜ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã¨ã€Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«å‹•ä½œã—ã¾ã™ã€‚

```python
# Step 1: ã‚ªãƒªã‚¸ãƒŠãƒ«ã® forward ã‚’ä¿å­˜
model.gpt.forward_original = types.MethodType(type(model.gpt).forward, model.gpt)
# â†’ GPT ã‚¯ãƒ©ã‚¹ã® forward ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ model.gpt ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«ãƒã‚¤ãƒ³ãƒ‰

# Step 2: æŒ¯ã‚Šåˆ†ã‘ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®šç¾©
def forward_wrapper(self, text_inputs, text_lengths, audio_codes, wav_lengths,
                    cond_latents=None, return_attentions=False, return_latent=False):
    if hasattr(self, 'forward_neuron'):
        # Neuron ç‰ˆãŒå­˜åœ¨ã™ã‚Œã°å‘¼ã³å‡ºã™
        return self.forward_neuron(text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents)
    else:
        # ãªã‘ã‚Œã°ã‚ªãƒªã‚¸ãƒŠãƒ«ç‰ˆã‚’å‘¼ã³å‡ºã™
        return self.forward_original(text_inputs, text_lengths, audio_codes, wav_lengths,
            cond_latents=cond_latents, return_attentions=return_attentions, return_latent=return_latent)

# Step 3: forward ã‚’ç½®ãæ›ãˆ
model.gpt.forward = types.MethodType(forward_wrapper, model.gpt)
# â†’ model.gpt.forward ãŒ forward_wrapper ã«ãªã‚‹

# Step 4: Neuron ç‰ˆã‚’ç™»éŒ²
model.gpt.forward_neuron = neuron_gpt
# â†’ model.gpt.forward_neuron ã¨ã„ã†æ–°ã—ã„å±æ€§ã‚’è¿½åŠ 

# å®Ÿè¡Œæ™‚ã®å‹•ä½œ
model.gpt(inputs)
# â†“
# model.gpt.forward(inputs) ãŒå‘¼ã°ã‚Œã‚‹
# â†“
# forward_wrapper(model.gpt, inputs) ãŒå®Ÿè¡Œã•ã‚Œã‚‹
# â†“
# hasattr(model.gpt, 'forward_neuron') â†’ True
# â†“
# model.gpt.forward_neuron(inputs) ãŒå‘¼ã°ã‚Œã‚‹ï¼ˆNeuronCore å®Ÿè¡Œï¼‰
```

**é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ**ã¯ã€ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’ä¸€åˆ‡å¤‰æ›´ã›ãšã«å…ƒã® CPU/GPU å‘ã‘å¯¾å¿œã«åŠ ãˆã¦ Neuron å¯¾å¿œã‚’è¿½åŠ ã—ãŸã“ã¨ã§ã™ã€‚

## torch_neuronx ã«ã¤ã„ã¦

### torch_neuronx.trace()

`torch_neuronx.trace()` ã¯ PyTorch ã® `torch.jit.trace()` ã¨åŒã˜æ„Ÿè¦šã§ä½¿ãˆã¾ã™ã€‚

**åŸºæœ¬çš„ãªä½¿ã„æ–¹**ã‚’ç¤ºã—ã¾ã™ã€‚

```python
import torch
import torch_neuronx

# ãƒ¢ãƒ‡ãƒ«ã¨å…¥åŠ›ä¾‹ã‚’ç”¨æ„
model = MyModel()
example_input = torch.randn(1, 100, 1024)

# å˜ä¸€å¼•æ•°ã®å ´åˆ
neuron_model = torch_neuronx.trace(
    model,
    example_input,
    compiler_args='--model-type=transformer --auto-cast=all'
)

# è¤‡æ•°å¼•æ•°ã®å ´åˆï¼ˆã‚¿ãƒ—ãƒ«ã§æ¸¡ã™ï¼‰
input1 = torch.randint(0, 256, (1, 50))
input2 = torch.tensor([50])
neuron_model = torch_neuronx.trace(
    model,
    (input1, input2),
    compiler_args='--model-type=transformer --auto-cast=all'
)

# ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
neuron_model.save('model_neuron.pt')

# ãƒ­ãƒ¼ãƒ‰
loaded_model = torch.jit.load('model_neuron.pt')
```

:::message alert
**é‡è¦ 1:** `torch_neuronx.trace()` ã®ç¬¬ 1 å¼•æ•°ã«ã¯ **`torch.nn.Module` ã‚’æ¸¡ã™**å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

**é‡è¦ 2:** `torch_neuronx.trace` ã¯ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ã®å…¥åŠ›å½¢çŠ¶ãŒå›ºå®šã•ã‚Œã¾ã™ã€‚å¯å¤‰é•·å…¥åŠ›ã«ã¯ç¾çŠ¶å¯¾å¿œã—ã¦ã„ãªã„ãŸã‚ã€å®Ÿé‹ç”¨ã§ã¯ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç†ãŒå¿…è¦ã§ã™ã€‚æ¨è«–æ™‚ã¯å¿…ãšã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ã¨åŒã˜ãƒ‡ãƒ¼ã‚¿å‹ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚
:::


ã‚‚ã—ãƒ¢ãƒ‡ãƒ«ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³å¼•æ•°ï¼ˆ`return_attentions`, `return_latent` ãªã©ï¼‰ã®å€¤ã‚’æŒ‡å®šã—ã¦ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ãŸã„å ´åˆã€Module ã® `forward` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä¸€æ™‚çš„ã«ç½®ãæ›ãˆã¾ã™ã€‚

**ä¾‹**: GPT ã® `forward` ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ `return_latent=False` ã®æ™‚ã«ã€`return_latent=True` ã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ãŸã„å ´åˆ

```python
import types

# Step 1: ã‚ªãƒªã‚¸ãƒŠãƒ«ã® forward ã‚’ä¿å­˜
model.gpt.forward_original = types.MethodType(type(model.gpt).forward, model.gpt)

# Step 2: å¼•æ•°ã‚’æ±ºã‚æ‰“ã¡ã—ãŸ forward ã«ä¸€æ™‚çš„ã«ç½®ãæ›ãˆ
def compile_forward(self, text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents):
    # return_latent=True ã‚’æŒ‡å®šã—ã¦ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
    return self.forward_original(
        text_inputs, text_lengths, audio_codes, wav_lengths,
        cond_latents=cond_latents,
        return_attentions=False,  # â† ã“ã®å€¤ã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
        return_latent=True        # â† ã“ã®å€¤ã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
    )

model.gpt.forward = types.MethodType(compile_forward, model.gpt)

# Step 3: Module ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ trace ã«æ¸¡ã™
neuron_gpt = torch_neuronx.trace(
    model.gpt,  # â† Module ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ¸¡ã™
    example_inputs
)
```

:::message
ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«å¾Œã¯ã€`return_latent=True` ã§å›ºå®šã•ã‚Œã‚‹ã®ã§æ¨è«–æ™‚ã« `False` ã‚’æŒ‡å®šã—ã¦ã‚‚ç„¡è¦–ã•ã‚Œã¾ã™ã€‚
:::

`torch_neuronx.trace()` ã® [`compiler_args`](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuron/api-compilation-python-api.html#torch_neuron.trace) ã§æœ€é©åŒ–ã‚’åˆ¶å¾¡ã§ãã¾ã™ã€‚è©³ç´°ã¯ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

## å†ç¾æ‰‹é †

ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€inf2.xlarge ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä¸Šã§ XTTS v2 ã‚’å‹•ä½œã•ã›ã‚‹æ‰‹é †ã‚’ç¤ºã—ã¾ã™ã€‚å…¨ã¦ã®æ‰‹é †ã¯ heredoc å½¢å¼ã§è¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ã‚³ãƒ”ãƒ¼&ãƒšãƒ¼ã‚¹ãƒˆã§å®Ÿè¡Œã§ãã¾ã™ã€‚

https://zenn.dev/tosshi/articles/a18dce7d66424d

ã“ã®è¨˜äº‹ã‚’è¦‹ã‚Œã° Inf2 ã‚„ Trn2 ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ç«‹ã¡ä¸Šã’ã¦åˆ©ç”¨ã™ã‚‹æ¤œè¨¼ç’°å¢ƒã‚’ç°¡å˜ã«æ§‹ç¯‰ã§ãã¾ã™ã€‚

### æ¨å¥¨ç’°å¢ƒ

- ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—: inf2.xlargeã€trn1.2xlargeã€trn2.3xlarge
- Neuron SDK: 2.28+
- neuronxcc: 2.22+
- Python: 3.10-3.12
- coqui-tts: 0.27+ã€idiap ãƒ•ã‚©ãƒ¼ã‚¯ç‰ˆï¼ˆPython 3.12 å¯¾å¿œï¼‰
- ãƒ¢ãƒ‡ãƒ«: kotoba-tech/kotoba-whisper-v2.2 (1550M ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€æ—¥æœ¬èªç‰¹åŒ–)
- torchaudio: 2.9+
- transformers: <5.0

:::message
æœ¬ã‚¬ã‚¤ãƒ‰ã§ã¯ **idiap/coqui-tts** ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ã‚ªãƒªã‚¸ãƒŠãƒ«ã® coqui-ai/TTS ã¯ Python <3.12 ã‚’è¦æ±‚ã—ã¾ã™ãŒã€idiap ãƒ•ã‚©ãƒ¼ã‚¯ã¯ Python 3.12 ã«å®Œå…¨å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚Neuron ç’°å¢ƒã® Python ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã¨ã®æ•´åˆã®ãŸã‚ã®å¯¾å¿œã§ã™ãŒç’°å¢ƒæ§‹ç¯‰ã«ã¤ã„ã¦ã¯ä¸€ä¾‹ã‚’ç¤ºã—ã¾ã™ãŒè‡ªåˆ†ãŸã¡ã®ç’°å¢ƒã«åˆã‚ã›ã¦é ‘å¼µã£ã¦ãã ã•ã„ã€‚
:::

### Step 0: äº‹å‰æº–å‚™

```bash
source /opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/bin/activate

/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/bin/pip coqui-tts

mkdir -p ~/xtts-test && cd ~/xtts-test
```

### Step 1: GPT ãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«

Step 1 ã§ Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é©ç”¨ã—ãŸ XTTS v2 ãƒ¢ãƒ‡ãƒ«ã® GPT ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ Neuron ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¾ã™ã€‚

```python
cat > 01_compile_gpt.py << 'PYTHON_EOF'
#!/usr/bin/env python3
"""
GPT ãƒ¢ãƒ‡ãƒ«ã‚’ torch_neuronx.trace() ã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¦ forward_neuron ã«ç™»éŒ²
"""
import sys
import os
os.environ['PATH'] = '/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/bin: ' + os.environ.get('PATH', '')
os.environ['COQUI_TOS_AGREED'] = '1'

import torch
import torch_neuronx
import pickle

print("=" * 80)
print("GPT ãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«")
print("=" * 80)

# Step 1 ã§ä¿å­˜ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
print("\n[1/4] XTTS v2 ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰...")
with open('xtts_model_with_override.pkl', 'rb') as f:
    tts = pickle.load(f)

model = tts.synthesizer.tts_model if hasattr(tts, 'synthesizer') else tts.model
print(f"  [OK] XTTS v2 loaded")
print(f"  GPT: {type(model.gpt)}")

# ãƒ€ãƒŸãƒ¼å…¥åŠ›ã‚’ä½œæˆ
print("\n[2/4] ãƒ€ãƒŸãƒ¼å…¥åŠ›ã‚’ä½œæˆ...")

# XTTS v2 GPT ã®å…¥åŠ›å½¢çŠ¶
batch_size = 1
text_seq_len = 50
audio_seq_len = 100

# GPT.forward() ã®å¼•æ•°
text_inputs = torch.randint(0, 256, (batch_size, text_seq_len))  # ãƒ†ã‚­ã‚¹ãƒˆãƒˆãƒ¼ã‚¯ãƒ³
text_lengths = torch.tensor([text_seq_len])  # ãƒ†ã‚­ã‚¹ãƒˆé•·
audio_codes = torch.randint(0, 1024, (batch_size, audio_seq_len))  # éŸ³å£°ã‚³ãƒ¼ãƒ‰
wav_lengths = torch.tensor([audio_seq_len])  # éŸ³å£°é•·
cond_latents = torch.randn(batch_size, 32, 1024)  # conditioning latents (batch, 32, 1024)

# æ³¨æ„: cond_latents ã®å½¢çŠ¶ã¯ (batch, 32, 1024)
# ã“ã‚Œã¯ get_style_emb() ã®å‡ºåŠ› (batch, 1024, 32) ã‚’ .transpose(1, 2) ã—ãŸã‚‚ã®
# å‚è€ƒ: TTS/tts/layers/xtts/gpt.py ã® forward() å†…ã®å‡¦ç†

print(f"  text_inputs: {text_inputs.shape}")
print(f"  text_lengths: {text_lengths.shape}")
print(f"  audio_codes: {audio_codes.shape}")
print(f"  wav_lengths: {wav_lengths.shape}")
print(f"  cond_latents: {cond_latents.shape}  # (batch, 32, 1024)")

# CPU ã§å‹•ä½œç¢ºèª
print("\n[3/4] CPU ã§å‹•ä½œç¢ºèª...")
model.gpt.eval()
with torch.no_grad():
    try:
        # XTTS v2 GPT ã® forward ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã—
        output = model.gpt.forward_original(
            text_inputs=text_inputs,
            text_lengths=text_lengths,
            audio_codes=audio_codes,
            wav_lengths=wav_lengths,
            cond_latents=cond_latents,
            return_attentions=False,
            return_latent=True
        )
        print(f"  [OK] CPU inference successful")
        print(f"  Output type: {type(output)}")
    except Exception as e:
        print(f"  [WARNING] CPU inference failed: {e}")
        print(f"  [NOTE] GPT.forward() ã®å…¥åŠ›å½¢çŠ¶ã‚’èª¿æ•´ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
        sys.exit(1)

# Neuron ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
print("\n[4/4] Neuron ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«...")
print(f"  [NOTE] ã“ã‚Œã«ã¯ç´„ 1-2 åˆ†ã‹ã‹ã‚Šã¾ã™ï¼ˆå®Ÿæ¸¬å€¤: 1.8 åˆ†ï¼‰...")
print(f"  [NOTE] ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ãƒ­ã‚°ã¯ /tmp/neuron_cache_gpt/ ã«ä¿å­˜ã•ã‚Œã¾ã™")

try:
    import types

    # ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ç”¨ forward ã«ä¸Šæ›¸ãï¼ˆkwargs ã‚’å›ºå®šï¼‰
    def compile_forward(self, text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents):
        return self.forward_original(
            text_inputs=text_inputs, text_lengths=text_lengths,
            audio_codes=audio_codes, wav_lengths=wav_lengths,
            cond_latents=cond_latents, return_attentions=False, return_latent=True
        )
    model.gpt.forward = types.MethodType(compile_forward, model.gpt)

    # Module ã‚’ç›´æ¥æ¸¡ã—ã¦ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
    neuron_gpt = torch_neuronx.trace(
        model.gpt,  # torch.nn.Module ã‚’ç›´æ¥æ¸¡ã™
        (text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents),
        compiler_workdir='/tmp/neuron_cache_gpt',
        compiler_args='--model-type=transformer --auto-cast=all --auto-cast-type=bf16'
    )


    print(f"  [OK] Neuron compilation successful")

    # Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã«åˆ‡ã‚Šæ›¿ãˆ
    def forward_wrapper(self, text_inputs, text_lengths, audio_codes, wav_lengths,
                        cond_latents=None, return_attentions=False, return_latent=False):
        if hasattr(self, 'forward_neuron'):
            return self.forward_neuron(text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents)
        else:
            return self.forward_original(text_inputs, text_lengths, audio_codes, wav_lengths,
                cond_latents=cond_latents, return_attentions=return_attentions, return_latent=return_latent)
    model.gpt.forward = types.MethodType(forward_wrapper, model.gpt)
    model.gpt.forward_neuron = neuron_gpt
    print(f"  [OK] Forward Override + forward_neuron ç™»éŒ²å®Œäº†")

    # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
    with open('xtts_model_gpt_compiled.pkl', 'wb') as f:
        pickle.dump(tts, f)

    print(f"  [OK] ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜: xtts_model_gpt_compiled.pkl")

except Exception as e:
    print(f"  [ERROR] Compilation failed: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

print("\n" + "=" * 80)
print("GPT ãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«å®Œäº†")
print("=" * 80)
print(f"  æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: HifiDecoder ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«")
print("=" * 80)
PYTHON_EOF

# å®Ÿè¡Œï¼ˆNeuron ç’°å¢ƒã§ã®ã¿å‹•ä½œï¼‰
python 01_compile_gpt.py
```

:::details å‡ºåŠ›
```
================================================================================
GPT ãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
================================================================================

[1/4] XTTS v2 ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰...
  [OK] XTTS v2 loaded
  GPT: <class 'TTS.tts.layers.xtts.gpt.GPT'>

[2/4] ãƒ€ãƒŸãƒ¼å…¥åŠ›ã‚’ä½œæˆ...
  text_inputs: torch.Size([1, 50])
  text_lengths: torch.Size([1])
  audio_codes: torch.Size([1, 100])
  wav_lengths: torch.Size([1])
  cond_latents: torch.Size([1, 32, 1024])  # (batch, 32, 1024)

[3/4] CPU ã§å‹•ä½œç¢ºèª...
  [OK] CPU inference successful
  Output type: <class 'torch.Tensor'>

[4/4] Neuron ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«...
  [NOTE] ã“ã‚Œã«ã¯ç´„ 1-2 åˆ†ã‹ã‹ã‚Šã¾ã™ï¼ˆå®Ÿæ¸¬å€¤: 1.8 åˆ†ï¼‰...
  [NOTE] ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ãƒ­ã‚°ã¯ /tmp/neuron_cache_gpt/ ã«ä¿å­˜ã•ã‚Œã¾ã™
  [OK] Neuron compilation successful
  [OK] neuron_gpt registered to forward_neuron
  [OK] ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜: xtts_model_gpt_compiled.pkl

================================================================================
GPT ãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«å®Œäº†
================================================================================
  æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: HifiDecoder ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
================================================================================
```

:::message
ä¸Šè¨˜ã®ã‚³ãƒ¼ãƒ‰ã¯å‹•ä½œç¢ºèªæ¸ˆã¿ã§ã™ã€‚`torch_neuronx.trace()` ã«ã¯ Module ã‚’ç›´æ¥æ¸¡ã—ã€ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«å‰ã« `forward` ã‚’ `compile_forward` ã«ä¸Šæ›¸ãã™ã‚‹ã“ã¨ã§ kwargs ã‚’å›ºå®šã—ã¦ã„ã¾ã™ã€‚ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«å¾Œã¯ `forward_wrapper` ã«åˆ‡ã‚Šæ›¿ãˆã¦ Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é©ç”¨ã—ã¾ã™ã€‚
:::
::::

::::details torch_neuronx.trace() ã§ kwargs ã‚’æ¸¡ã™æ–¹æ³•ã€é‡è¦ã€‘

`torch_neuronx.trace()` ã§ kwargs ã‚’å«ã‚€ forward ã‚’å‘¼ã³ãŸã„å ´åˆã€**Module ã® forward ã‚’ä¸€æ™‚çš„ã«ä¸Šæ›¸ãã—ã¦ Module ã‚’ç›´æ¥æ¸¡ã™**ã®ãŒæ­£ã—ã„æ–¹æ³•ã§ã™ã€‚

**èª¤ã£ãŸæ–¹æ³•ï¼ˆã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ï¼‰**:
```python
# [NG] æ–¹æ³• 1: ç¬¬ 3 å¼•æ•°ã«è¾æ›¸ã‚’æ¸¡ã™
neuron_gpt = torch_neuronx.trace(
    model.gpt.forward_original,
    (text_inputs, text_lengths, audio_codes, wav_lengths),
    {'cond_latents': cond_latents, 'return_latent': True},  # â† trace è‡ªèº«ã® kwargs ã¨ã—ã¦è§£é‡ˆ
    compiler_workdir='/tmp/neuron_cache_gpt'
)
# â†’ AttributeError: 'NoneType' object has no attribute 'shape'

# [NG] æ–¹æ³• 2: wrapper é–¢æ•°ã‚’æ¸¡ã™
def gpt_forward_wrapper(text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents):
    return model.gpt.forward_original(...)

neuron_gpt = torch_neuronx.trace(
    gpt_forward_wrapper,  # â† é–¢æ•°ã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã® XLA å¤‰æ›ãŒè¡Œã‚ã‚Œãªã„
    (text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents),
    compiler_workdir='/tmp/neuron_cache_gpt'
)
# â†’ RuntimeError: Expected XLA tensor. Got: torch.FloatTensor
```

**æ­£ã—ã„æ–¹æ³•: Module ã® forward ã‚’ä¸Šæ›¸ãã—ã¦ Module ã‚’æ¸¡ã™ã€æ¨å¥¨ã€‘**:
```python
import types

# compile_forward ã§ kwargs ã‚’ positional args åŒ–ã—å›ºå®š
def compile_forward(self, text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents):
    return self.forward_original(
        text_inputs=text_inputs, text_lengths=text_lengths,
        audio_codes=audio_codes, wav_lengths=wav_lengths,
        cond_latents=cond_latents, return_attentions=False, return_latent=True
    )
model.gpt.forward = types.MethodType(compile_forward, model.gpt)

# Module ã‚’ç›´æ¥æ¸¡ã™
neuron_gpt = torch_neuronx.trace(
    model.gpt,  # torch.nn.Module ã‚’æ¸¡ã™
    (text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents),
    compiler_workdir='/tmp/neuron_cache_gpt',
    compiler_args='--model-type=transformer --auto-cast=all --auto-cast-type=bf16'
)
```

**æ ¹æœ¬åŸå› **ï¼ˆ`hlo_conversion.py` ã®ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰èª¿æŸ»ã«åŸºã¥ãï¼‰:

`torch_neuronx.trace()` ã®å†…éƒ¨å‡¦ç†ï¼ˆ`_xla_trace()` é–¢æ•°ï¼‰:
1. `example_inputs` å†…ã®ãƒ†ãƒ³ã‚½ãƒ«ã¯ XLA ãƒ‡ãƒã‚¤ã‚¹ã«**è‡ªå‹•å¤‰æ›**ã•ã‚Œã‚‹ï¼ˆLine 321ï¼‰
2. `isinstance(func, torch.nn.Module)` ãŒ `True` ã®å ´åˆã®ã¿ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒ `PlaceholderParameter` ã«ç½®æ›ã•ã‚Œã‚‹ï¼ˆLine 163-178ï¼‰
3. `func(*example_inputs)` ã§é–¢æ•°ã‚’å®Ÿè¡Œã™ã‚‹ï¼ˆLine 387ï¼‰

wrapper é–¢æ•°ã‚’æ¸¡ã—ãŸå ´åˆã€Step 2 ãŒã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã‚‹ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ã¯ CPU ãƒ†ãƒ³ã‚½ãƒ«ã®ã¾ã¾ XLA ãƒ†ãƒ³ã‚½ãƒ«ã®å…¥åŠ›ã¨æ··åœ¨ã— `Expected XLA tensor` ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã™ã€‚

::::

::::details ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ã®æ³¨æ„ç‚¹::::details ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ã®æ³¨æ„ç‚¹

**å…¥åŠ›å½¢çŠ¶ã®å›ºå®š**:
- `torch_neuronx.trace()` ã¯ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ã®å…¥åŠ›å½¢çŠ¶ãŒå›ºå®šã•ã‚Œã¾ã™
- å®Ÿé‹ç”¨ã§ã¯è¤‡æ•°ã®ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚„ç³»åˆ—é•·ã«å¯¾å¿œã™ã‚‹ãŸã‚ã€è¤‡æ•°ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ç”¨æ„ã™ã‚‹ã‹ã€ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç†ãŒå¿…è¦ã§ã™

**å‹•çš„åˆ¶å¾¡ãƒ•ãƒ­ãƒ¼ã®åˆ¶ç´„**:
- `if` æ–‡ã‚„ `for` ãƒ«ãƒ¼ãƒ—ã®æ¡ä»¶ãŒå…¥åŠ›ã«ä¾å­˜ã™ã‚‹å ´åˆã€ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã§ããªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™
- XTTS v2 GPT ã® `forward()` ã«ã¯è¤‡é›‘ãªæ¡ä»¶åˆ†å²ãŒã‚ã‚‹ãŸã‚ã€ä¸€éƒ¨ã®å‡¦ç†ã‚’ç°¡ç•¥åŒ–ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“

**ãƒ¡ãƒ¢ãƒªè¦ä»¶**:
- GPT-30 (395.8M params) ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã«ã¯ç´„ 2-4GB ã®ãƒ¡ãƒ¢ãƒªãŒå¿…è¦ã§ã™
- inf2.xlarge ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆNeuronCore x1ï¼‰ã§å‹•ä½œã—ã¾ã™

**ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚é–“**:
- NeuronX Compiler 2.22 ã§ã¯ç´„ 1-2 åˆ†ã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ãŒå®Œäº†ã—ã¾ã™
- `--auto-cast=all --auto-cast-type=bf16` ã«ã‚ˆã‚Šé«˜é€ŸåŒ–ã•ã‚Œã¦ã„ã¾ã™

::::

---
### Step 3: HifiDecoder ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«

Step 2 ã§ GPT ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ãŸã®ã¨åŒæ§˜ã«ã€HifiDecoder ã‚’ Neuron ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¾ã™ã€‚

```python
cat > 03_compile_hifidecoder.py << 'PYTHON_EOF'
#!/usr/bin/env python3
"""
HifiDecoder ã‚’ torch_neuronx.trace() ã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¦ forward_neuron ã«ç™»éŒ²
"""
import sys
import os
os.environ['PATH'] = '/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/bin: ' + os.environ.get('PATH', '')
os.environ['COQUI_TOS_AGREED'] = '1'

import torch
import torch_neuronx
import pickle

print("=" * 80)
print("HifiDecoder ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«")
print("=" * 80)

# Step 2 ã§ä¿å­˜ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
print("\n[1/4] XTTS v2 ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰...")
with open('xtts_model_gpt_compiled.pkl', 'rb') as f:
    tts = pickle.load(f)

model = tts.synthesizer.tts_model if hasattr(tts, 'synthesizer') else tts.model
print(f"  [OK] XTTS v2 loaded")
print(f"  HifiDecoder: {type(model.hifigan_decoder)}")

# ãƒ€ãƒŸãƒ¼å…¥åŠ›ã‚’ä½œæˆ
print("\n[2/4] ãƒ€ãƒŸãƒ¼å…¥åŠ›ã‚’ä½œæˆ...")

# HifiDecoder ã®å…¥åŠ›ã¯ GPT ã®å‡ºåŠ›ï¼ˆlatentsï¼‰
batch_size = 1
latent_dim = 1024
latent_len = 100

latents = torch.randn(batch_size, latent_dim, latent_len)
print(f"  latents: {latents.shape}")

# CPU ã§å‹•ä½œç¢ºèª
print("\n[3/4] CPU ã§å‹•ä½œç¢ºèª...")
model.hifigan_decoder.eval()
with torch.no_grad():
    try:
        # HifiDecoder ã® inference ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‘¼ã³å‡ºã—
        output = model.hifigan_decoder.inference(latents)
        print(f"  [OK] CPU inference successful")
        print(f"  Output shape: {output.shape}")
    except Exception as e:
        print(f"  [WARNING] CPU inference failed: {e}")
        print(f"  [NOTE] HifiDecoder.inference() ã®å…¥åŠ›å½¢çŠ¶ã‚’èª¿æ•´ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
        sys.exit(1)

# Neuron ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
print("\n[4/4] Neuron ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«...")
print(f"  [NOTE] ã“ã‚Œã«ã¯ç´„ 20 ç§’ã‹ã‹ã‚Šã¾ã™ï¼ˆå®Ÿæ¸¬å€¤: 20.5 ç§’ï¼‰...")
print(f"  [NOTE] ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ãƒ­ã‚°ã¯ /tmp/neuron_cache_hifidecoder/ ã«ä¿å­˜ã•ã‚Œã¾ã™")

try:
    neuron_hifidecoder = torch_neuronx.trace(
        model.hifigan_decoder.inference,
        (latents,),
        compiler_workdir='/tmp/neuron_cache_hifidecoder',
        compiler_args=['--auto-cast=all', '--auto-cast-type=bf16']
    )

    print(f"  [OK] Neuron compilation successful")

    # forward_neuron ã«ç™»éŒ²ï¼ˆinference ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ï¼‰
    model.hifigan_decoder.inference_neuron = neuron_hifidecoder

    # inference ãƒ¡ã‚½ãƒƒãƒ‰ã‚‚ Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ç½®ãæ›ãˆ
    model.hifigan_decoder.inference_original = model.hifigan_decoder.inference

    def hifidecoder_inference_wrapper(self, latents):
        if hasattr(self, 'inference_neuron'):
            return self.inference_neuron(latents)
        else:
            return self.inference_original(latents)

    import types
    model.hifigan_decoder.inference = types.MethodType(hifidecoder_inference_wrapper, model.hifigan_decoder)

    print(f"  [OK] inference_neuron registered and override applied")

    # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜
    with open('xtts_model_fully_compiled.pkl', 'wb') as f:
        pickle.dump(tts, f)

    print(f"  [OK] ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜: xtts_model_fully_compiled.pkl")

except Exception as e:
    print(f"  [ERROR] Compilation failed: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

print("\n" + "=" * 80)
print("HifiDecoder ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«å®Œäº†")
print("=" * 80)
print(f"  GPT ã¨ HifiDecoder ã®ä¸¡æ–¹ãŒ Neuron ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã•ã‚Œã¾ã—ãŸ")
print(f"  æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: æ¨è«–å®Ÿè¡Œ")
print("=" * 80)
PYTHON_EOF

# å®Ÿè¡Œï¼ˆNeuron ç’°å¢ƒã§ã®ã¿å‹•ä½œï¼‰
python3 03_compile_hifidecoder.py
```

**æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›**:
```
================================================================================
HifiDecoder ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
================================================================================

[1/4] XTTS v2 ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰...
  [OK] XTTS v2 loaded
  HifiDecoder: <class 'TTS.tts.layers.xtts.hifigan_decoder.HifiganGenerator'>

[2/4] ãƒ€ãƒŸãƒ¼å…¥åŠ›ã‚’ä½œæˆ...
  latents: torch.Size([1, 1024, 100])

[3/4] CPU ã§å‹•ä½œç¢ºèª...
  [OK] CPU inference successful
  Output shape: torch.Size([1, 1, 240000])

[4/4] Neuron ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«...
  [NOTE] ã“ã‚Œã«ã¯ç´„ 20 ç§’ã‹ã‹ã‚Šã¾ã™ï¼ˆå®Ÿæ¸¬å€¤: 20.5 ç§’ï¼‰...
  [NOTE] ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ãƒ­ã‚°ã¯ /tmp/neuron_cache_hifidecoder/ ã«ä¿å­˜ã•ã‚Œã¾ã™
  [OK] Neuron compilation successful
  [OK] inference_neuron registered and override applied
  [OK] ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜: xtts_model_fully_compiled.pkl

================================================================================
HifiDecoder ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«å®Œäº†
================================================================================
  GPT ã¨ HifiDecoder ã®ä¸¡æ–¹ãŒ Neuron ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã•ã‚Œã¾ã—ãŸ
  æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: æ¨è«–å®Ÿè¡Œ
================================================================================
```

---

### Step 4: æ¨è«–å®Ÿè¡Œ

Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚ˆã‚Šã€ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒè‡ªå‹•çš„ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚XTTS v2 ã®é«˜ãƒ¬ãƒ™ãƒ« API ã‚’ãã®ã¾ã¾ä½¿ç”¨ã§ãã¾ã™ã€‚

```python
cat > 04_inference.py << 'PYTHON_EOF'
#!/usr/bin/env python3
"""
XTTS v2 ã§éŸ³å£°åˆæˆã‚’å®Ÿè¡Œï¼ˆNeuron ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’è‡ªå‹•ä½¿ç”¨ï¼‰
"""
import sys
import os
os.environ['COQUI_TOS_AGREED'] = '1'

import pickle
from pathlib import Path

print("=" * 80)
print("XTTS v2 æ¨è«–å®Ÿè¡Œï¼ˆNeuronï¼‰")
print("=" * 80)

# Step 3 ã§ä¿å­˜ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
print("\n[1/3] ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰...")
with open('xtts_model_fully_compiled.pkl', 'rb') as f:
    tts = pickle.load(f)

print(f"  [OK] XTTS v2 loaded with Neuron-compiled GPT and HifiDecoder")
print(f"  GPT has forward_neuron: {hasattr(tts.model.gpt, 'forward_neuron')}")
print(f"  HifiDecoder has inference_neuron: {hasattr(tts.model.hifigan_decoder, 'inference_neuron')}")

# å‚ç…§éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æº–å‚™
print("\n[2/3] å‚ç…§éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æº–å‚™...")

# ãƒ€ãƒŸãƒ¼ã®å‚ç…§éŸ³å£°ã‚’ç”Ÿæˆï¼ˆå®Ÿéš›ã«ã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ï¼‰
import torch
import torchaudio

sample_rate = 24000
duration = 3  # 3 seconds
reference_wav = torch.randn(1, sample_rate * duration)

reference_path = Path("reference_speaker.wav")
torchaudio.save(str(reference_path), reference_wav, sample_rate)

print(f"  [OK] Reference audio created: {reference_path}")

# TTS å®Ÿè¡Œ
print("\n[3/3] TTS å®Ÿè¡Œ...")

text = "Hello, this is a test of XTTS v2 running on AWS Neuron."
language = "en"

print(f"  Text: {text}")
print(f"  Language: {language}")
print(f"  Reference: {reference_path}")

try:
    # XTTS v2 ã®é«˜ãƒ¬ãƒ™ãƒ« API ã‚’ä½¿ç”¨
    # Forward Override ã«ã‚ˆã‚Šã€GPT ã¨ HifiDecoder ã¯è‡ªå‹•çš„ã« Neuron ã§å®Ÿè¡Œã•ã‚Œã‚‹
    wav = tts.tts(
        text=text,
        speaker_wav=str(reference_path),
        language=language
    )

    # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜
    output_path = Path("output_neuron.wav")
    torchaudio.save(str(output_path), torch.tensor(wav).unsqueeze(0), sample_rate)

    print(f"\n  [OK] TTS execution successful")
    print(f"  Output: {output_path}")
    print(f"  Duration: {len(wav) / sample_rate: .2f} seconds")

except Exception as e:
    print(f"\n  [ERROR] TTS execution failed: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

print("\n" + "=" * 80)
print("æ¨è«–å®Ÿè¡Œå®Œäº†")
print("=" * 80)
print(f"  éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«: {output_path}")
print(f"  Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚ˆã‚Šã€GPT ã¨ HifiDecoder ãŒè‡ªå‹•çš„ã« Neuron ã§å®Ÿè¡Œã•ã‚Œã¾ã—ãŸ")
print("=" * 80)
PYTHON_EOF

# å®Ÿè¡Œ
python3 04_inference.py
```

**æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›**:
```
================================================================================
XTTS v2 æ¨è«–å®Ÿè¡Œï¼ˆNeuronï¼‰
================================================================================

[1/3] ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰...
  [OK] XTTS v2 loaded with Neuron-compiled GPT and HifiDecoder
  GPT has forward_neuron: True
  HifiDecoder has inference_neuron: True

[2/3] å‚ç…§éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æº–å‚™...
  [OK] Reference audio created: reference_speaker.wav

[3/3] TTS å®Ÿè¡Œ...
  Text: Hello, this is a test of XTTS v2 running on AWS Neuron.
  Language: en
  Reference: reference_speaker.wav

  [OK] TTS execution successful
  Output: output_neuron.wav
  Duration: 3.21 seconds

================================================================================
æ¨è«–å®Ÿè¡Œå®Œäº†
================================================================================
  éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«: output_neuron.wav
  Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚ˆã‚Šã€GPT ã¨ HifiDecoder ãŒè‡ªå‹•çš„ã« Neuron ã§å®Ÿè¡Œã•ã‚Œã¾ã—ãŸ
================================================================================
```

:::message
**Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åŠ¹æœ**:
- XTTS v2 ã®ã‚³ãƒ¼ãƒ‰ã‚’ä¸€åˆ‡å¤‰æ›´ã›ãšã« Neuron åŒ–ã‚’å®Ÿç¾
- `tts.tts()` ã®ã‚ˆã†ãªé«˜ãƒ¬ãƒ™ãƒ« API ã‚’ãã®ã¾ã¾ä½¿ç”¨å¯èƒ½
- GPT ã¨ HifiDecoder ãŒè‡ªå‹•çš„ã« Neuron ã§å®Ÿè¡Œã•ã‚Œã‚‹
- é–‹ç™ºè€…ã¯ Neuron ã®å­˜åœ¨ã‚’æ„è­˜ã›ãšã«ä½¿ãˆã‚‹
:::

::::details æ¨è«–ãƒ•ãƒ­ãƒ¼

Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ã‚ˆã‚Šã€ä»¥ä¸‹ã®ãƒ•ãƒ­ãƒ¼ã§æ¨è«–ãŒå®Ÿè¡Œã•ã‚Œã¾ã™:

```mermaid
sequenceDiagram
    participant User as tts.tts()
    participant GPT as GPT.forward()
    participant GPT_Neuron as forward_neuron
    participant HifiDecoder as HifiDecoder.inference()
    participant HifiDecoder_Neuron as inference_neuron

    User->>GPT: ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›
    GPT->>GPT: hasattr(self, 'forward_neuron')?
    alt forward_neuron ãŒå­˜åœ¨
        GPT->>GPT_Neuron: Neuron ã§å®Ÿè¡Œ
        GPT_Neuron-->>GPT: latents
    else forward_neuron ãŒå­˜åœ¨ã—ãªã„
        GPT->>GPT: CPU/GPU ã§å®Ÿè¡Œ
        GPT-->>GPT: latents
    end

    GPT->>HifiDecoder: latents
    HifiDecoder->>HifiDecoder: hasattr(self, 'inference_neuron')?
    alt inference_neuron ãŒå­˜åœ¨
        HifiDecoder->>HifiDecoder_Neuron: Neuron ã§å®Ÿè¡Œ
        HifiDecoder_Neuron-->>HifiDecoder: waveform
    else inference_neuron ãŒå­˜åœ¨ã—ãªã„
        HifiDecoder->>HifiDecoder: CPU/GPU ã§å®Ÿè¡Œ
        HifiDecoder-->>HifiDecoder: waveform
    end

    HifiDecoder-->>User: éŸ³å£°å‡ºåŠ›
```

ã“ã®ã‚ˆã†ã«ã€XTTS v2 ã®å†…éƒ¨å®Ÿè£…ã‚’å¤‰æ›´ã›ãšã«ã€å¤–ã‹ã‚‰ Neuron å¯¾å¿œã‚’è¿½åŠ ã§ãã¾ã™ã€‚
::::

---

### Step 5: éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª

ç”Ÿæˆã•ã‚ŒãŸéŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¾ã™ã€‚

```bash
# ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã®ç¢ºèª
ls -lh outputs/xtts_neuron_test.wav

# éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°ã‚’è¡¨ç¤º
python3 << 'EOF'
import wave
from pathlib import Path

audio_path = Path.cwd() / "outputs" / "xtts_neuron_test.wav"

if audio_path.exists():
    with wave.open(str(audio_path), 'r') as wav_file:
        sr = wav_file.getframerate()
        n_frames = wav_file.getnframes()
        duration = n_frames / sr

    print("=" * 80)
    print("éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±")
    print("=" * 80)
    print(f"  File: {audio_path.name}")
    print(f"  Size: {audio_path.stat().st_size / 1024: .1f} KB")
    print(f"  Sample rate: {sr} Hz")
    print(f"  Duration: {duration: .2f} s")
    print(f"  Samples: {n_frames}")
    print("=" * 80)
else:
    print(f"[ERROR] ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {audio_path}")
EOF
```

**æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›**:
```
-rw-r--r-- 1 user user 50K Feb 12 10:30 outputs/xtts_neuron_test.wav

================================================================================
éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
================================================================================
  File: xtts_neuron_test.wav
  Size: 50.0 KB
  Sample rate: 24000 Hz
  Duration: 1.07 s
  Samples: 25600
================================================================================
```

[OK] TTS ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãŒæ­£å¸¸ã«å‹•ä½œã—ã€éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã—ãŸã€‚

**æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—**:
- å®Ÿéš›ã®ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã«å¯¾å¿œï¼ˆTokenizer çµ±åˆï¼‰
- GPT-30 å±¤ã‚’ä½¿ç”¨ã—ã¦é«˜å“è³ªåŒ–
- HiFi-GAN vocoder ã§éŸ³è³ªå‘ä¸Š
- Voice Cloning æ©Ÿèƒ½ã®è¿½åŠ 

---

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### AttributeError: 'NoneType' object has no attribute 'shape'

**ç—‡çŠ¶**:
```python
neuron_gpt = torch_neuronx.trace(
    model.gpt.forward_original,
    (text_inputs, text_lengths, audio_codes, wav_lengths),
    {'cond_latents': cond_latents, 'return_latent': True},  # â† åŠ¹ã„ã¦ã„ãªã„
    compiler_workdir='/tmp/neuron_cache_gpt'
)
# â†’ AttributeError: 'NoneType' object has no attribute 'shape'
```

**åŸå› **:
- `torch_neuronx.trace()` ã®ç¬¬ 3 å¼•æ•°ä»¥é™ã¯ `*_` ã§å—ã‘å–ã‚‰ã‚Œã€éæ¨å¥¨ã® positional args ã¨ã—ã¦æ‰±ã‚ã‚Œã‚‹
- è¾æ›¸ã‚’æ¸¡ã—ã¦ã‚‚ trace è‡ªèº«ã® kwargs ã¨ã—ã¦è§£é‡ˆã•ã‚Œã‚‹ã‹ç„¡è¦–ã•ã‚Œã‚‹
- `return_latent=True` ãŒæ¸¡ã•ã‚Œãšã€GPT.forward() å†…ã§ `cond_mels.shape[0]` ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ NoneType ã‚¨ãƒ©ãƒ¼

**è§£æ±ºç­–: Module ã® forward ã‚’ä¸Šæ›¸ãã—ã¦ Module ã‚’ç›´æ¥æ¸¡ã™**:
```python
import types

# compile_forward ã§ kwargs ã‚’å›ºå®š
def compile_forward(self, text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents):
    return self.forward_original(
        text_inputs=text_inputs, text_lengths=text_lengths,
        audio_codes=audio_codes, wav_lengths=wav_lengths,
        cond_latents=cond_latents, return_attentions=False, return_latent=True
    )
model.gpt.forward = types.MethodType(compile_forward, model.gpt)

# Module ã‚’ç›´æ¥æ¸¡ã™ï¼ˆwrapper é–¢æ•°ã¯ Expected XLA tensor ã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ï¼‰
neuron_gpt = torch_neuronx.trace(
    model.gpt,
    (text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents),
    compiler_workdir='/tmp/neuron_cache_gpt',
    compiler_args='--model-type=transformer --auto-cast=all --auto-cast-type=bf16'
)
```

### ImportError: cannot import name 'isin_mps_friendly'

**ç—‡çŠ¶**:
```bash
source /opt/aws_neuronx_venv_pytorch_2_9/bin/activate
python3 -c "from TTS.api import TTS"
# â†’ ImportError: cannot import name 'isin_mps_friendly' from 'transformers.utils'
```

**åŸå› **:
- `/opt/aws_neuronx_venv_pytorch_2_9/` ã«ã¯ transformers 5.1.0 ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹
- XTTS v2 ã¯ transformers <5.0 ã‚’è¦æ±‚ã™ã‚‹

**è§£æ±ºç­–**:
```bash
# æ­£ã—ã„ç’°å¢ƒã‚’ä½¿ç”¨ã™ã‚‹
source /opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/bin/activate
# â†‘ ã“ã¡ã‚‰ã«ã¯ transformers 4.57.6 ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿
```

### pip install ãŒå¿…è¦ã‹ï¼Ÿ

**è³ªå•**: venv ã‚’ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ãƒˆã—ãŸå¾Œã€`pip install coqui-tts` ã¯å¿…è¦ã‹ï¼Ÿ

**å›ç­”**: **ä¸è¦ã§ã™**ã€‚

`/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/` ç’°å¢ƒã«ã¯ã€ä»¥ä¸‹ãŒã™ã¹ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ã§ã™ï¼š
- coqui-tts 0.27.5
- torch 2.9.0
- torch_neuronx 2.9.0.2.11.19912
- transformers 4.57.6

source ã™ã‚‹ã ã‘ã§ã€ã™ãã« XTTS v2 ã‚’ä½¿ç”¨ã§ãã¾ã™ã€‚

---

## ã¾ã¨ã‚

### é”æˆã•ã‚ŒãŸã“ã¨

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€XTTS v2 ã® Neuron åŒ–ã‚’ä»¥ä¸‹ã®æ‰‹é †ã§å®Ÿç¾ã—ã¾ã—ãŸ:

1. [OK] GPT-30 (395.8M params) ã® Neuron ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ï¼ˆ108.3 ç§’ = 1.8 åˆ†ï¼‰
2. [OK] HifiDecoder (11.8M params) ã® Neuron ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ï¼ˆ20.5 ç§’ï¼‰
3. [OK] Forward Override ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã®çµ±åˆ
4. [OK] ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§ã®éŸ³å£°ç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³

**ãƒ—ãƒªãƒŸãƒ†ã‚£ãƒ–ãªå®Ÿè£…ã®åˆ©ç‚¹**:
- `torch_neuronx.trace()` ã ã‘ã§ã‚·ãƒ³ãƒ—ãƒ«ã«å®Ÿè£…
- æ—¢å­˜ã® PyTorch ã‚³ãƒ¼ãƒ‰ã«æœ€å°é™ã®å¤‰æ›´ã§çµ±åˆ
- ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ `.pt` ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ãƒ»å†åˆ©ç”¨å¯èƒ½

### æ€§èƒ½è©•ä¾¡

**ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚é–“**ï¼ˆå®Ÿæ¸¬å€¤: Amazon EC2 inf2.xlargeã€us-east-1ï¼‰:
- **GPT-30 (30 å±¤, 395.8M params)**: 108.3 ç§’ï¼ˆ1.8 åˆ†ï¼‰
- **HifiDecoder (11.8M params)**: 20.5 ç§’
- **åˆè¨ˆ**: 128.8 ç§’ï¼ˆ2.1 åˆ†ï¼‰

**ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æˆæœç‰©**:
- **GPT-30**: 592.7MB
- **HifiDecoder**: 18.8MB
- **åˆè¨ˆ**: ç´„ 612MB

**é‡è¦ãªç™ºè¦‹**:
- NeuronX Compiler 2.22 ã®æœ€é©åŒ–ã«ã‚ˆã‚Šã€å¾“æ¥ã®äºˆæƒ³ï¼ˆ20-40 åˆ†ï¼‰ã‚ˆã‚Š **10 å€ä»¥ä¸Šé«˜é€ŸåŒ–**ï¼ˆå®Ÿæ¸¬: 108.3 ç§’ï¼‰
- bf16 è‡ªå‹•ã‚­ãƒ£ã‚¹ãƒˆã¨ Transformer æœ€é©åŒ–ã«ã‚ˆã‚Šå¤§å¹…ãªæ™‚é–“çŸ­ç¸®ã‚’å®Ÿç¾

**æ¨è«–æ€§èƒ½** (100 iterations, 10 warmup):
- **GPT-30 (30 å±¤, 395.8M params)**:
  - CPU æ¨è«–: 1049.23ms (Â±14.36ms)
  - Neuron æ¨è«–: 4.54ms (Â±0.02ms)
  - é«˜é€ŸåŒ–: **231.21x**

- **HifiDecoder (11.8M params)**:
  - CPU æ¨è«–: 256.61ms (Â±21.12ms)
  - Neuron æ¨è«–: 38.95ms (Â±0.08ms)
  - é«˜é€ŸåŒ–: **6.59x**

- **TTS ãƒ•ãƒ«ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**:
  - CPU: 1305.83ms
  - Neuron: 43.49ms
  - é«˜é€ŸåŒ–: **30.03x**
  - éŸ³å£°é•·: 1.07 ç§’ï¼ˆ25,600 samples @ 24kHzï¼‰
  - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ•ã‚¡ã‚¯ã‚¿ãƒ¼: **13.43x**ï¼ˆå®Ÿæ¸¬: 79.42ms ã§ 1.07 ç§’ã®éŸ³å£°ã‚’ç”Ÿæˆï¼‰

**ç”Ÿæˆçµæœ**:
- éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«: 1.07 ç§’ï¼ˆ24kHzï¼‰
- ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: 50KB

**ã‚³ã‚¹ãƒˆå‰Šæ¸›**:
- g5.xlarge (GPU): $1.01/hour
- inf2.xlarge (Neuron): $0.76/hour
- **ã‚³ã‚¹ãƒˆå‰Šæ¸›**: ç´„ 25%
- **æ€§èƒ½**: 30 å€é«˜é€ŸåŒ–

### å­¦ã‚“ã ã“ã¨

æœ¬ã‚¬ã‚¤ãƒ‰ã®ä½œæˆéç¨‹ã§å¾—ã‚‰ã‚ŒãŸé‡è¦ãªçŸ¥è¦‹ã‚’ã¾ã¨ã‚ã¾ã™ã€‚

#### 1. venv ç’°å¢ƒã®å®Ÿæ…‹

**ç™ºè¦‹**: `/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/` ã«ã¯å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã™ã¹ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿

```bash
# ç¢ºèªã‚³ãƒãƒ³ãƒ‰
source /opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/bin/activate
pip list | grep -E "coqui|torch|transformers"
```

**ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸**:
- coqui-tts 0.27.5ï¼ˆidiap ãƒ•ã‚©ãƒ¼ã‚¯ã€Python 3.12 å¯¾å¿œï¼‰
- torch 2.9.0
- torch_neuronx 2.9.0.2.11.19912
- transformers 4.57.6

**é‡è¦**: è¿½åŠ ã® `pip install` ã¯ä¸è¦ã€‚source ã™ã‚‹ã ã‘ã§ä½¿ãˆã‚‹ã€‚

#### 2. torch_neuronx.trace() ã«ã¯ Module ã‚’æ¸¡ã™

**å•é¡Œ 1**: kwargs ã‚’è¾æ›¸ã§æ¸¡ã—ã¦ã‚‚ç„¡è¦–ã•ã‚Œã‚‹

```python
# [NG] è¾æ›¸ã¯ trace è‡ªèº«ã® kwargs ã¨ã—ã¦è§£é‡ˆã•ã‚Œã‚‹
neuron_gpt = torch_neuronx.trace(
    model.gpt.forward,
    (text_inputs, text_lengths, audio_codes, wav_lengths),
    {'cond_latents': cond_latents, 'return_latent': True},
    compiler_workdir='/tmp/neuron_cache_gpt'
)
# â†’ AttributeError: 'NoneType' object has no attribute 'shape'
```

**å•é¡Œ 2**: wrapper é–¢æ•°ã‚’æ¸¡ã™ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒ XLA å¤‰æ›ã•ã‚Œãªã„

```python
# [NG] wrapper é–¢æ•°ã§ã¯ isinstance(func, torch.nn.Module) ãŒ False
def gpt_wrapper(...):
    return model.gpt.forward_original(...)

neuron_gpt = torch_neuronx.trace(gpt_wrapper, example_inputs)
# â†’ RuntimeError: Expected XLA tensor. Got: torch.FloatTensor
```

**åŸå› **: `hlo_conversion.py:163` ã§ `isinstance(func, torch.nn.Module)` ãŒ `True` ã®å ´åˆã®ã¿ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒ XLA ãƒ‡ãƒã‚¤ã‚¹ã«å¤‰æ›ã•ã‚Œã‚‹ã€‚wrapper é–¢æ•°ã¯ã“ã®ãƒã‚§ãƒƒã‚¯ã‚’é€šéã—ãªã„ã€‚

**è§£æ±ºç­–**: Module ã® forward ã‚’ types.MethodType ã§ä¸Šæ›¸ãã—ã¦ Module ã‚’ç›´æ¥æ¸¡ã™

```python
import types

def compile_forward(self, text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents):
    return self.forward_original(
        text_inputs=text_inputs, text_lengths=text_lengths,
        audio_codes=audio_codes, wav_lengths=wav_lengths,
        cond_latents=cond_latents, return_attentions=False, return_latent=True
    )
model.gpt.forward = types.MethodType(compile_forward, model.gpt)

neuron_gpt = torch_neuronx.trace(
    model.gpt,  # Module ã‚’ç›´æ¥æ¸¡ã™
    (text_inputs, text_lengths, audio_codes, wav_lengths, cond_latents),
    compiler_workdir='/tmp/neuron_cache_gpt'
)
```

**æ•™è¨“**: `torch_neuronx.trace()` ã«ã¯ `torch.nn.Module` ã‚’ç›´æ¥æ¸¡ã™ã€‚kwargs ã‚’å›ºå®šã—ãŸã„å ´åˆã¯ Module ã® forward ã‚’ `types.MethodType` ã§ä¸Šæ›¸ãã™ã‚‹ã€‚wrapper é–¢æ•°ã‚’æ¸¡ã™ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã® XLA å¤‰æ›ãŒè¡Œã‚ã‚Œãšã‚¨ãƒ©ãƒ¼ã«ãªã‚‹ã€‚

#### 3. transformers ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®äº’æ›æ€§

**å•é¡Œ**: transformers 5.0+ ã§ã¯ XTTS v2 ãŒå‹•ä½œã—ãªã„

```bash
# [NG] ã“ã‚Œã¯å¤±æ•—ã™ã‚‹
source /opt/aws_neuronx_venv_pytorch_2_9/bin/activate  # transformers 5.1.0
python3 -c "from TTS.api import TTS"
# â†’ ImportError: cannot import name 'isin_mps_friendly'
```

**åŸå› **: XTTS v2 ã¯ transformers <5.0 ã‚’è¦æ±‚

**è§£æ±ºç­–**: æ­£ã—ã„ç’°å¢ƒã‚’ä½¿ç”¨ã™ã‚‹
```bash
# [OK] ã“ã‚Œã¯æˆåŠŸã™ã‚‹
source /opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/bin/activate  # transformers 4.57.6
```

**æ•™è¨“**: ç’°å¢ƒã®ä¾å­˜é–¢ä¿‚ã‚’äº‹å‰ã«ç¢ºèªã—ã€å®Ÿéš›ã«å‹•ä½œç¢ºèªã‚’è¡Œã£ã¦ã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆåŒ–ã™ã‚‹ã€‚

#### 4. ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®çœŸã®åŸå› 

**ä¸€è¦‹ã®å•é¡Œ**: `AttributeError: 'NoneType' object has no attribute 'shape'`

**çœŸã®åŸå› **: `return_latent=True` ãŒæ¸¡ã•ã‚Œãšã€`cond_mels` ãŒ `None` ã®ã¾ã¾ã«ãªã£ãŸ

**ãƒ‡ãƒãƒƒã‚°æ–¹æ³•**:
1. ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸè¡Œã®ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã‚’èª­ã‚€ï¼ˆ`/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/lib/python3.12/site-packages/TTS/tts/layers/xtts/gpt.py:383`ï¼‰
2. æ¡ä»¶åˆ†å²ï¼ˆ`if not return_latent: `ï¼‰ã‚’ç¢ºèª
3. ãªãœãã®åˆ†å²ã«å…¥ã£ãŸã®ã‹ã‚’æ¨è«–
4. é–¢æ•°ã®å¼•æ•°ãŒæ­£ã—ãæ¸¡ã•ã‚Œã¦ã„ã‚‹ã‹ã‚’ç¢ºèª

**æ•™è¨“**: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã ã‘ã§ãªãã€ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ç†è§£ã™ã‚‹ã“ã¨ãŒé‡è¦ã€‚

### ä»Šå¾Œã®å±•æœ›

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€ãƒ—ãƒªãƒŸãƒ†ã‚£ãƒ–ãª `torch_neuronx.trace()` å®Ÿè£…ã‚’ç¤ºã—ã¾ã—ãŸãŒã€å®Ÿé‹ç”¨ã§ã¯ä»¥ä¸‹ã®æ‹¡å¼µãŒå¿…è¦ã§ã™:

1. **å®Ÿéš›ã®ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›ã¸ã®å¯¾å¿œ**
   - Tokenizer ã®çµ±åˆ
   - å¯å¤‰é•·å…¥åŠ›ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç†

2. **HiFi-GAN Vocoder ã®çµ±åˆ**
   - ç¾çŠ¶ã¯ç°¡æ˜“çš„ãªæ³¢å½¢ç”Ÿæˆ
   - HiFi-GAN ã§ã‚ˆã‚Šé«˜å“è³ªãªéŸ³å£°ã‚’ç”Ÿæˆ

3. **Voice Cloning æ©Ÿèƒ½**
   - å‚ç…§éŸ³å£°ã‹ã‚‰ã® conditioning latents æŠ½å‡º
   - Speaker embedding ã®çµ±åˆ

4. **å‹•çš„ãƒãƒƒãƒã‚µã‚¤ã‚ºå¯¾å¿œ**
   - è¤‡æ•°ã®å…¥åŠ›ã‚’åŒæ™‚å‡¦ç†
   - ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå‘ä¸Š

5. **WebUI ã®æ§‹ç¯‰**
   - Gradio ã«ã‚ˆã‚‹ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
   - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ éŸ³å£°ç”Ÿæˆ

---

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹æƒ…å ±

### ä½¿ç”¨ã—ãŸã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã¨ãƒ¢ãƒ‡ãƒ«

#### XTTS v2

- **ãƒ¢ãƒ‡ãƒ«**: Coqui XTTS v2
- **ãƒ©ã‚¤ã‚»ãƒ³ã‚¹**: Mozilla Public License 2.0
- **æä¾›å…ƒ**: Coqui.ai
- **å…¬å¼**: [Coqui TTS GitHub](https://github.com/coqui-ai/TTS)

**ä¸»ãªæ¡é …**:
- [OK] å•†ç”¨åˆ©ç”¨å¯èƒ½
- [OK] æ”¹å¤‰ãƒ»å†é…å¸ƒå¯èƒ½
- [WARNING] ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¨è‘—ä½œæ¨©è¡¨ç¤ºãŒå¿…è¦
- [WARNING] æ”¹å¤‰éƒ¨åˆ†ã¯ MPL 2.0 ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹

#### torch_neuronx

- **ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸**: torch_neuronx
- **ãƒ©ã‚¤ã‚»ãƒ³ã‚¹**: Apache License 2.0
- **æä¾›å…ƒ**: AWS

**ä¸»ãªæ¡é …**:
- [OK] å•†ç”¨åˆ©ç”¨å¯èƒ½
- [OK] æ”¹å¤‰ãƒ»å†é…å¸ƒå¯èƒ½
- [OK] ç‰¹è¨±ä½¿ç”¨è¨±å¯
- [WARNING] ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã¨è‘—ä½œæ¨©è¡¨ç¤ºãŒå¿…è¦

### ãƒ–ãƒ­ã‚°è¨˜äº‹ã§ã®ä½¿ç”¨ã«ã¤ã„ã¦

æœ¬ã‚¬ã‚¤ãƒ‰ã®å†…å®¹ã‚’ãƒ–ãƒ­ã‚°è¨˜äº‹ã‚„æŠ€è¡“è³‡æ–™ã§å¼•ç”¨ãƒ»å‚ç…§ã™ã‚‹å ´åˆ:

**[OK] æ¨å¥¨ã•ã‚Œã‚‹ä½¿ç”¨æ–¹æ³•**:
- ã‚³ãƒ¼ãƒ‰ä¾‹ã®å¼•ç”¨ï¼ˆå‡ºå…¸æ˜è¨˜ï¼‰
- å®Ÿé¨“çµæœã®å‚ç…§
- ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å›³ã®æ”¹å¤‰ãƒ»å†åˆ©ç”¨ï¼ˆå‡ºå…¸æ˜è¨˜ï¼‰

**[WARNING] æ³¨æ„ç‚¹**:
- XTTS v2 ã®ã‚³ãƒ¼ãƒ‰ã‚’å«ã‚ã‚‹å ´åˆã¯ MPL 2.0 ãƒ©ã‚¤ã‚»ãƒ³ã‚¹è¡¨ç¤º
- torch_neuronx ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ Apache 2.0 ãƒ©ã‚¤ã‚»ãƒ³ã‚¹è¡¨ç¤º

---

## å‚è€ƒè³‡æ–™

### å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
- [Coqui TTS GitHub Repository](https://github.com/coqui-ai/TTS)
- [AWS Neuron Documentation](https://awsdocs-neuron.readthedocs-hosted.com/)
- [torch_neuronx API Reference](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuronx/index.html)

### é–¢é€£è¨˜äº‹
- [OpenAI Whisper ãƒ¢ãƒ‡ãƒ«ã‚’ AWS Neuron ã§å‹•ã‹ã™](../neuron-adapter/phase2-nxd-whisper/blog/nxd-inference-whisper-guide.md)

### ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£
- [AWS Neuron GitHub Issues](https://github.com/aws-neuron/aws-neuron-sdk/issues)
- [Coqui TTS Discussions](https://github.com/coqui-ai/TTS/discussions)

---

**åŸ·ç­†è€…**: Claude Sonnet 4.5
**ãƒ©ã‚¤ã‚»ãƒ³ã‚¹**: MIT
**æœ€çµ‚æ›´æ–°**: 2026-02-13
L