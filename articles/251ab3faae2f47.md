---
title: "Verl の概要調査と v0.7 リリースノート解説"
emoji: "🔨"
type: "tech"
topics: ["RLHF", "GRPO", "DPO", "Verl", "SAPO"]
published: false
---

## はじめに

:::message
この記事は、Verl の概要と [v0.7 のリリースノート](https://verl.readthedocs.io/en/latest/blog/v0.7.html)を ML インフラストラクチャ視点で整理・解説したものです。重要なアップデートに焦点を当てて整理します。強化学習の理論面は専門外ですが、インフラ設計の観点から整理してみます。リリースノートのブログ自体が非常にわかりやすいのでそちらも併せて読むことをお勧めします。
:::

Verl は、GRPO、DPO、CISPO、SAPO などの強化学習ベースの LLM ポストトレーニング手法を大規模クラスタで実行するためのフレームワークです。本記事では v0.7 の重要なアップデートに絞って解説します。

前提知識として強化学習アルゴリズムの基礎は理解しているものとします。以下はこれまでの強化学習の背景や基礎的な内容を把握する上で非常にわかりやすかったです。

https://note.com/olachin/n/n9706c13c8678

## Generation/Training フェーズ

全てのアルゴリズムに共通するかは把握できていませんが強化学習ベースのポストトレーニング手法には特徴があります。これらの手法では、**推論（Generation Phase）が訓練時間の約 80% を占める**ことです。つまり、コストにおいても Generation phase を無視することはできません。Generation/Training でリソースをうまくシェアする、フェーズに応じてアロケーションの仕方を変える、別の計算リソースを利用する、といった視点が重要になってくると認識しています。

::::details Genration phase のコスト

https://github.com/OpenRLHF/OpenRLHF

https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-chat

:::message alert
**パフォーマンス特性の分析**:

[OpenRLHF の公式データ](https://github.com/OpenRLHF/OpenRLHF)によると、RLHF 訓練では Generation が**全体時間の約 80%** を占めます（"RLHF training spends 80% of the time on sample generation"）。

[DeepSpeed-Chat の分析](https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-chat/README.md)も、**FLOPs** では Generation が 20%、Training が 80% ですが、Generation は各トークン生成ごとにモデル重み（例: 70B パラメータ × 2 bytes = 約 140 GB）をメモリから読み込む必要があり、**メモリバウンド**で GPU の計算性能を活かせません。一方、Training は Forward + Backward Pass で大量の演算を実行し、**計算バウンド**で GPU をフル活用できます。この違いにより、**実時間では Generation が大きな割合を占めます**（"can take a large portion of the e2e time"）。
:::

**コストへの影響**:

同じ GPU を使用する場合、実行時間がそのままコストに比例するため、**Generation Phase がコストの大部分を占めます**。OpenRLHF のデータ（80%）を参考にすると、例えば 8 GPU で $100/h/GPU の場合、総コスト: $800/h、Generation Phase: 約 $640/h（80%）、Training Phase: 約 $160/h（20%）となります。

これが、Verl v0.7 で推論エンジンの最適化（Rollout Engine の server mode 移行、vLLM/SGLang 統合）が重視される理由です。**推論の最適化こそが、RLHF/GRPO におけるコスト削減の要点**であり、Verl の設計思想もこの特性を深く理解した上で構築されています。
::::

### GRPO のフェーズ

GRPO の訓練プロセスは、**Generation Phase** と **Training Phase** の 2 つのフェーズから構成されます。[Verl の公式ドキュメント](https://verl.readthedocs.io/en/latest/algo/grpo.html)によると、GRPO は各プロンプトに対して複数の応答を生成し、それらの報酬を相対的に評価することで、Critic ネットワークなしでポリシーを更新します。

::::details GRPO の処理フロー
```mermaid
sequenceDiagram
    participant Controller as Controller<br/>(Single-Controller)
    participant ActorRollout as ActorRolloutRef Worker<br/>(Actor + vLLM/SGLang)
    participant Reward as Reward Worker<br/>(Reward Model)

    Note over Controller,Reward: Generation Phase（推論が支配的、全体の約 80%）

    rect rgb(255, 220, 220)
        Controller->>ActorRollout: プロンプト送信<br/>(train_batch_size=32)
        Note over ActorRollout: vLLM/SGLang で推論<br/>rollout.n=8（8 個の応答/プロンプト）<br/>合計 256 個の応答を生成
        ActorRollout-->>Controller: 生成された応答<br/>(256 sequences)

        Controller->>ActorRollout: Reference ログ確率計算
        Note over ActorRollout: Reference Model で<br/>KL divergence 用の<br/>ログ確率を計算
        ActorRollout-->>Controller: Reference log probs

        Controller->>Reward: 報酬計算リクエスト
        Note over Reward: 各応答にスコアを付与<br/>（Function-based or Model-based）
        Reward-->>Controller: 報酬スコア (256 values)
    end

    Note over Controller,Reward: Training Phase（学習、全体の約 20%）

    rect rgb(200, 255, 200)
        Note over Controller: Advantage 計算<br/>group_baseline = mean(rewards)<br/>advantages = rewards - baseline

        Controller->>ActorRollout: Forward Pass リクエスト<br/>(生成済み応答で確率再計算)
        Note over ActorRollout: Actor Model の Forward Pass<br/>（FSDP による分散訓練）<br/>Teacher Forcing で並列処理
        ActorRollout-->>Controller: 新しいログ確率

        Note over Controller: Loss 計算<br/>PPO Clipping + KL Loss

        Controller->>ActorRollout: Backward Pass + Update
        Note over ActorRollout: Gradient 計算<br/>Parameter Update
        ActorRollout-->>Controller: 更新完了
    end
```
::::

**Generation Phase** では、各プロンプトに対して複数の応答を生成し、**ActorRolloutRefWorker** が Actor、Rollout（推論エンジン）、Reference Model を統合しており、以下の処理を実行します。1. 各プロンプトに対して `rollout.n` 個の応答を生成します。**v0.7 では server mode に移行**し、vLLM/SGLang などの最新の推論バックエンドと統合され、デフォルトでオフラインではなくオンラインサービスとして動作してマルチターンの会話のスループット効率が向上します。2. 訓練開始時の初期ポリシー（Reference Model）を使用して、生成された応答ログ確率を計算します。これは KL divergence 計算に使用され、ポリシーの急激な変化を防ぎます。3. RewardManager が各応答にスコアを付与します。Function-based（ルールベース）または Model-based（学習済みモデル）の報酬計算がサポートされています。

:::message
Generation Phase は自己回帰生成のため逐次処理が必要です。前述の通り、この Phase が訓練時間の大半を占めるようです。v0.7 の server mode 移行により vLLM などとの統合が図られました。
:::

:::message
**vLLM と統合されたことでコスト影響の大きい Generation phase が疎結合化され、vLLM 側の推論技術の重要性がポストトレーニング側でも重要になってきていると感じました。AWS Neuron 推しの私としては vLLM が AWS Neuron に対応していることから Trainium などを利用することで Generation フェーズをお安くできないか、今後調査してみたいです。**
:::

**Training Phase** では、生成された応答と報酬を使用して Actor モデルのポリシーを更新します。アルゴリズムの解説がメインではないので詳細は割愛します。Training Phase は Forward + Backward Pass を含みますが、生成済みの完全なシーケンスに対して並列処理が可能なため、Generation Phase より高速です。

:::message alert
このように、Generation Phase はメモリバウンド・逐次処理、Training Phase は計算バウンド・並列処理、であり、異なる特性を持ちます。これら 2 つのフェーズを効率的に管理するインフラが、GRPO のようなポストトレーニング手法では重要です。
:::

## Verl のアーキテクチャ概要

> [公式ドキュメント](https://github.com/verl-project/verl?tab=readme-ov-file)より

![画像の説明](https://raw.githubusercontent.com/verl-project/verl-data/refs/heads/main/images/verl-arch.png)

### Hybrid-Controller アーキテクチャ -- HybridFlow

Verl は **Hybrid-Controller アーキテクチャ（HybridFlow）** を採用しています。このアーキテクチャは、Google Pathways などの非同期シャーディングデータフローシステムと設計原理を共有しており、PPO、GRPO、DAPO などの強化学習アルゴリズムを、**マルチステージ、マルチモデル、並列化可能なデータフローグラフ**としてモデル化します。

::::details GRPO のフローグラフ例

### マルチステージ（3 段階）
1. **Generation Stage**: Rollout Engine が複数の応答を生成
2. **Reward Stage**: Reward Model が各応答にスコアを付与
3. **Training Stage**: Actor Model がポリシーを更新

### マルチモデル（4 つのモデル）
- Actor Model（訓練対象）
- Rollout Engine（推論）
- Reference Model（KL 制約）
- Reward Model（評価）

### GRPO のデータフローグラフ

:::message
各ステージを独立した処理単位として扱うことで、複数バッチを同時処理できます。各ステージで異なる複数のモデルを処理することができます。
:::

```mermaid
graph TB
    subgraph "Stage 1: Generation"
        Prompt[プロンプト入力]
        Rollout["[Model 1]<br/>Rollout Engine<br/>推論専用"]
        Ref["[Model 2]<br/>Reference Model<br/>KL 制約用"]
        Prompt --> Rollout
        Rollout --> Responses[複数応答生成<br/>group_size=8]
        Ref --> LogProbs[Reference Log Probs<br/>KL 制約計算用]
    end

    subgraph "Stage 2: Reward"
        RewardModel["[Model 3]<br/>Reward Model<br/>評価"]
        Responses --> RewardModel
        RewardModel --> Scores[各応答にスコア付与]
    end

    subgraph "Stage 3: Training"
        Actor["[Model 4]<br/>Actor Model<br/>訓練対象"]
        Scores --> Advantage[Advantage 計算]
        LogProbs --> Advantage
        Advantage --> Actor
        Actor --> Updated[ポリシー更新]
    end

    style Rollout fill: #4fc3f7,stroke: #0277bd,stroke-width:3px
    style Ref fill: #ffb74d,stroke: #e65100,stroke-width:3px
    style RewardModel fill: #ba68c8,stroke: #6a1b9a,stroke-width:3px
    style Actor fill: #66bb6a,stroke: #2e7d32,stroke-width:3px
```
::::

柔軟性とパフォーマンスのバランスをとるために、Verl は **2 つの異なる並列計算モデル**を統合しています。

**高レベル Single-Controller**（MPMD: Multiple Program Multiple Data）では、異なるコンポーネント（Actor、Rollout、Critic、Reward）が異なるプログラムを実行し、Controller が Ray のような分散フレームワークを介して協調動作を制御します。オーケストレーションレベルでは、**単一プロセスの `RLTrainer` がグローバル計算グラフを管理**して Rollout のスケジューリングやジョブディスパッチなどを実施します。

**内部 Multi-Controller**（SPMD: Single Program Multiple Data）では、各コンポーネント（Model Engine / Rollout Engine）は、**内部的に標準的な分散訓練/推論モードで動作**します。全 Worker が同一のプログラムを実行し、Collective Communication を介して同期しながら、大量の分散計算を実行します。


| 計算モデル | Controller | 具体の処理例 | エンジン | 説明 |
|----------|--------|------------|---------|------|
| MPMD | Single | Rollout スケジューリング | RLTrainer | 異なるコンポーネントが異なるプログラムを実行。Ray で協調動作を制御 |
| | | Reward スコアリング | | |
| | | 訓練ジョブディスパッチ | | |
| | | Experience Buffer 管理 | | |
| SPMD | Multi | Model Engine | FSDP、Megatron etc | 全 Worker が同一プログラムを実行し、Collective Communication で同期 |
| | | Rollout Engine | vLLM、SGLang | |

## Verl の 4 つの Core コンポーネント

```mermaid
graph TB
    subgraph "訓練側"
        ModelEngine["Model Engine<br/>━━━━━━━━━━━<br/>訓練バックボーン<br/>FSDP/Megatron-Core..<br/><br/>*v0.7 変更* <br/>FP8 訓練<br/>VLM 対応<br/>LoRA サポート"]
    end

    subgraph "推論側"
        RolloutEngine["Rollout Engine<br/>━━━━━━━━━━━<br/>推論エンジン<br/>vLLM/SGLang..<br/><br/>*v0.7 変更* <br/>SPMD → server mode 移行<br/>FP8 量子化"]
    end

    subgraph "データ転送コンポーネント"
        CheckpointEngine["Checkpoint Engine<br/>━━━━━━━━━━━<br/>重み同期エンジン<br/><br/>*v0.7 変更* <br/>NCCL/NIXL バックエンド統合<br/>ストリーミング転送"]

        TransferQueue["TransferQueue<br/>━━━━━━━━━━━<br/>ゼロコピーテンソル転送<br/><br/>*v0.7 変更* <br/>シリアライズ最適化<br/>マルチパーティション対応"]
    end

    ModelEngine -->|更新された重み| CheckpointEngine
    CheckpointEngine -->|重み同期<br/>GPU→GPU| RolloutEngine

    RolloutEngine -->|生成データ<br/>Experience Buffer| TransferQueue
    TransferQueue -->|テンソル転送<br/>ゼロコピー/RDMA| ModelEngine

    style ModelEngine fill: #66bb6a,stroke: #2e7d32,stroke-width:3px
    style RolloutEngine fill: #4fc3f7,stroke: #0277bd,stroke-width:3px
    style CheckpointEngine fill: #ba68c8,stroke: #6a1b9a,stroke-width:3px
    style TransferQueue fill: #ffb74d,stroke: #e65100,stroke-width:3px
```

この 4 つのコンポーネントは、Fully async パイプラインの中核を担っています。Model Engine（訓練側）と Rollout Engine（推論側）が完全に分離され、Checkpoint Engine が重みの同期を、TransferQueue が Experience Buffer の転送をそれぞれ効率的に処理します。v0.7 のリリースノートでは、これらのコンポーネントに対する多数の改良が報告されています。

### Model Engine の概要

Model Engine は、プラグイン可能なバックエンドをサポートする**抽象インターフェース**を定義しています。すべてのバックエンドは SPMD モードで動作し、以下の統一インターフェースを実装します。

```python
class BaseEngine:
    def initialize(self):
        """モデル、オプティマイザ、学習率スケジューラをインスタンス化または読み込み"""
        raise NotImplementedError

    def optimizer_zero_grad(self):
        """オプティマイザの勾配をゼロにリセット"""
        raise NotImplementedError

    def optimizer_step(self):
        """オプティマイザを使用して最適化ステップを実行"""
        raise NotImplementedError

    def lr_scheduler_step(self):
        """学習率スケジューラを 1 ステップ進める"""
        raise NotImplementedError

    def forward_backward_batch(self, data: TensorDict, loss_function: Callable, forward_only=False) -> Any:
        """データバッチに対して Forward Pass を実行し、オプションで Backward Pass も実行"""
        raise NotImplementedError

    def get_per_tensor_param(self) -> tuple[Generator[tuple[str, torch.Tensor], None, None], Optional[dict]]:
        """テンソル単位のパラメータを yield する Generator と、オプションの PEFT 設定を取得"""
        raise NotImplementedError

    def to(self, device: str, model: bool = True, optimizer: bool = True, grad: bool = True):
        """モデルパラメータ、オプティマイザ状態、勾配を指定デバイスに移動"""
        raise NotImplementedError
```

:::message
**重要な設計原則**: 新しい訓練エンジンを統合するには、これらのインターフェースを継承して実装するだけで済みます。**呼び出し側のコードは一切変更不要**です。
:::

**サポートされているバックエンド**

並列化戦略の記号の意味は[こちら](https://ailzhang.github.io/posts/distributed-compute-in-transformer/)を参照。

| バックエンド | 並列化戦略 | パフォーマンス | サポート範囲 | 統合時間 |
|------------|-----------|--------------|------------|----------|
| FSDP | FSDP+SP | Dense/MoE 低 | 全 Transformer | Day 0 |
| Megatron-Core | DP+TP+PP+EP+CP | 高 | 特定モデル | 数週間〜数ヶ月 |
| VeOmni | FSDP+SP+EP | 中 | VLM/MoE | 約 1 週間 |

この抽象化により、**ユーザーは RL アルゴリズムの実装に集中でき、分散訓練の詳細に煩わされることがありません**。

### Rollout Engine の概要

> リリースノートブログより

![](https://raw.githubusercontent.com/wuxibin89/verl/refs/heads/wuxibin/doc_images/docs/_static/rollout_engine.png)

v0.7 では、レガシーの SPMD rollout モードが削除され、**server mode がデフォルト**になりました。server mode は、オフライン・バッチ推論ではなく、**オンライン・サービングとして動作**します。これによって、vLLM、SGLang などのバックエンドがサポートされ、動的バッチングやその他の推論エンジンの恩恵を受けることが可能になります。

:::message alert
Generation phase は基本的にはメモリバウンドな処理であり、メモリの集積度に対するコスト効率の良い trn2 インスタンスを vllm-neruon 経由で統合できるとコストを下げられる可能性があるため今後 Verl と vllm-neuron を組み合わせて利用する実験をしてみたいと思います。
:::

深く触れませんが AgentLoop コンポーネントによって単一ターン応答だけではなくツール呼び出しを含む複数ターン対話に対する対応が図られているようです。

### TransferQueue の概要

![](https://github.com/wuxibin89/verl/blob/wuxibin/doc_images/docs/_static/transfer_queue.png?raw=true)

TransferQueue は、**制御フローとデータフローを分離**し、大量データ転送時のボトルネックを解決します。

**従来の Ray 方式の問題点**: Ray の `ray.put()`/`ray.get()` は Python pickle でオブジェクトをシリアライズし、Ray Object Store（共有メモリ）に格納します。PyTorch テンソルを転送する場合、GPU → CPU → pickle シリアライズ → Object Store → pickle デシリアライズ → CPU → GPU という多段コピーが発生し、メモリ帯域幅を無駄に消費します。

**TransferQueue の解決策**: RLTrainer は指示とメタデータ（テンソルのサイズや型情報）のみを Ray 経由で送信し、実際のテンソルデータは別の専用経路で転送します。PyTorch テンソルのメモリバッファ（`torch.Tensor.data_ptr()` が指すメモリ領域）を直接扱うことで、pickle によるシリアライゼーションを完全に回避します。バックエンドとして ZeroMQ、NIXL、Ray RDT が利用可能で、RDMA を使用すると GPU メモリを CPU を介さずにノード間で直接転送できます（AWS EFA 経由など）。v0.8 ではデフォルトの転送メソッドになる予定のようです。

NIXL と Ray については以下で軽く解説しているので確認ください。

https://zenn.dev/tosshi/articles/940015c30b5558

https://zenn.dev/tosshi/articles/5a5139b6748b99

### Checkpoint Engine の詳細

![](https://github.com/wuxibin89/verl/blob/wuxibin/doc_images/docs/_static/checkpoint_engine.png?raw=true)

分離アーキテクチャ（rollout/training ノードが別々）におけるクロスノード・パラメータ同期の統一抽象化。

**コア API**:

```python
class CheckpointEngine(ABC):
    def send_weights(self, state_dict):
        """パラメータをストリーミング転送"""
        pass

    def receive_weights(self):
        """重みを受信（Generator）"""
        for bucket in stream:
            yield bucket
```

トランスポート・バックエンドとして、**NCCL** と **NIXL** の 2 つが提供されています。NCCL は Collective broadcast（同報通信）方式で、1 つの訓練ノードから複数の推論ノードへ一斉に重みを配信します。同一ノード内や NVLink/NVSwitch で接続された環境では特に効率的です。一方、NIXL はポイント・ツー・ポイント（P2P）通信方式で、より柔軟なノード間転送が可能です。これらにより、「効率的なクロスノード・パラメータ同期」を Off-policy 非同期訓練シナリオで実現します。

### 設計上のトレードオフ

**柔軟性 vs パフォーマンス**: ハイブリッド・アプローチにより、「計算グラフ内の複雑な制約を動的に管理し、柔軟なデータ依存関係、多様なリソース割り当て」を実現しています。

**抽象化レベル**: ユーザーは高レベルの Trainer インターフェースのみを扱い、分散の複雑さはカプセル化されています。「ユーザーは分散訓練の詳細に煩わされることなく、RL アルゴリズムの実装に完全に集中できます。」

**リソース共有**: Ray の PlacementGroup により、`ResourcePool` と `WorkerGroup` の抽象化が可能になり、「様々なロール（actor、critic、reward、rollout）間で柔軟な GPU 共有を実現しつつ、分離性を維持」できます。

### 3 つの訓練パイプライン・モデル

Verl は、ユースケースに応じて 3 つの異なるパイプライン戦略を提供します。

#### 1. On-Policy（同期）

```mermaid
sequenceDiagram
    participant Rollout as Rollout Phase
    participant Training as Training Phase

    Rollout->>Training: 生成完了
    Note over Training: 訓練実行
    Training->>Rollout: 更新完了
    Note over Rollout: 次の生成
```

- **実行方式**: Rollout と Training を順次実行
- **リソース**: GPU を共有（Colocated 配置）
- **アルゴリズム的正確性**: 厳密な on-policy
- **ユースケース**: ベースライン実験、正確性優先のシナリオ

#### 2. One-Step-Off-Policy（非同期）

```mermaid
sequenceDiagram
    participant Rollout as Rollout Phase
    participant Training as Training Phase

    par Rollout と Training が並列実行
        Rollout->>Rollout: 古いパラメータで生成
    and
        Training->>Training: 前ステップのデータで訓練
    end
```

- **実行方式**: Generation と Training をオーバーラップで並列化
- **リソース**: 前ステップのパラメータで rollout を実行（部分的分離）
- **パフォーマンス向上**: 20-40%
- **トレードオフ**: 厳密な on-policy からの最小限のアルゴリズム的乖離

#### 3. Fully Async（完全分離・ストリーミング）

```mermaid
graph LR
    subgraph "Trainer ノード"
        T[Actor Training<br/>最新モデル更新]
    end

    subgraph "Rollouter ノード"
        R[Rollout Generation<br/>やや古いモデル]
    end

    T -->|Checkpoint Engine<br/>重み同期| R
    R -->|TransferQueue<br/>Experience Buffer| T
```

- **実行方式**: Trainer と Rollouter ノードを完全分離
- **データ転送**: ストリーミング転送と Staleness 制御
- **部分的 rollout**: 必要に応じて段階的に生成
- **ユースケース**: 大規模訓練（128+ GPU）、長時間推論タスク（Chain-of-Thought）

**3 モデルの比較表**:

| 特性 | On-Policy | One-Step-Off-Policy | Fully Async |
|------|-----------|-------------------|-------------|
| リソース分離 | なし（共有） | 部分的 | 完全分離 |
| スループット向上 | ベースライン | 20-40% | 2.35x-2.67x（128 GPU） |
| 実装複雑度 | 低 | 中 | 高 |
| アルゴリズム的乖離 | なし | 最小 | Staleness により制御 |
| 推奨シナリオ | 正確性検証 | 中規模訓練 | 大規模クラスタ |

### Worker の柔軟な配置管理

GPU リソースは **ResourcePoolManager** が管理します。`resource_pool_spec` で各プールのノード構成を定義し、Role（Actor, Critic, Reward Model 等）をプールにマッピングします。中核となるのは `ActorRolloutRefWorker` クラスで、`role` パラメータに応じて Actor（ポリシー訓練）、Rollout（推論エンジン）、Reference（KL 制約用ログ確率計算）の 3 つの役割を 1 つの GPU セット上で切り替えて実行できるハイブリッドエンジンとして機能します。

```python
# Resource Pool 設定例
resource_pool_spec = {
    "global_pool": [8, 8, 8, 8],  # 4 ノード x 8 GPU: Actor/Rollout/Ref 共有
    "reward_pool": [8, 8],         # 2 ノード x 8 GPU: Reward Model 専用
}
```

`max_colocate_count` パラメータ（デフォルト 3）により、actor_critic_ref、rollout、reward model の 3 つの WorkerGroup を同一 GPU 上に共存させることができます。Reward Model を別プールで専用実行しながら、Actor/Rollout/Ref は同一 GPU セットで時分割実行するという柔軟な構成が可能です。

次のセクションでは、特に重要な 5 つのアップデートを詳しく解説します。

## v0.7 の重要なアップデート

v0.7 では 5 つの主要なアップデートが導入されました。これらは相互に関連しており、以下のような関係性があります：

1. **Rollout Engine の server mode 移行**が起点となり、推論エンジンを独立プロセスに分離
2. **Checkpoint Engine** と **TransferQueue** が、分離された訓練・推論プロセス間の効率的なデータ転送を支援
3. これらのインフラコンポーネントにより **Fully async パイプライン**として統合され、大規模クラスタでの並列性を最大化
4. 新しい RL アルゴリズム **CISPO** と **SAPO** が追加され、GRPO の課題を解決

以下では、各アップデートについて「実装の"なぜ"」を中心に解説します。

### 1. Rollout Engine の server mode 移行

前述の通り、これらの強化学習ベースのポストトレーニング手法では Generation Phase（推論）が訓練時間の大半を占めるため、推論エンジンの効率化がフレームワーク全体の性能に直結します。v0.6 以前の Rollout Engine は SPMD（Single Program Multiple Data）アーキテクチャを採用していました。SPMD では全プロセスが同じコードを実行し、静的バッチングによって推論を行います。

SPMD の本質的な問題は、全 GPU が同期的に同じバッチを処理する点にあります。バッチ内の最長シーケンスに合わせてパディングが必要になるため、短いシーケンスが多い場合に GPU 利用率が大きく低下します。これらのアルゴリズムでは 1 プロンプトから複数（例: 5-64 個）の応答を生成しますが、生成される応答のシーケンス長は大きくばらつきます。SPMD では最長の応答が完了するまで全 GPU が待機するため、短い応答を生成した GPU は遊休状態になります。さらに、バッチ全体が完了するまで結果を返せないため、リクエストの到着パターンに柔軟に対応できず、スループットも制限されます。

v0.7 では、Rollout Engine が server mode（オンラインサービングモード）に移行しました。server mode では、推論エンジンが独立したプロセスとして起動し、訓練プロセスからサンプル単位でリクエストを受け付けます。バックエンドとして vLLM、SGLang、TensorRT-LLM などの高性能推論エンジンを利用できるため、Continuous Batching（動的バッチング）による GPU 利用率の向上が実現します。

Continuous Batching では、推論エンジンが到着したリクエストを動的にバッチに詰め込みます。あるサンプルが完了したら即座に結果を返し、空いたスロットに新しいリクエストを追加します。これにより、パディングによる無駄が最小化され、GPU 利用率が大幅に向上します。これは Verl 独自のイノベーションというより、vLLM/SGLang のオンライン推論機能を訓練パイプラインに組み込んだ設計判断です。

また、v0.7 では `AgentLoopBase` 抽象化も導入されました。`SingleTurnAgentLoop` は従来の単一ターンの応答生成に対応し、`ToolAgentLoop` はツール呼び出しを含む複数ターンのエージェントタスクに対応します。これにより、SWEAgent や GUIAgent のような複雑なエージェント行動の学習が可能になりました。

設定は Hydra CLI で行います。`actor_rollout_ref.rollout.name=vllm` を指定することで vLLM バックエンドが有効化されます。Tensor Parallel サイズは `actor_rollout_ref.rollout.tensor_model_parallel_size=8` で制御できます。

::::details v0.6 以前の 3D-HybridEngine（時分割実行）からの進化

v0.6 以前の VERL は、3D-HybridEngine と呼ばれるアーキテクチャにより、同一 GPU セット上で Generation Phase と Training Phase を時分割で実行していました。これは GPU 数を 50% 削減できるメモリ効率の高い設計でした。

**時分割実行のフロー**:

1. **Generation Phase 開始**: FSDP モデルから state_dict を取得 → CPU にオフロード → vLLM/SGLang を Wake Up → CUDA IPC で重みを Zero-copy 転送 → KV Cache をリストア
2. **Generation 実行**: vLLM/SGLang で高速推論
3. **Training Phase 開始**: vLLM/SGLang を Sleep（KV Cache 解放） → FSDP モデルを GPU にロード → Optimizer をロード
4. **Training 実行**: PPO/GRPO 更新

切り替えコストは CUDA IPC（Inter-Process Communication）による Zero-copy 転送のため、モデルサイズに応じて異なります。大規模モデル（DeepSeek-V3 671B クラス）で約 5-10 秒、中規模モデル（Llama-3.1-70B クラス）で約 1-2 秒程度の切り替え時間が想定されます（筆者推定。実際の値はハードウェアとネットワーク構成に依存）。

v0.7 の server mode は、この時分割実行の延長線上にあります。v0.6 以前は同一プロセス内での切り替えでしたが、v0.7 では推論エンジンを独立プロセスに分離することで、Continuous Batching の恩恵を受けられるようになりました。この分離に伴い、Checkpoint Engine による効率的な重み同期メカニズムが必要になった、という経緯です。

::::

### 2. Checkpoint Engine による重み同期の最適化

Checkpoint Engine が v0.7 で重要な理由は、Rollout Engine の server mode 移行と直結しています。v0.6 以前の SPMD モードでは、訓練と推論が同一プロセス内で時分割実行されていたため、重みはメモリ上で共有されており、転送は比較的単純でした。しかし server mode では推論エンジンが独立プロセスになったため、訓練で更新された Actor モデルの重みを別プロセスの推論サーバーに「送る」仕組みが必須になりました。これらのアルゴリズムでは各訓練ステップ後に重みを同期する必要があるため、この転送が効率化の鍵となります。

大規模モデル（例えば 70B パラメータ）では重みが 140GB 以上になるため、効率的な転送メカニズムが必要です。従来の方式では GPU → CPU → 中間ストレージ → CPU → GPU という多段コピーが発生し、転送時間が数分に及ぶこともあります。ノード数が増えるとこの転送がボトルネックになり、スケーラビリティに限界がありました。

v0.7 では、Checkpoint Engine が導入され、ノード間のパラメータ同期が統一抽象化されました。Checkpoint Engine は、訓練ノードと推論ノード間で NCCL または NIXL を使用して GPU メモリを直接転送します。これにより、ファイルシステムを経由せず、GPU → GPU のゼロコピー転送が実現します。

NCCL バックエンドは broadcast 方式で、1 つの訓練ノードから複数の推論ノードへ一斉に重みを配信します。同一ノード内や NVLink/NVSwitch で接続された環境では特に効率的です。一方、NIXL バックエンドはポイントツーポイント（P2P）方式で、より柔軟なノード間転送が可能です。NIXL は NVIDIA が開発したライブラリで、ノード間のストリーミング転送に向いています。

大規模モデルでは、重みをパーティション単位に分割してストリーミング転送する機能もサポートされています。例えば 140GB のモデルを 1GB ずつ分割して転送することで、メモリピーク使用量を抑えながら並列転送できます。

::::details Checkpoint Engine の内部アーキテクチャ

Checkpoint Engine は 4 つのバックエンドを提供しており、配置パターンに応じて使い分けます。

| バックエンド | トポロジー | 用途 | 特徴 |
|---|---|---|---|
| Naive（Colocate） | ローカルメモリ | On-policy 同居配置 | 最小オーバーヘッド |
| NCCL | Broadcast | 固定クラスタ Off-policy | 高帯域幅 |
| HCCL | Broadcast | Ascend NPU | NPU 対応（実験的） |
| NIXL | Ring P2P | Elastic rollout | 動的トポロジー |

NCCL バックエンドの内部では、**コントロールプレーンとデータプレーンが分離**されています。コントロールプレーンは ZeroMQ の PUB/SUB パターンで Bucket メタデータ（テンソル名、形状、dtype、オフセット）を配信し、データプレーンは NCCL Broadcast で実際のテンソルデータを転送します。受信側の Rollout Worker では、次の Bucket を受信しながら前の Bucket のテンソルを yield するパイプライン方式により、転送と処理をオーバーラップさせています。

Bucket ベース転送により、大規模モデルでもメモリピークを抑えながら効率的な重み同期が可能になります。

::::


### 3. TransferQueue のゼロコピー転送と RDMA 対応

Verl では、Ray Actor 間でテンソル（生成されたトークン、報酬値など）を頻繁に転送します。v0.6 以前は、Python の `ray.get()` と `ray.put()` を使用していました。Ray の `ray.put()` は Python pickle でオブジェクトをシリアライズし、Ray Object Store（共有メモリ）に格納します。PyTorch テンソル（数百 MB 〜数 GB）を pickle する場合、CPU でのシリアライズと Object Store へのコピーが大きなオーバーヘッドになります。さらに、GPU → CPU → Ray Object Store → CPU → GPU という多段コピーが発生し、メモリ帯域幅を無駄に消費していました。

大規模クラスタでは、このテンソル転送がボトルネックとなり、全体のスループットを制限します。特に Fully async パイプラインでは、訓練ノードと推論ノード間で Experience Buffer（生成されたトークン列と報酬のペア）を大量に転送するため、転送効率が重要になります。

v0.7 では、TransferQueue の最適化が進みました。TransferQueue は、制御フロー（コントローラ）とデータフロー（テンソル転送）を分離します。Ray のメッセージパッシング（`ray.get()`/`ray.put()`）は制御信号のみに使用し、実際のテンソルデータは別の経路で転送します。

TransferQueue は PyTorch テンソルに特化することで、pickle を回避しゼロコピー転送を実現します。バックエンドとして ZeroMQ、NIXL、Ray RDT をサポートしています。AWS 環境では EFA（Elastic Fabric Adapter）経由で RDMA を活用可能です。RDMA を使用すると、GPU メモリを直接ノード間で転送でき、CPU オーバーヘッドを最小化できます。

また、大規模テンソル（数 GB）を複数パーティションに分割して並列転送する機能もサポートされました。例えば 10GB のテンソルを 256MB ずつ分割して転送することで、ネットワーク帯域幅を最大限活用できます。TransferQueue はまだ実験的機能（experimental）であり、一部の構成では不安定な場合があります。本番環境では、まず小規模で検証することを推奨します。

### 4. Fully async パイプラインの実装

v0.6 以前の Verl では、On-policy パイプライン（Generation → Training をシリアル実行）と One-step-off-policy パイプライン（1 ステップずらして並列実行）の 2 つがサポートされていました。On-policy は実装がシンプルですが、訓練と推論を同じ GPU で時分割実行するため効率が低くなります。One-step-off-policy は Generation と Training を 1 ステップずらすことで並列化し、20-40% の効率向上を実現しますが、リソースは部分的にしか分離されていません。

大規模クラスタ（128+ GPU）では、これらのパイプラインに限界がありました。訓練と推論のリソース要件が異なるため、どちらかが遊休状態になりリソース利用率が低下します。ノード数が増えるにつれて同期待ちが増加し、特にマルチターンエージェントのような推論時間が長いタスクでは、一方のリソースが遊休状態になる時間が無視できなくなります。

v0.7 では、Fully async パイプラインが導入されました。これは、訓練と推論のリソースを完全に分離し、ストリーミングデータ転送によってパイプライン並列化を実現する設計です。訓練ノードと推論ノードが完全に独立しているため、訓練は p5.48xlarge（GPU）、推論は trn2.48xlarge（Trainium）のように異なるインスタンスタイプを使用できます。

Fully async パイプラインの鍵は Staleness 制御です。Verl の staleness パラメータ（`staleness_threshold`）は 0.0-1.0 のスケールで指定します。`staleness_threshold=0` は同期的（更新後の重みでのみ推論）を意味し、`staleness_threshold=0.5` は古いサンプルの最大割合を 50% に制御することを意味します。推論側は最新の重みを待たず、少し古いモデルで Generation を続行することで、訓練と推論の同期待ちを最小化し、両方のリソースを常にフル稼働させられます。

TransferQueue と Checkpoint Engine を活用することで、訓練と推論の間でデータと重みを非同期に転送します。Experience Buffer（生成されたトークン列と報酬のペア）は TransferQueue でストリーミング転送され、更新された重みは Checkpoint Engine で GPU 間直接転送されます。

リリースノートによると、Fully async は On-policy と比較して大幅なスループット向上を達成しています。128 GPU での実験では 2.35x から 2.67x の速度向上が報告されています（測定環境: DAPO アルゴリズム、Qwen2.5-Math-7B, 128 H20 GPU）。設定は Hydra CLI で行い、`async_training.staleness_threshold=0.5` のように指定します。staleness を大きくしすぎると、古いポリシーで生成されたデータが訓練に使用され、学習の安定性が低下する可能性があります。

::::details GPU 数別スケーリング性能と Worker 配置パターン

**GPU 数別の Fully async 速度向上**:

| GPU 数 | 同期モード比 速度向上 |
|---|---|
| 32 GPU | 1.72x |
| 64 GPU | 2.09x |
| 128 GPU | 2.35x - 2.67x |

GPU 数が増えるほど速度向上の効果が大きくなります。これは、同期モードではノード間の同期待ちがスケールに比例して増加するのに対し、Fully async ではこの同期オーバーヘッドが排除されるためです。

**VERL の 3 つの Worker 配置パターン**:

Fully async は VERL が提供する 3 つの配置パターンの最も積極的な構成です。

```mermaid
graph TB
    subgraph "Colocate 配置（標準）"
        C_GPU["GPU プール<br/>Actor + Rollout + Ref<br/>時分割実行"]
    end

    subgraph "Split 配置（スループット重視）"
        S_GPU1["GPU プール 1<br/>Actor + Rollout + Ref"]
        S_GPU2["GPU プール 2<br/>Critic"]
        S_GPU1 -.->|"非同期"| S_GPU2
    end

    subgraph "Fully Async（最大スループット）"
        FA_GPU1["訓練 GPU プール<br/>Actor（FSDP/Megatron）"]
        FA_GPU2["推論 GPU プール<br/>Rollout（vLLM/SGLang）"]
        FA_GPU1 -.->|"Checkpoint Engine<br/>重み転送"| FA_GPU2
        FA_GPU2 -.->|"TransferQueue<br/>Experience Buffer"| FA_GPU1
    end
```

- **Colocate 配置**: 全ロールを同一 GPU プールで時分割実行します。メモリ効率重視で GPU 数を 50% 削減可能です。v0.6 以前の標準構成です
- **Split 配置**: Actor/Rollout と Critic を別プールに分離し非同期実行します。スループットとメモリ効率のバランスが良い構成です
- **Fully Async**: Trainer と Rollouter を完全分離します。128 GPU で 2.67x の速度向上を達成しました。v0.7 の主要アップデートです

v0.7 の Fully async パイプラインは、Checkpoint Engine と TransferQueue という 2 つのインフラコンポーネントによって実現されています。Checkpoint Engine が重みの同期を、TransferQueue が Experience Buffer の転送をそれぞれ担当し、訓練と推論の完全な非同期実行を可能にしています。

::::

### 5. Algorithm: CISPO と SAPO の追加

v0.7 では、GRPO の課題を解決する 2 つの発展的アルゴリズムが追加されました。

**CISPO（Clipped IS-weight Policy Optimization）** は、Importance Sampling の重み自体をクリップします。従来の GRPO はトークンレベルでの更新値をクリップしていましたが、CISPO は重要度サンプリング重みをクリップすることで、より安定した学習を実現します。リリースノートによると、CISPO は他の競合 RL 手法を上回るパフォーマンスを示しています。

**SAPO（Soft Adaptive Policy Optimization）** は、GRPO の「ハードなトークンレベルクリッピング」を温度制御されたソフトゲーティングに置き換えます。ハードクリッピングでは、閾値を超えたトークンの勾配が完全に切り捨てられるため、学習効率が低下します。SAPO は連続的な信頼領域を形成し、高度にオフポリシーなトークンのみを選択的に下げ加重することで、より情報的で安定した更新を実現します。特に MoE（Mixture of Experts）モデルにおいて、トークンレベルの重要度比の高分散問題に効果的です。

これらのアルゴリズムは Verl に統合されており、Hydra CLI で `policy_loss=cispo` または `policy_loss=sapo` と指定することで利用できます。GRPO との比較実験も実施されており、同一の設定下（Qwen2.5-0.5B-Base、バッチサイズ 256、コンテキスト長 8192、gsm8k データセット）で性能が評価されています。

::::details GRPO 固有の効率化と v0.7 での改善

GRPO（Group Relative Policy Optimization）は PPO と比較して Critic モデルが不要という特徴がありますが、v0.7 の機能により さらなる効率化が可能になりました。

**GRPO の効率化機会**:

| 手法 | v0.7 での対応 | 効果 |
|------|-------------|------|
| **Prefix Caching** | `enable_prefix_caching: true` | 同一プロンプト KV-cache 再利用 |
| **Continuous Batching** | server mode デフォルト | group 内の異なる長さを効率処理 |
| **Critic 不要** | リソースプール簡素化 | GPU メモリ節約 |
| **async mode** | Fully async パイプライン | 2.67x スループット向上 |

**Prefix Caching の重要性**:

GRPO では同一プロンプトに対して複数の応答（例: `group_size=8`）を生成します。v0.7 の Prefix Caching により：

- 最初の応答生成時にプロンプトの KV-cache を計算
- 2 個目以降の応答生成では KV-cache を再利用
- `group_size=8` なら 7/8 = **87.5% のプロンプト処理を削減**

```yaml
rollout:
  name: vllm
  enable_prefix_caching: true  # GRPO では必須
  gpu_memory_utilization: 0.6  # KV-cache 用メモリを多めに確保
  mode: async
```

**リソースプールの簡素化**:

PPO では Actor と Critic の両方が必要ですが、GRPO は Critic 不要のため：

```python
# GRPO の場合: Actor/Rollout/Ref のみ
resource_pool_spec = {
    "global_pool": [8, 8, 8, 8],  # 4 ノード x 8 GPU
}
mapping = {
    Role.ActorRollout: "global_pool",
    Role.Ref: "global_pool",       # Critic 不要
    Role.RewardModel: "global_pool",
}
```

Critic 用 GPU が不要なため、全リソースを Actor/Rollout/Ref に集中でき、Split Placement の場合でも Actor 側により多くの GPU を割り当て可能です。

**v0.7 の Fully async との組み合わせ**:

```yaml
# GRPO + Fully async の推奨構成
async_training:
  staleness_threshold: 0.5  # 適度な非同期性
actor_rollout_ref:
  actor:
    strategy: fsdp2
  rollout:
    name: vllm
    enable_prefix_caching: true
    enable_chunked_prefill: true
    mode: async
```

GRPO の軽量な設計（Critic 不要）と v0.7 の Fully async パイプラインを組み合わせることで、大規模クラスタでの高効率訓練が実現します。

::::

### 6. その他の v0.7 機能追加

v0.7 では、上記の主要アップデート以外にも、多数の機能追加と改善が行われました。

#### Model Engine の機能追加

**Megatron-Bridge 統合**:
- LoRA/PEFT サポート: Parameter-Efficient Fine-Tuning に対応
- 統合時間の大幅短縮: 従来数週間かかっていた統合が、Megatron-Bridge により数日で可能に

**実験的機能**:
- **FP8 訓練**: H100/H200 向けの FP8 訓練サポート（Megatron バックエンド）
- **VLM サポート**: Vision-Language Model の SFT と RL 訓練に対応
  - サポートモデル: Llama 3.2 Multimodal、Pixtral
- **新モデル統合**: GPT-OSS、Qwen3-Next

**VeOmni エンジン（Alpha）**:
- VLM と MoE に最適化された中性能バックエンド
- FSDP+SP+EP 並列化をサポート
- 約 1 週間で新モデルを統合可能

#### Rollout Engine の機能追加

**FP8 最適化**:
- **Blockwise FP8 Rollout**: vLLM と SGLang で blockwise FP8 をサポート
- **オンライン量子化**: vLLM で torchao によるオンライン量子化

**高度な機能**:
- **ルーター・リプレイ（実験的）**: MoE モデルのルーティング決定を再利用
- **ビデオ入力サポート**: マルチモーダル入力（ビデオ）に対応
- **vLLM 0.12.0 / SGLang 0.5.6 対応**: 最新バージョンの推論エンジンと統合

**AgentLoop 拡張**:
- `SingleTurnAgentLoop`: 従来の単一ターン応答生成
- `ToolAgentLoop`: ツール呼び出しを含む複数ターン対話
- カスタム実装例: SWEAgent（コード編集）、GUIAgent（UI 操作）

#### Reward System の強化

**ハイブリッド報酬シナリオ**:
- **Generative Reward**: 生成的報酬モデル（LLM-as-a-Judge）
- **Discriminative Reward**: 識別的報酬モデル（従来の報酬モデル）
- **Rule-based Reward**: ルールベースの報酬関数（関数ベース）

これら 3 つの報酬タイプを組み合わせて使用可能です。例えば、「文法の正確性はルールベース、内容の質は LLM-as-a-Judge、全体的な好ましさは訓練済み報酬モデル」のように、複数の観点から報酬を計算できます。

**Server Mode デプロイ**:
- Reward Model も server mode で独立デプロイ可能
- Rollout Engine と同様に、オンライン推論として動作

#### Recipes と新手法

**VLA（Vision-Language-Action）実験サポート**:
- ロボティクス向けの VLA モデル訓練に対応
- ビジョン入力とアクション出力の統合

**rhymeRL（History Rhymes）**:
- 加速手法のレシピサポート
- 過去の経験を活用した効率的な学習

これらの機能により、v0.7 は従来の言語モデル訓練だけでなく、VLM、ロボティクス、複雑なエージェントタスクにも対応する包括的なフレームワークとなりました。

## AWS での実践的活用

v0.7 の新機能、特に Fully async パイプラインは、AWS の異種インスタンス構成と組み合わせることでコスト最適化が可能です。

### Fully async + Trainium/GPU 混在構成

前述の通り、これらの強化学習ベースのポストトレーニング手法では推論が支配的なワークロードです。Verl の Fully async パイプラインは訓練と推論を完全に分離するため、それぞれに最適なインスタンスタイプを選択できます。

推奨構成:

```text
┌──────────────────────────────────────────────────────────┐
│  Amazon EKS / Amazon SageMaker HyperPod                  │
│                                                          │
│  [GPU Node Pool] p5.48xlarge x 8                         │
│  ├── Verl Model Engine（Actor 訓練）                      │
│  ├── FSDP/Megatron-Core                                  │
│  └── Checkpoint Engine（重み同期）                        │
│                                                          │
│  [Trainium Node Pool] trn2.48xlarge x 4-8                │
│  ├── Verl Rollout Engine（vLLM server mode）             │
│  ├── Reference Model 推論（重み固定）                     │
│  └── Reward Model 推論（重み固定）                        │
│                                                          │
│  [Storage] Amazon FSx for Lustre                         │
│  └── モデル重み、チェックポイント、Experience Buffer      │
└──────────────────────────────────────────────────────────┘
```

Actor Model の訓練は p5.48xlarge（GPU）で行います。訓練は GPU が最適です。Actor Model の推論も p5.48xlarge（GPU）を使用します。重みが頻繁に更新されるため、GPU で推論を続ける方が効率的です。一方、Reference Model と Reward Model は重みが固定されているため、trn2.48xlarge（Trainium）で推論を行います。Trainium は訓練と推論の両方に対応するアクセラレータであり、固定重みの推論では GPU と同等以上の性能を発揮します。

Reference/Reward を Trainium に移行することで、推論コストの大幅な削減が可能です（p5 比）。

### NxD Inference / Neuron vLLM との統合

Verl の Rollout Engine は vLLM バックエンドをサポートしており、**Neuron vLLM**（NxD Inference）と統合できます。

NxD Inference は Continuous Batching、KV-Cache 管理、Tensor Parallelism、Disaggregated Inference（trn1.32xlarge / trn2.48xlarge）に対応しています。vLLM と同等の機能を Trainium 上で実行できるため、Verl の Rollout Engine と統合可能です。

Neuron コンパイルの課題として、初回コンパイルが大規模モデルで数十分～数時間かかります。また、バッチサイズ、シーケンス長、Tensor Parallelism 数の変更時には再コンパイルが必要です。回避策として、コンパイルキャッシュを `NEURON_COMPILE_CACHE_URL=s3://bucket/cache` で共有する方法があります。また、頻繁に更新される Actor は GPU 維持し、Reference/Reward のみ Trainium に配置する構成も推奨されます。

### Elastic Training との組み合わせ

:::message
詳細は前回の記事を参照: [GRPO のための次世代インフラアーキテクチャ](/articles/54a29cb1388cd7)
:::

Verl の Fully async パイプラインは、Elastic Training 戦略と組み合わせることでさらなるコスト最適化が可能です。

訓練クラスタは固定（8 x p5.48xlarge）で運用し、推論プールは弾力的（2-16 x trn2.48xlarge）にスケールする構成が推奨されます。Kubernetes HPA または KubeRay Autoscaler を使用することで、推論の負荷に応じて動的にノード数を調整できます。

コスト削減効果（概算）:
```text
[ベースライン] GPU 統一構成
p5.48xlarge x 8（訓練） + p5.48xlarge x 16（推論） = $2,360/h

[最適化構成] Verl Fully async + Trn/GPU + Elastic
p5.48xlarge x 8（訓練、固定） + trn2.48xlarge x 2-16（推論、弾力的） = 削減可能

Trainium を活用することで、GPU 統一構成に比べてコスト削減が期待できます。
```

注: 上記は概算です。最新のオンデマンド価格で再計算してください。

## まとめ

### v0.7 の主要アップデートのおさらい

| アップデート | 重要度 | 主な効果 |
|---|---|---|
| Rollout Engine の server mode 移行 | 高 | 動的バッチング、vLLM/SGLang 統合、GPU 利用率向上 |
| Checkpoint Engine | 高 | GPU 間ゼロコピー転送、NCCL/NIXL バックエンド |
| TransferQueue 最適化 | 中 | ゼロコピー転送、RDMA 対応、シリアライゼーション最適化 |
| Fully async パイプライン | 高 | 2.35-2.67x スループット向上、リソース完全分離 |
| CISPO/SAPO アルゴリズム追加 | 中 | GRPO の課題解決、安定した学習、MoE 対応 |

その他にも、H100/H200 向け FP8 訓練/推論サポート、Llama 3.2 Multimodal や Pixtral の VLM 対応、AgentLoopBase 抽象化（SWEAgent、GUIAgent）が追加されています。

### RLHF/GRPO における重要な知見

v0.7 の設計思想を理解する上で重要なのは、**Generation Phase が訓練時間の約 80% を占める**という事実です。[OpenRLHF の公式データ](https://github.com/OpenRLHF/OpenRLHF)によると、RLHF 訓練では Generation（サンプル生成）が全体の 80% の時間を消費します。[DeepSpeed-Chat の分析](https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-chat/README.md)でも、計算量は Generation 20%、Training 80% ですが、Generation がメモリバウンドのため実時間では大きな割合を占めることが示されています。

この特性が、v0.7 の主要アップデートの設計判断に影響を与えています：

- **Rollout Engine の server mode 移行**: Generation Phase（80%）の高速化が全体のスループット向上に直結するため、vLLM/SGLang の Continuous Batching を活用
- **Checkpoint Engine の最適化**: 各訓練ステップ後の重み同期が頻繁に発生するため、GPU 間ゼロコピー転送が重要
- **Fully async パイプライン**: Generation（80%）と Training（20%）を完全に分離し、並行実行することで 2.35-2.67x のスループット向上を実現

このように、VERL v0.7 は Generation Phase が支配的という RLHF/GRPO の本質的な特性を深く理解し、それに最適化された設計となっています。

### 今後のロードマップ

公式リリースノートによると、v0.8 と v0.9 では以下の大規模な変更が予定されています。

#### v0.8 の予定

**コア・アーキテクチャの刷新**:
- **DataProto → Tensordict への移行**: 現在の DataProto を PyTorch ネイティブの Tensordict に置換
  - より柔軟なデータ構造
  - PyTorch エコシステムとの統合性向上
- **新しいモデルエンジンをデフォルト化**: VeOmni などの新バックエンドを標準に

**推論エンジンの拡張**:
- **TensorRT-LLM Rollout Engine**: NVIDIA TensorRT-LLM を正式サポート
  - v0.7 では実験的サポートのみ
  - v0.8 で本番環境対応
- **VeOmni エンジンの本番化**: VLM と MoE に最適化されたバックエンドが安定版に

**データ転送の最適化**:
- **TransferQueue がデフォルトの転送メソッドに**: v0.7 では実験的だった TransferQueue が標準に
  - ゼロコピー転送がデフォルト動作
  - Ray Object Store の利用を最小化

**新機能**:
- **MTP（Multi-Turn Policy）RL 訓練サポート**: 複数ターン対話の RL 訓練に対応
- **DeepSeek V3.2 サポート**: 最新の大規模モデルに対応

#### v0.9 の予定

**完全非同期訓練のメイン統合**:
- **Fully async パイプラインがデフォルト**: v0.7 で導入された Fully async が標準パイプラインに
- **レガシーエンジンの削除**: 従来のモデルエンジン（SPMD ベース）を削除
  - コードベースの簡素化
  - メンテナンス負荷の軽減

**マルチモーダル・モデルの拡張**:
- **Omni-model サポート**: Qwen3-Omni、BAGEL などのオムニモーダルモデルに対応
  - テキスト、ビジョン、オーディオの統合
  - より複雑なマルチモーダル RL 訓練

**エージェント・トレーニング・レシピ**:
- **SWEAgent レシピ**: コード編集エージェントの訓練レシピ
- **GUIAgent レシピ**: UI 操作エージェントの訓練レシピ
- カスタム AgentLoop 実装のベストプラクティス

このロードマップにより、Verl は言語モデルだけでなく、マルチモーダル、エージェント、ロボティクスなど、幅広い領域に対応する包括的な RL 訓練フレームワークへと進化していきます。

### 参考リンク

公式ドキュメント:
- Verl 公式ドキュメント: https://verl.readthedocs.io/
- Verl GitHub: https://github.com/verl-project/verl
- v0.7 リリースノート: https://verl.readthedocs.io/en/latest/blog/v0.7.html

関連記事:
- [GRPO のための次世代インフラアーキテクチャ](/articles/54a29cb1388cd7)（前回記事）
- [LLM のための強化学習手法 2025](https://note.com/olachin/n/n9706c13c8678)（GRPO/DPO 基礎理論）

RLHF/GRPO のパフォーマンス特性:
- **OpenRLHF**: [Generation が 80% の時間を占める公式データ](https://github.com/OpenRLHF/OpenRLHF)
  - 引用: "RLHF training spends 80% of the time on sample generation"
- **DeepSpeed-Chat**: [計算量 vs 実時間の詳細分析](https://github.com/deepspeedai/DeepSpeed/blob/master/blogs/deepspeed-chat/README.md)
  - 計算量: Generation 20%、Training 80%
  - 実時間: Generation がメモリバウンドのため大きな割合を占める
- **TRL (Hugging Face)**: [Generation が主要なボトルネック](https://huggingface.co/docs/trl/main/en/grpo_trainer)
  - 引用: "Generation is often the main bottleneck when training with online methods"

論文:
- GRPO: [DeepSeekMath - GRPO 提案論文](https://arxiv.org/abs/2402.03300)
- CISPO: [MiniMax-M1 論文内で提案](https://arxiv.org/abs/2506.13585)
- SAPO: [Soft Adaptive Policy Optimization](https://arxiv.org/abs/2511.20347)

AWS リソース:
- NxD Inference: https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/
- SageMaker HyperPod: https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod.html
