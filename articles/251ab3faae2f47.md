---
title: "Verl v0.7 リリースノート解説"
emoji: "😉"
type: "tech"
topics: ["RLHF", "GRPO", "DPO", "Verl", "AWS"]
published: false
---

## はじめに

https://verl.readthedocs.io/en/latest/blog/v0.7.html

:::message
この記事は、Verl v0.7 のリリースノートブログを ML インフラエンジニア向けに解説したものです。重要なアップデートに焦点を当て、自分のキャッチアップのためにまとめています。強化学習何もわからんマンです。
:::

Verl は、GRPO（Group Relative Policy Optimization）や DPO（Direct Preference Optimization）といった LLM のポストトレーニング手法を大規模クラスタで実行するための分散訓練フレームワークです。本記事では v0.7 の重要なアップデート 4 件に絞って解説します。

前提知識として GRPO/DPO の基礎は理解しているものとします。以下はわかりやすかったです。

https://note.com/olachin/n/n9706c13c8678

## Verl と GRPO/DPO の関係

GRPO と DPO は、それぞれ異なるアプローチで LLM のポストトレーニングを効率化します。GRPO は PPO から Critic モデルを削減し、グループ内の相対評価で学習します。一方、DPO は報酬モデルを介さず、人間の選好ペアから直接学習します。

これらの手法に共通する特徴は、**推論（Generation Phase）が訓練時間の約 80% を占める** ことです。つまり、大規模な分散推論と訓練を効率的に管理するインフラが重要になります。Verl は、この課題に対応するために設計された分散訓練フレームワークです。

## Verl のアーキテクチャ概要

Verl（ByteDance Seed チーム開発）は、GRPO のような強化学習アルゴリズムを大規模クラスタで実行するための分散訓練フレームワークです。

公式リポジトリ: https://github.com/volcengine/verl

Verl の中心的な設計思想は **Hybrid-Controller アーキテクチャ（HybridFlow）** です。Single-Controller（オーケストレーション層）と Multi-Controller（分散計算層）の 2 層構造により、訓練全体の流れを制御しながら、各コンポーネント（Actor、Rollout、Reward）を独立して並列実行できます。

![Verl のアーキテクチャ](https://verl.readthedocs.io/en/latest/_images/hybridflow.png)

Verl は以下の 4 つのコア・コンポーネントで構成されています。

| コンポーネント | 役割 | v0.7 での主な変更 |
|---|---|---|
| Model Engine | 訓練バックボーン（FSDP/Megatron-Core/VeOmni） | FP8 訓練、VLM 対応、LoRA サポート |
| Rollout Engine | 推論エンジン（vLLM/SGLang/TensorRT-LLM） | SPMD → server mode 移行、FP8 量子化 |
| TransferQueue | ゼロコピーテンソル転送 | シリアライゼーション最適化、マルチパーティション対応 |
| Checkpoint Engine | 重み同期エンジン | NCCL/NIXL バックエンド統合、ストリーミング転送 |

v0.7 のリリースノートでは、これらのコンポーネントに対する多数の改良が報告されています。次のセクションでは、特に重要な 4 つのアップデートを詳しく解説します。

## v0.7 の重要なアップデート

以下では、v0.7 で導入された重要な 4 つのアップデートについて、「実装の"なぜ"」を中心に解説します。

### 1. Rollout Engine の server mode 移行

前述の通り、GRPO のワークロードでは Generation Phase（推論）が訓練時間の大半を占めるため、推論エンジンの効率化がフレームワーク全体の性能に直結します。v0.6 以前の Rollout Engine は SPMD（Single Program Multiple Data）アーキテクチャを採用していました。SPMD では全プロセスが同じコードを実行し、静的バッチングによって推論を行います。

SPMD の本質的な問題は、全 GPU が同期的に同じバッチを処理する点にあります。バッチ内の最長シーケンスに合わせてパディングが必要になるため、短いシーケンスが多い場合に GPU 利用率が大きく低下します。GRPO では 1 プロンプトから複数（例: 5-64 個）の応答を生成しますが、生成される応答のシーケンス長は大きくばらつきます。SPMD では最長の応答が完了するまで全 GPU が待機するため、短い応答を生成した GPU は遊休状態になります。さらに、バッチ全体が完了するまで結果を返せないため、リクエストの到着パターンに柔軟に対応できず、スループットも制限されます。

v0.7 では、Rollout Engine が server mode（オンラインサービングモード）に移行しました。server mode では、推論エンジンが独立したプロセスとして起動し、訓練プロセスからサンプル単位でリクエストを受け付けます。バックエンドとして vLLM、SGLang、TensorRT-LLM などの高性能推論エンジンを利用できるため、Continuous Batching（動的バッチング）による GPU 利用率の向上が実現します。

![Rollout Engine のアーキテクチャ](https://verl.readthedocs.io/en/latest/_images/rollout_engine.png)

Continuous Batching では、推論エンジンが到着したリクエストを動的にバッチに詰め込みます。あるサンプルが完了したら即座に結果を返し、空いたスロットに新しいリクエストを追加します。これにより、パディングによる無駄が最小化され、GPU 利用率が大幅に向上します。これは Verl 独自のイノベーションというより、vLLM/SGLang のオンライン推論機能を訓練パイプラインに組み込んだ設計判断です。

また、v0.7 では `AgentLoopBase` 抽象化も導入されました。`SingleTurnAgentLoop` は従来の GRPO 訓練のような単一ターンの応答生成に対応し、`ToolAgentLoop` はツール呼び出しを含む複数ターンのエージェントタスクに対応します。これにより、SWEAgent や GUIAgent のような複雑なエージェント行動の学習が可能になりました。

設定は Hydra CLI で行います。`actor_rollout_ref.rollout.name=vllm` を指定することで vLLM バックエンドが有効化されます。Tensor Parallel サイズは `actor_rollout_ref.rollout.tensor_model_parallel_size=8` で制御できます。

### 2. Checkpoint Engine による重み同期の最適化

Checkpoint Engine が v0.7 で重要な理由は、Rollout Engine の server mode 移行と直結しています。v0.6 以前の SPMD モードでは、訓練と推論が同一プロセス内で時分割実行されていたため、重みはメモリ上で共有されており、転送は比較的単純でした。しかし server mode では推論エンジンが独立プロセスになったため、訓練で更新された Actor モデルの重みを別プロセスの推論サーバーに「送る」仕組みが必須になりました。GRPO では各訓練ステップ後に重みを同期する必要があるため、この転送が効率化の鍵となります。

大規模モデル（例えば 70B パラメータ）では重みが 140GB 以上になるため、効率的な転送メカニズムが必要です。従来の方式では GPU → CPU → 中間ストレージ → CPU → GPU という多段コピーが発生し、転送時間が数分に及ぶこともあります。ノード数が増えるとこの転送がボトルネックになり、スケーラビリティに限界がありました。

v0.7 では、Checkpoint Engine が導入され、ノード間のパラメータ同期が統一抽象化されました。Checkpoint Engine は、訓練ノードと推論ノード間で NCCL または NIXL を使用して GPU メモリを直接転送します。これにより、ファイルシステムを経由せず、GPU → GPU のゼロコピー転送が実現します。

![Checkpoint Engine](https://verl.readthedocs.io/en/latest/_images/checkpoint_engine.png)

NCCL バックエンドは broadcast 方式で、1 つの訓練ノードから複数の推論ノードへ一斉に重みを配信します。同一ノード内や NVLink/NVSwitch で接続された環境では特に効率的です。一方、NIXL バックエンドはポイントツーポイント（P2P）方式で、より柔軟なノード間転送が可能です。NIXL は NVIDIA が開発したライブラリで、ノード間のストリーミング転送に向いています。

大規模モデルでは、重みをパーティション単位に分割してストリーミング転送する機能もサポートされています。例えば 140GB のモデルを 1GB ずつ分割して転送することで、メモリピーク使用量を抑えながら並列転送できます。

### 3. TransferQueue のゼロコピー転送と RDMA 対応

Verl では、Ray Actor 間でテンソル（生成されたトークン、報酬値など）を頻繁に転送します。v0.6 以前は、Python の `ray.get()` と `ray.put()` を使用していました。Ray の `ray.put()` は Python pickle でオブジェクトをシリアライズし、Ray Object Store（共有メモリ）に格納します。PyTorch テンソル（数百 MB 〜数 GB）を pickle する場合、CPU でのシリアライズと Object Store へのコピーが大きなオーバーヘッドになります。さらに、GPU → CPU → Ray Object Store → CPU → GPU という多段コピーが発生し、メモリ帯域幅を無駄に消費していました。

大規模クラスタでは、このテンソル転送がボトルネックとなり、全体のスループットを制限します。特に Fully async パイプラインでは、訓練ノードと推論ノード間で Experience Buffer（生成されたトークン列と報酬のペア）を大量に転送するため、転送効率が重要になります。

v0.7 では、TransferQueue の最適化が進みました。TransferQueue は、制御フロー（コントローラ）とデータフロー（テンソル転送）を分離します。Ray のメッセージパッシング（`ray.get()`/`ray.put()`）は制御信号のみに使用し、実際のテンソルデータは別の経路で転送します。

![TransferQueue のデータフロー](https://verl.readthedocs.io/en/latest/_images/transfer_queue.png)

TransferQueue は PyTorch テンソルに特化することで、pickle を回避しゼロコピー転送を実現します。バックエンドとして ZeroMQ、NIXL、Ray RDT をサポートしています。AWS 環境では EFA（Elastic Fabric Adapter）経由で RDMA を活用可能です。RDMA を使用すると、GPU メモリを直接ノード間で転送でき、CPU オーバーヘッドを最小化できます。

また、大規模テンソル（数 GB）を複数パーティションに分割して並列転送する機能もサポートされました。例えば 10GB のテンソルを 256MB ずつ分割して転送することで、ネットワーク帯域幅を最大限活用できます。TransferQueue はまだ実験的機能（experimental）であり、一部の構成では不安定な場合があります。本番環境では、まず小規模で検証することを推奨します。

### 4. Fully async パイプラインの実装

v0.6 以前の Verl では、On-policy パイプライン（Generation → Training をシリアル実行）と One-step-off-policy パイプライン（1 ステップずらして並列実行）の 2 つがサポートされていました。On-policy は実装がシンプルですが、訓練と推論を同じ GPU で時分割実行するため効率が低くなります。One-step-off-policy は Generation と Training を 1 ステップずらすことで並列化し、20-40% の効率向上を実現しますが、リソースは部分的にしか分離されていません。

大規模クラスタ（128+ GPU）では、これらのパイプラインに限界がありました。訓練と推論のリソース要件が異なるため、どちらかが遊休状態になりリソース利用率が低下します。ノード数が増えるにつれて同期待ちが増加し、特にマルチターンエージェントのような推論時間が長いタスクでは、一方のリソースが遊休状態になる時間が無視できなくなります。

v0.7 では、Fully async パイプラインが導入されました。これは、訓練と推論のリソースを完全に分離し、ストリーミングデータ転送によってパイプライン並列化を実現する設計です。訓練ノードと推論ノードが完全に独立しているため、訓練は p5.48xlarge（GPU）、推論は trn2.48xlarge（Trainium）のように異なるインスタンスタイプを使用できます。

![完全非同期パイプライン](https://verl.readthedocs.io/en/latest/_images/fully_async.png)

Fully async パイプラインの鍵は Staleness 制御です。Verl の staleness パラメータ（`staleness_threshold`）は 0.0-1.0 のスケールで指定します。`staleness_threshold=0` は同期的（更新後の重みでのみ推論）を意味し、`staleness_threshold=0.5` は「最大 1.5 倍のサンプル生成を古い重みで許容」を意味します。推論側は最新の重みを待たず、少し古いモデルで Generation を続行することで、訓練と推論の同期待ちを最小化し、両方のリソースを常にフル稼働させられます。

TransferQueue と Checkpoint Engine を活用することで、訓練と推論の間でデータと重みを非同期に転送します。Experience Buffer（生成されたトークン列と報酬のペア）は TransferQueue でストリーミング転送され、更新された重みは Checkpoint Engine で GPU 間直接転送されます。

リリースノートによると、Fully async は On-policy と比較して 2.35-2.67 倍のスループット向上を達成しています（測定環境: Qwen2.5-Math-7B, 128 H20 GPU）。設定は Hydra CLI で行い、`async_training.staleness_threshold=0.5` のように指定します。staleness を大きくしすぎると、古いポリシーで生成されたデータが訓練に使用され、学習の安定性が低下する可能性があります。

## AWS での実践的活用

v0.7 の新機能、特に Fully async パイプラインは、AWS の異種インスタンス構成と組み合わせることでコスト最適化が可能です。

### Fully async + Trainium/GPU 混在構成

前述の通り、GRPO では推論が支配的なワークロードです。Verl の Fully async パイプラインは訓練と推論を完全に分離するため、それぞれに最適なインスタンスタイプを選択できます。

推奨構成:

```text
┌──────────────────────────────────────────────────────────┐
│  Amazon EKS / HyperPod Cluster                           │
│                                                          │
│  [GPU Node Pool] p5.48xlarge x 8                         │
│  ├── Verl Model Engine（Actor 訓練）                      │
│  ├── FSDP/Megatron-Core                                  │
│  └── Checkpoint Engine（重み同期）                        │
│                                                          │
│  [Trainium Node Pool] trn2.48xlarge x 4-8                │
│  ├── Verl Rollout Engine（vLLM server mode）             │
│  ├── Reference Model 推論（重み固定）                     │
│  └── Reward Model 推論（重み固定）                        │
│                                                          │
│  [Storage] FSx for Lustre                                │
│  └── モデル重み、チェックポイント、Experience Buffer      │
└──────────────────────────────────────────────────────────┘
```

Actor Model の訓練は p5.48xlarge（GPU）で行います。訓練は GPU が最適です。Actor Model の推論も p5.48xlarge（GPU）を使用します。重みが頻繁に更新されるため、GPU で推論を続ける方が効率的です。一方、Reference Model と Reward Model は重みが固定されているため、trn2.48xlarge（Trainium）で推論を行います。Trainium は訓練と推論の両方に対応するアクセラレータであり、固定重みの推論では GPU と同等以上の性能を発揮します。

Reference/Reward を Trainium に移行することで、推論コスト 60-87% 削減（p5 比）が可能です。

### NxD Inference / Neuron vLLM との統合

Verl の Rollout Engine は vLLM バックエンドをサポートしており、**Neuron vLLM**（NxD Inference）と統合できます。

NxD Inference は Continuous Batching、KV-Cache 管理、Tensor Parallelism、Disaggregated Inference（trn1.32xlarge / trn2.48xlarge）に対応しています。vLLM と同等の機能を Trainium 上で実行できるため、Verl の Rollout Engine と統合可能です。

Neuron コンパイルの課題として、初回コンパイルが大規模モデルで数十分～数時間かかります。また、バッチサイズ、シーケンス長、Tensor Parallelism 数の変更時には再コンパイルが必要です。回避策として、コンパイルキャッシュを `NEURON_COMPILE_CACHE_URL=s3://bucket/cache` で共有する方法があります。また、頻繁に更新される Actor は GPU 維持し、Reference/Reward のみ Trainium に配置する構成も推奨されます。

### Elastic Training との組み合わせ

:::message
詳細は前回の記事を参照: [GRPO のための次世代インフラアーキテクチャ](/articles/54a29cb1388cd7)
:::

Verl の Fully async パイプラインは、Elastic Training 戦略と組み合わせることでさらなるコスト最適化が可能です。

訓練クラスタは固定（8 x p5.48xlarge）で運用し、推論プールは弾力的（2-16 x trn2.48xlarge）にスケールする構成が推奨されます。Kubernetes HPA または KubeRay Autoscaler を使用することで、推論の負荷に応じて動的にノード数を調整できます。

コスト削減効果（概算）:
```text
[ベースライン] GPU 統一構成
p5.48xlarge x 8（訓練） + p5.48xlarge x 16（推論） = $2,360/h

[最適化構成] Verl Fully async + Trn/GPU + Elastic
p5.48xlarge x 8（訓練、固定） + trn2.48xlarge x 2-16（推論、弾力的） = 削減可能

Trainium を活用することで、GPU 統一構成に比べてコスト削減が期待できます。
```

注: 上記は概算です。最新のオンデマンド価格で再計算してください。

## まとめ

### v0.7 の主要アップデートのおさらい

| アップデート | 重要度 | 主な効果 |
|---|---|---|
| Rollout Engine の server mode 移行 | 高 | 動的バッチング、vLLM/SGLang 統合、GPU 利用率向上 |
| Checkpoint Engine | 高 | GPU 間ゼロコピー転送、NCCL/NIXL バックエンド |
| TransferQueue 最適化 | 中 | ゼロコピー転送、RDMA 対応、シリアライゼーション最適化 |
| Fully async パイプライン | 高 | 2.35-2.67x スループット向上、リソース完全分離 |

その他にも、H100/H200 向け FP8 訓練/推論サポート、Llama 3.2 Multimodal や Pixtral の VLM 対応、AgentLoopBase 抽象化（SWEAgent、GUIAgent）、新アルゴリズム（CISPO、SAPO）が追加されています。

### 今後のロードマップ

公式ドキュメントによると、v0.8 では DataProto を Tensordict に置換し、新しいモデルエンジンをデフォルト化する予定です。また、TensorRT-LLM 対応の Rollout Engine が実装され、VeOmni エンジンが本番環境対応になります。v0.9 では完全非同期トレーニングがメインに統合され、従来のモデルエンジン（SPMD ベース）が削除されます。さらに、Qwen3-Omni や BAGEL などのマルチモーダルモデル対応、SWEAgent や GUIAgent などのエージェントトレーニングレシピが追加される予定です。

### 参考リンク

公式ドキュメント:
- Verl 公式ドキュメント: https://verl.readthedocs.io/
- Verl GitHub: https://github.com/volcengine/verl
- v0.7 リリースノート: https://verl.readthedocs.io/en/latest/blog/v0.7.html

関連記事:
- [GRPO のための次世代インフラアーキテクチャ](/articles/54a29cb1388cd7)（前回記事）
- [LLM のための強化学習手法 2025](https://note.com/olachin/n/n9706c13c8678)（GRPO/DPO 基礎理論）

AWS リソース:
- NxD Inference: https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/
- SageMaker HyperPod: https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod.html
