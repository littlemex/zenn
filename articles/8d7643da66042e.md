---
title: "vllm-neuron ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ç·´ç¿’ã§å¾—ãŸçŸ¥è¦‹"
emoji: "ğŸ”"
type: "tech"
topics: ["vllm", "AWSNeuron", "Profiler", "Python", "æ€§èƒ½æœ€é©åŒ–"]
published: false
---

## ã¯ã˜ã‚ã«

[å‰å›ã®è¨˜äº‹](https://zenn.dev/tosshi/articles/d68bd091d1934d) ã§ã¯åˆ¥ã«ãªãã¦ã‚‚ä»Šå›ã®è¨˜äº‹è‡ªä½“ã¯æ›¸ã‘ã‚‹ã«ã‚‚é–¢ã‚ã‚‰ãšãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ„ãƒ¼ãƒ«ã®é–‹ç™ºã«ã»ã¼è¶£å‘³ã§é›†ä¸­ã—ã¦ã—ã¾ã—ãŸãŒã€ä»Šå›ã¯ï¼ˆçœŸé¢ç›®ã«ï¼‰ AWS Inferentia2 ä¸Šã§ vllm-neuron ã‚’ä½¿ç”¨ã—ãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°åˆ†æã«ãƒãƒ£ãƒ¬ãƒ³ã‚¸ã—ã¾ã™ã€‚

æœ¬è¨˜äº‹ã§ã¯ã€3 ã¤ã®èª¿æŸ»ãƒ•ã‚§ãƒ¼ã‚ºã«åˆ†ã‘ã¦èª¬æ˜ã—ã¾ã™ã€‚ã¾ãš Phase 1 ã§ã¯ AWS Neuron Profiler ã«ã‚ˆã‚‹ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒ¬ãƒ™ãƒ«ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‚’è©¦è¡ŒéŒ¯èª¤ã—ã¾ã—ãŸã€‚æ¬¡ã« Phase 2 ã§ã¯ line_profiler ã«ã‚ˆã‚‹ Python ãƒ¬ãƒ™ãƒ«ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‚’å®Ÿæ–½ã—ã¾ã—ãŸã€‚æœ€å¾Œã« Phase 3 ã§ã¯ NxD Inference ã‚’ç›´æ¥ä½¿ç”¨ã—ãŸæ¸¬å®šã‚’è¡Œã„ã€vLLM ã¨ã®è©³ç´°æ¯”è¼ƒã‚’å®Ÿæ–½ã—ã¾ã—ãŸã€‚

ã•ã‚‰ã«ã€bucketing è¨­å®šã®å½±éŸ¿ã‚’æ­£ç¢ºã«ç†è§£ã™ã‚‹ãŸã‚ã€å›ºå®šé•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆ97ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã¨å¯å¤‰é•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆ18-125ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã®ä¸¡æ–¹ã§æ¸¬å®šã‚’å®Ÿæ–½ã—ã¾ã—ãŸã€‚ã¾ãŸã€prefix caching ã®æ­£ã—ã„ãƒ•ãƒ©ã‚°åã¨ vllm-neuron ã®å†…éƒ¨å®Ÿè£…ã‚‚èª¿æŸ»ã—ã¾ã—ãŸã€‚

æœ€çµ‚çš„ã«ã€vllm-neuronã€NeuronCore å´ã®å‡¦ç†ã®å®Ÿè¡Œæ™‚é–“ã€bucketing è¨­å®šã‚„ prefix caching ã®æŒ™å‹•ã«ã¤ã„ã¦ç†è§£ã‚’æ·±ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã—ãŸã€‚ç‰¹ã« **bucketing ã®åŠ¹æœã¯ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã«ä¾å­˜ã™ã‚‹** ã¨ã„ã†çŸ¥è¦‹ãŒã‚ã‚Šã¾ã—ãŸã€‚

:::message
**ä»Šå›ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã®é€²ã‚æ–¹ã¯çµæœçš„ã«ã‹ãªã‚Šé–“é•ã£ã¦ã„ã¾ã—ãŸï¼** è‰²ã€…è©¦è¡ŒéŒ¯èª¤ã—ãŸã‚“ã ãªã€ã¨æ€ã„ãªãŒã‚‰æœ¬è¨˜äº‹ã‚’èª­ã‚“ã§ãã ã•ã„ã€‚æœ€å¾Œã«ã©ã†ã„ã†æµã‚Œã«ã™ã¹ãã ã£ãŸã¨æ€ã†ã®ã‹æ•´ç†ã—ã¾ã—ãŸã€‚
:::

:::message alert
æœ¬è¨˜äº‹ã¯åˆå­¦è€…å‘ã‘ã§ã¯ãªã„ãŸã‚ã‚ã‚‹ç¨‹åº¦ LLM æ¨è«–ã®åŸºç¤çŸ¥è­˜ã€vLLM ã®åŸºç¤çŸ¥è­˜ã€AWS Neuron ã®åŸºç¤çŸ¥è­˜ãŒã‚ã‚‹ã“ã¨ãŒå‰æã§ã™ã€‚
:::

---

## Phase 1: AWS Neuron Profiler ã§ã®è©¦è¡ŒéŒ¯èª¤

### 1.1 ãªãœãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‹ã‚‰å§‹ã‚ã‚‹ã®ã‹

æ€§èƒ½æœ€é©åŒ–ã‚’è¡Œã†éš›ã€ã¾ãšç¾çŠ¶ã‚’æŠŠæ¡ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¸¬å®šã§ã¯æ€§èƒ½ã®çµæœã¯åˆ†ã‹ã‚Šã¾ã™ãŒã€æ€§èƒ½ã®ç†ç”±ã€ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®å ´æ‰€ã€ãã—ã¦æ”¹å–„ã®ä½™åœ°ã¯åˆ†ã‹ã‚Šã¾ã›ã‚“ã€‚

ä»¥ä¸‹ã«å®Ÿé¨“ç’°å¢ƒã¨è¨­å®šæƒ…å ±ã‚’ã¾ã¨ã‚ã¦ãŠãã¾ã™ã€‚ä»¥å‰ã® Zenn è¨˜äº‹ã®å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼æœ€é©å€¤ã‹ã‚‰ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™ã€‚

::::details å®Ÿé¨“ç’°å¢ƒã¨è¨­å®šæƒ…å ±

æœ¬èª¿æŸ»ã§ä½¿ç”¨ã—ãŸå®Ÿé¨“ç’°å¢ƒã¨è¨­å®šã®è©³ç´°ã‚’è¨˜è¼‰ã—ã¾ã™ã€‚

**ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ç’°å¢ƒ**:
- ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—: `inf2.xlarge`

**ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ãƒãƒ¼ã‚¸ãƒ§ãƒ³**:
- Neuron SDK: 2.27.x
- vLLM: 0.13.0ï¼ˆNeuron å¯¾å¿œç‰ˆï¼‰
- neuronx-distributed-inference (NxD Inference): 0.7.0
- Python: 3.12

**ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿**:
- ãƒ¢ãƒ‡ãƒ«: Qwen3-0.6B-Reranker
- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·:
  - Phase 1-3 åˆæœŸæ¸¬å®š: 97 ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå›ºå®šé•·ï¼‰
  - è¿½åŠ èª¿æŸ»: 18-125 ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå¯å¤‰é•·ã€16 ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰
- ã‚¿ã‚¹ã‚¯: Rerankerï¼ˆæ–‡æ›¸ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼‰
- ãƒãƒƒãƒã‚µã‚¤ã‚º: 4

**vLLM è¨­å®šï¼ˆéå»ã® Zenn è¨˜äº‹ã®å®Ÿé¨“ã§ã®æœ€é©å€¤ï¼‰**:
```yaml
vllm:
  tensor_parallel_size: 2           # 2 NeuronCore ä½¿ç”¨
  max_num_seqs: 4                   # åŒæ™‚å‡¦ç†æ•°
  block_size: 32                    # KV cache block size
  max_model_len: 2048
  max_num_batched_tokens: 256
  num_gpu_blocks_override: 512
  enable_prefix_caching: false      # Phase 1-5 ã§ã¯ç„¡åŠ¹
  dtype: "bfloat16"

  additional_config:
    override_neuron_config:
      skip_warmup: True
      enable_bucketing: true        # å‹•çš„ãƒãƒƒãƒãƒ³ã‚°æœ‰åŠ¹
      pa_num_blocks: 512
      pa_block_size: 32
```

ã“ã‚Œã‚‰ã®è¨­å®šã¯ã€[å‰å›ã® Zenn è¨˜äº‹](https://zenn.dev/tosshi/articles/ef61e14fe73399) ã§æœ€é©åŒ–ã—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚

**æ¸¬å®šã®ç„¦ç‚¹**:
æœ¬èª¿æŸ»ã§ã¯ã€ã“ã®ç‰¹å®šã®è¨­å®šã«ãŠã‘ã‚‹ vllm-neuron ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã€ã‚ˆã‚Šå¤§è¦æ¨¡ãªãƒãƒƒãƒã‚µã‚¤ã‚ºã§ã¯ã€ç•°ãªã‚‹æ€§èƒ½ç‰¹æ€§ã‚’ç¤ºã™å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

:::message
**è¿½åŠ èª¿æŸ»**: å›ºå®šé•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆ97 ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã ã‘ã§ãªãã€å¯å¤‰é•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆ18-125 ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã§ã® bucketing ã®å½±éŸ¿ã‚‚æ¸¬å®šã—ã¾ã—ãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã«ã‚ˆã‚‹æ€§èƒ½ã®é•ã„ã‚’æ˜ç¢ºã«ã™ã‚‹ã“ã¨ãŒã§ãã¾ã—ãŸã€‚
:::

:::message
**NxD Inference ã¯ vllm-neuron ã§å†…éƒ¨çš„ã«æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã¨ã—ã¦åˆ©ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚å®Ÿæ…‹ã¨ã—ã¦ã¯ `override_neuron_config` ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒ NxD Inference ã«æ¸¡ã•ã‚Œã¦ã„ã‚‹å½¢ã§ã™ã€‚**
:::
::::

### 1.2 Perfetto ãƒˆãƒ¬ãƒ¼ã‚¹ã®åŸºæœ¬åˆ†æ

å‰å›ã‚‚å°‘ã—ç´¹ä»‹ã—ãŸ Perfetto ã«ã¤ã„ã¦ç´¹ä»‹ã—ã¾ã™ã€‚Perfetto ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã¯ SQLite ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ‰±ãˆã¾ã™ã€‚ã¾ãšä»¥ä¸‹ã®ã‚ˆã†ãªåˆ†æã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œã—ã¾ã—ãŸã€‚

:::details Perfetto ãƒˆãƒ¬ãƒ¼ã‚¹åˆ†æã‚³ãƒ¼ãƒ‰

```python
from perfetto.trace_processor import TraceProcessor
tp = TraceProcessor(trace='profile_output/trace.perfetto-trace')

# Operation ã”ã¨ã®é›†è¨ˆ
sql = """
SELECT name, COUNT(*) as count,
       SUM(dur) / 1e9 as total_seconds,
       AVG(dur) / 1e9 as avg_seconds
FROM slice WHERE dur > 0
GROUP BY name ORDER BY total_seconds DESC LIMIT 10
"""
```

**çµæœã®ä¸€éƒ¨**:
```
                  name   count total_seconds avg_seconds
0              unknown  156427      0.038387         0.0
1               MATMUL   21582      0.010941    0.000001
2 custom_call.17_sg0002      36      0.007028    0.000195
3            LDWEIGHTS   21212      0.004914         0.0
```

**ã‚¯ã‚¨ãƒªã®è¦‹æ–¹**:
`slice` ãƒ†ãƒ¼ãƒ–ãƒ«ã«ã¯å„ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®å®Ÿè¡Œè¨˜éŒ²ãŒæ ¼ç´ã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã®ã‚¯ã‚¨ãƒªã¯ä»¥ä¸‹ã‚’å–å¾—ã—ã¾ã™ã€‚
- `name`: ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åï¼ˆMATMUL ãªã©ã€Neuron ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ãŒç”Ÿæˆã—ãŸæ¼”ç®—ã®ç¨®é¡ï¼‰
- `count`: ãã®ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒå®Ÿè¡Œã•ã‚ŒãŸå›æ•°
- `dur`: å„å®Ÿè¡Œã®ç¶™ç¶šæ™‚é–“ï¼ˆãƒŠãƒç§’å˜ä½ã§è¨˜éŒ²ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€1e9 ã§å‰²ã£ã¦ç§’ã«å¤‰æ›ï¼‰
- `total_seconds`: ãã®ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆè¨ˆå®Ÿè¡Œæ™‚é–“ï¼ˆç§’å˜ä½ï¼‰
- `avg_seconds`: 1 å›ã‚ãŸã‚Šã®å¹³å‡å®Ÿè¡Œæ™‚é–“ï¼ˆç§’å˜ä½ï¼‰
:::

çµæœã¨ã—ã¦ã€ã¾ãšã€`custom_call.17_sg0002` ã¨ã„ã†æ“ä½œãŒãŸã£ãŸ 36 å›ã®å®Ÿè¡Œã§ 7ms ã‚‚æ¶ˆè²»ã—ã¦ã„ã‚‹ã“ã¨ãŒåˆ¤æ˜ã—ã¾ã—ãŸã€‚æ¬¡ã«ã€MATMUL ã¨ LDWEIGHTS ãŒã»ã¼åŒã˜å›æ•°å®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã™ã€‚ã•ã‚‰ã«ã€`unknown` ã¨ã„ã†åˆ†é¡ã®æ“ä½œãŒ 38ms ã§æœ€å¤§ã®æ™‚é–“ã‚’æ¶ˆè²»ã—ã¦ã„ã¾ã—ãŸã€‚

`custom_call.17_sg0002`ã€‚ã€‚ã€‚ä½•ã“ã‚Œã€‚ã€‚ã€‚

:::details [ç™ºå±•çš„å†…å®¹] NEFF åˆ†æã«ã‚ˆã‚‹ custom_call ã®èª¿æŸ»

**ç–‘å•**: `custom_call.17_sg0002` ã¨ã¯ä½•ã‹ï¼ŸRoPEï¼Ÿæ´»æ€§åŒ–é–¢æ•°ï¼Ÿä½•ã‚‰ã‹ã®ã‚«ã‚¹ã‚¿ãƒ ã‚«ãƒ¼ãƒãƒ«ï¼Ÿ

Perfetto ãƒˆãƒ¬ãƒ¼ã‚¹ã§ã¯å®Ÿè¡Œå›æ•°ã¨æ™‚é–“ã—ã‹åˆ†ã‹ã‚‰ãªã„ãŸã‚ã€NEFF (Neuron Executable File Format) ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ [unpacking](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-runtime/explore/work-with-neff-files.html) ã—ã¦é™çš„ãªæ§‹é€ ã‚’èª¿æŸ»ã—ã¾ã—ãŸã€‚(uppack ã«ã¯ `neuron-packager unpack` ã‚³ãƒãƒ³ãƒ‰ã‚’åˆ©ç”¨ã—ã¦ã‚‚è‰¯ã„ã§ã™)

**NEFF ã‹ã‚‰åˆ¤æ˜ã—ãŸã“ã¨**:

```bash
# NEFF ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ unpacking
$ dd if=neff_322059935237836.neff of=neff.tar.gz bs=1024 skip=1
$ tar -xzf neff.tar.gz

# tensor_map.json ã‚’ç¢ºèª
$ cat sg00/tensor_map.json | jq '.["custom_call.17_sg0002"]'
{
  "dtype": "float32",
  "sim_shape": [256, 1, 1],
  "kind": null,
  "is_const": false,
  "layer_name": "custom_call.17"
}
```

**åˆ†ã‹ã‚‹ã“ã¨**:
- ãƒ‡ãƒ¼ã‚¿å‹: `float32`ï¼ˆç²¾åº¦é‡è¦–ã®æ¼”ç®—ï¼‰
- ãƒ†ãƒ³ã‚½ãƒ«å½¢çŠ¶: `[256, 1, 1]`ï¼ˆæ¯”è¼ƒçš„å°ã•ã„ï¼‰
- ã‚µãƒ–ã‚°ãƒ©ãƒ•: `sg0002`
- å‹•çš„ã«è¨ˆç®—ã•ã‚Œã‚‹ä¸­é–“ãƒ†ãƒ³ã‚½ãƒ«
- `custom_call.14` ï½ `17` ã®é€£ç¶šã—ãŸæ¼”ç®—ã‚·ãƒ¼ã‚±ãƒ³ã‚¹

**Qwen3 ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‹ã‚‰æ¨æ¸¬**:

å½¢çŠ¶ `[256, 1, 1]` ã¨å‘¨è¾ºã® `dot` (MATMUL) æ“ä½œã‹ã‚‰ã€ä»¥ä¸‹ã®ã‚ˆã†ãªæ“ä½œã¨æ¨æ¸¬
- **RoPE (Rotary Position Embedding)**: ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¨ˆç®—
- **RMSNorm**: æ­£è¦åŒ–å±¤ã®çµ±è¨ˆå€¤è¨ˆç®—
- **ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹**: Softmax å‰ã®ä¸­é–“è¨ˆç®—

NEFF ã¯ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ã®é™çš„ãªæƒ…å ±ï¼ˆã‚°ãƒ©ãƒ•æ§‹é€ ã€ãƒ†ãƒ³ã‚½ãƒ«å½¢çŠ¶ã€ãƒ‡ãƒ¼ã‚¿å‹ï¼‰ã‚’å«ã¿ã¾ã™ãŒã€ä»¥ä¸‹ã¯åˆ¤æ˜ã—ãªã„ã‚ˆã†ã§ã™ã€‚
- å…·ä½“çš„ãªæ¼”ç®—ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆã©ã®ã‚«ãƒ¼ãƒãƒ«ãŒå®Ÿè¡Œã•ã‚Œã‚‹ã‹ï¼‰
- å®Ÿè¡Œå›æ•°: 36 å›ï¼ˆPerfetto ãƒˆãƒ¬ãƒ¼ã‚¹ã§æ¸¬å®šå¯èƒ½ï¼‰
- å®Ÿè¡Œæ™‚é–“: 7msï¼ˆPerfetto ãƒˆãƒ¬ãƒ¼ã‚¹ã§æ¸¬å®šå¯èƒ½ï¼‰
- åˆå›å®Ÿè¡Œæ™‚ã®é…å»¶ï¼ˆskip_warmup ã«ã‚ˆã‚‹å‹•ä½œã®é•ã„ï¼‰

NEFF åˆ†æã‹ã‚‰ã¯ã€ä½•ãŒä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹ã¯åˆ†ã‹ã‚Šã¾ã™ãŒã€ã©ã†å‹•ãã‹ã¯ Perfetto ãƒˆãƒ¬ãƒ¼ã‚¹ã§å®Ÿè¡Œæ™‚ã«æ¸¬å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ãã—ã¦ä¸Šè¨˜ä»¥ä¸Šã®è©³ç´°ãªç‰¹å®šã¯ç¾æ™‚ç‚¹ã§ã¯ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã€‚
:::

### 1.3 skip_warmup è¨­å®šã®å½±éŸ¿

è©³ç´°ã«è©¦è¡ŒéŒ¯èª¤ã—ãªãŒã‚‰å®Ÿè¡Œã—ãŸã‚¯ã‚¨ãƒªã‚’å…¨ã¦ç´¹ä»‹ã—ã¦ã„ã‚‹ã¨è†¨å¤§ã«ãªã£ã¦ã—ã¾ã†ãŸã‚å‰²æ„›ã—ã¾ã™ãŒ Phase 1 ã®æ™‚ç³»åˆ—ã®å‘½ä»¤å®Ÿè¡Œã«é–¢ã™ã‚‹èª¿æŸ»çµæœã‹ã‚‰ã€custom_call ãŒåˆå›å®Ÿè¡Œæ™‚ã«å¤§ããªé…å»¶ã‚’èµ·ã“ã—ã¦ã„ã‚‹ã“ã¨ãŒåˆ†ã‹ã£ãŸãŸã‚ã€NxD Inference ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã‚ã‚‹ `skip_warmup=False` ã‚’è©¦ã—ã¾ã—ãŸã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¯ `False` ãªã®ã§ã™ãŒä»¥å‰ã®å®Ÿé¨“ã®è©¦è¡ŒéŒ¯èª¤ã§ `True` ã«ã—ã¦ã„ã¾ã—ãŸã€‚ã“ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å¾Œã« 1 å›ã® forward å®Ÿè¡Œã‚’è¡Œã„ã€é…å»¶åˆæœŸåŒ–ã‚’å®Œäº†ã•ã›ã‚‹ã¨ã„ã†ã‚‚ã®ã§ã™ã€‚
                                                                                                                  
| è¨­å®š | å¹³å‡æ™‚é–“ |
|------|---------|
| Baseline (skip_warmup=True) | 2.992ç§’ |
| Warmup (skip_warmup=False) | 3.110ç§’ (+3.9%) |

ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã™ã‚Œã°é€Ÿããªã‚‹ã¨äºˆæƒ³ã—ã¦ã„ã¾ã—ãŸãŒã€å®Ÿéš›ã«ã¯ç´„ 4% é…ããªã‚Šã¾ã—ãŸã€‚ï¼ˆæ¸¬å®šã®ãŸã³ã«çµæœã¯å¤šå°‘å¤‰å‹•ã—ã¾ã™ï¼‰å†åº¦ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã—ã¦ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®å¤‰åŒ–ã‚’ç¢ºèªã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

:::details Operation æ•°ã®å¤‰åŒ–

```
Baseline (skip_warmup=True):
  MATMUL: 21,582å›, 10.94ms
  LDWEIGHTS: 21,212å›, 4.91ms
  ACTIVATE: 4,702å›, 1.65ms
  COPY: 83å›, 0.03ms

Warmup (skip_warmup=False):
  MATMUL: 13,497å› (-37%), 2.64ms (-76%)
  LDWEIGHTS: 13,497å› (-36%), 1.17ms (-76%)
  ACTIVATE: 4,207å› (-11%), 2.94ms (+78%)
  COPY: 554å› (+567%), 1.01ms (+3,267%)
```
:::

`skip_warmup=False` ã§ MATMUL/LDWEIGHTS ã®ä¸»è¦ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¯å®Ÿè¡Œæ™‚é–“ãŒ 76% æ¸›å°‘ã—ãŸã‚‚ã®ã®ã€ACTIVATE ã®å®Ÿè¡Œæ™‚é–“ãŒ +78%ã€COPY ã®å®Ÿè¡Œæ™‚é–“ãŒ +3,267% å¢—åŠ ã—ã€ãƒˆãƒ¼ã‚¿ãƒ«ã§ã¯é…ããªã‚Šã¾ã—ãŸã€‚


### 1.4 Neuron Profiler ã®æ¸¬å®šç¯„å›²ã®é™ç•Œ

:::message alert
ã“ã“ã§é‡è¦ãªæ°—ã¥ãï¼šNeuron Profiler ã®ãƒˆãƒ¬ãƒ¼ã‚¹æ™‚é–“ã¯ 16-17ms ãªã®ã«ã€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å…¨ä½“ã¯ç´„ 3 ç§’ã‹ã‹ã£ã¦ã„ã‚‹ã€‚**ã“ã® 16-17ms ã£ã¦ã©ã“ã‹ã‚‰ã©ã“ã¾ã§ã®ãªã‚“ã®å€¤ï¼Ÿ**
:::

èª¿æŸ»ã®çµæœã€Neuron Profiler ã®æ¸¬å®šç¯„å›²ã«é–¢ã™ã‚‹é‡è¦ãªç‰¹æ€§ãŒåˆ¤æ˜ã—ã¾ã—ãŸã€‚Neuron Profiler ã¯å®Ÿè¡Œæ™‚ã« NTFF (Neuron Trace File Format) ã¨ã„ã†ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã—ã€ãã‚Œã‚’ Perfetto ãƒˆãƒ¬ãƒ¼ã‚¹ã«å¤‰æ›ã—ã¾ã™ã€‚å„ NTFF ãƒ•ã‚¡ã‚¤ãƒ«ã¯ 1 ã¤ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ã‚°ãƒ©ãƒ•ã®å®Ÿè¡Œè¨˜éŒ²ã‚’è¡¨ã—ã¦ãŠã‚Šã€ç•°ãªã‚‹ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚„ sequence length ç”¨ã®è¤‡æ•°ã‚°ãƒ©ãƒ•ãŒå­˜åœ¨ã—ã¾ã™ã€‚

```bash
$ find profile_output -name "*.ntff" | wc -l
22  # 11ç¨®é¡ã®ã‚°ãƒ©ãƒ• Ã— 2ã‚³ã‚¢(tensor_parallel_size=2)

# NTFF ã¯ Perfetto ã«å¤‰æ›ã•ã‚Œã‚‹
$ ls profile_output/trace.perfetto-trace
trace.perfetto-trace  # ã“ã‚Œã‚’ TraceProcessor ã‚„ Perfetto UI ã§åˆ†æ
```

::::details [ç™ºå±•çš„å†…å®¹] NEFF ãƒ•ã‚¡ã‚¤ãƒ«ã¨ bucketing ã®é–¢ä¿‚

**NEFF (Neuron Executable File Format)** ã¯ã€NeuronCore ä¸Šã§å®Ÿè¡Œã•ã‚Œã‚‹ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ã‚°ãƒ©ãƒ•ã®å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ã§ã™ã€‚bucketing ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã¨ã€è¤‡æ•°ã® (batch_size, sequence_length) ã®çµ„ã¿åˆã‚ã›ã«å¯¾å¿œã™ã‚‹è¤‡æ•°ã®ã‚°ãƒ©ãƒ•ãŒäº‹å‰ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã•ã‚Œã¾ã™ã€‚

```bash
# NEFF ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†æ
$ find profile_output -name "*.neff" | wc -l
77  # è¤‡æ•°ã® PID ã‹ã‚‰ 11 ç¨®é¡ã®ã‚°ãƒ©ãƒ• Ã— è¤‡æ•°å›ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«

$ ls -lh profile_output/*/neff_*.neff | awk '{print $5}' | sort -u
801K   # Graph 1: æœ€å°ãƒã‚±ãƒƒãƒˆ
881K   # Graph 2
991K   # Graph 3
1.1M   # Graph 4
1.3M   # Graph 5
2.1M   # Graph 6
2.3M   # Graph 7
2.4M   # Graph 8
2.6M   # Graph 9
3.0M   # Graph 10
       # (åˆè¨ˆ 11 ç¨®é¡ã€124 MB)
```

**11 ã‚°ãƒ©ãƒ•ã®å®Ÿè¡Œæ™‚é–“åˆ†å¸ƒ**
```
Graph 1:  10.81 ms   Graph 2:  69.79 ms   Graph 3:  11.61 ms
Graph 4:  25.84 ms   Graph 5:  23.02 ms   Graph 6:  17.62 ms
Graph 7:  19.75 ms   Graph 8:  42.67 ms   Graph 9:  12.20 ms
Graph 10: 21.70 ms   Graph 11:  8.51 ms
```

**bucketing ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰**
1. **ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰**: 11 ã‚°ãƒ©ãƒ•ã‚’äº‹å‰ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ï¼ˆåˆå›èµ·å‹•æ™‚ï¼‰
2. **ãƒ¡ãƒ¢ãƒªã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰**: 124 MB ã® NEFF ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚°ãƒ©ãƒ•æ•°ãŒå¤šã„ã®ã§ãƒ¡ãƒ¢ãƒªã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒå¢—ãˆã‚‹ï¼‰
3. **é¸æŠã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰**: å®Ÿè¡Œæ™‚ã«å…¥åŠ›ã‚µã‚¤ã‚ºã«å¿œã˜ãŸæœ€é©ã‚°ãƒ©ãƒ•ã‚’é¸æŠ

**å›ºå®šé•·ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ï¼ˆ97 ãƒˆãƒ¼ã‚¯ãƒ³ Rerankerï¼‰ã®å ´åˆ**
- å®Ÿéš›ã«ä½¿ç”¨ã™ã‚‹ã®ã¯ 11 ã‚°ãƒ©ãƒ•ã®ã†ã¡ 1 ã¤ã ã‘
- æ®‹ã‚Š 10 ã‚°ãƒ©ãƒ•ã¯ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹ãŒä½¿ç”¨ã•ã‚Œãªã„
- bucketing=OFF ãªã‚‰ 1 ã‚°ãƒ©ãƒ•ã®ã¿ã‚³ãƒ³ãƒ‘ã‚¤ãƒ« â†’ ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãªã—

**å¯å¤‰é•·ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ï¼ˆä¸€èˆ¬çš„ãª LLM ã®ç”Ÿæˆï¼‰ã®å ´åˆ**
- ç•°ãªã‚‹é•·ã•ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒ 11 ã‚°ãƒ©ãƒ•ã«åˆ†æ•£
- å†ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ä¸è¦ã§åŠ¹ç‡çš„ãªãƒãƒƒãƒãƒ³ã‚°
- bucketing=ON ãŒé«˜é€ŸåŒ–ã«è²¢çŒ®

:::message
**ã“ã®ã‚ˆã†ã«ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã«ã‚ˆã£ã¦ bucketing ã®æ€§èƒ½ã¯ ON/OFF ã§ã©ã¡ã‚‰ãŒè‰¯ã„ã‹å¤‰å‹•ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã“ã¨ã‚’è¦šãˆã¦ãŠã„ã¦ãã ã•ã„ï¼**
:::
::::

:::message alert
**ä»Šå›ã®ç¬¬ä¸€ã®éã¡**: Neuron Profiler ã¯ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒ¬ãƒ™ãƒ«ã®åˆ†æã«ã¯æœ‰ç”¨ã ãŒã€vllm-neuron å…¨ä½“ã®æœ€é©åŒ–ã«ãŠã„ã¦åˆæ‰‹ã§ä½¿ã†ã‚‚ã®ã§ã¯ãªã„ã€‚
:::

ã‚«ã‚¹ã‚¿ãƒ ã‚«ãƒ¼ãƒãƒ«ã‚’å®Ÿè£…ã™ã‚‹ã‚ˆã†ãªã‚±ãƒ¼ã‚¹ã§ã¯ Neuron Profiler ã¯å¿…é ˆã¨è¨€ãˆã¾ã™ãŒã€æœ€é©ãªè¨­å®šã‚’æ¢ã™éš›ã®åˆæ‰‹ã§å®Ÿæ–½ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãã—ã¦ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒ¬ãƒ™ãƒ«ã®æ”¹å–„ã‚’ã™ã‚‹å‰ã« vllm-neuron å…¨ä½“ã®å®Ÿè¡Œæ™‚é–“ã®å†…ã®ã©ã®ç¨‹åº¦ã‚’ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢å´ã®æ¨è«–å‡¦ç†ãŒå ã‚ã¦ã„ã‚‹ã®ã‹ã«ã‚ˆã£ã¦æ”¹å–„ã®å„ªå…ˆåº¦ãŒå¤‰ã‚ã£ã¦ãã‚‹ã®ã§ vllm-neuron å…¨ä½“ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‚’ã—ãªã„ã¨æ„å‘³ãªã„ãã€ã¨æ€ã„ã¾ã—ãŸã€‚ã€‚ã¨ã¯ã„ãˆã€ä»Šå›å¾—ãŸ Neuron Profiler ã«é–¢ã™ã‚‹çŸ¥è¦‹ã¯æœ‰ç”¨ãªãŸã‚ã‚·ã‚§ã‚¢ã®æ„å‘³ã‚’è¾¼ã‚ã¦ Phase 1 ã‚’æ¶ˆã•ãšã«ãã®ã¾ã¾å…¬é–‹ã—ã¾ã™ã€‚

### 1.5 NEFFã€Perfetto ã¨ã¯

Phase 1 ã§ç™»å ´ã—ãŸãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¤ã„ã¦æ•´ç†ã—ã¾ã™ã€‚

:::message
NEFFï¼ˆã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ï¼‰ â†’ NTFFï¼ˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°å®Ÿè¡Œæ™‚ã®ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ â†’ **Perfetto ãƒˆãƒ¬ãƒ¼ã‚¹**ï¼ˆåˆ†æã«ä½¿ç”¨ï¼‰
:::

#### NEFF (Neuron Executable File Format)

[å‚è€ƒ: Work with NEFF Files](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-runtime/explore/work-with-neff-files.html)

**å½¹å‰²**: ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ã‚°ãƒ©ãƒ•ã®å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«

```bash
# NEFF ã®æ§‹é€ 
neff_322059935237836.neff (801KB)
â”œâ”€â”€ [1024 byte header]
â””â”€â”€ [tar.gz archive]
    â”œâ”€â”€ info.json              # ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æƒ…å ±
    â”œâ”€â”€ hlo_stats.json         # æ¼”ç®—çµ±è¨ˆï¼ˆHloMacCount: 29.2B ãªã©ï¼‰
    â”œâ”€â”€ metrics.json           # æ¨å®šãƒ¬ã‚¤ãƒ†ãƒ³ã‚·
    â”œâ”€â”€ neff.json             # ã‚°ãƒ©ãƒ•å®šç¾©ï¼ˆ373 ãƒãƒ¼ãƒ‰ï¼‰
    â””â”€â”€ sg00/                  # ã‚µãƒ–ã‚°ãƒ©ãƒ• 0
        â”œâ”€â”€ tensor_map.json   # ãƒ†ãƒ³ã‚½ãƒ«æƒ…å ±ï¼ˆ458 ãƒ†ãƒ³ã‚½ãƒ«ï¼‰
        â”œâ”€â”€ PE.bin            # Processing Element å‘½ä»¤
        â”œâ”€â”€ Activation.bin    # æ´»æ€§åŒ–é–¢æ•°å‘½ä»¤
        â”œâ”€â”€ DVE.bin           # Data Vector Engine å‘½ä»¤
        â””â”€â”€ debug_info_*.dbg  # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
```

#### NTFF (Neuron Trace File Format) - ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«

**å½¹å‰²**: Perfetto ãƒˆãƒ¬ãƒ¼ã‚¹ã«å¤‰æ›ã•ã‚Œã‚‹å‰ã®ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«

```bash
# NTFF ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¾‹ï¼ˆNeuron Profiler ãŒç”Ÿæˆï¼‰
profile_output/i-0049acfde6046f237_pid_520024/
â”œâ”€â”€ 322059935237836_instid_0_vnc_0.ntff  # Graph 1, Core 0
â”œâ”€â”€ 322059935237836_instid_0_vnc_1.ntff  # Graph 1, Core 1
â”œâ”€â”€ 729292360268366_instid_0_vnc_0.ntff  # Graph 4, Core 0
â”œâ”€â”€ 729292360268366_instid_0_vnc_1.ntff  # Graph 4, Core 1
...
â””â”€â”€ (22 files = 11 graphs Ã— 2 cores)

# Neuron Profiler ã§ Perfetto ã«å¤‰æ›
$ neuron-profile view --output-format perfetto profile_output
```

#### Perfetto ãƒˆãƒ¬ãƒ¼ã‚¹

**å½¹å‰²**: NeuronCore ä¸Šã®ä½ãƒ¬ãƒ™ãƒ«å®Ÿè¡Œãƒˆãƒ¬ãƒ¼ã‚¹

```bash
# Perfetto ãƒˆãƒ¬ãƒ¼ã‚¹
trace.perfetto-trace (110 MB)
â””â”€â”€ SQLite ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
    â”œâ”€â”€ slice ãƒ†ãƒ¼ãƒ–ãƒ«          # ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œè¨˜éŒ²
    â”‚   â””â”€â”€ MATMUL: 21,582å›, 10.94ms
    â”‚       COPY: 83å›, 0.03ms
    â”‚       custom_call.17: 36å›, 7ms
    â”œâ”€â”€ thread ãƒ†ãƒ¼ãƒ–ãƒ«         # ã‚¹ãƒ¬ãƒƒãƒ‰æƒ…å ±
    â””â”€â”€ process ãƒ†ãƒ¼ãƒ–ãƒ«        # ãƒ—ãƒ­ã‚»ã‚¹æƒ…å ±
```

::::details NEFF ã¨ Perfetto ã®æ¯”è¼ƒ

| æƒ…å ± | NEFF | Perfetto | å‚™è€ƒ |
|------|------|----------|------|
| **é™çš„æ§‹é€ ** | | | |
| ã‚°ãƒ©ãƒ•æ§‹é€ ï¼ˆãƒãƒ¼ãƒ‰ã€ãƒ†ãƒ³ã‚½ãƒ«æ•°ï¼‰ | âœ… | âŒ | NEFF unpacking ã§å–å¾— |
| ãƒ†ãƒ³ã‚½ãƒ«å½¢çŠ¶ãƒ»ãƒ‡ãƒ¼ã‚¿å‹ | âœ… | âŒ | tensor_map.json |
| æ¼”ç®—é‡ï¼ˆç†è«–å€¤ï¼‰ | âœ… | âŒ | hlo_stats.json |
| ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆç†è«–å€¤ï¼‰ | âœ… | âŒ | IfmapSizeã€OfmapSize |
| DMA ã‚­ãƒ¥ãƒ¼æ§‹æˆ | âœ… | âŒ | def.json |
| **ã‚°ãƒ©ãƒ•ãƒ¬ãƒ™ãƒ«å®Ÿè¡Œ** | | | |
| ã‚°ãƒ©ãƒ•ã”ã¨ã®å®Ÿè¡Œæ™‚é–“ | âŒ | âš ï¸ | SQL é›†è¨ˆã§è¨ˆç®—å¯èƒ½ |
| NeuronCore ã”ã¨ã®å†…è¨³ | âŒ | âš ï¸ | ã‚¹ãƒ¬ãƒƒãƒ‰åˆ¥ã«é›†è¨ˆ |
| ä½¿ç”¨ã•ã‚ŒãŸã‚°ãƒ©ãƒ•ã®è­˜åˆ¥ | âŒ | âš ï¸ | slice åã‹ã‚‰æ¨å®š |
| ã‚°ãƒ©ãƒ•é–“ã®é·ç§»æ™‚é–“ | âŒ | âš ï¸ | ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‹ã‚‰æ¨å®š |
| **ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¬ãƒ™ãƒ«å®Ÿè¡Œ** | | | |
| å®Ÿè¡Œæ™‚é–“ï¼ˆå®Ÿæ¸¬å€¤ï¼‰ | âŒ | âœ… | slice.dur |
| å®Ÿè¡Œå›æ•° | âŒ | âœ… | COUNT(*) |
| ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è©³ç´°ï¼ˆMATMULã€COPY ãªã©ï¼‰ | âŒ | âœ… | slice.name |
| ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã¨å®Ÿè¡Œé †åº | âŒ | âœ… | slice.ts |
| ä¸¦åˆ—å®Ÿè¡Œã®å¯è¦–åŒ– | âŒ | âœ… | Perfetto UI |
| åˆæœŸåŒ–é…å»¶ï¼ˆskip_warmup åŠ¹æœï¼‰ | âŒ | âœ… | åˆå›å®Ÿè¡Œæ™‚é–“ã®æ¯”è¼ƒ |
| **é«˜ãƒ¬ãƒ™ãƒ«æƒ…å ±** | | | |
| Python ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ | âŒ | âŒ | line_profiler ãªã©ãŒå¿…è¦ |
| æ¼”ç®—å†…å®¹ã®æ„å‘³ï¼ˆRoPEã€RMSNorm ãªã©ï¼‰ | âš ï¸ | âŒ | å½¢çŠ¶ã‹ã‚‰æ¨æ¸¬ã®ã¿ |

**å‡¡ä¾‹**: âœ… ç›´æ¥å–å¾—å¯èƒ½ã€âš ï¸ æ¨æ¸¬ãƒ»è¨ˆç®—ãŒå¿…è¦ã€âŒ å–å¾—ä¸å¯èƒ½
::::

---

## Phase 2: line_profiler ã«ã‚ˆã‚‹ Python ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°

Phase 1 ã§ã¯ Neuron Profiler ã«ã‚ˆã‚Š NeuronCore ãƒ¬ãƒ™ãƒ«ã®è©³ç´°ãªåˆ†æã‚’è¡Œã„ã¾ã—ãŸãŒã€Python ãƒ¬ãƒ™ãƒ«ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ï¼ˆãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã€ãƒ‡ãƒ¼ã‚¿æº–å‚™ãªã©ï¼‰ã®æ¸¬å®šã«ã¯åˆ¥ã®ãƒ„ãƒ¼ãƒ«ãŒå¿…è¦ã§ã™ã€‚ãã“ã§ line_profiler ã‚’ä½¿ç”¨ã—ã¦ Python ã‚³ãƒ¼ãƒ‰ã®è¡Œã”ã¨ã®å®Ÿè¡Œæ™‚é–“ã‚’æ¸¬å®šã—ã¾ã™ã€‚

### æ¸¬å®šã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æº–å‚™

Phase 1 ã§ä½¿ç”¨ã—ãŸ `test_reranker.py` ã¯ pytest + benchmark_capture ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ãŒã€line_profiler ã¨çµ„ã¿åˆã‚ã›ã‚‹ã¨å‡ºåŠ›ãŒè¤‡é›‘ã«ãªã‚Šã¾ã™ã€‚ãã“ã§ã€line_profiler å°‚ç”¨ã®ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¹ã‚¯ãƒªãƒ—ãƒˆ `profile_line.py` ã‚’ä½œæˆã—ã¾ã—ãŸã€‚ï¼ˆã“ã®è¾ºã‚Šã‚‚ vllm-neuron ã® Python ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã®ãŸã‚ã«ã†ã¾ãå–ã‚Œã‚‹ã‚ˆã†ã«ä»Šå¾Œ benchmark_capture ã®å®Ÿè£…ã‚’æ”¹å–„ã—ã¾ã™ï¼‰

::::details ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æ§‹é€ 

```python:profile_line.py
try:
    profile
except NameError:
    def profile(func):
        return func

# config.yaml ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿ï¼ˆtest_reranker.py ã¨åŒã˜ï¼‰
config_path = Path(__file__).parent / 'config.yaml'
with open(config_path, 'r') as f:
    config = yaml.safe_load(f)

# æ¸¬å®šå¯¾è±¡ã®é–¢æ•°ã« @profile ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’è¿½åŠ 
@profile
def build_prompts_for_vllm(pairs, tokenizer, prefix_tokens, suffix_tokens):
    """ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰"""
    # ... (test_reranker.py ã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯)

@profile
def run_reranker(llm, tokenizer, token_true_id, token_false_id,
                 prefix_tokens, suffix_tokens):
    """ãƒªãƒ©ãƒ³ã‚«ãƒ¼ã®ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # ... (test_reranker.py ã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯)

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼ˆpytest éä¾å­˜ï¼‰"""
    llm = vllm.LLM(model=model_path, **vllm_config)
    # ... åˆæœŸåŒ–ã¨ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°å®Ÿè¡Œ

if __name__ == "__main__":
    main()
```

::::

::::details ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Œå…¨ç‰ˆ
```python:profile_line.py
"""
Line profiler script for vLLM-Neuron Reranker

Run with:
    kernprof -l -v profile_line.py

Or for more detailed output:
    kernprof -l profile_line.py
    python -m line_profiler profile_line.py.lprof
"""

# line_profiler compatibility: make @profile decorator optional
try:
    profile
except NameError:
    # If not running under kernprof, @profile is a no-op
    def profile(func):
        return func

import csv
import gc
import logging
import os
import sys
from pathlib import Path

import yaml

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load configuration
config_path = Path(__file__).parent / 'config.yaml'
with open(config_path, 'r') as f:
    config = yaml.safe_load(f)

# Get model path
model_path = config['model']['path']

# Get vLLM config
vllm_config = {
    "tensor_parallel_size": config['vllm']['tensor_parallel_size'],
    "max_num_seqs": config['vllm']['max_num_seqs'],
    "block_size": config['vllm']['block_size'],
    "max_model_len": config['vllm']['max_model_len'],
    "max_num_batched_tokens": config['vllm']['max_num_batched_tokens'],
    "num_gpu_blocks_override": config['vllm']['num_gpu_blocks_override'],
    "enable_prefix_caching": config['vllm']['enable_prefix_caching'],
    "dtype": config['vllm']['dtype'],
    "disable_log_stats": config['vllm'].get('disable_log_stats', False),
}

# Add additional_config if present (Zenn article optimal settings)
if 'additional_config' in config['vllm']:
    vllm_config['additional_config'] = config['vllm']['additional_config']

# Get reranker config
reranker_config = config['reranker']
benchmark_config = config['benchmark']

# Reranker prompts
reranker_prompts = {
    'instruction': reranker_config['instruction'],
    'prefix': reranker_config['prefix'],
    'suffix': reranker_config['suffix']
}

# Token IDs
token_ids = {
    'true': reranker_config['token_true'],
    'false': reranker_config['token_false']
}

# Load CSV data
csv_file = Path(__file__).parent / reranker_config['input_file']
with open(csv_file, 'r', encoding='utf-8') as f:
    reader = csv.DictReader(f)
    rows = list(reader)

num_queries = min(len(rows), benchmark_config['num_test_queries'])
search_num = reranker_config['search_num']
batch_size = reranker_config['batch_size']
max_length = reranker_config['max_length']

logger.info(f"Loaded {len(rows)} queries from {csv_file}")
logger.info(f"Testing with first {num_queries} queries")


def format_instruction(query: str, doc: str) -> str:
    """Format instruction for reranker"""
    instruction = reranker_prompts['instruction']
    output = f"<Instruct>: {instruction}\n<Query>: {query}\n<Document>: {doc}"
    # Truncate if too long
    if len(output) >= 2000:
        output = output[:2000]
    return output


@profile
def build_prompts_for_vllm(pairs, tokenizer, prefix_tokens, suffix_tokens):
    """Build prompts with proper tokenization - PROFILING TARGET"""
    prompts = []
    budget = max_length - len(prefix_tokens) - len(suffix_tokens)

    # Tokenize pairs
    enc = tokenizer(
        list(pairs),
        padding=False,
        truncation="longest_first",
        return_attention_mask=False,
        add_special_tokens=False,
        max_length=max(8, budget),
    )

    # Build final prompts: prefix + content + suffix
    for ids in enc["input_ids"]:
        final_ids = prefix_tokens + ids + suffix_tokens
        text = tokenizer.decode(final_ids, skip_special_tokens=False)
        prompts.append(text)

    return prompts


@profile
def run_reranker(llm, tokenizer, token_true_id, token_false_id, prefix_tokens, suffix_tokens):
    """Run reranker on queries - MAIN PROFILING TARGET"""

    import vllm
    from vllm import SamplingParams

    # Get use_tqdm setting from benchmark config
    use_tqdm = benchmark_config.get('use_tqdm', True)

    # Create SamplingParams
    sampling_params = SamplingParams(
        max_tokens=1,
        temperature=0.0,
        logprobs=20,
        detokenize=True,
        allowed_token_ids=[token_true_id, token_false_id]
    )

    logger.info(f"SamplingParams configured: max_tokens=1, "
                f"allowed_tokens=[{token_ids['true']}, {token_ids['false']}]")

    # Process each query
    total_processed = 0
    for query_idx, row in enumerate(rows[:num_queries]):
        query = row["query"]

        # Get candidates
        candidates = [
            row[f"answer_{i}"]
            for i in range(search_num)
            if f"answer_{i}" in row
        ]

        # Format query-document pairs
        pairs = [format_instruction(query, doc) for doc in candidates[:search_num]]

        # Build prompts with tokenization
        prompts = build_prompts_for_vllm(pairs, tokenizer, prefix_tokens, suffix_tokens)

        # Process in batches
        query_outputs = []
        for s in range(0, len(prompts), batch_size):
            batch_prompts = prompts[s:s + batch_size]
            outputs = llm.generate(batch_prompts, sampling_params, use_tqdm=use_tqdm)
            query_outputs.extend(outputs)

        total_processed += len(query_outputs)

        if query_idx == 0:
            # Show first result for verification
            logger.info(f"Query 1: {query[:80]}...")
            logger.info(f"Generated {len(query_outputs)} scores for "
                       f"{len(candidates[:search_num])} candidates")
            if query_outputs:
                first_output = query_outputs[0]
                logger.info(f"First output: {first_output.outputs[0].text} "
                           f"(token_ids={first_output.outputs[0].token_ids})")

    logger.info(f"Profiling completed: processed {total_processed} reranker pairs")
    return total_processed


def main():
    """Main profiling function"""
    import vllm

    logger.info("Initializing vLLM-Neuron reranker...")
    logger.info(f"Model: {model_path}")
    logger.info(f"Config: block_size={vllm_config['block_size']}, "
               f"max_num_seqs={vllm_config['max_num_seqs']}, "
               f"tensor_parallel_size={vllm_config['tensor_parallel_size']}")

    # Initialize vLLM
    llm = vllm.LLM(model=model_path, **vllm_config)

    # Get tokenizer and token IDs
    tokenizer = llm.get_tokenizer()
    token_false_id = tokenizer.convert_tokens_to_ids(token_ids['false'])
    token_true_id = tokenizer.convert_tokens_to_ids(token_ids['true'])

    logger.info(f"Token IDs: {token_ids['true']}={token_true_id}, "
               f"{token_ids['false']}={token_false_id}")

    # Encode prompt templates
    prefix_tokens = tokenizer.encode(
        reranker_prompts['prefix'], add_special_tokens=False
    )
    suffix_tokens = tokenizer.encode(
        reranker_prompts['suffix'], add_special_tokens=False
    )

    logger.info(f"Prefix tokens: {len(prefix_tokens)}, Suffix tokens: {len(suffix_tokens)}")

    # Run profiling
    logger.info("Starting profiling run...")
    total = run_reranker(llm, tokenizer, token_true_id, token_false_id, prefix_tokens, suffix_tokens)

    logger.info(f"Profiling complete. Processed {total} pairs.")

    # Cleanup
    del llm
    gc.collect()


if __name__ == "__main__":
    main()
````

```yaml:config.yaml
# vLLM-Neuron Reranker Benchmark Configuration

# Model configuration
model:
  # Path to the reranker model
  # Example: "/path/to/models/Qwen3-0.6B-Reranker"
  # Use environment variable: export RERANKER_MODEL_PATH="/your/model/path"
  path: "/home/coder/data-science/investigations/inf2-vllm-performance/models/Qwen3-0.6B-Reranker"

# vLLM-Neuron engine settings
vllm:
  tensor_parallel_size: 2           # Number of NeuronCores
  max_num_seqs: 4                   # Batch size
  block_size: 32                    # KV cache block size (32 for Zenn best case, 128 for stability)
  max_model_len: 2048               # Maximum sequence length
  max_num_batched_tokens: 256       # Performance optimization
  num_gpu_blocks_override: 512      # pa_num_blocks equivalent
  enable_prefix_caching: false      # Explicit disable
  dtype: "bfloat16"                 # Data type

  # Neuron-specific overrides (Zenn article optimal settings)
  additional_config:
    override_neuron_config:
      skip_warmup: true             # Phase 1-5 ã®è¨­å®šï¼ˆè¨˜äº‹ã¨ä¸€è‡´ï¼‰
      enable_bucketing: true        # å‹•çš„ãƒãƒƒãƒãƒ³ã‚°æœ‰åŠ¹
      pa_num_blocks: 512
      pa_block_size: 32

# Reranker-specific settings
reranker:
  # Input data
  input_file: "input_sample.csv"    # CSV file with queries and candidates

  # Processing parameters
  search_num: 20                    # Number of candidates per query to process
  batch_size: 8                     # Batch size for processing prompts
  max_length: 1500                  # Maximum prompt length

  # Model-specific tokens (for Qwen3-Reranker)
  # Change these for other reranker models
  token_true: "yes"
  token_false: "no"

  # Prompt templates (for Qwen3-Reranker)
  # Customize these for your model
  prefix: |
    <|im_start|>system
    Judge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be "yes" or "no".<|im_end|>
    <|im_start|>user

  # Note: "assitant" typo is intentional for Qwen3-Reranker compatibility
  suffix: |
    <|im_end|>
    <|im_start|>assitant
    <think>

    </think>


  instruction: "Given a web search query, retrieve relevant passages that answer the query"

# Benchmark settings
benchmark:
  rounds: 5                         # Number of benchmark rounds
  warmup_rounds: 1                  # Number of warmup rounds
  num_test_queries: 10              # Number of queries to use for testing (è¨˜äº‹ã¨åŒã˜æ¡ä»¶)

# Profiler settings (optional)
profiler:
  # Clear Neuron compilation cache before benchmark
  # WARNING: First run after clearing will recompile (10-15 minutes)
  # Useful when:
  # - Model configuration changed (batch size, sequence length, etc.)
  # - Neuron SDK version changed
  # - Testing clean compilation performance
  clear_cache_before: false

  # Clear cache after benchmark (useful for CI/CD to save disk space)
  clear_cache_after: false
```
::::

ã“ã‚Œã«ã‚ˆã‚Šã€**Phase 1 ã¨åŒã˜æ¸¬å®šæ¡ä»¶**ï¼ˆåŒã˜ config.yamlã€åŒã˜å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ï¼‰ã‚’ç¶­æŒã—ãªãŒã‚‰ã€line_profiler ã«ã‚ˆã‚‹è©³ç´°ãª Python ãƒ¬ãƒ™ãƒ«ã®åˆ†æãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

### 2.1 æ¸¬å®šå¯¾è±¡ã®ç†è§£

**æ¸¬å®šå¯¾è±¡**: 1 ã‚¯ã‚¨ãƒªï¼ˆ20 å€™è£œæ–‡æ›¸ã®ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼‰ã‚’å‡¦ç†ã™ã‚‹æ™‚é–“

```yaml
reranker:
  search_num: 20        # 1 ã‚¯ã‚¨ãƒªã‚ãŸã‚Š 20 å€™è£œæ–‡æ›¸
  batch_size: 8         # 8 ãƒšã‚¢ãšã¤ãƒãƒƒãƒå‡¦ç†

vllm:
  max_num_seqs: 4       # vLLM ã®åŒæ™‚å‡¦ç†æ•°
```

```
1  ã‚¯ã‚¨ãƒª = 20 ãƒšã‚¢ Ã· batch_size=8 = 3 ãƒãƒƒãƒ
10 ã‚¯ã‚¨ãƒª = 30 ãƒãƒƒãƒ
åˆè¨ˆæ™‚é–“ = 2,992ms â†’ 1 ã‚¯ã‚¨ãƒªã‚ãŸã‚Šç´„ 300ms
```

### 2.2 line_profiler æ¸¬å®šçµæœ

::::details line_profiler ã®å®Ÿè¡Œ

**å®Ÿè¡Œç’°å¢ƒã®æº–å‚™**:

```bash
# vLLM-Neuron ç’°å¢ƒã‚’ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ãƒˆ
source /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/bin/activate

# PATH ã« Neuron SDK ã®ãƒ„ãƒ¼ãƒ«ã‚’è¿½åŠ 
export PATH="/opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/bin:$PATH"

# line_profiler ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„å ´åˆ
pip install line-profiler
```

**ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°å®Ÿè¡Œ**:

```bash
cd /path/to/my-reranker
kernprof -l -v -p vllm.v1.engine profile_line.py
```

:::message
**kernprof ã‚ªãƒ—ã‚·ãƒ§ãƒ³èª¬æ˜**:
- `-l` (--line-by-line): è¡Œã”ã¨ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‚’æœ‰åŠ¹åŒ–
- `-v` (--view): çµæœã‚’å³åº§ã«è¡¨ç¤º
- `-p vllm.v1.engine` (--prof-mod): **vllm.v1.engine ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’è‡ªå‹•ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°å¯¾è±¡ã«æŒ‡å®š**ï¼ˆã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å†…ã®å…¨é–¢æ•°ã‚’è‡ªå‹•çš„ã«ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ï¼‰
:::

å®Ÿè¡Œå¾Œã€`profile_line.py.lprof` ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã€ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«è©³ç´°ãªè¡Œã”ã¨ã®å®Ÿè¡Œæ™‚é–“ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚
::::

ä»¥ä¸‹ã«å®Ÿéš›ã« line_profiler ã®çµæœã‹ã‚‰å¾—ã‚‰ã‚ŒãŸæƒ…å ±ã‚’ç¤ºã—ã¾ã™ã€‚

```python
# llm.generate() - 30ãƒãƒƒãƒå‡¦ç†
Line 157: outputs = llm.generate(batch_prompts, sampling_params)
  - Hits: 30 batches
  - Time: 3781.560 ms (3.78ç§’)
  - Per Hit: 126.052 ms/batch
  - % Time: 99.1%

# LLMEngine.step() ã®å†…è¨³
Line 293: outputs = self.engine_core.get_output()
  - Hits: 229 steps (7.6 steps/batch)
  - Time: 3197.372 ms
  - Per Hit: 13.962 ms/step
  - % Time: 95.3%
```

line_profiler ã«ã‚ˆã‚‹æ¸¬å®šã®çµæœã€10 ã‚¯ã‚¨ãƒªï¼ˆ30 ãƒãƒƒãƒï¼‰ã®å‡¦ç†ã«åˆè¨ˆ 3.78 ç§’ã‹ã‹ã‚Šã€ãã®ã†ã¡ `llm.generate()` ã®å‘¼ã³å‡ºã—ã ã‘ã§ **99.1%ï¼ˆ3.78 ç§’ï¼‰** ã‚’å ã‚ã‚‹ã“ã¨ãŒåˆ¤æ˜ã—ã¾ã—ãŸã€‚ç´„ 3 ç§’ã‹ã‚‰æ™‚é–“ãŒå¢—ãˆã¦ã„ã‚‹ã®ã¯ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã§ã™ã€‚

ã•ã‚‰ã«é‡è¦ãªç™ºè¦‹ã¨ã—ã¦ã€**1 ãƒãƒƒãƒã‚ãŸã‚Šã®å‡¦ç†æ™‚é–“ãŒ 126.052ms** ã¨ã„ã†æ¸¬å®šå€¤ãŒå¾—ã‚‰ã‚Œã¾ã—ãŸã€‚ã“ã®å€¤ã¯ Phase 3 ã§ NxD Inference ã¨ç›´æ¥æ¯”è¼ƒã™ã‚‹éš›ã®åŸºæº–å€¤ã¨ãªã‚Šã¾ã™ã€‚ã¾ãŸã€vLLM ã®å†…éƒ¨å‡¦ç†ã‚’è©³ã—ãè¦‹ã‚‹ã¨ã€`LLMEngine.step()` ãŒ 229 å›å‘¼ã°ã‚Œã¦ãŠã‚Šã€30 ãƒãƒƒãƒã«å¯¾ã—ã¦ **å¹³å‡ 7.6 steps/batch** ã¨ã„ã†è¬ã®å€¤ãŒè¦³æ¸¬ã•ã‚Œã¾ã—ãŸã€‚ãªãœ 1 ãƒãƒƒãƒã®å‡¦ç†ã« 7.6 å›ã‚‚ã®ã‚¹ãƒ†ãƒƒãƒ—ãŒå¿…è¦ãªã®ã‹ã€ã“ã®æ™‚ç‚¹ã§ã¯ç†è§£ã§ãã¦ã„ã¾ã›ã‚“ã§ã—ãŸã€‚

### 2.3 7.6 steps/batch ã®ç†ç”±ã‚’è¿½ã†

ã“ã®è¬ã‚’è§£æ˜ã™ã‚‹ãŸã‚ã€`LLMEngine.step()` ã®ä¸­èº«ã‚’ã•ã‚‰ã«è©³ã—ãèª¿ã¹ã‚‹ã“ã¨ã«ã—ã¾ã—ãŸã€‚line_profiler ã® `-p vllm.v1.engine` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚Šã€vLLM å†…éƒ¨ã®ã‚³ãƒ¼ãƒ‰ã‚‚è‡ªå‹•çš„ã«ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã•ã‚Œã¦ã„ã¾ã™ã€‚

`LLMEngine.step()` ã®å‡¦ç†æ™‚é–“ã®ã»ã¼å…¨ã¦ï¼ˆ95.3%ï¼‰ãŒ `engine_core.get_output()` ã¨ã„ã†å˜ä¸€ã®é–¢æ•°å‘¼ã³å‡ºã—ã§è²»ã‚„ã•ã‚Œã¦ã„ã¾ã—ãŸã€‚ã•ã‚‰ã«ãã® `get_output()` ã®ä¸­èº«ã‚’è¦‹ã‚‹ã¨ã€**100% ãŒ `outputs_queue.get()` ã¨ã„ã†ã‚­ãƒ¥ãƒ¼ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿å–å¾—å‡¦ç†**ã§ã—ãŸã€‚

```python
# LLMEngine.step() ã®ä¸­èº«
Line 293: outputs = self.engine_core.get_output()
  - Time: 3197.372 ms (95.3% of step())

# get_output() ã®ä¸­èº«
Line 715: outputs = self.outputs_queue.get()
  - Time: 3194.6 ms
  - % Time: 100.0% of get_output()
```

ã¤ã¾ã‚Šã€ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã¯ `outputs_queue.get()` ã§ã‚­ãƒ¥ãƒ¼ã‹ã‚‰çµæœãŒé€ã‚‰ã‚Œã¦ãã‚‹ã®ã‚’ãŸã **å¾…ã£ã¦ã„ã‚‹ã ã‘**ã§ã—ãŸã€‚ã“ã‚Œã¯å®Ÿéš›ã®æ¨è«–å‡¦ç†ãŒåˆ¥ãƒ—ãƒ­ã‚»ã‚¹ã§è¡Œã‚ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚ã“ã“ã§ vLLM v1 ã®ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å­˜åœ¨ã‚’æ€ã„å‡ºã—ã¾ã—ãŸã€‚

ï¼ˆä»¥ä¸‹ã®è¨˜äº‹ã«å†…éƒ¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è§£èª¬ãŒã‚ã‚Šã¾ã™ï¼‰

https://zenn.dev/tosshi/articles/f64ba0b86e330b

vLLM v1 ã§ã¯ã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å—ã‘å–ã‚‹ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã¨ã€å®Ÿéš›ã«æ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹ Worker ãƒ—ãƒ­ã‚»ã‚¹ãŒåˆ†é›¢ã•ã‚Œã¦ã„ã¾ã™ã€‚ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã¯ `llm.generate()` ã‚’å‘¼ã³å‡ºã™ã¨ã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ Worker ãƒ—ãƒ­ã‚»ã‚¹ã«é€ä¿¡ã—ã€`outputs_queue.get()` ã§ãƒ–ãƒ­ãƒƒã‚¯ã—ã¦çµæœã‚’å¾…ã¡ã¾ã™ã€‚ä¸€æ–¹ã€Worker ãƒ—ãƒ­ã‚»ã‚¹ã¯ NeuronCore ã§ã®æ¨è«–å®Ÿè¡Œã€çµæœã®ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã€ãã—ã¦ãƒ—ãƒ­ã‚»ã‚¹é–“é€šä¿¡ã‚’é€šã˜ã¦ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã«çµæœã‚’è¿”ã—ã¾ã™ã€‚ã“ã®æ§‹é€ ã‚’å›³ç¤ºã™ã‚‹ã¨ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

**vLLM v1 ã®ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ (vllm-neuron)**

```mermaid
%%{init: {'theme':'dark'}}%%
graph LR
    subgraph MainProcess["Main Process"]
        A[LLMEngine.step] --> B[get_output]
        B --> C[outputs_queue.get<br/>13.962 ms/step]
    end

    subgraph WorkerProcess["Worker Process"]
        D[EngineCore] --> E[execute_model]
        E --> F[Neuron å®Ÿè¡Œ]
        F --> G[ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³]
        G --> H[ZMQ é€ä¿¡]
    end

    H -->|IPC| C

    style MainProcess fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style WorkerProcess fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style C fill:#0f3460,stroke:#16213e,color:#fff
    style F fill:#0f3460,stroke:#16213e,color:#fff
```

line_profiler ã¯ Python ã®æ¨™æº–çš„ãªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ã¨åŒæ§˜ã«ã€**å®Ÿè¡Œä¸­ã®ãƒ—ãƒ­ã‚»ã‚¹ã®ã‚³ãƒ¼ãƒ‰ã—ã‹æ¸¬å®šã§ãã¾ã›ã‚“**ã€‚ã¤ã¾ã‚Šã€Worker ãƒ—ãƒ­ã‚»ã‚¹ã§å®Ÿè¡Œã•ã‚Œã‚‹ `execute_model()` ã‚„ NeuronCore ã§ã®æ¨è«–å‡¦ç†ã¯ã€ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã‹ã‚‰è¦‹ã‚‹ã¨ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã§ã™ã€‚

æ¸¬å®šã§ããŸã®ã¯ `outputs_queue.get()` ã§å¾…æ©Ÿã—ã¦ã„ã‚‹æ™‚é–“ï¼ˆ13.962ms/stepï¼‰ã ã‘ã§ã‚ã‚Šã€ã“ã®æ™‚é–“ã«ã¯æ¨è«–ã€IPC ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãªã©ã®ã™ã¹ã¦ã®æ™‚é–“ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚

ã§ã¯ã€ãªãœ 1 ãƒãƒƒãƒã®å‡¦ç†ã«å¹³å‡ 7.6 å›ã‚‚ `step()` ãŒå‘¼ã°ã‚Œã‚‹ã®ã§ã—ã‚‡ã†ã‹ã€‚ã“ã‚Œã¯ vLLM v1 ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã®å‹•ä½œæ–¹æ³•ã¨ã—ã¦ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ãŒä½•åº¦ã‚‚ `step()` ã‚’ç¢ºèªã—ã¦ã‚­ãƒ¥ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯ã—ç¶šã‘ã¦ã„ã‚‹ã‹ã‚‰ã§ã™ã€‚1 ãƒãƒƒãƒã‚ãŸã‚Šå¹³å‡ã—ã¦ 7.6 å›ã‚­ãƒ¥ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã„ãŸã¨ã„ã†ã“ã¨ã§ã™ã€‚

è¬ã¯è§£ã‘ã¾ã—ãŸãŒã€è‚å¿ƒã® **Worker ãƒ—ãƒ­ã‚»ã‚¹å†…ã§ã® NeuronCore ã®æ¨è«–å‡¦ç†æ™‚é–“**ã‚’åˆ†è§£ã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚`outputs_queue.get()` ã® 13.962 ms ã«ã¯ã€æ¨è«–å®Ÿè¡Œã€ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã€IPC é€šä¿¡ã®ã™ã¹ã¦ãŒå«ã¾ã‚Œã¦ãŠã‚Šã€line_profiler ã§ã¯ã“ã‚Œä»¥ä¸Šåˆ†è§£ã‚’ã™ã‚‹ã®ã¯å¤§å¤‰ãã†ã§ã™ã€‚

### 2.4 æ¸¬å®šã®é™ç•Œã¨ä»Šå¾Œã®æ–¹å‘æ€§

line_profiler ã«ã‚ˆã‚‹æ¸¬å®šã§åˆ¤æ˜ã—ãŸã“ã¨ã‚’æ•´ç†ã™ã‚‹ã¨ã€ä»¥ä¸‹ã®ã‚ˆã†ãªæ§‹é€ ã«ãªã‚Šã¾ã™ã€‚å…¨ä½“ã¨ã—ã¦ 126.052 ms/batch ã¨ã„ã†å‡¦ç†æ™‚é–“ã¯æ¸¬å®šã§ãã¾ã—ãŸãŒã€ãã®å†…è¨³ã®å¤§éƒ¨åˆ†ï¼ˆ84.2%ï¼‰ã®è©³ç´°ãŒä¸æ˜ã¨ã„ã†çŠ¶æ³ã§ã™ã€‚

ã“ã®çŠ¶æ³ã‚’æ‰“é–‹ã™ã‚‹ãŸã‚ã€NxD Inference ã‚’ç›´æ¥ä½¿ã£ãŸæ¸¬å®šã‚’è©¦ã¿ã¾ã—ãŸã€‚

## Phase 3: NxD Inference ç›´æ¥æ¸¬å®š

Worker ãƒ—ãƒ­ã‚»ã‚¹ã®ç›´æ¥æ¸¬å®šãŒå›°é›£ï¼ˆé¢å€’ï¼‰ãªãŸã‚ã€**vLLM ã‚’ä½¿ã‚ãšã« NxD Inference ã‚’ç›´æ¥ä½¿ç”¨**ã—ã¦ç´”ç²‹ãªæ¨è«–å®Ÿè¡Œæ™‚é–“ã‚’æ¸¬å®šã™ã‚‹ã“ã¨ã«ã—ã¾ã—ãŸã€‚è¨­å®šã¯ã“ã‚Œã¾ã§ã¨åŒã˜ã«ã—ã¦ãŠãã¾ã™ã€‚

### 3.1 vLLM æ¸¬å®šï¼ˆ30ãƒãƒƒãƒï¼‰

::::details ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æ§‹é€ 
```python:benchmark_30batches.py
import time
from vllm import LLM, SamplingParams

llm = LLM(model=model_path, **vllm_config)
sampling_params = SamplingParams(max_tokens=1, temperature=0.0, logprobs=20)

# 97ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ Ã— 4 = ãƒãƒƒãƒ
batch_prompts = [prompt_template] * 4

# Warmup
llm.generate(batch_prompts, sampling_params, use_tqdm=False)

# æ¸¬å®š: 30ãƒãƒƒãƒ
batch_times = []
for i in range(30):
    start = time.perf_counter()
    outputs = llm.generate(batch_prompts, sampling_params, use_tqdm=False)
    elapsed = (time.perf_counter() - start) * 1000
    batch_times.append(elapsed)
```
::::

::::details ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Œå…¨ç‰ˆ
```python:benchmark_30batches.py
"""
30ãƒãƒƒãƒã®çµ±ä¸€æ¸¬å®šï¼ˆvLLM bucketing=Trueï¼‰
"""
import logging
import time
import yaml
import vllm
from vllm import SamplingParams

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load config
with open('config.yaml', 'r') as f:
    config = yaml.safe_load(f)

model_path = config['model']['path']

# vLLM config with bucketing=True
vllm_config = {
    "tensor_parallel_size": config['vllm']['tensor_parallel_size'],
    "max_num_seqs": config['vllm']['max_num_seqs'],
    "block_size": config['vllm']['block_size'],
    "max_model_len": config['vllm']['max_model_len'],
    "max_num_batched_tokens": config['vllm']['max_num_batched_tokens'],
    "num_gpu_blocks_override": config['vllm']['num_gpu_blocks_override'],
    "enable_prefix_caching": config['vllm']['enable_prefix_caching'],
    "dtype": config['vllm']['dtype'],
}

# Add additional_config (bucketing=True)
if 'additional_config' in config['vllm']:
    vllm_config['additional_config'] = config['vllm']['additional_config']

logger.info("Initializing vLLM with bucketing=True...")
logger.info(f"Config: {vllm_config}")

llm = vllm.LLM(model=model_path, **vllm_config)
tokenizer = llm.get_tokenizer()

# Reranker prompt template (97 tokens avg)
prompt_template = """<|im_start|>system
Judge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be "yes" or "no".<|im_end|>
<|im_start|>user
<Instruct>: Given a web search query, retrieve relevant passages that answer the query
<Query>: ãƒ—ãƒ­ã‚»ã‚¹ã«ã¤ã„ã¦æ•™ãˆã¦
<Document>: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®çµ±åˆã•ã‚ŒãŸæ©Ÿèƒ½ã‚’åˆ†æã™ã‚‹æ–¹æ³•<|im_end|>
<|im_start|>assitant
<think>

</think>
"""

# Create 30 batches of 4 prompts each (max_num_seqs=4)
batch_prompts = [prompt_template] * 4

sampling_params = SamplingParams(
    max_tokens=1,
    temperature=0.0,
    logprobs=20,
    detokenize=True,
)

logger.info("Running 30-batch benchmark...")
batch_times = []

for i in range(30):
    start = time.perf_counter()
    outputs = llm.generate(batch_prompts, sampling_params, use_tqdm=False)
    elapsed = (time.perf_counter() - start) * 1000
    batch_times.append(elapsed)
    
    if (i + 1) % 10 == 0:
        logger.info(f"  Batch {i+1}/30: {elapsed:.2f} ms")

logger.info(f"\n=== Results (30 batches, bucketing=True) ===")
logger.info(f"Average: {sum(batch_times)/len(batch_times):.2f} ms/batch")
logger.info(f"Min: {min(batch_times):.2f} ms")
logger.info(f"Max: {max(batch_times):.2f} ms")
logger.info(f"Total: {sum(batch_times):.2f} ms")
```
::::

çµæœã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚

```
Average: 64.33 ms/batch
Min: 60.78 ms
Max: 85.76 ms
```

### 3.2 NxD Inference æ¸¬å®š

::::details NxD Inference æ¸¬å®šæ‰‹é †ï¼ˆ4ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰

**ç’°å¢ƒæº–å‚™**

```bash
# Neuron SDK ç’°å¢ƒã‚’ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ãƒˆ
source /opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/bin/activate

# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
mkdir -p ~/data-science/investigations/neuronx-distributed-inference
cd ~/data-science/investigations/neuronx-distributed-inference

# neuronx-distributed-inference ãƒªãƒã‚¸ãƒˆãƒªã‚¯ãƒ­ãƒ¼ãƒ³
git clone https://github.com/aws-neuron/neuronx-distributed-inference.git
cd neuronx-distributed-inference
```

**ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæº–å‚™**

Phase 1 ã¨åŒã˜ 97 ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ï¼š

```bash
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç’°å¢ƒå¤‰æ•°ã«ã‚»ãƒƒãƒˆï¼ˆ97ãƒˆãƒ¼ã‚¯ãƒ³ã€Qwen3-Reranker å½¢å¼ï¼‰
export PROMPT='<|im_start|>system
Judge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be "yes" or "no".<|im_end|>
<|im_start|>user
<Instruct>: Given a web search query, retrieve relevant passages that answer the query
<Query>: ãƒ—ãƒ­ã‚»ã‚¹ã«ã¤ã„ã¦æ•™ãˆã¦
<Document>: ã‚µãƒ³ãƒ—ãƒ«æ–‡æ›¸å†…å®¹<|im_end|>
<|im_start|>assitant
<think>

</think>
'

# ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’è¨­å®š
export MODEL_PATH="/path/to/Qwen3-0.6B-Reranker"
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³ A: bucketing=OFF, prefix-caching=OFFï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰**

```bash
python src/neuronx_distributed_inference/inference_demo.py \
  --model-type qwen3 \
  --task-type causal-lm \
  run \
  --model-path "$MODEL_PATH" \
  --prompt "$PROMPT" \
  --tp-degree 2 \
  --batch-size 4 \
  --seq-len 128 \
  --max-context-length 128 \
  --max-length 128 \
  --on-device-sampling \
  --top-k 1 \
  --benchmark \
  --benchmark-report-path /tmp/benchmark_A_v2.json

# çµæœç¢ºèª
cat /tmp/benchmark_A_v2.json | jq
```

**çµæœï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³ Aï¼‰:**
```json
{
  "context_encoding_model": {
    "latency_ms_avg": 11.972224712371826,
    "latency_ms_p50": 11.97052001953125,
    "latency_ms_p90": 12.050032615661621,
    "latency_ms_p95": 12.057197093963623,
    "throughput": 42781.77658389653
  }
}
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³ B: bucketing=ON, prefix-caching=OFFï¼ˆ+0.4%ï¼‰**

```bash
python src/neuronx_distributed_inference/inference_demo.py \
  --model-type qwen3 \
  --task-type causal-lm \
  run \
  --model-path "$MODEL_PATH" \
  --prompt "$PROMPT" \
  --tp-degree 2 \
  --batch-size 4 \
  --seq-len 128 \
  --max-context-length 128 \
  --max-length 128 \
  --on-device-sampling \
  --top-k 1 \
  --enable-bucketing \
  --context-encoding-buckets 32 64 96 128 \
  --token-generation-buckets 32 64 96 128 \
  --benchmark \
  --benchmark-report-path /tmp/benchmark_B_v2.json

# çµæœç¢ºèª
cat /tmp/benchmark_B_v2.json | jq
```

**çµæœï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³Bï¼‰:**
```json
{
  "context_encoding_model": {
    "latency_ms_avg": 12.021279335021973,
    "latency_ms_p50": 11.997222900390625,
    "latency_ms_p90": 12.201786041259766,
    "latency_ms_p95": 12.22454309463501,
    "throughput": 42607.99899294344
  }
}
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³ C: bucketing=OFF, prefix-caching=ONï¼ˆã‚¨ãƒ©ãƒ¼ï¼‰**

```bash
python src/neuronx_distributed_inference/inference_demo.py \
  --model-type qwen3 \
  --task-type causal-lm \
  run \
  --model-path "$MODEL_PATH" \
  --prompt "$PROMPT" \
  --tp-degree 2 \
  --batch-size 4 \
  --seq-len 128 \
  --max-context-length 128 \
  --max-length 128 \
  --on-device-sampling \
  --top-k 1 \
  --is-continuous-batching \
  --ctx-batch-size 1 \
  --enable-block-kv-layout \
  --enable-prefix-caching \
  --pa-num-blocks 16 \
  --pa-block-size 32 \
  --benchmark

# ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ
# TypeError: unsupported operand type(s) for -: 'NoneType' and 'NoneType'
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³ D: bucketing=ON, prefix-caching=ONï¼ˆã‚¨ãƒ©ãƒ¼ï¼‰**

```bash
python src/neuronx_distributed_inference/inference_demo.py \
  --model-type qwen3 \
  --task-type causal-lm \
  run \
  --model-path "$MODEL_PATH" \
  --prompt "$PROMPT" \
  --tp-degree 2 \
  --batch-size 4 \
  --seq-len 128 \
  --max-context-length 128 \
  --max-length 128 \
  --on-device-sampling \
  --top-k 1 \
  --enable-bucketing \
  --context-encoding-buckets 32 64 96 128 \
  --token-generation-buckets 32 64 96 128 \
  --is-continuous-batching \
  --ctx-batch-size 1 \
  --enable-block-kv-layout \
  --enable-prefix-caching \
  --pa-num-blocks 16 \
  --pa-block-size 32 \
  --benchmark

# ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ
# TypeError: can only concatenate list (not "NoneType") to list
```

**æ¸¬å®šçµæœã‚µãƒãƒªãƒ¼ï¼ˆå›ºå®šé•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: 97ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰:**

| ãƒ‘ã‚¿ãƒ¼ãƒ³ | Bucketing | Prefix Caching | çµæœ | å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· (ms) |
|---------|-----------|----------------|------|---------------------|
| **A** | OFF | OFF | âœ… æˆåŠŸ | **11.97** |
| **B** | ON | OFF | âœ… æˆåŠŸ | **12.02** (+0.4%) |
| **C** | OFF | ON | âŒ ã‚¨ãƒ©ãƒ¼ | - |
| **D** | ON | ON | âŒ ã‚¨ãƒ©ãƒ¼ | - |

:::message alert
**é‡è¦ãªç™ºè¦‹**: bucketing ã‚’æœ‰åŠ¹ã«ã—ã¦ã‚‚ã€å›ºå®šé•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆ97ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã®é€£ç¶šå‡¦ç†ã§ã¯ **+0.4%** ã¨ã»ã¼å½±éŸ¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã“ã‚Œã¯ Phase 1 ã§ vLLM çµŒç”±ã§æ¸¬å®šã—ãŸã¨ãã®çµæœï¼ˆbucketing ã§é«˜é€ŸåŒ–ï¼‰ã¨ã¯ç•°ãªã‚‹å‚¾å‘ã§ã™ã€‚

ã“ã®è¬ã¯å¾Œç¶šã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã€Œbucketing è¨­å®šã®äºˆæƒ³å¤–ã®å½±éŸ¿ã€ã§å¯å¤‰é•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ã—ãŸè¿½åŠ æ¸¬å®šã«ã‚ˆã‚Šè§£æ˜ã•ã‚Œã¾ã™ã€‚
:::

::::

### 3.3 ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰æ¯”è¼ƒ

| æ¸¬å®šå¯¾è±¡ | å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· (ms/batch) | Min (ms) | Max (ms) |
|---------|-------------------------|----------|----------|
| **vLLM** | **64.33** | 60.78 | 85.76 |
| **NxD Inference** | **11.97** | - | - |

**ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰å†…è¨³**

```mermaid
%%{init: {'theme':'dark'}}%%
graph LR
    subgraph NxD["NxD Inference (11.97 ms)"]
        A[Neuron å®Ÿè¡Œ<br/>11.97 ms<br/>100%]
    end

    subgraph vLLM["vLLM (64.33 ms)"]
        B[Neuron å®Ÿè¡Œ<br/>11.97 ms<br/>19%]
        C[Python ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰<br/>52.36 ms<br/>81%]
        B -.-> C
    end

    style NxD fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style vLLM fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style A fill:#0f3460,stroke:#16213e,color:#fff
    style B fill:#0f3460,stroke:#16213e,color:#fff
    style C fill:#8b4513,stroke:#16213e,color:#fff
```

ç´”ç²‹ãª Neuron å®Ÿè¡Œæ™‚é–“ã¯ 11.97 ms (19%) ã§ã‚ã‚‹ã®ã«å¯¾ã—ã€vLLM ã® Python ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒ 52.36 ms (81%) ã‚’å ã‚ã¦ã„ã¾ã™ã€‚

**vLLM ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ = 52ms** ã¨ã„ã†ã®ãŒã“ã‚Œã§ã‚ã‹ã‚Šã¾ã—ãŸï¼ãŸã ã—ä»Šå›ã®æ¸¬å®šã‚±ãƒ¼ã‚¹ã«ä¾å­˜ã™ã‚‹ã“ã¨ã«æ³¨æ„ãŒå¿…è¦ã§ã™ã€‚

## Phase 4: bucketing è¨­å®šå½±éŸ¿ã®æ¸¬å®š

### 4 ãƒ‘ã‚¿ãƒ¼ãƒ³æ¸¬å®šã®å®Ÿæ–½

NxD Inference ã§ä»¥ä¸‹ã® 4 ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¸¬å®šã—ã¾ã—ãŸï¼ˆå›ºå®šé•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: 97 ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ï¼š

| ãƒ‘ã‚¿ãƒ¼ãƒ³ | Prefix Caching | Bucketing | çµæœ | å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· (ms) |
|----------|----------------|-----------|------|---------------------|
| **A** | OFF | OFF | âœ… æˆåŠŸ | **11.97** |
| **B** | OFF | ON | âœ… æˆåŠŸ | **12.02** |
| **C** | ON | OFF | âŒ å¤±æ•—* | - |
| **D** | ON | ON | âŒ å¤±æ•—* | - |

\* Prefix caching ã‚¨ãƒ©ãƒ¼ï¼ˆå¾Œè¿°ï¼‰

```mermaid
%%{init: {'theme':'dark'}}%%
graph TD
    subgraph vLLM["vllm-neuron"]
        A1["bucketing=False<br/>126.05 ms"] -.->|2 å€é«˜é€ŸåŒ–| A2["bucketing=True<br/>64.33 ms âœ…"]
    end

    subgraph NxD["NxD Inference"]
        B1["bucketing=OFF<br/>11.97 ms âœ…"] -.->|+0.4%| B2["bucketing=ON<br/>12.02 ms"]
    end

    style vLLM fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style NxD fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style A1 fill:#8b4513,stroke:#16213e,color:#fff
    style A2 fill:#0f3460,stroke:#16213e,color:#fff
    style B1 fill:#0f3460,stroke:#16213e,color:#fff
    style B2 fill:#8b4513,stroke:#16213e,color:#fff
```

vLLM ã§ã¯ bucketing ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã“ã¨ã§ 126.05 ms ã‹ã‚‰ 64.33 ms ã¸ã¨ 2 å€é«˜é€ŸåŒ–ã•ã‚Œã¾ã—ãŸã€‚ä¸€æ–¹ã€NxD Inference ã‚’ç›´æ¥ä½¿ç”¨ã—ãŸå ´åˆã€bucketing ã‚’æœ‰åŠ¹ã«ã—ã¦ã‚‚ 11.97 ms ã‹ã‚‰ 12.02 ms ã¸ã¨ +0.4% ã®å¾®å¢—ã«ã¨ã©ã¾ã‚Šã¾ã—ãŸã€‚

### è€ƒãˆã‚‰ã‚Œã‚‹ç†ç”±

**æ¸¬å®šæ¡ä»¶ã®é•ã„: å›ºå®šé•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ**

ä»Šå›ã®æ¸¬å®šã§ã¯ã€å…¨ãåŒã˜ 97 ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç¹°ã‚Šè¿”ã—ä½¿ç”¨ã—ã¾ã—ãŸã€‚ä¸Šè¿°ã—ã¾ã—ãŸãŒ NEFF ã®åˆ†æã‹ã‚‰ `bucketing=OFF` ã®å ´åˆã¯ 1 ã‚°ãƒ©ãƒ•ã§å…¨ã¦ã®å‡¦ç†ãŒå®Ÿæ–½ã•ã‚Œã¾ã™ãŒã€`ON` ã®å ´åˆã¯è¤‡æ•°ã‚°ãƒ©ãƒ•ã‚’é¸æŠã—ãŸã‚Šãƒ¡ãƒ¢ãƒªã¨ã—ã¦ä¿æŒã™ã‚‹ã€ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹ãªã©ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒç™ºç”Ÿã—ã¾ã™ã€‚

**å›ºå®šé•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å ´åˆ**:
- å¸¸ã«åŒã˜ãƒã‚±ãƒƒãƒˆï¼ˆ128ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã‚’ä½¿ç”¨
- Bucketing ã®ãƒ¡ãƒªãƒƒãƒˆ: ãªã—ï¼ˆå‹•çš„ãªæœ€é©åŒ–ã®ä½™åœ°ãŒãªã„ï¼‰
- Bucketing ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰: ã‚°ãƒ©ãƒ•é¸æŠã€ãƒ¡ãƒ¢ãƒªç®¡ç†
- **çµæœ**: Bucketing OFF ã®æ–¹ãŒã‚ãšã‹ã«é«˜é€Ÿï¼ˆ+0.4% ã®å·®ï¼‰

ã¤ã¾ã‚Š bucketing è¨­å®šã¯ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚„ä½¿ç”¨ã‚±ãƒ¼ã‚¹ã«ä¾å­˜ã—ã€å›ºå®šé•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã¯ bucketing ã«ã‚ˆã£ã¦å¾—ã‚‰ã‚Œã‚‹æ©æµãŒã»ã¼ãªãã€ã‚ãšã‹ãªã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã ã‘ãŒè¦³æ¸¬ã•ã‚Œã¾ã—ãŸã€‚ä¸€æ–¹ã€å¯å¤‰é•·ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ï¼ˆä¸€èˆ¬çš„ãª LLM ã®ç”Ÿæˆã‚¿ã‚¹ã‚¯ï¼‰ã§ã¯åŠ¹ç‡çš„ãªå‹•çš„ãƒãƒƒãƒãƒ³ã‚°ã®æ©æµã‚’å¾—ã‚‰ã‚Œã‚‹ãŸã‚ bucketing=ON ãŒæ€§èƒ½ã‚’æ”¹å–„ã™ã‚‹ã¨äºˆæƒ³ã•ã‚Œã¾ã™ã€‚

**å®Ÿéš›ã«ã€97 ãƒˆãƒ¼ã‚¯ãƒ³ã®å›ºå®šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å¯å¤‰ã«ã—ã¦å®Ÿé¨“ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚**

### å¯å¤‰é•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã®æ¤œè¨¼

ä»®èª¬ã‚’æ¤œè¨¼ã™ã‚‹ãŸã‚ã€å¯å¤‰é•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã®æ¸¬å®šã‚’å®Ÿæ–½ã—ã¾ã—ãŸã€‚

**æ¸¬å®šæ¡ä»¶**:
- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ•°: 16å€‹ï¼ˆ4 ãƒã‚±ãƒƒãƒˆ Ã— 4 ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰
- ãƒã‚±ãƒƒãƒˆæ§‹æˆ: 32, 64, 96, 128 ãƒˆãƒ¼ã‚¯ãƒ³
- å®Ÿéš›ã®é•·ã•: 18-125 ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª¤å·®ã‚’å«ã‚€ï¼‰
- æ¸¬å®šãƒ„ãƒ¼ãƒ«: inference_demo.pyï¼ˆNxD Inference ç›´æ¥ä½¿ç”¨ï¼‰

::::details å¯å¤‰é•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```python:create_variable_prompts_v2.py
import json
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("/path/to/Qwen3-0.6B-Reranker")

# ãƒã‚±ãƒƒãƒˆã”ã¨ã®ç›®æ¨™ãƒˆãƒ¼ã‚¯ãƒ³æ•°
bucket_targets = {
    32: [20, 24, 28, 31],
    64: [52, 56, 60, 63],
    96: [84, 88, 92, 95],
    128: [116, 120, 124, 127]
}

prompts = []
for bucket, targets in bucket_targets.items():
    for target in targets:
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª¿æ•´ã—ã¦ç›®æ¨™ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã«è¿‘ã¥ã‘ã‚‹
        content = "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®çµ±åˆã•ã‚ŒãŸæ©Ÿèƒ½ã‚’åˆ†æã™ã‚‹" * (target // 10)
        prompt = f"""<|im_start|>system
Judge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be "yes" or "no".<|im_end|>
<|im_start|>user
<Instruct>: Given a web search query, retrieve relevant passages that answer the query
<Query>: ãƒ—ãƒ­ã‚»ã‚¹ã«ã¤ã„ã¦æ•™ãˆã¦
<Document>: {content}<|im_end|>
<|im_start|>assitant
<think>

</think>
"""
        token_ids = tokenizer.encode(prompt, add_special_tokens=False)
        actual_length = len(token_ids)
        prompts.append({
            "bucket": bucket,
            "target": target,
            "actual": actual_length,
            "prompt": prompt
        })

# ä¿å­˜
with open('/tmp/variable_prompts.json', 'w', encoding='utf-8') as f:
    json.dump(prompts, f, ensure_ascii=False, indent=2)

print(f"Generated {len(prompts)} prompts")
print(f"Length range: {min(p['actual'] for p in prompts)}-{max(p['actual'] for p in prompts)} tokens")
```
::::

**æ¸¬å®šçµæœ**:

| æ¡ä»¶ | Context Encoding å¹³å‡ | å·®åˆ† | æ”¹å–„ç‡ |
|------|---------------------|------|--------|
| **å›ºå®šé•· (97 ãƒˆãƒ¼ã‚¯ãƒ³)** | | | |
| Bucketing OFF | 11.97 ms | baseline | - |
| Bucketing ON | 12.02 ms | +0.05 ms | **+0.4%** (æ‚ªåŒ–) |
| **å¯å¤‰é•· (18-125 ãƒˆãƒ¼ã‚¯ãƒ³)** | | | |
| Bucketing OFF | 17.18 ms | baseline | - |
| Bucketing ON | 13.87 ms | -3.31 ms | **-19.3%** (æ”¹å–„) |

::::details å¯å¤‰é•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ¸¬å®šã®è©³ç´°çµæœ

**Bucketing OFF**:
```json
{
  "context_encoding_model": {
    "latency_ms_avg": 17.1828031539917,
    "latency_ms_p50": 15.299081802368164,
    "latency_ms_p90": 23.46792221069336,
    "latency_ms_p95": 23.482704162597656,
    "throughput": 29797.23363012853
  }
}
```

**Bucketing ON**:
```json
{
  "context_encoding_model": {
    "latency_ms_avg": 13.871526718139648,
    "latency_ms_p50": 11.935114860534668,
    "latency_ms_p90": 19.990873336791992,
    "latency_ms_p95": 20.034921169281006,
    "throughput": 36910.14049163478
  }
}
```

::::

### çµè«–: Bucketing ã®åŠ¹æœã¯ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã«ä¾å­˜

```mermaid
%%{init: {'theme':'dark'}}%%
graph TD
    subgraph Fixed["å›ºå®šé•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ (97 ãƒˆãƒ¼ã‚¯ãƒ³)"]
        A1["Bucketing OFF<br/>11.97 ms âœ…"] -.->|+0.4%| A2["Bucketing ON<br/>12.02 ms"]
    end

    subgraph Variable["å¯å¤‰é•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ (18-125 ãƒˆãƒ¼ã‚¯ãƒ³)"]
        B1["Bucketing OFF<br/>17.18 ms"] -.->|-19.3%| B2["Bucketing ON<br/>13.87 ms âœ…"]
    end

    style Fixed fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style Variable fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style A1 fill:#0f3460,stroke:#16213e,color:#fff
    style A2 fill:#8b4513,stroke:#16213e,color:#fff
    style B1 fill:#8b4513,stroke:#16213e,color:#fff
    style B2 fill:#0f3460,stroke:#16213e,color:#fff
```

**å›ºå®šé•·ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰**:
- å…¨ã¦ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒåŒã˜é•·ã•ï¼ˆ97ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰
- Bucketing ã®ãƒ¡ãƒªãƒƒãƒˆ: ãªã—ï¼ˆå¸¸ã«åŒã˜ãƒã‚±ãƒƒãƒˆã‚’ä½¿ç”¨ï¼‰
- Bucketing ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰: ã‚°ãƒ©ãƒ•é¸æŠã€ãƒ¡ãƒ¢ãƒªç®¡ç†
- **çµæœ**: Bucketing OFF ãŒ 0.4% é«˜é€Ÿ

**å¯å¤‰é•·ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰**:
- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒ 18-125 ãƒˆãƒ¼ã‚¯ãƒ³ã®ç¯„å›²ã§å¤‰å‹•
- Bucketing ã®ãƒ¡ãƒªãƒƒãƒˆ: é©åˆ‡ãªãƒã‚±ãƒƒãƒˆã«æŒ¯ã‚Šåˆ†ã‘ã¦ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’æœ€é©åŒ–
- Bucketing ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰: ã‚°ãƒ©ãƒ•é¸æŠã€ãƒ¡ãƒ¢ãƒªç®¡ç†ï¼ˆãƒ¡ãƒªãƒƒãƒˆãŒä¸Šå›ã‚‹ï¼‰
- **çµæœ**: Bucketing ON ãŒ 19.3% é«˜é€Ÿ

**æ¨å¥¨è¨­å®š**:
- **Rerankerï¼ˆå›ºå®šé•·ï¼‰**: Bucketing OFF
- **é€šå¸¸ã® LLM ç”Ÿæˆï¼ˆå¯å¤‰é•·ï¼‰**: Bucketing ON
- **vLLM ä½¿ç”¨æ™‚**: å¸¸ã« Bucketing ONï¼ˆvLLM ã®å‹•ä½œã«å¿…è¦ï¼‰

## NxD Inference å†…éƒ¨ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°

NxD Inference ã®å†…éƒ¨å‡¦ç†ã‚’ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã—ã¾ã—ãŸã€‚

### æ¸¬å®šã‚³ãƒ¼ãƒ‰ã¨çµæœ

:::details NxD Inference å†…éƒ¨ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‚³ãƒ¼ãƒ‰

```python
# profile_nxd_detailed.py (æŠœç²‹)
import time
import torch
from neuronx_distributed_inference.models.config import NeuronConfig
from neuronx_distributed_inference.models.qwen3.modeling_qwen3 import NeuronQwen3ForCausalLM

# Configuration
neuron_config = NeuronConfig(
    tp_degree=2,
    batch_size=4,
    max_context_length=128,
    seq_len=128,
    enable_bucketing=True,
)

# Load model
model = NeuronQwen3ForCausalLM(compiled_model_path)
model.load(compiled_model_path)

# Warmup
with torch.no_grad():
    _ = model(inputs.input_ids, attention_mask=inputs.attention_mask, position_ids=position_ids)

# Benchmark (30 iterations)
times_total = []
times_forward = []
times_output = []

for i in range(30):
    # Total time
    t_start = time.perf_counter()

    # Forward pass
    t_forward_start = time.perf_counter()
    with torch.no_grad():
        outputs = model(inputs.input_ids,
                       attention_mask=inputs.attention_mask,
                       position_ids=position_ids)
    t_forward_end = time.perf_counter()

    # Output processing (minimal)
    t_output_start = time.perf_counter()
    # (outputs processing would go here)
    t_output_end = time.perf_counter()

    t_end = time.perf_counter()

    times_total.append((t_end - t_start) * 1000)
    times_forward.append((t_forward_end - t_forward_start) * 1000)
    times_output.append((t_output_end - t_output_start) * 1000)
```

**æ¸¬å®šçµæœ**:
```
================================================================================
NxD Inference è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
================================================================================

Configuration: tp_degree=2, batch_size=4, bucketing=True

Total time:
  å¹³å‡: 12.70 ms
  æœ€å°: 12.60 ms
  æœ€å¤§: 13.28 ms

Forward pass:
  å¹³å‡: 12.70 ms  (100.0% of total)
  æœ€å°: 12.60 ms
  æœ€å¤§: 13.28 ms

Output processing:
  å¹³å‡: 0.00 ms   (0.0% of total)
  æœ€å°: 0.00 ms
  æœ€å¤§: 0.00 ms

================================================================================
å†…è¨³
================================================================================
Forward pass:        12.70 ms (100.0%)  â† Neuronå®Ÿè¡Œ
Output processing:   0.00 ms (0.0%)
Other overhead:      0.00 ms (0.0%)     â† Pythonã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãªã—
```

:::

### NxD Inference ã® Python ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã¯ã‚¼ãƒ­

NxD Inference ã®æ¸¬å®šçµæœã‹ã‚‰ Python ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã¯ã»ã¼ 0 ms ã§ã‚ã‚Šã€ã»ã¼å…¨ã¦ã®å®Ÿè¡Œæ™‚é–“ãŒ Neuron å®Ÿè¡Œã«ä½¿ã‚ã‚Œã¦ã„ã¾ã™ã€‚ã“ã‚Œã¯ã‚·ãƒ³ã‚°ãƒ«ãƒ—ãƒ­ã‚»ã‚¹ã®ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã‚ˆã‚‹ã‚‚ã®ã§ã—ã‚‡ã†ã€‚

ã“ã®çµæœã«ã‚ˆã‚Šã€**vLLM ã® 52.36ms ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã¯ vLLM å´ã®å®Ÿè£…ã‚³ã‚¹ãƒˆ**ã§ã‚ã‚‹ã“ã¨ãŒç¢ºèªã§ãã¾ã—ãŸã€‚

:::message
**æ³¨**: ä¸Šè¨˜ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°çµæœï¼ˆ12.70 msï¼‰ã¯ bucketing=True ã®è¨­å®šã§ã®æ¸¬å®šã§ã™ã€‚bucketing=OFF ã§ã¯ 11.97 msã€bucketing=ON ã§ã¯ 12.02 ms ã¨ã„ã†çµæœãŒå¾—ã‚‰ã‚Œã¦ãŠã‚Šã€Python ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒã»ã¼ã‚¼ãƒ­ã¨ã„ã†çµè«–ã¯ä¸¡æ–¹ã®è¨­å®šã§ä¸€è²«ã—ã¦ã„ã¾ã™ã€‚
:::

## è¿½åŠ èª¿æŸ»: ãƒ•ãƒ©ã‚°åã®è¨‚æ­£ã¨vLLMå†…éƒ¨å®Ÿè£…ã®è§£æ

ä¸Šè¨˜ã®æ¸¬å®šã§ Pattern C & Dï¼ˆprefix cachingæœ‰åŠ¹ï¼‰ã®æ¸¬å®šã‚’è©¦ã¿ã¾ã—ãŸãŒã€ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã“ã®èª¿æŸ»éç¨‹ã§é‡è¦ãªç™ºè¦‹ãŒã‚ã‚Šã¾ã—ãŸã€‚

### ãƒ•ãƒ©ã‚°åã®è¨‚æ­£

::::details å…ƒã€…ã®æƒ³å®šï¼ˆèª¤ã‚Šï¼‰

åˆæœŸã®èª¿æŸ»ã§ã¯ã€[AWSå…¬å¼ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/tutorials/trn2-llama3.3-70b-apc-tutorial.html)ã‚’å‚è€ƒã« `--enable-variable-length-prefill` ã¨ã„ã†ãƒ•ãƒ©ã‚°ã‚’ä½¿ç”¨ã—ã‚ˆã†ã¨ã—ã¦ã„ã¾ã—ãŸã€‚

ã—ã‹ã—ã€ã“ã®ãƒ•ãƒ©ã‚°ã¯ **inference_demo.py ã«ã¯å­˜åœ¨ã—ã¾ã›ã‚“**ã€‚

::::

**æ­£ã—ã„ãƒ•ãƒ©ã‚°**: `--enable-prefix-caching`

```python:inference_demo.py (line 249)
parser.add_argument(
    "--enable-prefix-caching",
    action="store_true",
    help="Enable prefix caching for the model"
)
```

ã“ã®ãƒ•ãƒ©ã‚°ã¯ inference_demo.py /home/coder/data-science/investigations/neuronx-distributed-inference/src/neuronx_distributed_inference/inference_demo.py:249 ã«ç¢ºã‹ã«å­˜åœ¨ã—ã¾ã™ã€‚

### vLLM-Neuron ã®å†…éƒ¨å®Ÿè£…ã‚’è§£æ

Prefix caching ã®è¨­å®šæ–¹æ³•ã‚’ç†è§£ã™ã‚‹ãŸã‚ã€vLLM-Neuron ã®ã‚³ãƒ¼ãƒ‰ã‚’èª¿æŸ»ã—ã¾ã—ãŸã€‚

**ç™ºè¦‹**: vLLM ã¯ `--enable-prefix-caching` ãƒ•ãƒ©ã‚°ã‹ã‚‰ **è‡ªå‹•çš„ã«è¤‡æ•°ã®è¨­å®šã‚’è¡Œã†**

```python:/work/vllm-neuron/vllm_neuron/worker/neuronx_distributed_model_loader.py (lines 782-786)
"is_block_kv_layout": (scheduler_config.chunked_prefill_enabled
                       or cache_config.enable_prefix_caching),
"is_prefix_caching": cache_config.enable_prefix_caching,
"is_continuous_batching": (batch_size > 1),
```

ã¤ã¾ã‚Šã€vLLM ã§ `enable_prefix_caching=True` ã‚’è¨­å®šã™ã‚‹ã¨:
1. `is_block_kv_layout=True` ãŒè‡ªå‹•çš„ã«è¨­å®šã•ã‚Œã‚‹
2. `is_prefix_caching=True` ãŒè¨­å®šã•ã‚Œã‚‹
3. `batch_size > 1` ã®å ´åˆã€`is_continuous_batching=True` ãŒè¨­å®šã•ã‚Œã‚‹

ã“ã‚Œã‚‰ã®è¨­å®šãŒ NxD Inference ã«æ¸¡ã•ã‚Œã¾ã™ã€‚

### inference_demo.py ã§ã®ç›´æ¥ä½¿ç”¨ã®åˆ¶ç´„

:::message alert
**é‡è¦ãªç™ºè¦‹**: `inference_demo.py` ã§ prefix caching ã‚’ç›´æ¥ä½¿ç”¨ã—ã‚ˆã†ã¨ã™ã‚‹ã¨ã€ä»¥ä¸‹ã®ã‚ˆã†ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã™ã€‚
:::

**Pattern Cï¼ˆbucketing OFF, prefix caching ONï¼‰ã®ã‚¨ãƒ©ãƒ¼**:
```
TypeError: unsupported operand type(s) for -: 'NoneType' and 'NoneType'
```

**Pattern Dï¼ˆbucketing ON, prefix caching ONï¼‰ã®ã‚¨ãƒ©ãƒ¼**:
```
TypeError: can only concatenate list (not "NoneType") to list
```

**ç†ç”±**:
- Prefix caching ã®å®Ÿè£…ã¯ **vLLM ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯çµŒç”±ã§ã®ä½¿ç”¨ãŒæƒ³å®šã•ã‚Œã¦ã„ã‚‹**
- inference_demo.py ã§ã®ç›´æ¥ä½¿ç”¨ã«ã¯ã€è¿½åŠ ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒå¿…è¦
- [AWSå…¬å¼ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/tutorials/trn2-llama3.3-70b-apc-tutorial.html)ã§ã‚‚ vLLM ã‚µãƒ¼ãƒãƒ¼çµŒç”±ã®ä½¿ç”¨ã‚’æ¨å¥¨

::::details Pattern C & D ã®æ¸¬å®šã‚¹ã‚¯ãƒªãƒ—ãƒˆä¾‹ï¼ˆã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿï¼‰

**Pattern Cï¼ˆã‚¨ãƒ©ãƒ¼ç™ºç”Ÿï¼‰**:
```bash
python inference_demo.py \
  --model-type qwen3 \
  --task-type causal-lm run \
  --model-path "$MODEL_PATH" \
  --prompt "$PROMPT" \
  --tp-degree 2 \
  --batch-size 4 \
  --seq-len 128 \
  --max-context-length 128 \
  --max-length 128 \
  --on-device-sampling \
  --top-k 1 \
  --is-continuous-batching \
  --ctx-batch-size 1 \
  --enable-block-kv-layout \
  --enable-prefix-caching \
  --pa-num-blocks 16 \
  --pa-block-size 32 \
  --benchmark

# ã‚¨ãƒ©ãƒ¼: TypeError: unsupported operand type(s) for -: 'NoneType' and 'NoneType'
```

**Pattern Dï¼ˆã‚¨ãƒ©ãƒ¼ç™ºç”Ÿï¼‰**:
```bash
python inference_demo.py \
  --model-type qwen3 \
  --task-type causal-lm run \
  --model-path "$MODEL_PATH" \
  --prompt "$PROMPT" \
  --tp-degree 2 \
  --batch-size 4 \
  --seq-len 128 \
  --max-context-length 128 \
  --max-length 128 \
  --on-device-sampling \
  --top-k 1 \
  --enable-bucketing \
  --context-encoding-buckets 32 64 96 128 \
  --token-generation-buckets 32 64 96 128 \
  --is-continuous-batching \
  --ctx-batch-size 1 \
  --enable-block-kv-layout \
  --enable-prefix-caching \
  --pa-num-blocks 16 \
  --pa-block-size 32 \
  --benchmark

# ã‚¨ãƒ©ãƒ¼: TypeError: can only concatenate list (not "NoneType") to list
```

::::

### ä»Šå›ã®çµè«–ã¸ã®å½±éŸ¿

Pattern C & Dï¼ˆprefix cachingæœ‰åŠ¹ï¼‰ã®æ¸¬å®šã«å¤±æ•—ã—ã¾ã—ãŸãŒã€**ä»Šå›ã®ä¸»è¦ãªçµè«–ã«ã¯å½±éŸ¿ã—ã¾ã›ã‚“**ã€‚

**æ¸¬å®šã§ããŸã“ã¨**:
1. âœ… vLLM ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ï¼ˆPattern A & B ã§æ¸¬å®šï¼‰
2. âœ… Bucketing ã®å½±éŸ¿ï¼ˆå›ºå®šé•· & å¯å¤‰é•·ã§æ¸¬å®šï¼‰
3. âœ… NxD Inference ã®ç´”ç²‹ãªå®Ÿè¡Œæ™‚é–“

**æ¸¬å®šã§ããªã‹ã£ãŸã“ã¨**:
- âŒ inference_demo.py ã§ã® prefix caching ã®ç›´æ¥æ¸¬å®š
- ãŸã ã—ã€vLLM çµŒç”±ã§ã® prefix caching ã¯æ­£å¸¸ã«å‹•ä½œï¼ˆãƒªãƒªãƒ¼ã‚¹ãƒãƒ¼ãƒˆã§3.2xæ”¹å–„ã‚’ç¢ºèªï¼‰

## Prefix Cachingã®çœŸå®Ÿ


### ãƒªãƒªãƒ¼ã‚¹ãƒãƒ¼ãƒˆç¢ºèª

[NxD Inference Release Notes](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/neuronx-distributed-inference/neuronx-distributed-inference.html)ã‚ˆã‚Šï¼š

**Neuron 2.24.0**
- **Automatic Prefix Caching (APC) ã‚µãƒãƒ¼ãƒˆé–‹å§‹**
- **vLLM çµŒç”±ã§ã®ã¿å‹•ä½œ**
- 3.2x TTFTæ”¹å–„ï¼ˆ90%ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆæ™‚ã€Llama3.3 70Bï¼‰

### vllm-neuron ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```mermaid
%%{init: {'theme':'dark'}}%%
graph TD
    subgraph GPU["é€šå¸¸ã® vLLM (GPU ãªã©)"]
        A1[Scheduler<br/>ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°] --> A2[KVCacheCoordinator]
        A2 --> A3[Hash-based prefix caching<br/>PagedAttention<br/>Block allocation/eviction<br/>LRU eviction policy]
        A3 --> A4[GPU Worker<br/>å®Ÿè¡Œ]
    end

    subgraph Neuron["vllm-neuron"]
        B1[Scheduler<br/>full_context_lens è¨ˆç®—<br/>computed_context_lens è¨ˆç®—] --> B2["âŒ KVCacheCoordinator<br/>(ä½¿ç”¨ã•ã‚Œãªã„)"]
        B2 -.->|bypass| B3[vllm-neuron Plugin]
        B3 --> B4[is_prefix_caching è¨­å®š<br/>ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ Neuron ã«æ¸¡ã™]
        B4 --> B5[NxD Inference<br/>Neuron ãƒ¬ãƒ™ãƒ«]
        B5 --> B6[Prefix caching å®Ÿè£…<br/>KV ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†<br/>ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³]
    end

    style GPU fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style Neuron fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style A2 fill:#0f3460,stroke:#16213e,color:#fff
    style B2 fill:#8b0000,stroke:#16213e,color:#fff
    style B6 fill:#0f3460,stroke:#16213e,color:#fff
```

**vLLM ä½¿ç”¨æ™‚ã€prefix caching ã¯ Neuron å´ã§å®Ÿè¡Œã•ã‚Œã‚‹**ã“ã¨ãŒåˆ¤æ˜ã—ã¾ã—ãŸã€‚

ã¾ãšã€vLLM å´ã®ç‹¬è‡ª prefix caching å®Ÿè£…ï¼ˆ`KVCacheCoordinator`ï¼‰ã¯å®Œå…¨ã«ãƒã‚¤ãƒ‘ã‚¹ã•ã‚Œã¾ã™ã€‚é€šå¸¸ã® vLLM ã§ã¯ hash-based ã‚„ PagedAttention-based ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ãŒè¡Œã‚ã‚Œã¾ã™ãŒã€vllm-neuron ã§ã¯ã“ã®æ©Ÿæ§‹ã¯ä½¿ç”¨ã•ã‚Œã¾ã›ã‚“ã€‚

ä»£ã‚ã‚Šã«ã€Neuron å´ã§ prefix caching ãŒå®Œå…¨ã«å®Ÿè£…ã•ã‚Œã¾ã™ã€‚vLLM å´ã¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¨ˆç®—ï¼ˆ`full_context_lens`, `computed_context_lens`ï¼‰ã®ã¿ã‚’æ‹…å½“ã—ã€å®Ÿéš›ã® prefix caching å®Ÿè¡Œã¯ Neuron å´ã§ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒ¬ãƒ™ãƒ«ã§è¡Œã‚ã‚Œã¾ã™ã€‚

ã“ã®è¨­è¨ˆã«ã‚ˆã‚Šã€vLLM å´ã®è¿½åŠ ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã¯æœ€å°é™ã«æŠ‘ãˆã‚‰ã‚Œã¦ã„ã¾ã™ã€‚ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¨ˆç®—ã®ã¿ã§æ¸ˆã‚€ãŸã‚ã€æ—¢å­˜ã® 52ms ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã«æ¯”ã¹ã¦ç„¡è¦–ã§ãã‚‹ç¨‹åº¦ã®å½±éŸ¿ã—ã‹ã‚ã‚Šã¾ã›ã‚“ã€‚

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å½±éŸ¿ï¼ˆæ¨å®šï¼‰

:::message alert
**æ³¨æ„**: æœ¬æ¸¬å®šã§ã¯ prefix caching ã¯ç„¡åŠ¹ã«ã—ã¦ã„ã¾ã™ï¼ˆ`enable_prefix_caching: false`ï¼‰ã€‚ä»¥ä¸‹ã®æ•°å€¤ã¯ã€[NxD Inference Release Notes](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/neuronx-distributed-inference/neuronx-distributed-inference.html)ã«è¨˜è¼‰ã•ã‚ŒãŸã€Œ3.2x TTFTæ”¹å–„ï¼ˆ90% cache hitæ™‚ï¼‰ã€ã®ãƒ‡ãƒ¼ã‚¿ã‚’åŸºã«æ¨å®šã—ãŸã‚‚ã®ã§ã™ã€‚
:::

#### Rerankerï¼ˆçŸ­ã„æ¨è«–ã€ä»Šå›ã®æ¸¬å®šï¼‰

```
Prefix cachingç„¡åŠ¹ï¼ˆå®Ÿæ¸¬ï¼‰:
- vLLM: 64.33 ms/batch (Neuron 11.97ms + ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ 52.36ms)
- NxDç›´æ¥: 11.97 ms/batch

Prefix cachingæœ‰åŠ¹ï¼ˆæ¨å®šã€90% cache hitæ™‚ï¼‰:
- vLLM: ~56 ms/batch (Neuron 3.74ms + ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ 52.36ms)
  â†’ 13%æ”¹å–„ï¼ˆåŠ¹æœã¯é™å®šçš„ï¼‰
- NxDç›´æ¥: ~3.74 ms/batch
  â†’ 69%æ”¹å–„ï¼ˆå¤§ããªåŠ¹æœï¼‰

æ¨å®šæ ¹æ‹ : 11.97ms Ã· 3.2 â‰ˆ 3.74ms
```

**çµè«–**: **çŸ­ã„æ¨è«–ã§ã¯ã€vLLM ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒæ”¯é…çš„**ãªãŸã‚ã€prefix caching ã®åŠ¹æœã¯é™å®šçš„ã€‚

#### é€šå¸¸ã®ç”Ÿæˆï¼ˆé•·ã„æ¨è«–ï¼‰

```
ä¾‹: 100ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ @ 12 ms/token = 1200 ms

Prefix cachingç„¡åŠ¹:
- vLLM: 1252 ms (1200 + 52)
- NxD: 1200 ms

Prefix cachingæœ‰åŠ¹ï¼ˆ90% cache hitï¼‰:
- vLLM: 427 ms (375 + 52) â†’ 2.9xé«˜é€ŸåŒ–
- NxD: 375 ms â†’ 3.2xé«˜é€ŸåŒ–
```

**çµè«–**: **é•·ã„æ¨è«–ã§ã¯ã€prefix caching ã®åŠ¹æœãŒæ”¯é…çš„**ã€‚vLLM ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã®å‰²åˆãŒå°ã•ããªã‚‹ã€‚

## ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã®çŸ¥è¦‹

ä»Šå›ã®èª¿æŸ»ã‹ã‚‰å¾—ã‚‰ã‚ŒãŸãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã™ã‚‹éš›ã®çŸ¥è¦‹ã‚’ã¾ã¨ã‚ã¾ã™ã€‚ä»Šå¾Œã‚‚ã£ã¨æ›¸ç±ãªã©ã‚’èª­ã‚“ã§å‹‰å¼·ã—ã‚ˆã†ã¨æ€ã£ã¦ãŠã‚Šã€ç¾æ™‚ç‚¹ã§ã®çŸ¥è¦‹ã¨ã„ã†ã“ã¨ã§ã‚ˆã‘ã‚Œã°å‚è€ƒã«ã—ã¦ãã ã•ã„ã€‚

### 1: ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã«æ¸¬å®šã™ã‚‹

è¤‡é›‘ãªã‚·ã‚¹ãƒ†ãƒ ã¯è¤‡æ•°ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã«ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ„ãƒ¼ãƒ«ãŒç•°ãªã‚‹ãŸã‚å¿…è¦ã«å¿œã˜ã¦å€‹åˆ¥ã«æ¸¬å®šã—ã€å·®åˆ†ã‚’å–ã‚‹ã“ã¨ã§ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã®æ‰€åœ¨ã‚’ç‰¹å®šã§ãã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚

```
L1: ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒ¬ãƒ™ãƒ«
L2: SDK ãƒ¬ãƒ™ãƒ«
L3: ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãƒ¬ãƒ™ãƒ«
L4: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¬ãƒ™ãƒ«
```

**ä¾‹**: ä»Šå›ã¯ L2 (NxD) ã‚’æ¸¬å®šã—ã¦ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ç¢ºç«‹ã—ãŸã“ã¨ã§ã€L3 (vLLM) ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’å®šé‡åŒ–ã§ãã¾ã—ãŸã€‚

### 2: ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‹ã‚‰å§‹ã‚ã‚‹

æœ€ã‚‚å˜ç´”ãªå®Ÿè£…ã‚’ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã—ã¦æ¸¬å®šã—ã¦ã‹ã‚‰ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’è¿½åŠ ã—ã¦ã„ãã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ã‚³ã‚¹ãƒˆã‚’æ˜ç¢ºã«ã§ãã¾ã™ã€‚

**ä¾‹**:
1. NxD Inference ç›´æ¥ä½¿ç”¨ï¼ˆæœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ï¼‰â†’ 11.97ms
2. vLLM çµŒç”±ï¼ˆè¤‡é›‘ã•è¿½åŠ ï¼‰â†’ 64.33ms
3. å·®åˆ† = vLLM ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ = 52.36ms

ã‚ˆã‚Šå³å¯†ã«ã¯æ§˜ã€…ãªå…¥åŠ›ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å…¥ã‚ŒãŸä¸Šã§çµ±è¨ˆçš„ãªæœ‰æ„æ€§ã‚„ä¿¡é ¼åŒºé–“ã‚’å°å‡ºã™ã¹ãã§ã—ã‚‡ã†ã€‚

### 3: æ¸¬å®šãƒ„ãƒ¼ãƒ«ã®é™ç•Œã‚’ç†è§£ã™ã‚‹

å„ãƒ„ãƒ¼ãƒ«ã«ã¯å›ºæœ‰ã®é™ç•ŒãŒã‚ã‚Šã¾ã™ã€‚å˜ä¸€ãƒ„ãƒ¼ãƒ«ã ã‘ã§ã¯å…¨ä½“åƒã‚’æŠŠæ¡ã§ããªã„ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚

**ä¾‹**: ä»Šå›ã¯ Neuron Profiler ã ã‘ã§ã¯ vLLM ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’æ¸¬å®šã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚

### 4: æ¸¬å®šæ¡ä»¶ã®å½±éŸ¿ã‚’è€ƒæ…®ã™ã‚‹

æ¸¬å®šæ¡ä»¶ãŒçµæœã«å¤§ããå½±éŸ¿ã—ã¾ã™ã€‚å®Ÿéš›ã®ä½¿ç”¨æ¡ä»¶ã«è¿‘ã„ç’°å¢ƒã§æ¸¬å®šã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚

**ä¾‹**: ä»¥ä¸‹ã®æ¡ä»¶ä¸‹ã§ã¯ bucketing=OFF ãŒæœ€é€Ÿã¨ãªã‚‹ç¾å®Ÿã‚±ãƒ¼ã‚¹ã¨ã®ã‚®ãƒ£ãƒƒãƒ—ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã€‚

```python
# å…¨ã¦åŒã˜ 97 ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
batch_prompts = [prompt_97tokens] * 4
```

æ¸¬å®šæ¡ä»¶ã¨å®Ÿç’°å¢ƒã®ã‚®ãƒ£ãƒƒãƒ—ã‚’èªè­˜ã—ã€çµæœã®é©ç”¨ç¯„å›²ã‚’æ˜ç¤ºã™ã‚‹ã¹ãã§ã—ã‚‡ã†ã€‚

### 5: äºˆæƒ³å¤–ã®çµæœã‚’æ˜ã‚Šä¸‹ã’ã‚‹

äºˆæƒ³ã¨ç•°ãªã‚‹çµæœã¯ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã—ã¦æ˜ã‚Šä¸‹ã’ã‚‹ã¹ãã§ã™ã€‚

**ä¾‹**: ä»Šå›ã¯ bucketing=ON ã§ vLLM ã¯é«˜é€ŸåŒ–ã—ãŸãŒ NxD Inference ã¯æ‚ªåŒ–ã—ãŸã€‚ã“ã‚Œã¯ã©ã†ã„ã†æ™‚ã«èµ·ã“ã‚‹ã®ã‹ã‚’æ˜ç¢ºã«ã™ã‚‹ã€‚

### 6: ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’æ˜ç¢ºã«ã™ã‚‹

æ€§èƒ½ã ã‘ã§ãªãã€æ©Ÿèƒ½ã¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’å®šé‡åŒ–ã—ã¾ã™ã€‚

**ä¾‹**: æ€§èƒ½ãŒä¸ŠãŒã‚‹ã®ã§ã‚ã‚Œã°æ¯å›ã‚«ã‚¹ã‚¿ãƒ ã‚«ãƒ¼ãƒãƒ«ã‚’å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã®ãŸã‚ã«å®Ÿè£…ã™ã¹ãã‹ï¼Ÿ

## ã¾ã¨ã‚

1. **vLLM ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰**ï¼ˆreranker ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã€å‰å›ã® Zenn æœ€é©å€¤è¨­å®šï¼‰
 - vLLM: 64.33 ms/batch
 - NxD: 11.97 ms/batch
 - ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰: +52.36 ms (81%)
 - NxD Inference ã® Python ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰: ~0 ms
 - vLLM ã®ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹æ§‹é€ ã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã€é€šä¿¡ã‚³ã‚¹ãƒˆãŒè¦å› 

2. **bucketing è¨­å®šã¯ä½¿ç”¨ã‚±ãƒ¼ã‚¹ã«ä¾å­˜**
 - **å›ºå®šé•·ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ï¼ˆ97ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰**: bucketing OFF: 11.97 ms vs ON: 12.02 msï¼ˆ+0.4%ã€ã»ã¼åŒç­‰ï¼‰
 - **å¯å¤‰é•·ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ï¼ˆ18-125ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰**: bucketing OFF: 17.18 ms vs ON: 13.87 msï¼ˆ**-19.3%æ”¹å–„**ï¼‰
 - å›ºå®šé•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã¯ã€bucketing ã®ãƒ¡ãƒªãƒƒãƒˆãŒãªãã‚ãšã‹ãªã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã®ã¿
 - å¯å¤‰é•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã¯ã€bucketing ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°æœ€é©åŒ–ãŒåŠ¹æœã‚’ç™ºæ®

3. **Prefix Caching ã®æ­£ã—ã„ä½¿ã„æ–¹**
 - âŒ ãƒ•ãƒ©ã‚°å: `--enable-variable-length-prefill`ï¼ˆå­˜åœ¨ã—ãªã„ï¼‰
 - âœ… ãƒ•ãƒ©ã‚°å: `--enable-prefix-caching`ï¼ˆæ­£ã—ã„ï¼‰
 - inference_demo.py ã§ã®ç›´æ¥ä½¿ç”¨ã¯å›°é›£ï¼ˆTypeErrorç™ºç”Ÿï¼‰
 - **vLLM çµŒç”±ã§ã®ä½¿ç”¨ãŒæ¨å¥¨**ï¼ˆNeuron 2.24.0ä»¥é™ï¼‰
 - vLLM å´ã®ç‹¬è‡ªå®Ÿè£…ã¯å®Œå…¨ã«ãƒã‚¤ãƒ‘ã‚¹ã•ã‚Œã€Neuron å´ã§å®Ÿè¡Œ
 - vLLM ãŒè‡ªå‹•çš„ã« `is_block_kv_layout`ã€`is_prefix_caching`ã€`is_continuous_batching` ã‚’è¨­å®š

4. **vLLM-Neuron ã®å†…éƒ¨å®Ÿè£…ã®ç™ºè¦‹**
 - `/work/vllm-neuron/vllm_neuron/worker/neuronx_distributed_model_loader.py:782-786`
 - `enable_prefix_caching=True` ã§è¤‡æ•°ã®è¨­å®šãŒè‡ªå‹•çš„ã«æœ‰åŠ¹åŒ–ã•ã‚Œã‚‹
 - NxD Inference ã¸ã®è¨­å®šå€¤ã®æ©‹æ¸¡ã—ãƒ­ã‚¸ãƒƒã‚¯ãŒæ˜ç¢ºã«

5. **æ¸¬å®šã®é›£ã—ã•**:
 - Neuron Profiler ã¯ graph variation ã‚’æ¸¬å®šã™ã‚‹ãŸã‚ã€ãƒãƒƒãƒå‡¦ç†æ™‚é–“ã‚’ç›´æ¥æ¸¬å®šã§ããªã„
 - line_profiler ã¯ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ã®å£ã‚’è¶Šãˆã‚‰ã‚Œãªã„
 - å˜ä¸€ãƒ„ãƒ¼ãƒ«ã§ã¯å…¨ä½“åƒã‚’æŠŠæ¡ã§ããªã„
 - æ¸¬å®šæ¡ä»¶ï¼ˆå›ºå®šé•· vs å¯å¤‰é•·ï¼‰ãŒçµæœã«å¤§ããå½±éŸ¿ã™ã‚‹

### å‚è€ƒè³‡æ–™

- [Zenn è¨˜äº‹ - Inf2ã§ vLLM ã‚’å‹•ã‹ã™éš›ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°](https://zenn.dev/tosshi/articles/ef61e14fe73399)
- [NxD Inference Release Notes](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/neuronx-distributed-inference/neuronx-distributed-inference.html)
- [vLLM User Guide for NxD Inference](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/developer_guides/vllm-user-guide-v1.html)
- [AWS Neuron Profiler Guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/tools/neuron-sys-tools/neuron-profile-user-guide.html)
- [neuronx-distributed-inference GitHub](https://github.com/aws-neuron/neuronx-distributed-inference)