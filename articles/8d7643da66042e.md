---
title: "vllm-neuron ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã§å¾—ãŸçŸ¥è¦‹"
emoji: "ğŸ”"
type: "tech"
topics: ["vllm", "AWSNeuron", "Profiler", "Python", "æ€§èƒ½æœ€é©åŒ–"]
published: false
---

## ã¯ã˜ã‚ã«

[å‰å›ã®è¨˜äº‹](https://zenn.dev/tosshi/articles/d68bd091d1934d) ã§ã¯ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ„ãƒ¼ãƒ«ã®é–‹ç™ºã«ã»ã¼è¶£å‘³ã§é›†ä¸­ã—ã¦ã—ã¾ã—ãŸãŒã€ä»Šå›ã¯ï¼ˆçœŸé¢ç›®ã«ï¼‰ AWS Inferentia2 ä¸Šã§ vllm-neuron ã‚’ä½¿ç”¨ã—ãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°åˆ†æã«ãƒãƒ£ãƒ¬ãƒ³ã‚¸ã—ã¾ã™ã€‚

æœ¬è¨˜äº‹ã§ã¯ã€4 ã¤ã®èª¿æŸ»ãƒ•ã‚§ãƒ¼ã‚ºã«åˆ†ã‘ã¦èª¬æ˜ã—ã¾ã™ã€‚ã¾ãš Phase 1 ã§ã¯ AWS Neuron Profiler ã«ã‚ˆã‚‹ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒ¬ãƒ™ãƒ«ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‚’è©¦è¡ŒéŒ¯èª¤ã—ã¾ã—ãŸã€‚æ¬¡ã« Phase 2 ã§ã¯ line_profiler ã«ã‚ˆã‚‹ Python ãƒ¬ãƒ™ãƒ«ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‚’å®Ÿæ–½ã—ã¾ã—ãŸã€‚Phase 3 ã§ã¯ NxD Inference ã‚’ç›´æ¥ä½¿ç”¨ã—ãŸæ¸¬å®šã‚’è¡Œã„ã€vLLM ã¨ã®è©³ç´°æ¯”è¼ƒã‚’å®Ÿæ–½ã—ã¾ã—ãŸã€‚æœ€å¾Œã« Phase 4 ã§ã¯ vLLM ã® bucketing ON/OFF ã®å†…éƒ¨å‹•ä½œã‚’ line_profiler ã§è©³ç´°ã«åˆ†æã—ã¾ã—ãŸã€‚

ã•ã‚‰ã«ã€bucketing è¨­å®šã®å½±éŸ¿ã‚’æ­£ç¢ºã«ç†è§£ã™ã‚‹ãŸã‚ã€å›ºå®šé•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆ97ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã¨å¯å¤‰é•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆ18-125ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã®ä¸¡æ–¹ã§æ¸¬å®šã‚’å®Ÿæ–½ã—ã¾ã—ãŸã€‚

æœ€çµ‚çš„ã«ã€vllm-neuronã€NeuronCore å´ã®å‡¦ç†ã®å®Ÿè¡Œæ™‚é–“ã€bucketing è¨­å®šã‚„ prefix caching ã®æŒ™å‹•ã«ã¤ã„ã¦ç†è§£ã‚’æ·±ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã—ãŸã€‚ç‰¹ã« **bucketing ã®åŠ¹æœã¯ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¨ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã«ä¾å­˜ã™ã‚‹** ã¨ã„ã†çŸ¥è¦‹ãŒã‚ã‚Šã¾ã—ãŸã€‚Phase 4 ã®èª¿æŸ»ã§ã¯ã€**vLLM Worker Process å†…éƒ¨ã«å¤§ããªã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒå­˜åœ¨ã™ã‚‹**ã“ã¨ãŒåˆ¤æ˜ã—ã¾ã—ãŸã€‚

:::message
**ä»Šå›ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã®é€²ã‚æ–¹ã¯çµæœçš„ã«ã‹ãªã‚Šé–“é•ã£ã¦ã„ã¾ã—ãŸï¼** è‰²ã€…è©¦è¡ŒéŒ¯èª¤ã—ãŸã‚“ã ãªã€ã¨æ€ã„ãªãŒã‚‰æœ¬è¨˜äº‹ã‚’èª­ã‚“ã§ãã ã•ã„ã€‚
:::

:::message alert
æœ¬è¨˜äº‹ã¯åˆå­¦è€…å‘ã‘ã§ã¯ãªã„ãŸã‚ã‚ã‚‹ç¨‹åº¦ LLM æ¨è«–ã®åŸºç¤çŸ¥è­˜ã€vLLM ã®åŸºç¤çŸ¥è­˜ã€AWS Neuron ã®åŸºç¤çŸ¥è­˜ãŒã‚ã‚‹ã“ã¨ãŒå‰æã§ã™ã€‚
:::

---

## Phase 1: AWS Neuron Profiler ã§ã®è©¦è¡ŒéŒ¯èª¤

### 1.1 ãªãœãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‹ã‚‰å§‹ã‚ã‚‹ã®ã‹

æ€§èƒ½æœ€é©åŒ–ã‚’è¡Œã†éš›ã€ã¾ãšç¾çŠ¶ã‚’æŠŠæ¡ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¸¬å®šã§ã¯æ€§èƒ½ã®çµæœã¯åˆ†ã‹ã‚Šã¾ã™ãŒã€æ€§èƒ½ã®ç†ç”±ã€ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®å ´æ‰€ã€ãã—ã¦æ”¹å–„ã®ä½™åœ°ã¯åˆ†ã‹ã‚Šã¾ã›ã‚“ã€‚

ä»¥ä¸‹ã«å®Ÿé¨“ç’°å¢ƒã¨è¨­å®šæƒ…å ±ã‚’ã¾ã¨ã‚ã¦ãŠãã¾ã™ã€‚ä»¥å‰ã® Zenn è¨˜äº‹ã®å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼æœ€é©å€¤ã‹ã‚‰ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™ã€‚

::::details å®Ÿé¨“ç’°å¢ƒã¨è¨­å®šæƒ…å ±

æœ¬èª¿æŸ»ã§ä½¿ç”¨ã—ãŸå®Ÿé¨“ç’°å¢ƒã¨è¨­å®šã®è©³ç´°ã‚’è¨˜è¼‰ã—ã¾ã™ã€‚

**ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ç’°å¢ƒ**:
- ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—: `inf2.xlarge`

**ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ãƒãƒ¼ã‚¸ãƒ§ãƒ³**:
- Neuron SDK: 2.27.x
- vLLM: 0.13.0ï¼ˆNeuron å¯¾å¿œç‰ˆï¼‰
- neuronx-distributed-inference (NxD Inference): 0.7.0
- Python: 3.12

**ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿**:
- ãƒ¢ãƒ‡ãƒ«: Qwen3-0.6B-Reranker
- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·:
  - Phase 1-3 åˆæœŸæ¸¬å®š: 97 ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå›ºå®šé•·ï¼‰
  - è¿½åŠ èª¿æŸ»: 18-125 ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå¯å¤‰é•·ã€16 ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰
- ã‚¿ã‚¹ã‚¯: Rerankerï¼ˆæ–‡æ›¸ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼‰
- ãƒãƒƒãƒã‚µã‚¤ã‚º: 4

**vLLM è¨­å®šï¼ˆéå»ã® Zenn è¨˜äº‹ã®å®Ÿé¨“ã§ã®æœ€é©å€¤ï¼‰**:
```yaml
vllm:
  tensor_parallel_size: 2           # 2 NeuronCore ä½¿ç”¨
  max_num_seqs: 4                   # åŒæ™‚å‡¦ç†æ•°
  block_size: 32                    # KV cache block size
  max_model_len: 2048
  max_num_batched_tokens: 256
  num_gpu_blocks_override: 512
  enable_prefix_caching: false      # Phase 1-5 ã§ã¯ç„¡åŠ¹
  dtype: "bfloat16"

  additional_config:
    override_neuron_config:
      skip_warmup: True
      enable_bucketing: true        # å‹•çš„ãƒãƒƒãƒãƒ³ã‚°æœ‰åŠ¹
      pa_num_blocks: 512
      pa_block_size: 32
```

ã“ã‚Œã‚‰ã®è¨­å®šã¯ã€[å‰å›ã® Zenn è¨˜äº‹](https://zenn.dev/tosshi/articles/ef61e14fe73399) ã§æœ€é©åŒ–ã—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚

**æ¸¬å®šã®ç„¦ç‚¹**:
æœ¬èª¿æŸ»ã§ã¯ã€ã“ã®ç‰¹å®šã®è¨­å®šã«ãŠã‘ã‚‹ vllm-neuron ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã€ã‚ˆã‚Šå¤§è¦æ¨¡ãªãƒãƒƒãƒã‚µã‚¤ã‚ºã§ã¯ã€ç•°ãªã‚‹æ€§èƒ½ç‰¹æ€§ã‚’ç¤ºã™å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

:::message
**è¿½åŠ èª¿æŸ»**: å›ºå®šé•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆ97 ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã ã‘ã§ãªãã€å¯å¤‰é•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆ18-125 ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã§ã® bucketing ã®å½±éŸ¿ã‚‚æ¸¬å®šã—ã¾ã—ãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã«ã‚ˆã‚‹æ€§èƒ½ã®é•ã„ã‚’æ˜ç¢ºã«ã™ã‚‹ã“ã¨ãŒã§ãã¾ã—ãŸã€‚
:::

:::message
**NxD Inference ã¯ vllm-neuron ã§å†…éƒ¨çš„ã«æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã¨ã—ã¦åˆ©ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚å®Ÿæ…‹ã¨ã—ã¦ã¯ `override_neuron_config` ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒ NxD Inference ã«åˆã†å½¢ã«å¤‰æ›ã•ã‚Œã¦æ¸¡ã•ã‚Œã¦ã„ã¾ã™ã€‚**
:::
::::

### 1.2 Perfetto ãƒˆãƒ¬ãƒ¼ã‚¹ã®åŸºæœ¬åˆ†æ

å‰å›ã‚‚å°‘ã—ç´¹ä»‹ã—ãŸ Perfetto ã«ã¤ã„ã¦ç´¹ä»‹ã—ã¾ã™ã€‚Perfetto ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã¯ SQLite ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ‰±ãˆã¾ã™ã€‚ã¾ãšä»¥ä¸‹ã®ã‚ˆã†ãªåˆ†æã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œã—ã¾ã—ãŸã€‚

:::details Perfetto ãƒˆãƒ¬ãƒ¼ã‚¹åˆ†æã‚³ãƒ¼ãƒ‰

```python
from perfetto.trace_processor import TraceProcessor
tp = TraceProcessor(trace='profile_output/trace.perfetto-trace')

# Operation ã”ã¨ã®é›†è¨ˆ
sql = """
SELECT name, COUNT(*) as count,
       SUM(dur) / 1e9 as total_seconds,
       AVG(dur) / 1e9 as avg_seconds
FROM slice WHERE dur > 0
GROUP BY name ORDER BY total_seconds DESC LIMIT 10
"""
```

**çµæœã®ä¸€éƒ¨**:
```
                  name   count total_seconds avg_seconds
0              unknown  156427      0.038387         0.0
1               MATMUL   21582      0.010941    0.000001
2 custom_call.17_sg0002      36      0.007028    0.000195
3            LDWEIGHTS   21212      0.004914         0.0
```

**ã‚¯ã‚¨ãƒªã®è¦‹æ–¹**:
`slice` ãƒ†ãƒ¼ãƒ–ãƒ«ã«ã¯å„ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®å®Ÿè¡Œè¨˜éŒ²ãŒæ ¼ç´ã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã®ã‚¯ã‚¨ãƒªã¯ä»¥ä¸‹ã‚’å–å¾—ã—ã¾ã™ã€‚
- `name`: ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åï¼ˆMATMUL ãªã©ã€Neuron ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ãŒç”Ÿæˆã—ãŸæ¼”ç®—ã®ç¨®é¡ï¼‰
- `count`: ãã®ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒå®Ÿè¡Œã•ã‚ŒãŸå›æ•°
- `dur`: å„å®Ÿè¡Œã®ç¶™ç¶šæ™‚é–“ï¼ˆãƒŠãƒç§’å˜ä½ã§è¨˜éŒ²ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€1e9 ã§å‰²ã£ã¦ç§’ã«å¤‰æ›ï¼‰
- `total_seconds`: ãã®ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆè¨ˆå®Ÿè¡Œæ™‚é–“ï¼ˆç§’å˜ä½ï¼‰
- `avg_seconds`: 1 å›ã‚ãŸã‚Šã®å¹³å‡å®Ÿè¡Œæ™‚é–“ï¼ˆç§’å˜ä½ï¼‰
:::

çµæœã¨ã—ã¦ã€ã¾ãšã€`custom_call.17_sg0002` ã¨ã„ã†æ“ä½œãŒãŸã£ãŸ 36 å›ã®å®Ÿè¡Œã§ 7ms ã‚‚æ¶ˆè²»ã—ã¦ã„ã‚‹ã“ã¨ãŒåˆ¤æ˜ã—ã¾ã—ãŸã€‚æ¬¡ã«ã€MATMUL ã¨ LDWEIGHTS ãŒã»ã¼åŒã˜å›æ•°å®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã™ã€‚ã•ã‚‰ã«ã€`unknown` ã¨ã„ã†åˆ†é¡ã®æ“ä½œãŒ 38ms ã§æœ€å¤§ã®æ™‚é–“ã‚’æ¶ˆè²»ã—ã¦ã„ã¾ã—ãŸã€‚

`custom_call.17_sg0002`ã€‚ã€‚ã€‚ä½•ã§ã™ã‹ã­ã“ã‚Œã¯ã€‚ã€‚

:::details [ç™ºå±•çš„å†…å®¹] NEFF åˆ†æã«ã‚ˆã‚‹ custom_call ã®èª¿æŸ»

**ç–‘å•**: `custom_call.17_sg0002` ã¨ã¯ä½•ã‹ï¼ŸRoPEï¼Ÿæ´»æ€§åŒ–é–¢æ•°ï¼Ÿä½•ã‚‰ã‹ã®ã‚«ã‚¹ã‚¿ãƒ ã‚«ãƒ¼ãƒãƒ«ï¼Ÿ

Perfetto ãƒˆãƒ¬ãƒ¼ã‚¹ã§ã¯å®Ÿè¡Œå›æ•°ã¨æ™‚é–“ã—ã‹åˆ†ã‹ã‚‰ãªã„ãŸã‚ã€NEFF (Neuron Executable File Format) ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ [unpacking](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-runtime/explore/work-with-neff-files.html) ã—ã¦é™çš„ãªæ§‹é€ ã‚’èª¿æŸ»ã—ã¾ã—ãŸã€‚(uppack ã«ã¯ `neuron-packager unpack` ã‚³ãƒãƒ³ãƒ‰ã‚’åˆ©ç”¨ã—ã¦ã‚‚è‰¯ã„ã§ã™)

**NEFF ã‹ã‚‰åˆ¤æ˜ã—ãŸã“ã¨**:

```bash
# NEFF ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ unpacking
$ dd if=neff_322059935237836.neff of=neff.tar.gz bs=1024 skip=1
$ tar -xzf neff.tar.gz

# tensor_map.json ã‚’ç¢ºèª
$ cat sg00/tensor_map.json | jq '.["custom_call.17_sg0002"]'
{
  "dtype": "float32",
  "sim_shape": [256, 1, 1],
  "kind": null,
  "is_const": false,
  "layer_name": "custom_call.17"
}
```

**åˆ†ã‹ã‚‹ã“ã¨**:
- ãƒ‡ãƒ¼ã‚¿å‹: `float32`ï¼ˆç²¾åº¦é‡è¦–ã®æ¼”ç®—ï¼‰
- ãƒ†ãƒ³ã‚½ãƒ«å½¢çŠ¶: `[256, 1, 1]`ï¼ˆæ¯”è¼ƒçš„å°ã•ã„ï¼‰
- ã‚µãƒ–ã‚°ãƒ©ãƒ•: `sg0002`
- å‹•çš„ã«è¨ˆç®—ã•ã‚Œã‚‹ä¸­é–“ãƒ†ãƒ³ã‚½ãƒ«
- `custom_call.14` ï½ `17` ã®é€£ç¶šã—ãŸæ¼”ç®—ã‚·ãƒ¼ã‚±ãƒ³ã‚¹

**Qwen3 ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‹ã‚‰æ¨æ¸¬**:

å½¢çŠ¶ `[256, 1, 1]` ã¨å‘¨è¾ºã® `dot` (MATMUL) æ“ä½œã‹ã‚‰ã€ä»¥ä¸‹ã®ã‚ˆã†ãªæ“ä½œã¨æ¨æ¸¬
- **RoPE (Rotary Position Embedding)**: ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¨ˆç®—
- **RMSNorm**: æ­£è¦åŒ–å±¤ã®çµ±è¨ˆå€¤è¨ˆç®—
- **ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹**: Softmax å‰ã®ä¸­é–“è¨ˆç®—

NEFF ã¯ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ã®é™çš„ãªæƒ…å ±ï¼ˆã‚°ãƒ©ãƒ•æ§‹é€ ã€ãƒ†ãƒ³ã‚½ãƒ«å½¢çŠ¶ã€ãƒ‡ãƒ¼ã‚¿å‹ï¼‰ã‚’å«ã¿ã¾ã™ãŒã€ä»¥ä¸‹ã¯åˆ¤æ˜ã—ãªã„ã‚ˆã†ã§ã™ã€‚
- å…·ä½“çš„ãªæ¼”ç®—ãƒ­ã‚¸ãƒƒã‚¯
- å®Ÿè¡Œå›æ•°
- å®Ÿè¡Œæ™‚é–“
- åˆå›å®Ÿè¡Œæ™‚ã®é…å»¶

NEFF åˆ†æã‹ã‚‰ã¯ã€ä½•ãŒä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹ã¯åˆ†ã‹ã‚Šã¾ã™ãŒã€ã©ã†å‹•ãã‹ã¯ Perfetto ãƒˆãƒ¬ãƒ¼ã‚¹ã§å®Ÿè¡Œæ™‚ã«æ¸¬å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ãã—ã¦ä¸Šè¨˜ä»¥ä¸Šã®è©³ç´°ãªç‰¹å®šã¯ç¾æ™‚ç‚¹ã§ã¯ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã€‚
:::

### 1.3 skip_warmup è¨­å®šã®å½±éŸ¿

è©³ç´°ã«è©¦è¡ŒéŒ¯èª¤ã—ãªãŒã‚‰å®Ÿè¡Œã—ãŸã‚¯ã‚¨ãƒªã‚’å…¨ã¦ç´¹ä»‹ã—ã¦ã„ã‚‹ã¨è†¨å¤§ã«ãªã£ã¦ã—ã¾ã†ãŸã‚å‰²æ„›ã—ã¾ã™ãŒ Phase 1 ã®æ™‚ç³»åˆ—ã®å‘½ä»¤å®Ÿè¡Œã«é–¢ã™ã‚‹èª¿æŸ»çµæœã‹ã‚‰ã€custom_call ãŒåˆå›å®Ÿè¡Œæ™‚ã«å¤§ããªé…å»¶ã‚’èµ·ã“ã—ã¦ã„ã‚‹ã“ã¨ãŒåˆ†ã‹ã£ãŸãŸã‚ã€NxD Inference ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã‚ã‚‹ `skip_warmup=False` ã‚’è©¦ã—ã¾ã—ãŸã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¯ `False` ãªã®ã§ã™ãŒä»¥å‰ã®å®Ÿé¨“ã®è©¦è¡ŒéŒ¯èª¤ã§ `True` ã«ã—ã¦ã„ã¾ã—ãŸã€‚ã“ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å¾Œã« 1 å›ã® forward å®Ÿè¡Œã‚’è¡Œã„ã€é…å»¶åˆæœŸåŒ–ã‚’å®Œäº†ã•ã›ã‚‹ã¨ã„ã†ã‚‚ã®ã§ã™ã€‚
                                                                                                                  
| è¨­å®š | å¹³å‡æ™‚é–“ |
|------|---------|
| Baseline (skip_warmup=True) | 2.992ç§’ |
| Warmup (skip_warmup=False) | 3.110ç§’ (+3.9%) |

ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã™ã‚Œã°é€Ÿããªã‚‹ã¨äºˆæƒ³ã—ã¦ã„ã¾ã—ãŸãŒã€å®Ÿéš›ã«ã¯ç´„ 4% é…ããªã‚Šã¾ã—ãŸã€‚ï¼ˆæ¸¬å®šã®ãŸã³ã«çµæœã¯å¤šå°‘å¤‰å‹•ã—ã¾ã™ï¼‰å†åº¦ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã—ã¦ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®å¤‰åŒ–ã‚’ç¢ºèªã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

:::details Operation æ•°ã®å¤‰åŒ–

```
Baseline (skip_warmup=True):
  MATMUL: 21,582å›, 10.94ms
  LDWEIGHTS: 21,212å›, 4.91ms
  ACTIVATE: 4,702å›, 1.65ms
  COPY: 83å›, 0.03ms

Warmup (skip_warmup=False):
  MATMUL: 13,497å› (-37%), 2.64ms (-76%)
  LDWEIGHTS: 13,497å› (-36%), 1.17ms (-76%)
  ACTIVATE: 4,207å› (-11%), 2.94ms (+78%)
  COPY: 554å› (+567%), 1.01ms (+3,267%)
```
:::

`skip_warmup=False` ã§ MATMUL/LDWEIGHTS ã®ä¸»è¦ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¯å®Ÿè¡Œæ™‚é–“ãŒ 76% æ¸›å°‘ã—ãŸã‚‚ã®ã®ã€ACTIVATE ã®å®Ÿè¡Œæ™‚é–“ãŒ +78%ã€COPY ã®å®Ÿè¡Œæ™‚é–“ãŒ +3,267% å¢—åŠ ã—ã€ãƒˆãƒ¼ã‚¿ãƒ«ã§ã¯é…ããªã‚Šã¾ã—ãŸã€‚


### 1.4 Neuron Profiler ã®æ¸¬å®šç¯„å›²ã®é™ç•Œ

:::message alert
**ã“ã“ã§é‡è¦ãªæ°—ã¥ã**ï¼šNeuron Profiler ã®ãƒˆãƒ¬ãƒ¼ã‚¹æ™‚é–“ã¯ 16-17ms ãªã®ã«ã€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å…¨ä½“ã¯ç´„ 3 ç§’ã‹ã‹ã£ã¦ã„ã‚‹ã€‚**ã“ã® 16-17ms ã£ã¦ã©ã“ã‹ã‚‰ã©ã“ã¾ã§ã®ãªã‚“ã®å€¤ï¼Ÿ**
:::

æ¶™ã®èª¿æŸ»ã®çµæœã€Neuron Profiler ã®æ¸¬å®šç¯„å›²ã«é–¢ã™ã‚‹é‡è¦ãªç‰¹æ€§ãŒåˆ¤æ˜ã—ã¾ã—ãŸã€‚Neuron Profiler ã¯å®Ÿè¡Œæ™‚ã« NTFF (Neuron Trace File Format) ã¨ã„ã†ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã—ã€ãã‚Œã‚’ Perfetto ãƒˆãƒ¬ãƒ¼ã‚¹ã«å¤‰æ›ã—ã¾ã™ã€‚å„ NTFF ãƒ•ã‚¡ã‚¤ãƒ«ã¯ 1 ã¤ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ã‚°ãƒ©ãƒ•ã®å®Ÿè¡Œè¨˜éŒ²ã‚’è¡¨ã—ã¦ãŠã‚Šã€ç•°ãªã‚‹ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚„ sequence length ç”¨ã®è¤‡æ•°ã‚°ãƒ©ãƒ•ãŒå­˜åœ¨ã—ã¾ã™ã€‚ãã®ãŸã‚ã“ã‚Œã ã‘ã‚’è¦‹ã‚Œã° NeuronCore ã®ãƒˆãƒ¼ã‚¿ãƒ«ã®å®Ÿè¡Œæ™‚é–“ãŒç¢ºå®Ÿã«ã‚ã‹ã‚‹ã¨ã„ã†ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

```bash
$ find profile_output -name "*.ntff" | wc -l
22  # 11ç¨®é¡ã®ã‚°ãƒ©ãƒ• Ã— 2ã‚³ã‚¢(tensor_parallel_size=2)

# NTFF ã¯ Perfetto ã«å¤‰æ›ã•ã‚Œã‚‹
$ ls profile_output/trace.perfetto-trace
trace.perfetto-trace  # ã“ã‚Œã‚’ TraceProcessor ã‚„ Perfetto UI ã§åˆ†æ
```

::::details NEFF ãƒ•ã‚¡ã‚¤ãƒ«ã¨ bucketing ã®é–¢ä¿‚

**NEFF (Neuron Executable File Format)** ã¯ã€NeuronCore ä¸Šã§å®Ÿè¡Œã•ã‚Œã‚‹ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ã‚°ãƒ©ãƒ•ã®å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ã§ã™ã€‚bucketing ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã¨ã€è¤‡æ•°ã® (batch_size, sequence_length) ã®çµ„ã¿åˆã‚ã›ã«å¯¾å¿œã™ã‚‹è¤‡æ•°ã®ã‚°ãƒ©ãƒ•ãŒäº‹å‰ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã•ã‚Œã¾ã™ã€‚

```bash
# NEFF ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†æ
$ find profile_output -name "*.neff" | wc -l
77  # è¤‡æ•°ã® PID ã‹ã‚‰ 11 ç¨®é¡ã®ã‚°ãƒ©ãƒ• Ã— è¤‡æ•°å›ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«

$ ls -lh profile_output/*/neff_*.neff | awk '{print $5}' | sort -u
801K   # Graph 1: æœ€å°ãƒã‚±ãƒƒãƒˆ
881K   # Graph 2
991K   # Graph 3
1.1M   # Graph 4
1.3M   # Graph 5
2.1M   # Graph 6
2.3M   # Graph 7
2.4M   # Graph 8
2.6M   # Graph 9
3.0M   # Graph 10
       # (åˆè¨ˆ 11 ç¨®é¡ã€124 MB)
```

bucketing ã‚’æœ‰åŠ¹ã«ã™ã‚‹å ´åˆã€è¤‡æ•°ã®ã‚µã‚¤ã‚ºã®ã‚°ãƒ©ãƒ•ã‚’äº‹å‰ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¦ãŠãã€ãã‚Œã‚‰ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦æ¨è«–ã«åˆ©ç”¨ã—ã¾ã™ã€‚å®Ÿè¡Œæ™‚ã«å…¥åŠ›ã‚µã‚¤ã‚ºã«å¿œã˜ãŸæœ€é©ã‚°ãƒ©ãƒ•ã‚’é¸æŠã™ã‚‹ã“ã¨ã‹ã‚‰ã€ã‚°ãƒ©ãƒ•é¸æŠã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãªã©ãŒç™ºç”Ÿã—ã¾ã™ã€‚å›ºå®šé•·ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã®å ´åˆã€å®Ÿéš›ã«ä½¿ç”¨ã™ã‚‹ã®ã¯è¤‡æ•°ã‚°ãƒ©ãƒ•ã®ã†ã¡ 1 ã¤ã ã‘ã§ã‚ã‚Šã€ç‰¹ã« bucketing ã®æ©æµã‚’å—ã‘ã‚‹ã“ã¨ãªãã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒè¿½åŠ ã•ã‚Œã‚‹ã¨æ€ã‚ã‚Œã¾ã™ã€‚ä¸€èˆ¬çš„ãª LLM ã®ç”Ÿæˆã®ã‚ˆã†ãªå¯å¤‰é•·ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã®å ´åˆã¯ç•°ãªã‚‹é•·ã•ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¤‡æ•°ã‚°ãƒ©ãƒ•ã«åˆ†æ•£ã•ã‚Œã‚‹ã®ã§å†ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã™ã‚‹ã“ã¨ãªãåŠ¹ç‡çš„ãªãƒãƒƒãƒãƒ³ã‚°ãŒå¯èƒ½ãªãŸã‚ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’å·®ã—å¼•ã„ã¦ã‚‚é«˜é€ŸåŒ–ã«è²¢çŒ®ã™ã‚‹ã¨æ€ã‚ã‚Œã¾ã™ã€‚

:::message
**ã“ã®ã‚ˆã†ã«ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã«ã‚ˆã£ã¦ bucketing ã®æ€§èƒ½ã¯ ON/OFF ã§ã©ã¡ã‚‰ãŒè‰¯ã„ã‹å¤‰å‹•ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã“ã¨ã‚’è¦šãˆã¦ãŠã„ã¦ãã ã•ã„ï¼**
:::
::::

:::message alert
**ä»Šå›ã®éã¡ã‹ã‚‰ã®å­¦ã³**: Neuron Profiler ã¯ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒ¬ãƒ™ãƒ«ã®åˆ†æã«ã¯æœ‰ç”¨ã ãŒã€vllm-neuron å…¨ä½“ã®æœ€é©åŒ–ã«ãŠã„ã¦åˆæ‰‹ã§ä½¿ã†ã‚‚ã®ã§ã¯ãªã„ã€‚
:::

ã‚«ã‚¹ã‚¿ãƒ ã‚«ãƒ¼ãƒãƒ«ã‚’å®Ÿè£…ã™ã‚‹ã‚ˆã†ãªã‚±ãƒ¼ã‚¹ã§ã¯ Neuron Profiler ã¯å¿…é ˆã¨è¨€ãˆã¾ã™ãŒã€æœ€é©ãªè¨­å®šã‚’æ¢ã™éš›ã®åˆæ‰‹ã§å®Ÿæ–½ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãã—ã¦ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒ¬ãƒ™ãƒ«ã®æ”¹å–„ã‚’ã™ã‚‹å‰ã« vllm-neuron å…¨ä½“ã®å®Ÿè¡Œæ™‚é–“ã®å†…ã®ã©ã®ç¨‹åº¦ã‚’ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢å´ã®æ¨è«–å‡¦ç†ãŒå ã‚ã¦ã„ã‚‹ã®ã‹ã«ã‚ˆã£ã¦æ”¹å–„ã®å„ªå…ˆåº¦ãŒå¤‰ã‚ã£ã¦ãã‚‹ã®ã§ vllm-neuron å…¨ä½“ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‚’ã—ãªã„ã¨æ„å‘³ãªã„ãã€ã¨æ€ã„ã¾ã—ãŸã€‚ã€‚ã¨ã¯ã„ãˆã€ä»Šå›å¾—ãŸ Neuron Profiler ã«é–¢ã™ã‚‹çŸ¥è¦‹ã¯æœ‰ç”¨ãªãŸã‚ã‚·ã‚§ã‚¢ã®æ„å‘³ã‚’è¾¼ã‚ã¦ Phase 1 ã‚’æ¶ˆã•ãšã«ãã®ã¾ã¾å…¬é–‹ã—ã¾ã™ã€‚

### 1.5 NEFFã€Perfetto ã¨ã¯

Phase 1 ã§ç™»å ´ã—ãŸãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¤ã„ã¦æ•´ç†ã—ã¾ã™ã€‚

:::message
NEFFï¼ˆã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ï¼‰ â†’ NTFFï¼ˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°å®Ÿè¡Œæ™‚ã®ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ â†’ **Perfetto ãƒˆãƒ¬ãƒ¼ã‚¹**ï¼ˆåˆ†æã«ä½¿ç”¨ï¼‰
:::

#### NEFF (Neuron Executable File Format)

[å‚è€ƒ: Work with NEFF Files](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-runtime/explore/work-with-neff-files.html)

**å½¹å‰²**: ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ã‚°ãƒ©ãƒ•ã®å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«

```bash
# NEFF ã®æ§‹é€ 
neff_322059935237836.neff (801KB)
â”œâ”€â”€ [1024 byte header]
â””â”€â”€ [tar.gz archive]
    â”œâ”€â”€ info.json              # ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æƒ…å ±
    â”œâ”€â”€ hlo_stats.json         # æ¼”ç®—çµ±è¨ˆï¼ˆHloMacCount: 29.2B ãªã©ï¼‰
    â”œâ”€â”€ metrics.json           # æ¨å®šãƒ¬ã‚¤ãƒ†ãƒ³ã‚·
    â”œâ”€â”€ neff.json             # ã‚°ãƒ©ãƒ•å®šç¾©ï¼ˆ373 ãƒãƒ¼ãƒ‰ï¼‰
    â””â”€â”€ sg00/                  # ã‚µãƒ–ã‚°ãƒ©ãƒ• 0
        â”œâ”€â”€ tensor_map.json   # ãƒ†ãƒ³ã‚½ãƒ«æƒ…å ±ï¼ˆ458 ãƒ†ãƒ³ã‚½ãƒ«ï¼‰
        â”œâ”€â”€ PE.bin            # Processing Element å‘½ä»¤
        â”œâ”€â”€ Activation.bin    # æ´»æ€§åŒ–é–¢æ•°å‘½ä»¤
        â”œâ”€â”€ DVE.bin           # Data Vector Engine å‘½ä»¤
        â””â”€â”€ debug_info_*.dbg  # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
```

#### NTFF (Neuron Trace File Format) - ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«

**å½¹å‰²**: Perfetto ãƒˆãƒ¬ãƒ¼ã‚¹ã«å¤‰æ›ã•ã‚Œã‚‹å‰ã®ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«

```bash
# NTFF ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¾‹ï¼ˆNeuron Profiler ãŒç”Ÿæˆï¼‰
profile_output/i-0049acfde6046f237_pid_520024/
â”œâ”€â”€ 322059935237836_instid_0_vnc_0.ntff  # Graph 1, Core 0
â”œâ”€â”€ 322059935237836_instid_0_vnc_1.ntff  # Graph 1, Core 1
â”œâ”€â”€ 729292360268366_instid_0_vnc_0.ntff  # Graph 4, Core 0
â”œâ”€â”€ 729292360268366_instid_0_vnc_1.ntff  # Graph 4, Core 1
...
â””â”€â”€ (22 files = 11 graphs Ã— 2 cores)

# Neuron Profiler ã§ Perfetto ã«å¤‰æ›
$ neuron-profile view --output-format perfetto profile_output
```

#### Perfetto ãƒˆãƒ¬ãƒ¼ã‚¹

**å½¹å‰²**: NeuronCore ä¸Šã®ä½ãƒ¬ãƒ™ãƒ«å®Ÿè¡Œãƒˆãƒ¬ãƒ¼ã‚¹

```bash
# Perfetto ãƒˆãƒ¬ãƒ¼ã‚¹
trace.perfetto-trace (110 MB)
â””â”€â”€ SQLite ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
    â”œâ”€â”€ slice ãƒ†ãƒ¼ãƒ–ãƒ«          # ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œè¨˜éŒ²
    â”‚   â””â”€â”€ MATMUL: 21,582å›, 10.94ms
    â”‚       COPY: 83å›, 0.03ms
    â”‚       custom_call.17: 36å›, 7ms
    â”œâ”€â”€ thread ãƒ†ãƒ¼ãƒ–ãƒ«         # ã‚¹ãƒ¬ãƒƒãƒ‰æƒ…å ±
    â””â”€â”€ process ãƒ†ãƒ¼ãƒ–ãƒ«        # ãƒ—ãƒ­ã‚»ã‚¹æƒ…å ±
```

::::details NEFF ã¨ Perfetto ã®æ¯”è¼ƒ

ä»¥ä¸‹ã¯ã¾ã å®Œå…¨ã«ã¯ç†è§£ãƒ»æ•´ç†ã—ãã‚Œã¦ã„ãªã„ãŸã‚å‚è€ƒç¨‹åº¦ã«ç¢ºèªã—ã¦ãã ã•ã„ã€‚

| æƒ…å ± | NEFF | Perfetto | å‚™è€ƒ |
|------|------|----------|------|
| **é™çš„æ§‹é€ ** | | | |
| ã‚°ãƒ©ãƒ•æ§‹é€ ï¼ˆãƒãƒ¼ãƒ‰ã€ãƒ†ãƒ³ã‚½ãƒ«æ•°ï¼‰ | âœ… | âŒ | NEFF unpacking ã§å–å¾— |
| ãƒ†ãƒ³ã‚½ãƒ«å½¢çŠ¶ãƒ»ãƒ‡ãƒ¼ã‚¿å‹ | âœ… | âŒ | tensor_map.json |
| æ¼”ç®—é‡ï¼ˆç†è«–å€¤ï¼‰ | âœ… | âŒ | hlo_stats.json |
| ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆç†è«–å€¤ï¼‰ | âœ… | âŒ | IfmapSizeã€OfmapSize |
| DMA ã‚­ãƒ¥ãƒ¼æ§‹æˆ | âœ… | âŒ | def.json |
| **ã‚°ãƒ©ãƒ•ãƒ¬ãƒ™ãƒ«å®Ÿè¡Œ** | | | |
| ã‚°ãƒ©ãƒ•ã”ã¨ã®å®Ÿè¡Œæ™‚é–“ | âŒ | âš ï¸ | SQL é›†è¨ˆã§è¨ˆç®—å¯èƒ½ |
| NeuronCore ã”ã¨ã®å†…è¨³ | âŒ | âš ï¸ | ã‚¹ãƒ¬ãƒƒãƒ‰åˆ¥ã«é›†è¨ˆ |
| ä½¿ç”¨ã•ã‚ŒãŸã‚°ãƒ©ãƒ•ã®è­˜åˆ¥ | âŒ | âš ï¸ | slice åã‹ã‚‰æ¨å®š |
| ã‚°ãƒ©ãƒ•é–“ã®é·ç§»æ™‚é–“ | âŒ | âš ï¸ | ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‹ã‚‰æ¨å®š |
| **ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¬ãƒ™ãƒ«å®Ÿè¡Œ** | | | |
| å®Ÿè¡Œæ™‚é–“ï¼ˆå®Ÿæ¸¬å€¤ï¼‰ | âŒ | âœ… | slice.dur |
| å®Ÿè¡Œå›æ•° | âŒ | âœ… | COUNT(*) |
| ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è©³ç´°ï¼ˆMATMULã€COPY ãªã©ï¼‰ | âŒ | âœ… | slice.name |
| ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã¨å®Ÿè¡Œé †åº | âŒ | âœ… | slice.ts |
| ä¸¦åˆ—å®Ÿè¡Œã®å¯è¦–åŒ– | âŒ | âœ… | Perfetto UI |
| åˆæœŸåŒ–é…å»¶ï¼ˆskip_warmup åŠ¹æœï¼‰ | âŒ | âœ… | åˆå›å®Ÿè¡Œæ™‚é–“ã®æ¯”è¼ƒ |
| **é«˜ãƒ¬ãƒ™ãƒ«æƒ…å ±** | | | |
| Python ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ | âŒ | âŒ | line_profiler ãªã©ãŒå¿…è¦ |
| æ¼”ç®—å†…å®¹ã®æ„å‘³ï¼ˆRoPEã€RMSNorm ãªã©ï¼‰ | âš ï¸ | âŒ | å½¢çŠ¶ã‹ã‚‰æ¨æ¸¬ã®ã¿ |

**å‡¡ä¾‹**: âœ… ç›´æ¥å–å¾—å¯èƒ½ã€âš ï¸ æ¨æ¸¬ãƒ»è¨ˆç®—ãŒå¿…è¦ã€âŒ å–å¾—ä¸å¯èƒ½
::::

---

## Phase 2: line_profiler ã«ã‚ˆã‚‹ Python ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°

Phase 1 ã§ã¯ Neuron Profiler ã«ã‚ˆã‚Š NeuronCore ãƒ¬ãƒ™ãƒ«ã®è©³ç´°ãªåˆ†æã‚’è¡Œã„ã¾ã—ãŸãŒã€Python ãƒ¬ãƒ™ãƒ«ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ï¼ˆãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã€ãƒ‡ãƒ¼ã‚¿æº–å‚™ãªã©ï¼‰ã®æ¸¬å®šã«ã¯åˆ¥ã®ãƒ„ãƒ¼ãƒ«ãŒå¿…è¦ã§ã™ã€‚ãã“ã§ line_profiler ã‚’ä½¿ç”¨ã—ã¦ Python ã‚³ãƒ¼ãƒ‰ã®è¡Œã”ã¨ã®å®Ÿè¡Œæ™‚é–“ã‚’æ¸¬å®šã—ã¾ã™ã€‚

### æ¸¬å®šã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æº–å‚™

Phase 1 ã§ä½¿ç”¨ã—ãŸ `test_reranker.py` ã¯ pytest + benchmark_capture ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ãŒã€line_profiler ã¨çµ„ã¿åˆã‚ã›ã‚‹ã¨å‡ºåŠ›ãŒè¤‡é›‘ã«ãªã‚Šã¾ã™ã€‚ãã“ã§ã€line_profiler å°‚ç”¨ã®ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¹ã‚¯ãƒªãƒ—ãƒˆ `profile_line.py` ã‚’ä½œæˆã—ã¾ã—ãŸã€‚ï¼ˆã“ã®è¾ºã‚Šã‚‚ vllm-neuron ã® Python ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã®ãŸã‚ã«ã†ã¾ãå–ã‚Œã‚‹ã‚ˆã†ã«ä»Šå¾Œ benchmark_capture ã®å®Ÿè£…ã‚’æ”¹å–„ã—ã¾ã™ï¼‰

::::details ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æ§‹é€ 

```python:profile_line.py
try:
    profile
except NameError:
    def profile(func):
        return func

# config.yaml ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿ï¼ˆtest_reranker.py ã¨åŒã˜ï¼‰
config_path = Path(__file__).parent / 'config.yaml'
with open(config_path, 'r') as f:
    config = yaml.safe_load(f)

# æ¸¬å®šå¯¾è±¡ã®é–¢æ•°ã« @profile ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’è¿½åŠ 
@profile
def build_prompts_for_vllm(pairs, tokenizer, prefix_tokens, suffix_tokens):
    """ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰"""
    # ... (test_reranker.py ã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯)

@profile
def run_reranker(llm, tokenizer, token_true_id, token_false_id,
                 prefix_tokens, suffix_tokens):
    """ãƒªãƒ©ãƒ³ã‚«ãƒ¼ã®ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # ... (test_reranker.py ã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯)

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼ˆpytest éä¾å­˜ï¼‰"""
    llm = vllm.LLM(model=model_path, **vllm_config)
    # ... åˆæœŸåŒ–ã¨ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°å®Ÿè¡Œ

if __name__ == "__main__":
    main()
```

::::

::::details ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Œå…¨ç‰ˆ
```python:profile_line.py
"""
Line profiler script for vLLM-Neuron Reranker

Run with:
    kernprof -l -v profile_line.py

Or for more detailed output:
    kernprof -l profile_line.py
    python -m line_profiler profile_line.py.lprof
"""

# line_profiler compatibility: make @profile decorator optional
try:
    profile
except NameError:
    # If not running under kernprof, @profile is a no-op
    def profile(func):
        return func

import csv
import gc
import logging
import os
import sys
from pathlib import Path

import yaml

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load configuration
config_path = Path(__file__).parent / 'config.yaml'
with open(config_path, 'r') as f:
    config = yaml.safe_load(f)

# Get model path
model_path = config['model']['path']

# Get vLLM config
vllm_config = {
    "tensor_parallel_size": config['vllm']['tensor_parallel_size'],
    "max_num_seqs": config['vllm']['max_num_seqs'],
    "block_size": config['vllm']['block_size'],
    "max_model_len": config['vllm']['max_model_len'],
    "max_num_batched_tokens": config['vllm']['max_num_batched_tokens'],
    "num_gpu_blocks_override": config['vllm']['num_gpu_blocks_override'],
    "enable_prefix_caching": config['vllm']['enable_prefix_caching'],
    "dtype": config['vllm']['dtype'],
    "disable_log_stats": config['vllm'].get('disable_log_stats', False),
}

# Add additional_config if present (Zenn article optimal settings)
if 'additional_config' in config['vllm']:
    vllm_config['additional_config'] = config['vllm']['additional_config']

# Get reranker config
reranker_config = config['reranker']
benchmark_config = config['benchmark']

# Reranker prompts
reranker_prompts = {
    'instruction': reranker_config['instruction'],
    'prefix': reranker_config['prefix'],
    'suffix': reranker_config['suffix']
}

# Token IDs
token_ids = {
    'true': reranker_config['token_true'],
    'false': reranker_config['token_false']
}

# Load CSV data
csv_file = Path(__file__).parent / reranker_config['input_file']
with open(csv_file, 'r', encoding='utf-8') as f:
    reader = csv.DictReader(f)
    rows = list(reader)

num_queries = min(len(rows), benchmark_config['num_test_queries'])
search_num = reranker_config['search_num']
batch_size = reranker_config['batch_size']
max_length = reranker_config['max_length']

logger.info(f"Loaded {len(rows)} queries from {csv_file}")
logger.info(f"Testing with first {num_queries} queries")


def format_instruction(query: str, doc: str) -> str:
    """Format instruction for reranker"""
    instruction = reranker_prompts['instruction']
    output = f"<Instruct>: {instruction}\n<Query>: {query}\n<Document>: {doc}"
    # Truncate if too long
    if len(output) >= 2000:
        output = output[:2000]
    return output


@profile
def build_prompts_for_vllm(pairs, tokenizer, prefix_tokens, suffix_tokens):
    """Build prompts with proper tokenization - PROFILING TARGET"""
    prompts = []
    budget = max_length - len(prefix_tokens) - len(suffix_tokens)

    # Tokenize pairs
    enc = tokenizer(
        list(pairs),
        padding=False,
        truncation="longest_first",
        return_attention_mask=False,
        add_special_tokens=False,
        max_length=max(8, budget),
    )

    # Build final prompts: prefix + content + suffix
    for ids in enc["input_ids"]:
        final_ids = prefix_tokens + ids + suffix_tokens
        text = tokenizer.decode(final_ids, skip_special_tokens=False)
        prompts.append(text)

    return prompts


@profile
def run_reranker(llm, tokenizer, token_true_id, token_false_id, prefix_tokens, suffix_tokens):
    """Run reranker on queries - MAIN PROFILING TARGET"""

    import vllm
    from vllm import SamplingParams

    # Get use_tqdm setting from benchmark config
    use_tqdm = benchmark_config.get('use_tqdm', True)

    # Create SamplingParams
    sampling_params = SamplingParams(
        max_tokens=1,
        temperature=0.0,
        logprobs=20,
        detokenize=True,
        allowed_token_ids=[token_true_id, token_false_id]
    )

    logger.info(f"SamplingParams configured: max_tokens=1, "
                f"allowed_tokens=[{token_ids['true']}, {token_ids['false']}]")

    # Process each query
    total_processed = 0
    for query_idx, row in enumerate(rows[:num_queries]):
        query = row["query"]

        # Get candidates
        candidates = [
            row[f"answer_{i}"]
            for i in range(search_num)
            if f"answer_{i}" in row
        ]

        # Format query-document pairs
        pairs = [format_instruction(query, doc) for doc in candidates[:search_num]]

        # Build prompts with tokenization
        prompts = build_prompts_for_vllm(pairs, tokenizer, prefix_tokens, suffix_tokens)

        # Process in batches
        query_outputs = []
        for s in range(0, len(prompts), batch_size):
            batch_prompts = prompts[s:s + batch_size]
            outputs = llm.generate(batch_prompts, sampling_params, use_tqdm=use_tqdm)
            query_outputs.extend(outputs)

        total_processed += len(query_outputs)

        if query_idx == 0:
            # Show first result for verification
            logger.info(f"Query 1: {query[:80]}...")
            logger.info(f"Generated {len(query_outputs)} scores for "
                       f"{len(candidates[:search_num])} candidates")
            if query_outputs:
                first_output = query_outputs[0]
                logger.info(f"First output: {first_output.outputs[0].text} "
                           f"(token_ids={first_output.outputs[0].token_ids})")

    logger.info(f"Profiling completed: processed {total_processed} reranker pairs")
    return total_processed


def main():
    """Main profiling function"""
    import vllm

    logger.info("Initializing vLLM-Neuron reranker...")
    logger.info(f"Model: {model_path}")
    logger.info(f"Config: block_size={vllm_config['block_size']}, "
               f"max_num_seqs={vllm_config['max_num_seqs']}, "
               f"tensor_parallel_size={vllm_config['tensor_parallel_size']}")

    # Initialize vLLM
    llm = vllm.LLM(model=model_path, **vllm_config)

    # Get tokenizer and token IDs
    tokenizer = llm.get_tokenizer()
    token_false_id = tokenizer.convert_tokens_to_ids(token_ids['false'])
    token_true_id = tokenizer.convert_tokens_to_ids(token_ids['true'])

    logger.info(f"Token IDs: {token_ids['true']}={token_true_id}, "
               f"{token_ids['false']}={token_false_id}")

    # Encode prompt templates
    prefix_tokens = tokenizer.encode(
        reranker_prompts['prefix'], add_special_tokens=False
    )
    suffix_tokens = tokenizer.encode(
        reranker_prompts['suffix'], add_special_tokens=False
    )

    logger.info(f"Prefix tokens: {len(prefix_tokens)}, Suffix tokens: {len(suffix_tokens)}")

    # Run profiling
    logger.info("Starting profiling run...")
    total = run_reranker(llm, tokenizer, token_true_id, token_false_id, prefix_tokens, suffix_tokens)

    logger.info(f"Profiling complete. Processed {total} pairs.")

    # Cleanup
    del llm
    gc.collect()


if __name__ == "__main__":
    main()
````

```yaml:config.yaml
# vLLM-Neuron Reranker Benchmark Configuration

# Model configuration
model:
  # Path to the reranker model
  # Example: "/path/to/models/Qwen3-0.6B-Reranker"
  # Use environment variable: export RERANKER_MODEL_PATH="/your/model/path"
  path: "/home/coder/data-science/investigations/inf2-vllm-performance/models/Qwen3-0.6B-Reranker"

# vLLM-Neuron engine settings
vllm:
  tensor_parallel_size: 2           # Number of NeuronCores
  max_num_seqs: 4                   # Batch size
  block_size: 32                    # KV cache block size (32 for Zenn best case, 128 for stability)
  max_model_len: 2048               # Maximum sequence length
  max_num_batched_tokens: 256       # Performance optimization
  num_gpu_blocks_override: 512      # pa_num_blocks equivalent
  enable_prefix_caching: false      # Explicit disable
  dtype: "bfloat16"                 # Data type

  # Neuron-specific overrides (Zenn article optimal settings)
  additional_config:
    override_neuron_config:
      skip_warmup: true             # Phase 1-5 ã®è¨­å®šï¼ˆè¨˜äº‹ã¨ä¸€è‡´ï¼‰
      enable_bucketing: true        # å‹•çš„ãƒãƒƒãƒãƒ³ã‚°æœ‰åŠ¹
      pa_num_blocks: 512
      pa_block_size: 32

# Reranker-specific settings
reranker:
  # Input data
  input_file: "input_sample.csv"    # CSV file with queries and candidates

  # Processing parameters
  search_num: 20                    # Number of candidates per query to process
  batch_size: 8                     # Batch size for processing prompts
  max_length: 1500                  # Maximum prompt length

  # Model-specific tokens (for Qwen3-Reranker)
  # Change these for other reranker models
  token_true: "yes"
  token_false: "no"

  # Prompt templates (for Qwen3-Reranker)
  # Customize these for your model
  prefix: |
    <|im_start|>system
    Judge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be "yes" or "no".<|im_end|>
    <|im_start|>user

  # Note: "assitant" typo is intentional for Qwen3-Reranker compatibility
  suffix: |
    <|im_end|>
    <|im_start|>assitant
    <think>

    </think>


  instruction: "Given a web search query, retrieve relevant passages that answer the query"

# Benchmark settings
benchmark:
  rounds: 5                         # Number of benchmark rounds
  warmup_rounds: 1                  # Number of warmup rounds
  num_test_queries: 10              # Number of queries to use for testing (è¨˜äº‹ã¨åŒã˜æ¡ä»¶)

# Profiler settings (optional)
profiler:
  # Clear Neuron compilation cache before benchmark
  # WARNING: First run after clearing will recompile (10-15 minutes)
  # Useful when:
  # - Model configuration changed (batch size, sequence length, etc.)
  # - Neuron SDK version changed
  # - Testing clean compilation performance
  clear_cache_before: false

  # Clear cache after benchmark (useful for CI/CD to save disk space)
  clear_cache_after: false
```
::::

ã“ã‚Œã«ã‚ˆã‚Šã€**Phase 1 ã¨åŒã˜æ¸¬å®šæ¡ä»¶**ï¼ˆåŒã˜ config.yamlã€åŒã˜å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ï¼‰ã‚’ç¶­æŒã—ãªãŒã‚‰ã€line_profiler ã«ã‚ˆã‚‹è©³ç´°ãª Python ãƒ¬ãƒ™ãƒ«ã®åˆ†æãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

### 2.1 æ¸¬å®šå¯¾è±¡ã®ç†è§£

**æ¸¬å®šå¯¾è±¡**: 1 ã‚¯ã‚¨ãƒªï¼ˆ20 å€™è£œæ–‡æ›¸ã®ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼‰ã‚’å‡¦ç†ã™ã‚‹æ™‚é–“

```yaml
reranker:
  search_num: 20        # 1 ã‚¯ã‚¨ãƒªã‚ãŸã‚Š 20 å€™è£œæ–‡æ›¸
  batch_size: 8         # 8 ãƒšã‚¢ãšã¤ãƒãƒƒãƒå‡¦ç†

vllm:
  max_num_seqs: 4       # vLLM ã®åŒæ™‚å‡¦ç†æ•°
```

```
1  ã‚¯ã‚¨ãƒª = 20 ãƒšã‚¢ Ã· batch_size=8 = 3 ãƒãƒƒãƒ
10 ã‚¯ã‚¨ãƒª = 30 ãƒãƒƒãƒ
åˆè¨ˆæ™‚é–“ = 2,992ms â†’ 1 ã‚¯ã‚¨ãƒªã‚ãŸã‚Šç´„ 300ms
```

### 2.2 line_profiler æ¸¬å®šçµæœ

::::details line_profiler ã®å®Ÿè¡Œ

**å®Ÿè¡Œç’°å¢ƒã®æº–å‚™**:

```bash
# vLLM-Neuron ç’°å¢ƒã‚’ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ãƒˆ
source /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/bin/activate

# PATH ã« Neuron SDK ã®ãƒ„ãƒ¼ãƒ«ã‚’è¿½åŠ 
export PATH="/opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/bin:$PATH"

# line_profiler ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„å ´åˆ
pip install line-profiler
```

**ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°å®Ÿè¡Œ**:

```bash
cd /path/to/my-reranker
kernprof -l -v -p vllm.v1.engine profile_line.py
```

:::message
**kernprof ã‚ªãƒ—ã‚·ãƒ§ãƒ³èª¬æ˜**:
- `-l` (--line-by-line): è¡Œã”ã¨ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‚’æœ‰åŠ¹åŒ–
- `-v` (--view): çµæœã‚’å³åº§ã«è¡¨ç¤º
- `-p vllm.v1.engine` (--prof-mod): **vllm.v1.engine ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’è‡ªå‹•ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°å¯¾è±¡ã«æŒ‡å®š**ï¼ˆã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å†…ã®å…¨é–¢æ•°ã‚’è‡ªå‹•çš„ã«ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ï¼‰
:::

å®Ÿè¡Œå¾Œã€`profile_line.py.lprof` ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã€ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«è©³ç´°ãªè¡Œã”ã¨ã®å®Ÿè¡Œæ™‚é–“ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚
::::

ä»¥ä¸‹ã«å®Ÿéš›ã« line_profiler ã®çµæœã‹ã‚‰å¾—ã‚‰ã‚ŒãŸæƒ…å ±ã‚’ç¤ºã—ã¾ã™ã€‚

```python
# llm.generate() - 30ãƒãƒƒãƒå‡¦ç†
Line 157: outputs = llm.generate(batch_prompts, sampling_params)
  - Hits: 30 batches
  - Time: 3781.560 ms (3.78ç§’)
  - Per Hit: 126.052 ms/batch
  - % Time: 99.1%

# LLMEngine.step() ã®å†…è¨³
Line 293: outputs = self.engine_core.get_output()
  - Hits: 229 steps (7.6 steps/batch)
  - Time: 3197.372 ms
  - Per Hit: 13.962 ms/step
  - % Time: 95.3%
```

line_profiler ã«ã‚ˆã‚‹æ¸¬å®šã®çµæœã€10 ã‚¯ã‚¨ãƒªï¼ˆ30 ãƒãƒƒãƒï¼‰ã®å‡¦ç†ã«åˆè¨ˆ 3.78 ç§’ã‹ã‹ã‚Šã€ãã®ã†ã¡ `llm.generate()` ã®å‘¼ã³å‡ºã—ã ã‘ã§ **99.1%ï¼ˆ3.78 ç§’ï¼‰** ã‚’å ã‚ã‚‹ã“ã¨ãŒåˆ¤æ˜ã—ã¾ã—ãŸã€‚ç´„ 3 ç§’ã‹ã‚‰æ™‚é–“ãŒå¢—ãˆã¦ã„ã‚‹ã®ã¯ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã§ã™ã€‚

ã•ã‚‰ã«é‡è¦ãªç™ºè¦‹ã¨ã—ã¦ã€**1 ãƒãƒƒãƒã‚ãŸã‚Šã®å‡¦ç†æ™‚é–“ãŒ 126.052ms** ã¨ã„ã†æ¸¬å®šå€¤ãŒå¾—ã‚‰ã‚Œã¾ã—ãŸã€‚ãŸã ã—ã€ã“ã®å€¤ã¯ **ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’å«ã‚€** ãŸã‚ã€Phase 3 ã§ç´”ç²‹ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¸¬å®šï¼ˆ64.33 msï¼‰ã‚’åˆ¥é€”å®Ÿæ–½ã—ã¾ã™ã€‚ã¾ãŸã€vLLM ã®å†…éƒ¨å‡¦ç†ã‚’è¦‹ã‚‹ã¨ã€`LLMEngine.step()` ãŒ 229 å›å‘¼ã°ã‚Œã¦ãŠã‚Šã€30 ãƒãƒƒãƒã«å¯¾ã—ã¦ **å¹³å‡ 7.6 steps/batch** ã¨ã„ã†è¬ã®å€¤ãŒè¦³æ¸¬ã•ã‚Œã¾ã—ãŸã€‚ãªãœ 1 ãƒãƒƒãƒã®å‡¦ç†ã« 7.6 å›ã‚‚ã®ã‚¹ãƒ†ãƒƒãƒ—ãŒå¿…è¦ãªã®ã‹ã€ã“ã®æ™‚ç‚¹ã§ã¯ç†è§£ã§ãã¦ã„ã¾ã›ã‚“ã§ã—ãŸã€‚

### 2.3 7.6 steps/batch ã®ç†ç”±ã‚’è¿½ã†

ã“ã®æ•°å€¤ã®è§£æ˜ã™ã‚‹ãŸã‚ã€`LLMEngine.step()` ã®ä¸­èº«ã‚’ã•ã‚‰ã«è©³ã—ãèª¿ã¹ã¾ã—ãŸã€‚line_profiler ã® `-p vllm.v1.engine` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚Šã€vLLM å†…éƒ¨ã®ã‚³ãƒ¼ãƒ‰ã‚‚è‡ªå‹•ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã•ã‚Œã¦ã„ã¾ã™ã€‚

`LLMEngine.step()` ã®å‡¦ç†æ™‚é–“ã®ã»ã¼å…¨ã¦ï¼ˆ95.3%ï¼‰ãŒ `engine_core.get_output()` ã¨ã„ã†å˜ä¸€ã®é–¢æ•°å‘¼ã³å‡ºã—ã§è²»ã‚„ã•ã‚Œã¦ã„ã¾ã—ãŸã€‚ã•ã‚‰ã«ãã® `get_output()` é–¢æ•°ã®ä¸­èº«ã‚’è¦‹ã‚‹ã¨ã€**100% ãŒ `outputs_queue.get()` ã¨ã„ã†ã‚­ãƒ¥ãƒ¼ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿å–å¾—å‡¦ç†**ã§ã—ãŸã€‚

```python
# LLMEngine.step() ã®ä¸­èº«
Line 293: outputs = self.engine_core.get_output()
  - Time: 3197.372 ms (95.3% of step())

# get_output() ã®ä¸­èº«
Line 715: outputs = self.outputs_queue.get()
  - Time: 3194.6 ms
  - % Time: 100.0% of get_output()
```

ã¤ã¾ã‚Šã€ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã¯ `outputs_queue.get()` ã§ã‚­ãƒ¥ãƒ¼ã‹ã‚‰çµæœãŒé€ã‚‰ã‚Œã¦ãã‚‹ã®ã‚’ãŸã **å¾…ã£ã¦ã„ã‚‹ã ã‘**ã§ã—ãŸã€‚ã“ã‚Œã¯å®Ÿéš›ã®æ¨è«–å‡¦ç†ãŒåˆ¥ãƒ—ãƒ­ã‚»ã‚¹ã§è¡Œã‚ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚ã“ã“ã§ vLLM v1 ã®ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å­˜åœ¨ã‚’æ€ã„å‡ºã—ã¾ã—ãŸã€‚

ï¼ˆä»¥ä¸‹ã®è¨˜äº‹ã«å†…éƒ¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è§£èª¬ãŒã‚ã‚Šã¾ã™ï¼‰

https://zenn.dev/tosshi/articles/f64ba0b86e330b

vLLM v1 ã§ã¯ã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å—ã‘å–ã‚‹ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã¨ã€å®Ÿéš›ã«æ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹ Worker ãƒ—ãƒ­ã‚»ã‚¹ãŒåˆ†é›¢ã•ã‚Œã¦ã„ã¾ã™ã€‚ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã¯ `llm.generate()` ã‚’å‘¼ã³å‡ºã™ã¨ã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ Worker ãƒ—ãƒ­ã‚»ã‚¹ã«é€ä¿¡ã—ã€`outputs_queue.get()` ã§ãƒ–ãƒ­ãƒƒã‚¯ã—ã¦çµæœã‚’å¾…ã¡ã¾ã™ã€‚ä¸€æ–¹ã€Worker ãƒ—ãƒ­ã‚»ã‚¹ã¯ NeuronCore ã§ã®æ¨è«–å®Ÿè¡Œã€çµæœã®ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã€ãã—ã¦ãƒ—ãƒ­ã‚»ã‚¹é–“é€šä¿¡ã‚’é€šã˜ã¦ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã«çµæœã‚’è¿”ã—ã¾ã™ã€‚ã“ã®æ§‹é€ ã‚’å›³ç¤ºã™ã‚‹ã¨ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

**vLLM v1 ã®ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ (vllm-neuron)**

```mermaid
%%{init: {'theme':'dark'}}%%
graph LR
    subgraph MainProcess["Main Process"]
        A[LLMEngine.step] --> B[get_output]
        B --> C[outputs_queue.get<br/>13.962 ms/step]
    end

    subgraph WorkerProcess["Worker Process"]
        D[EngineCore] --> E[execute_model]
        E --> F[Neuron å®Ÿè¡Œ]
        F --> G[ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³]
        G --> H[ZMQ é€ä¿¡]
    end

    H -->|IPC| C

    style MainProcess fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style WorkerProcess fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style C fill:#0f3460,stroke:#16213e,color:#fff
    style F fill:#0f3460,stroke:#16213e,color:#fff
```

line_profiler ã¯ Python ã®æ¨™æº–çš„ãªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ã¨åŒæ§˜ã«ã€**å®Ÿè¡Œä¸­ã®ãƒ—ãƒ­ã‚»ã‚¹ã®ã‚³ãƒ¼ãƒ‰ã—ã‹æ¸¬å®šã§ãã¾ã›ã‚“**ã€‚ã¤ã¾ã‚Šã€Worker ãƒ—ãƒ­ã‚»ã‚¹ã§å®Ÿè¡Œã•ã‚Œã‚‹ `execute_model()` ã‚„ NeuronCore ã§ã®æ¨è«–å‡¦ç†ã¯ã€ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã‹ã‚‰è¦‹ã‚‹ã¨ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã§ã™ã€‚

æ¸¬å®šã§ããŸã®ã¯ `outputs_queue.get()` ã§å¾…æ©Ÿã—ã¦ã„ã‚‹æ™‚é–“ï¼ˆ13.962ms/stepï¼‰ã ã‘ã§ã‚ã‚Šã€ã“ã®æ™‚é–“ã«ã¯æ¨è«–ã€IPC ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãªã©ã®ã™ã¹ã¦ã®æ™‚é–“ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚

ã§ã¯ã€ãªãœ 1 ãƒãƒƒãƒã®å‡¦ç†ã«å¹³å‡ 7.6 å›ã‚‚ `step()` ãŒå‘¼ã°ã‚Œã‚‹ã®ã§ã—ã‚‡ã†ã‹ã€‚ã“ã‚Œã¯ vLLM v1 ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã®å‹•ä½œæ–¹æ³•ã¨ã—ã¦ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ãŒä½•åº¦ã‚‚ `step()` ã‚’ç¢ºèªã—ã¦ã‚­ãƒ¥ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯ã—ç¶šã‘ã¦ã„ã‚‹ã‹ã‚‰ã§ã™ã€‚1 ãƒãƒƒãƒã‚ãŸã‚Šå¹³å‡ã—ã¦ 7.6 å›ã‚­ãƒ¥ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã„ãŸã¨ã„ã†ã“ã¨ã§ã™ã€‚

è¬ã¯è§£ã‘ã¾ã—ãŸãŒã€è‚å¿ƒã® **Worker ãƒ—ãƒ­ã‚»ã‚¹å†…ã§ã® NeuronCore ã®æ¨è«–å‡¦ç†æ™‚é–“**ã‚’åˆ†è§£ã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚`outputs_queue.get()` ã® 13.962 ms ã«ã¯ã€æ¨è«–å®Ÿè¡Œã€ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã€IPC é€šä¿¡ã®ã™ã¹ã¦ãŒå«ã¾ã‚Œã¦ãŠã‚Šã€line_profiler ã§ã¯ã“ã‚Œä»¥ä¸Šåˆ†è§£ã‚’ã™ã‚‹ã®ã¯é›£ã—ãã†ã§ã™ã€‚

:::message alert
**ä»Šå›ã®éã¡ã‹ã‚‰ã®å­¦ã³**: line_profiler ã§ã¯ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã«é™ç•ŒãŒã‚ã‚‹ãŸã‚ã€ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ç†è§£ã—ãŸä¸Šã§æ¬²ã—ã„æƒ…å ±ã‚’å–å¾—ã§ãã‚‹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ„ãƒ¼ãƒ«ã‚’é¸å®šã—ãŸæ–¹ãŒè‰¯ã„ã€‚
:::

### 2.4 æ¸¬å®šã®é™ç•Œã¨ä»Šå¾Œã®æ–¹å‘æ€§

line_profiler ã«ã‚ˆã‚‹æ¸¬å®šã§åˆ¤æ˜ã—ãŸã“ã¨ã‚’æ•´ç†ã™ã‚‹ã¨ã€ä»¥ä¸‹ã®ã‚ˆã†ãªæ§‹é€ ã«ãªã‚Šã¾ã™ã€‚å…¨ä½“ã¨ã—ã¦ 126.052 ms/batch ã¨ã„ã†å‡¦ç†æ™‚é–“ï¼ˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰è¾¼ã¿ï¼‰ã¯æ¸¬å®šã§ãã¾ã—ãŸãŒã€ãã®å†…è¨³ã®å¤§éƒ¨åˆ†ï¼ˆ84.2%ï¼‰ã®è©³ç´°ãŒä¸æ˜ã¨ã„ã†çŠ¶æ³ã§ã™ã€‚

ã“ã®çŠ¶æ³ã‚’æ‰“é–‹ã™ã‚‹ãŸã‚ã€ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’å«ã¾ãªã„ç´”ç²‹ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¸¬å®šã¨ã€NxD Inference ã‚’ç›´æ¥ä½¿ã£ãŸæ¸¬å®šã‚’è©¦ã¿ã¾ã—ãŸã€‚

## Phase 3: NxD Inference ç›´æ¥æ¸¬å®š

Worker ãƒ—ãƒ­ã‚»ã‚¹ã®ç›´æ¥æ¸¬å®šãŒå›°é›£ï¼ˆé¢å€’ï¼‰ãªãŸã‚ã€**vLLM ã‚’ä½¿ã‚ãšã« NxD Inference ã‚’ç›´æ¥ä½¿ç”¨**ã—ã¦ç´”ç²‹ãªæ¨è«–å®Ÿè¡Œæ™‚é–“ã‚’æ¸¬å®šã™ã‚‹ã“ã¨ã«ã—ã¾ã—ãŸã€‚è¨­å®šã¯ã“ã‚Œã¾ã§ã¨åŒã˜ã«ã—ã¦ãŠãã¾ã™ã€‚

### 3.1 vLLM æ¸¬å®šï¼ˆ30ãƒãƒƒãƒï¼‰

::::details ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æ§‹é€ 
```python:benchmark_30batches.py
import time
from vllm import LLM, SamplingParams

llm = LLM(model=model_path, **vllm_config)
sampling_params = SamplingParams(max_tokens=1, temperature=0.0, logprobs=20)

# 97ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ Ã— 4 = ãƒãƒƒãƒ
batch_prompts = [prompt_template] * 4

# Warmup
llm.generate(batch_prompts, sampling_params, use_tqdm=False)

# æ¸¬å®š: 30ãƒãƒƒãƒ
batch_times = []
for i in range(30):
    start = time.perf_counter()
    outputs = llm.generate(batch_prompts, sampling_params, use_tqdm=False)
    elapsed = (time.perf_counter() - start) * 1000
    batch_times.append(elapsed)
```
::::

::::details ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Œå…¨ç‰ˆ
```python:benchmark_30batches.py
"""
30ãƒãƒƒãƒã®çµ±ä¸€æ¸¬å®šï¼ˆvLLM bucketing=Trueï¼‰
"""
import logging
import time
import yaml
import vllm
from vllm import SamplingParams

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load config
with open('config.yaml', 'r') as f:
    config = yaml.safe_load(f)

model_path = config['model']['path']

# vLLM config with bucketing=True
vllm_config = {
    "tensor_parallel_size": config['vllm']['tensor_parallel_size'],
    "max_num_seqs": config['vllm']['max_num_seqs'],
    "block_size": config['vllm']['block_size'],
    "max_model_len": config['vllm']['max_model_len'],
    "max_num_batched_tokens": config['vllm']['max_num_batched_tokens'],
    "num_gpu_blocks_override": config['vllm']['num_gpu_blocks_override'],
    "enable_prefix_caching": config['vllm']['enable_prefix_caching'],
    "dtype": config['vllm']['dtype'],
}

# Add additional_config (bucketing=True)
if 'additional_config' in config['vllm']:
    vllm_config['additional_config'] = config['vllm']['additional_config']

logger.info("Initializing vLLM with bucketing=True...")
logger.info(f"Config: {vllm_config}")

llm = vllm.LLM(model=model_path, **vllm_config)
tokenizer = llm.get_tokenizer()

# Reranker prompt template (97 tokens avg)
prompt_template = """<|im_start|>system
Judge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be "yes" or "no".<|im_end|>
<|im_start|>user
<Instruct>: Given a web search query, retrieve relevant passages that answer the query
<Query>: ãƒ—ãƒ­ã‚»ã‚¹ã«ã¤ã„ã¦æ•™ãˆã¦
<Document>: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®çµ±åˆã•ã‚ŒãŸæ©Ÿèƒ½ã‚’åˆ†æã™ã‚‹æ–¹æ³•<|im_end|>
<|im_start|>assitant
<think>

</think>
"""

# Create 30 batches of 4 prompts each (max_num_seqs=4)
batch_prompts = [prompt_template] * 4

sampling_params = SamplingParams(
    max_tokens=1,
    temperature=0.0,
    logprobs=20,
    detokenize=True,
)

logger.info("Running 30-batch benchmark...")
batch_times = []

for i in range(30):
    start = time.perf_counter()
    outputs = llm.generate(batch_prompts, sampling_params, use_tqdm=False)
    elapsed = (time.perf_counter() - start) * 1000
    batch_times.append(elapsed)
    
    if (i + 1) % 10 == 0:
        logger.info(f"  Batch {i+1}/30: {elapsed:.2f} ms")

logger.info(f"\n=== Results (30 batches, bucketing=True) ===")
logger.info(f"Average: {sum(batch_times)/len(batch_times):.2f} ms/batch")
logger.info(f"Min: {min(batch_times):.2f} ms")
logger.info(f"Max: {max(batch_times):.2f} ms")
logger.info(f"Total: {sum(batch_times):.2f} ms")
```
::::

çµæœã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚

```
Average: 64.33 ms/batch
Min: 60.78 ms
Max: 85.76 ms
```

### 3.2 NxD Inference æ¸¬å®š

::::details NxD Inference æ¸¬å®šæ‰‹é †ï¼ˆ4ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰

**ç’°å¢ƒæº–å‚™**

```bash
# Neuron SDK ç’°å¢ƒã‚’ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ãƒˆ
source /opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/bin/activate

# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
mkdir -p ~/data-science/investigations/neuronx-distributed-inference
cd ~/data-science/investigations/neuronx-distributed-inference

# neuronx-distributed-inference ãƒªãƒã‚¸ãƒˆãƒªã‚¯ãƒ­ãƒ¼ãƒ³
git clone https://github.com/aws-neuron/neuronx-distributed-inference.git
cd neuronx-distributed-inference
```

**ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæº–å‚™**

Phase 1 ã¨åŒã˜ 97 ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨ã—ã¾ã™ï¼š

```bash
# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç’°å¢ƒå¤‰æ•°ã«ã‚»ãƒƒãƒˆï¼ˆ97ãƒˆãƒ¼ã‚¯ãƒ³ã€Qwen3-Reranker å½¢å¼ï¼‰
export PROMPT='<|im_start|>system
Judge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be "yes" or "no".<|im_end|>
<|im_start|>user
<Instruct>: Given a web search query, retrieve relevant passages that answer the query
<Query>: ãƒ—ãƒ­ã‚»ã‚¹ã«ã¤ã„ã¦æ•™ãˆã¦
<Document>: ã‚µãƒ³ãƒ—ãƒ«æ–‡æ›¸å†…å®¹<|im_end|>
<|im_start|>assitant
<think>

</think>
'

# ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã‚’è¨­å®š
export MODEL_PATH="/path/to/Qwen3-0.6B-Reranker"
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³ A: bucketing=OFF, prefix-caching=OFFï¼ˆãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ï¼‰**

```bash
python src/neuronx_distributed_inference/inference_demo.py \
  --model-type qwen3 \
  --task-type causal-lm \
  run \
  --model-path "$MODEL_PATH" \
  --prompt "$PROMPT" \
  --tp-degree 2 \
  --batch-size 4 \
  --seq-len 128 \
  --max-context-length 128 \
  --max-length 128 \
  --on-device-sampling \
  --top-k 1 \
  --benchmark \
  --benchmark-report-path /tmp/benchmark_A_v2.json

# çµæœç¢ºèª
cat /tmp/benchmark_A_v2.json | jq
```

**çµæœï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³ Aï¼‰:**
```json
{
  "context_encoding_model": {
    "latency_ms_avg": 11.972224712371826,
    "latency_ms_p50": 11.97052001953125,
    "latency_ms_p90": 12.050032615661621,
    "latency_ms_p95": 12.057197093963623,
    "throughput": 42781.77658389653
  }
}
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³ B: bucketing=ON, prefix-caching=OFFï¼ˆ+0.4%ï¼‰**

```bash
python src/neuronx_distributed_inference/inference_demo.py \
  --model-type qwen3 \
  --task-type causal-lm \
  run \
  --model-path "$MODEL_PATH" \
  --prompt "$PROMPT" \
  --tp-degree 2 \
  --batch-size 4 \
  --seq-len 128 \
  --max-context-length 128 \
  --max-length 128 \
  --on-device-sampling \
  --top-k 1 \
  --enable-bucketing \
  --context-encoding-buckets 32 64 96 128 \
  --token-generation-buckets 32 64 96 128 \
  --benchmark \
  --benchmark-report-path /tmp/benchmark_B_v2.json

# çµæœç¢ºèª
cat /tmp/benchmark_B_v2.json | jq
```

**çµæœï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³Bï¼‰:**
```json
{
  "context_encoding_model": {
    "latency_ms_avg": 12.021279335021973,
    "latency_ms_p50": 11.997222900390625,
    "latency_ms_p90": 12.201786041259766,
    "latency_ms_p95": 12.22454309463501,
    "throughput": 42607.99899294344
  }
}
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³ C: bucketing=OFF, prefix-caching=ONï¼ˆã‚¨ãƒ©ãƒ¼ï¼‰**

```bash
python src/neuronx_distributed_inference/inference_demo.py \
  --model-type qwen3 \
  --task-type causal-lm \
  run \
  --model-path "$MODEL_PATH" \
  --prompt "$PROMPT" \
  --tp-degree 2 \
  --batch-size 4 \
  --seq-len 128 \
  --max-context-length 128 \
  --max-length 128 \
  --on-device-sampling \
  --top-k 1 \
  --is-continuous-batching \
  --ctx-batch-size 1 \
  --enable-block-kv-layout \
  --enable-prefix-caching \
  --pa-num-blocks 16 \
  --pa-block-size 32 \
  --benchmark

# ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ
# TypeError: unsupported operand type(s) for -: 'NoneType' and 'NoneType'
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³ D: bucketing=ON, prefix-caching=ONï¼ˆã‚¨ãƒ©ãƒ¼ï¼‰**

```bash
python src/neuronx_distributed_inference/inference_demo.py \
  --model-type qwen3 \
  --task-type causal-lm \
  run \
  --model-path "$MODEL_PATH" \
  --prompt "$PROMPT" \
  --tp-degree 2 \
  --batch-size 4 \
  --seq-len 128 \
  --max-context-length 128 \
  --max-length 128 \
  --on-device-sampling \
  --top-k 1 \
  --enable-bucketing \
  --context-encoding-buckets 32 64 96 128 \
  --token-generation-buckets 32 64 96 128 \
  --is-continuous-batching \
  --ctx-batch-size 1 \
  --enable-block-kv-layout \
  --enable-prefix-caching \
  --pa-num-blocks 16 \
  --pa-block-size 32 \
  --benchmark

# ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ
# TypeError: can only concatenate list (not "NoneType") to list
```
::::

**æ¸¬å®šçµæœã‚µãƒãƒªãƒ¼ï¼ˆå›ºå®šé•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: 97 ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰**

| ãƒ‘ã‚¿ãƒ¼ãƒ³ | Bucketing | Prefix Caching | çµæœ | å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· (ms) |
|---------|-----------|----------------|------|---------------------|
| **A** | OFF | OFF | æˆåŠŸ | **11.97** |
| **B** | ON | OFF | æˆåŠŸ | **12.02** (+0.4%) |
| **C** | OFF | ON | ã‚¨ãƒ©ãƒ¼ | - |
| **D** | ON | ON | ã‚¨ãƒ©ãƒ¼ | - |

:::message alert
**é‡è¦ãªç™ºè¦‹**: bucketing ã‚’æœ‰åŠ¹ã«ã—ã¦ã‚‚ã€å›ºå®šé•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆ97ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã®é€£ç¶šå‡¦ç†ã§ã¯å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒ **+0.4%** ã¨ã»ã¼å½±éŸ¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã“ã‚Œã¯ Phase 1 ã§ **vLLM çµŒç”±ã§æ¸¬å®šã—ãŸã¨ãã®çµæœï¼ˆbucketing ã§é«˜é€ŸåŒ–ï¼‰ã¨ã¯ç•°ãªã‚‹å‚¾å‘**ã§ã™ã€‚

ã¤ã¾ã‚Šã€vLLM ã¨ NxD Inference ã§ã¯ bucketing ã®åŠ¹æœãŒç•°ãªã£ã¦ã„ã‚‹ï¼ˆå¼·ãã„ã†ã¨é€†è»¢ã—ã¦ã„ã‚‹ï¼‰ã¨ã„ã†ãµã†ã«è¦‹ãˆã¦ã—ã¾ã„ã¾ã™ã€‚
:::

### 3.3 ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰æ¯”è¼ƒ

| æ¸¬å®šå¯¾è±¡ | å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· (ms/batch) | Min (ms) | Max (ms) |
|---------|-------------------------|----------|----------|
| **vLLM** | **64.33** | 60.78 | 85.76 |
| **NxD Inference** | **11.97** | - | - |

**ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰å†…è¨³**

```mermaid
%%{init: {'theme':'dark'}}%%
graph LR
    subgraph NxD["NxD Inference (11.97 ms)"]
        A[Neuron å®Ÿè¡Œ<br/>11.97 ms<br/>100%]
    end

    subgraph vLLM["vLLM (64.33 ms)"]
        B[Neuron å®Ÿè¡Œ<br/>11.97 ms<br/>19%]
        C[Python ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒ‰<br/>52.36 ms<br/>81%]
        B -.-> C
    end

    NxD o--o vLLM

    style NxD fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style vLLM fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style A fill:#0f3460,stroke:#16213e,color:#fff
    style B fill:#0f3460,stroke:#16213e,color:#fff
    style C fill:#8b4513,stroke:#16213e,color:#fff
```

ç´”ç²‹ãª Neuron å®Ÿè¡Œæ™‚é–“ã¯ 11.97 ms (19%) ã§ã‚ã‚‹ã®ã«å¯¾ã—ã€vLLM ã® Python ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒ 52.36 ms (81%) ã‚’å ã‚ã¦ã„ã¾ã™ã€‚

ä»Šå›ã®æ¡ä»¶ã§ã¯ **vLLM ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ = 52ms** ã¨ã„ã†ã®ãŒã“ã‚Œã§ã‚ã‹ã‚Šã¾ã—ãŸï¼ãŸã ã—ä»Šå›ã®æ¸¬å®šã‚±ãƒ¼ã‚¹ã«ä¾å­˜ã™ã‚‹ã“ã¨ã«æ³¨æ„ãŒå¿…è¦ã§ã™ã€‚

### 3.4: bucketing è¨­å®šå½±éŸ¿ã®è€ƒå¯Ÿ

NxD Inference ã§ä»¥ä¸‹ã® 4 ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŒ¯ã‚Šè¿”ã‚Šã¾ã—ã‚‡ã†ï¼ˆå›ºå®šé•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: 97 ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã€‚

| ãƒ‘ã‚¿ãƒ¼ãƒ³ | Prefix Caching | Bucketing | çµæœ | å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· (ms) |
|----------|----------------|-----------|------|---------------------|
| **A** | OFF | OFF | æˆåŠŸ | **11.97** |
| **B** | OFF | ON | æˆåŠŸ | **12.02** |
| **C** | ON | OFF | ã‚¨ãƒ©ãƒ¼ | - |
| **D** | ON | ON | ã‚¨ãƒ©ãƒ¼ | - |

:::message
NxD Inference ã‚’ç›´æ¥ä½¿ç”¨ã—ãŸå ´åˆã€bucketing ã‚’æœ‰åŠ¹ã«ã—ã¦ã‚‚ 11.97 ms ã‹ã‚‰ 12.02 ms ã¸ã¨ +0.4% ã®å¾®å¢—ã«ã¨ã©ã¾ã‚Šã¾ã—ãŸã€‚
:::

**è€ƒå¯Ÿ: å›ºå®šé•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã¯ Bucketing ã®åŠ¹æœãŒãªã„ï¼Ÿ**

NEFF ã®åˆ†æã‹ã‚‰ **Bucketing=OFF** ã§ 1 ã‚°ãƒ©ãƒ•ã§å…¨ã¦ã®å‡¦ç†ãŒå®Ÿæ–½ã•ã‚Œã€**Bucketing=ON** ã§è¤‡æ•°ã‚°ãƒ©ãƒ•ã‚’äº‹å‰ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã€å®Ÿè¡Œæ™‚ã«é¸æŠã™ã‚‹ã¨ã„ã†ã“ã¨ã‚’è§£èª¬ã—ã¾ã—ãŸã€‚ã¤ã¾ã‚Šä»Šå›ã®å›ºå®šé•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã®å®Ÿé¨“ã®å ´åˆã€Bucketing ã‚’æœ‰åŠ¹ã«ã—ãŸã¨ã—ã¦ã‚‚ã‚°ãƒ©ãƒ•é¸æŠãƒ­ã‚¸ãƒƒã‚¯ã‚„ç„¡é§„ãªã‚°ãƒ©ãƒ•ãƒ¡ãƒ¢ãƒªä¿æŒã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã®ã¿ãŒåŠ ç®—ã•ã‚Œã‚‹ã«ã‚‚ã‹ã‹ã‚ã‚‰ãš 1 ã‚°ãƒ©ãƒ•ã—ã‹åˆ©ç”¨ã—ãªã„ã®ã§ã‚ã‚Œã°åŠ¹æœã¯è–„ã„ã¨è¨€ãˆã¾ã™ã€‚å®Ÿéš›ã®çµæœã‹ã‚‰ã‚‚ã‚ãšã‹ã«å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãŒæ‚ªåŒ–ã—ã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚

ä¸€æ–¹ã§ã€**vLLM ã§ã® bucketing ã«ã¤ã„ã¦** ã¯ `enable_bucketing: true` ãŒæœ€é©è¨­å®šã§ã—ãŸã€‚ã“ã®åŸå› ã‚’æ¢ã‚‹å¿…è¦ãŒã‚ã‚Šãã†ã§ã™ã€‚ãã—ã¦ã€å¯å¤‰é•·ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ï¼ˆä¸€èˆ¬çš„ãª LLM ã®ç”Ÿæˆã‚¿ã‚¹ã‚¯ï¼‰ã§ã¯ã€ç•°ãªã‚‹é•·ã•ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã—ã¦é©åˆ‡ãªãƒã‚±ãƒƒãƒˆã‚’é¸æŠã™ã‚‹ã“ã¨ã§ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’æœ€é©åŒ–ã—ã€bucketing=ON ãŒæ€§èƒ½ã‚’æ”¹å–„ã™ã‚‹ã¯ãšã€ã¨ã„ã†ä»®èª¬ãŒç«‹ã¡ã¾ã™ã€‚

ãã“ã§å®Ÿéš›ã«ã€97 ãƒˆãƒ¼ã‚¯ãƒ³ã®å›ºå®šãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å¯å¤‰ã«ã—ã¦å®Ÿé¨“ã™ã‚‹ã“ã¨ã«ã—ã¾ã—ãŸã€‚

### 3.5 å¯å¤‰é•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã®æ¤œè¨¼

ä»®èª¬ã‚’æ¤œè¨¼ã™ã‚‹ãŸã‚ã€å¯å¤‰é•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã®æ¸¬å®šã‚’å®Ÿæ–½ã—ã¾ã—ãŸã€‚

**æ¸¬å®šæ¡ä»¶**
- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ•°: 16 å€‹ï¼ˆ4 ãƒã‚±ãƒƒãƒˆ Ã— 4 ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼‰
- ãƒã‚±ãƒƒãƒˆæ§‹æˆ: 32, 64, 96, 128 ãƒˆãƒ¼ã‚¯ãƒ³
- å®Ÿéš›ã®é•·ã•: 18-125 ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®èª¤å·®ã‚’å«ã‚€ï¼‰
- æ¸¬å®šãƒ„ãƒ¼ãƒ«: inference_demo.pyï¼ˆNxD Inference ç›´æ¥ä½¿ç”¨ï¼‰

::::details å¯å¤‰é•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```python:create_variable_prompts_v2.py
import json
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("/path/to/Qwen3-0.6B-Reranker")

# ãƒã‚±ãƒƒãƒˆã”ã¨ã®ç›®æ¨™ãƒˆãƒ¼ã‚¯ãƒ³æ•°
bucket_targets = {
    32: [20, 24, 28, 31],
    64: [52, 56, 60, 63],
    96: [84, 88, 92, 95],
    128: [116, 120, 124, 127]
}

prompts = []
for bucket, targets in bucket_targets.items():
    for target in targets:
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’èª¿æ•´ã—ã¦ç›®æ¨™ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã«è¿‘ã¥ã‘ã‚‹
        content = "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®çµ±åˆã•ã‚ŒãŸæ©Ÿèƒ½ã‚’åˆ†æã™ã‚‹" * (target // 10)
        prompt = f"""<|im_start|>system
Judge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be "yes" or "no".<|im_end|>
<|im_start|>user
<Instruct>: Given a web search query, retrieve relevant passages that answer the query
<Query>: ãƒ—ãƒ­ã‚»ã‚¹ã«ã¤ã„ã¦æ•™ãˆã¦
<Document>: {content}<|im_end|>
<|im_start|>assitant
<think>

</think>
"""
        token_ids = tokenizer.encode(prompt, add_special_tokens=False)
        actual_length = len(token_ids)
        prompts.append({
            "bucket": bucket,
            "target": target,
            "actual": actual_length,
            "prompt": prompt
        })

# ä¿å­˜
with open('/tmp/variable_prompts.json', 'w', encoding='utf-8') as f:
    json.dump(prompts, f, ensure_ascii=False, indent=2)

print(f"Generated {len(prompts)} prompts")
print(f"Length range: {min(p['actual'] for p in prompts)}-{max(p['actual'] for p in prompts)} tokens")
```
::::

**æ¸¬å®šçµæœ**

| æ¡ä»¶ | å¹³å‡ | å·®åˆ† | æ”¹å–„ç‡ |
|------|---------------------|------|--------|
| **å›ºå®šé•· (97 ãƒˆãƒ¼ã‚¯ãƒ³)** | | | |
| Bucketing OFF | 11.97 ms | baseline | - |
| Bucketing ON | 12.02 ms | +0.05 ms | **+0.4%** (æ‚ªåŒ–) |
| **å¯å¤‰é•· (18-125 ãƒˆãƒ¼ã‚¯ãƒ³)** | | | |
| Bucketing OFF | 17.18 ms | baseline | - |
| Bucketing ON | 13.87 ms | -3.31 ms | **-19.3%** (æ”¹å–„) |

::::details å¯å¤‰é•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ¸¬å®šã®è©³ç´°çµæœ

**Bucketing OFF**:
```json
{
  "context_encoding_model": {
    "latency_ms_avg": 17.1828031539917,
    "latency_ms_p50": 15.299081802368164,
    "latency_ms_p90": 23.46792221069336,
    "latency_ms_p95": 23.482704162597656,
    "throughput": 29797.23363012853
  }
}
```

**Bucketing ON**:
```json
{
  "context_encoding_model": {
    "latency_ms_avg": 13.871526718139648,
    "latency_ms_p50": 11.935114860534668,
    "latency_ms_p90": 19.990873336791992,
    "latency_ms_p95": 20.034921169281006,
    "throughput": 36910.14049163478
  }
}
```

::::

:::message
å¯å¤‰é•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã®æ¤œè¨¼ã‹ã‚‰ã€Bucketing ã®åŠ¹æœã¯ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã€ã¤ã¾ã‚Šå…¥åŠ›ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³ã«ä¾å­˜ã™ã‚‹ã“ã¨ãŒç¢ºèªã§ãã¾ã—ãŸã€‚

**ã¡ãªã¿ã«ã¾ã  vLLM ã®å ´åˆã¯ bucketing OFF ã®æ–¹ãŒè‰¯ã„ç†ç”±ã¯èª¬æ˜ãŒã¤ã„ã¦ã„ãªã„ãŸã‚ã“ã®å¾Œã§èª¿æŸ»ã‚’ç¶™ç¶šã—ã¾ã™ã€‚**
:::

```mermaid
%%{init: {'theme':'dark'}}%%
graph TD
    subgraph Fixed["å›ºå®šé•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ (97 ãƒˆãƒ¼ã‚¯ãƒ³)"]
        A1["Bucketing OFF<br/>11.97 ms âœ…"] -.->|+0.4%| A2["Bucketing ON<br/>12.02 ms"]
    end

    subgraph Variable["å¯å¤‰é•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ (18-125 ãƒˆãƒ¼ã‚¯ãƒ³)"]
        B1["Bucketing OFF<br/>17.18 ms"] -.->|-19.3%| B2["Bucketing ON<br/>13.87 ms âœ…"]
    end

    style Fixed fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style Variable fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style A1 fill:#0f3460,stroke:#16213e,color:#fff
    style A2 fill:#8b4513,stroke:#16213e,color:#fff
    style B1 fill:#8b4513,stroke:#16213e,color:#fff
    style B2 fill:#0f3460,stroke:#16213e,color:#fff
```

:::message alert
**ä»Šå›ã®éã¡ã‹ã‚‰ã®å­¦ã³**: ç°¡å˜ã®ãŸã‚ã«å›ºå®šã‚µã‚¤ã‚ºã®å…¥åŠ›ã§ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã™ã‚‹ã¨æ€ã‚ã¬çµæœã«ãªã‚‹ã“ã¨ãŒã‚ã‚‹ã€‚ãã®ãŸã‚ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚„ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã¯å‡ºæ¥ã‚‹ã ã‘å®Ÿéš›ã®åˆ©ç”¨çŠ¶æ³ã‚’åæ˜ ã—ãŸæ–¹ãŒè‰¯ã„ã€‚ï¼ˆå½“ãŸã‚Šå‰ã¨è¨€ãˆã°å½“ãŸã‚Šå‰ã§ã™ãŒã€‚ã€‚ï¼‰
:::

## 3.6 NxD Inference å†…éƒ¨ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°

NeuronCore ã§ã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†æ™‚é–“ã¨ NxD Inference ã® Python ã‚³ãƒ¼ãƒ‰å´ã§ã‹ã‹ã£ã¦ã„ã‚‹å‡¦ç†ã‚’æŠŠæ¡ã™ã‚‹ãŸã‚ã« NxD Inference ã®å†…éƒ¨å‡¦ç†ã‚’ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã—ã¾ã—ãŸã€‚

:::details NxD Inference å†…éƒ¨ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‚³ãƒ¼ãƒ‰

```python
# profile_nxd_detailed.py (æŠœç²‹)
import time
import torch
from neuronx_distributed_inference.models.config import NeuronConfig
from neuronx_distributed_inference.models.qwen3.modeling_qwen3 import NeuronQwen3ForCausalLM

# Configuration
neuron_config = NeuronConfig(
    tp_degree=2,
    batch_size=4,
    max_context_length=128,
    seq_len=128,
    enable_bucketing=True,
)

# Load model
model = NeuronQwen3ForCausalLM(compiled_model_path)
model.load(compiled_model_path)

# Warmup
with torch.no_grad():
    _ = model(inputs.input_ids, attention_mask=inputs.attention_mask, position_ids=position_ids)

# Benchmark (30 iterations)
times_total = []
times_forward = []
times_output = []

for i in range(30):
    # Total time
    t_start = time.perf_counter()

    # Forward pass
    t_forward_start = time.perf_counter()
    with torch.no_grad():
        outputs = model(inputs.input_ids,
                       attention_mask=inputs.attention_mask,
                       position_ids=position_ids)
    t_forward_end = time.perf_counter()

    # Output processing (minimal)
    t_output_start = time.perf_counter()
    # (outputs processing would go here)
    t_output_end = time.perf_counter()

    t_end = time.perf_counter()

    times_total.append((t_end - t_start) * 1000)
    times_forward.append((t_forward_end - t_forward_start) * 1000)
    times_output.append((t_output_end - t_output_start) * 1000)
```

**æ¸¬å®šçµæœ**:
```
================================================================================
NxD Inference è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
================================================================================

Configuration: tp_degree=2, batch_size=4, bucketing=True

Total time:
  å¹³å‡: 12.70 ms
  æœ€å°: 12.60 ms
  æœ€å¤§: 13.28 ms

Forward pass:
  å¹³å‡: 12.70 ms  (100.0% of total)
  æœ€å°: 12.60 ms
  æœ€å¤§: 13.28 ms

Output processing:
  å¹³å‡: 0.00 ms   (0.0% of total)
  æœ€å°: 0.00 ms
  æœ€å¤§: 0.00 ms

================================================================================
å†…è¨³
================================================================================
Forward pass:        12.70 ms (100.0%)  â† Neuronå®Ÿè¡Œ
Output processing:   0.00 ms (0.0%)
Other overhead:      0.00 ms (0.0%)     â† Pythonã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãªã—
```

:::

NxD Inference ã®æ¸¬å®šçµæœã‹ã‚‰ Python ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã¯ã»ã¼ 0 ms ã§ã‚ã‚Šã€ã»ã¼å…¨ã¦ã®å®Ÿè¡Œæ™‚é–“ãŒ Neuron å®Ÿè¡Œã«ä½¿ã‚ã‚Œã¦ã„ã¾ã™ã€‚ã“ã‚Œã¯ã‚·ãƒ³ã‚°ãƒ«ãƒ—ãƒ­ã‚»ã‚¹ã®ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã‚ˆã‚‹ã‚‚ã®ã§ã—ã‚‡ã†ã€‚

ã“ã®çµæœã«ã‚ˆã‚Šã€**vLLM ã® 52.36ms ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã¯ vLLM å´ã®å®Ÿè£…ã‚³ã‚¹ãƒˆ**ã§ã‚ã‚‹ã“ã¨ãŒç¢ºèªã§ãã¾ã—ãŸã€‚

:::message
**æ³¨**: ä¸Šè¨˜ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°çµæœï¼ˆ12.70 msï¼‰ã¯ bucketing=True ã®è¨­å®šã§ã®æ¸¬å®šã§ã™ã€‚bucketing=OFF ã§ã¯ 11.97 msã€bucketing=ON ã§ã¯ 12.02 ms ã¨ã„ã†çµæœãŒå¾—ã‚‰ã‚Œã¦ãŠã‚Šã€Python ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒã»ã¼ã‚¼ãƒ­ã¨ã„ã†çµè«–ã¯ä¸¡æ–¹ã®è¨­å®šã§ä¸€è²«ã—ã¦ã„ã¾ã™ã€‚
:::

## 3.7 çŸ¥è¦‹: Prefix Caching è¨­å®šã«é–¢ã™ã‚‹ vLLM å†…éƒ¨å®Ÿè£…

ä¸Šè¨˜ã® NxD Inference ã®ç›´æ¥æ¸¬å®šã§ Pattern C & Dï¼ˆprefix caching æœ‰åŠ¹ï¼‰ã®æ¸¬å®šã‚’è©¦ã¿ã¾ã—ãŸãŒã€ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã“ã®èª¿æŸ»éç¨‹ã§ç™ºè¦‹ãŒã‚ã‚Šã¾ã—ãŸã€‚

```python:inference_demo.py (line 249)
parser.add_argument(
    "--enable-prefix-caching",
    action="store_true",
    help="Enable prefix caching for the model"
)
```

ã“ã® `--enable-prefix-caching` ãƒ•ãƒ©ã‚°ã¯å®Ÿè£…ä¸Šå­˜åœ¨ã—ã¾ã™ã€‚ã§ã¯ãªãœã‚¨ãƒ©ãƒ¼ã—ãŸã®ã§ã—ã‚‡ã†ã‹ï¼Ÿ

https://github.com/aws-neuron/neuronx-distributed-inference/blob/aa7987ffc66ac2bd9894427621ca9b6f3fc40ed9/src/neuronx_distributed_inference/inference_demo.py#L248-L250

### vllm-neuron ã®å†…éƒ¨å®Ÿè£…ã‚’ç¢ºèª

Prefix caching ã®è¨­å®šæ–¹æ³•ã‚’ç†è§£ã™ã‚‹ãŸã‚ã€vllm-neuron ã®ã‚³ãƒ¼ãƒ‰ã‚’èª¿æŸ»ã—ã¾ã—ãŸã€‚çµè«–ã€vLLM ã¯ `--enable-prefix-caching` ãƒ•ãƒ©ã‚°ã‹ã‚‰ **è‡ªå‹•çš„ã«è¤‡æ•°ã®è¨­å®šã‚’è¡Œã†** ã“ã¨ãŒã‚ã‹ã‚Šã¾ã—ãŸã€‚

https://github.com/vllm-project/vllm-neuron/blob/139ffe574d0a2d662954946f59b38914e13473fe/vllm_neuron/worker/neuronx_distributed_model_loader.py#L880-L884

vLLM ã§ `enable_prefix_caching=True` ã‚’è¨­å®šã™ã‚‹ã¨:
1. `is_block_kv_layout=True` ãŒè‡ªå‹•çš„ã«è¨­å®šã•ã‚Œã‚‹
2. `is_prefix_caching=True` ãŒè¨­å®šã•ã‚Œã‚‹

ã“ã‚Œã‚‰ã®è¨­å®šãŒ NxD Inference ã«æ¸¡ã•ã‚Œã¾ã™ã€‚

### inference_demo.py ã§ã®ç›´æ¥ä½¿ç”¨ã®åˆ¶ç´„

:::message alert
**é‡è¦ãªç™ºè¦‹**: `inference_demo.py` ã§ prefix caching ã‚’ç›´æ¥ä½¿ç”¨ã—ã‚ˆã†ã¨ã™ã‚‹ã¨ã€ä»¥ä¸‹ã®ã‚ˆã†ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã™ã€‚
:::

**Pattern Cï¼ˆbucketing OFF, prefix caching ONï¼‰ã®ã‚¨ãƒ©ãƒ¼**:
```
TypeError: unsupported operand type(s) for -: 'NoneType' and 'NoneType'
```

**Pattern Dï¼ˆbucketing ON, prefix caching ONï¼‰ã®ã‚¨ãƒ©ãƒ¼**:
```
TypeError: can only concatenate list (not "NoneType") to list
```

**ç†ç”±**:
- Prefix caching ã®å®Ÿè£…ã¯ **vLLM ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯çµŒç”±ã§ã®ä½¿ç”¨ãŒæƒ³å®šã•ã‚Œã¦ã„ã‚‹**
- inference_demo.py ã§ã®ç›´æ¥ä½¿ç”¨ã«ã¯ã€è¿½åŠ ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒå¿…è¦
- [AWSå…¬å¼ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/tutorials/trn2-llama3.3-70b-apc-tutorial.html)ã§ã‚‚ vLLM ã‚µãƒ¼ãƒãƒ¼çµŒç”±ã®ä½¿ç”¨ã‚’æ¨å¥¨

::::details Pattern C & D ã®æ¸¬å®šã‚¹ã‚¯ãƒªãƒ—ãƒˆä¾‹ï¼ˆã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿï¼‰

**Pattern Cï¼ˆã‚¨ãƒ©ãƒ¼ç™ºç”Ÿï¼‰**:
```bash
python inference_demo.py \
  --model-type qwen3 \
  --task-type causal-lm run \
  --model-path "$MODEL_PATH" \
  --prompt "$PROMPT" \
  --tp-degree 2 \
  --batch-size 4 \
  --seq-len 128 \
  --max-context-length 128 \
  --max-length 128 \
  --on-device-sampling \
  --top-k 1 \
  --is-continuous-batching \
  --ctx-batch-size 1 \
  --enable-block-kv-layout \
  --enable-prefix-caching \
  --pa-num-blocks 16 \
  --pa-block-size 32 \
  --benchmark

# ã‚¨ãƒ©ãƒ¼: TypeError: unsupported operand type(s) for -: 'NoneType' and 'NoneType'
```

**Pattern Dï¼ˆã‚¨ãƒ©ãƒ¼ç™ºç”Ÿï¼‰**:
```bash
python inference_demo.py \
  --model-type qwen3 \
  --task-type causal-lm run \
  --model-path "$MODEL_PATH" \
  --prompt "$PROMPT" \
  --tp-degree 2 \
  --batch-size 4 \
  --seq-len 128 \
  --max-context-length 128 \
  --max-length 128 \
  --on-device-sampling \
  --top-k 1 \
  --enable-bucketing \
  --context-encoding-buckets 32 64 96 128 \
  --token-generation-buckets 32 64 96 128 \
  --is-continuous-batching \
  --ctx-batch-size 1 \
  --enable-block-kv-layout \
  --enable-prefix-caching \
  --pa-num-blocks 16 \
  --pa-block-size 32 \
  --benchmark

# ã‚¨ãƒ©ãƒ¼: TypeError: can only concatenate list (not "NoneType") to list
```

::::

### ä»Šå›ã®çµè«–ã¸ã®å½±éŸ¿

Pattern C & Dï¼ˆprefix cachingæœ‰åŠ¹ï¼‰ã®æ¸¬å®šã«å¤±æ•—ã—ã¾ã—ãŸãŒã€**ä»Šå›ã®ä¸»è¦ãªçµè«–ã«ã¯å½±éŸ¿ã—ã¾ã›ã‚“**ã€‚

**æ¸¬å®šã§ããŸã“ã¨**:
1. âœ… vLLM ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ï¼ˆPattern A & B ã§æ¸¬å®šï¼‰
2. âœ… Bucketing ã®å½±éŸ¿ï¼ˆå›ºå®šé•· & å¯å¤‰é•·ã§æ¸¬å®šï¼‰
3. âœ… NxD Inference ã®ç´”ç²‹ãªå®Ÿè¡Œæ™‚é–“

**æ¸¬å®šã§ããªã‹ã£ãŸã“ã¨**:
- âŒ inference_demo.py ã§ã® prefix caching ã®ç›´æ¥æ¸¬å®š
- ãŸã ã—ã€vLLM çµŒç”±ã§ã® prefix caching ã¯æ­£å¸¸ã«å‹•ä½œï¼ˆãƒªãƒªãƒ¼ã‚¹ãƒãƒ¼ãƒˆã§3.2xæ”¹å–„ã‚’ç¢ºèªï¼‰

## Prefix Cachingã®çœŸå®Ÿ


### ãƒªãƒªãƒ¼ã‚¹ãƒãƒ¼ãƒˆç¢ºèª

[NxD Inference Release Notes](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/neuronx-distributed-inference/neuronx-distributed-inference.html)ã‚ˆã‚Šï¼š

**Neuron 2.24.0**
- **Automatic Prefix Caching (APC) ã‚µãƒãƒ¼ãƒˆé–‹å§‹**
- **vLLM çµŒç”±ã§ã®ã¿å‹•ä½œ**
- 3.2x TTFTæ”¹å–„ï¼ˆ90%ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆæ™‚ã€Llama3.3 70Bï¼‰

### vllm-neuron ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```mermaid
%%{init: {'theme':'dark'}}%%
graph TD
    subgraph GPU["é€šå¸¸ã® vLLM (GPU ãªã©)"]
        A1[Scheduler<br/>ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°] --> A2[KVCacheCoordinator]
        A2 --> A3[Hash-based prefix caching<br/>PagedAttention<br/>Block allocation/eviction<br/>LRU eviction policy]
        A3 --> A4[GPU Worker<br/>å®Ÿè¡Œ]
    end

    subgraph Neuron["vllm-neuron"]
        B1[Scheduler<br/>full_context_lens è¨ˆç®—<br/>computed_context_lens è¨ˆç®—] --> B2["âŒ KVCacheCoordinator<br/>(ä½¿ç”¨ã•ã‚Œãªã„)"]
        B2 -.->|bypass| B3[vllm-neuron Plugin]
        B3 --> B4[is_prefix_caching è¨­å®š<br/>ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ Neuron ã«æ¸¡ã™]
        B4 --> B5[NxD Inference<br/>Neuron ãƒ¬ãƒ™ãƒ«]
        B5 --> B6[Prefix caching å®Ÿè£…<br/>KV ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†<br/>ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³]
    end

    style GPU fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style Neuron fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style A2 fill:#0f3460,stroke:#16213e,color:#fff
    style B2 fill:#8b0000,stroke:#16213e,color:#fff
    style B6 fill:#0f3460,stroke:#16213e,color:#fff
```

**vLLM ä½¿ç”¨æ™‚ã€prefix caching ã¯ Neuron å´ã§å®Ÿè¡Œã•ã‚Œã‚‹**ã“ã¨ãŒåˆ¤æ˜ã—ã¾ã—ãŸã€‚

ã¾ãšã€vLLM å´ã®ç‹¬è‡ª prefix caching å®Ÿè£…ï¼ˆ`KVCacheCoordinator`ï¼‰ã¯å®Œå…¨ã«ãƒã‚¤ãƒ‘ã‚¹ã•ã‚Œã¾ã™ã€‚é€šå¸¸ã® vLLM ã§ã¯ hash-based ã‚„ PagedAttention-based ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ãŒè¡Œã‚ã‚Œã¾ã™ãŒã€vllm-neuron ã§ã¯ã“ã®æ©Ÿæ§‹ã¯ä½¿ç”¨ã•ã‚Œã¾ã›ã‚“ã€‚

ä»£ã‚ã‚Šã«ã€Neuron å´ã§ prefix caching ãŒå®Œå…¨ã«å®Ÿè£…ã•ã‚Œã¾ã™ã€‚vLLM å´ã¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¨ˆç®—ï¼ˆ`full_context_lens`, `computed_context_lens`ï¼‰ã®ã¿ã‚’æ‹…å½“ã—ã€å®Ÿéš›ã® prefix caching å®Ÿè¡Œã¯ Neuron å´ã§ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒ¬ãƒ™ãƒ«ã§è¡Œã‚ã‚Œã¾ã™ã€‚

ã“ã®è¨­è¨ˆã«ã‚ˆã‚Šã€vLLM å´ã®è¿½åŠ ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã¯æœ€å°é™ã«æŠ‘ãˆã‚‰ã‚Œã¦ã„ã¾ã™ã€‚ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¨ˆç®—ã®ã¿ã§æ¸ˆã‚€ãŸã‚ã€æ—¢å­˜ã® 52ms ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã«æ¯”ã¹ã¦ç„¡è¦–ã§ãã‚‹ç¨‹åº¦ã®å½±éŸ¿ã—ã‹ã‚ã‚Šã¾ã›ã‚“ã€‚

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å½±éŸ¿ï¼ˆæ¨å®šï¼‰

:::message alert
**æ³¨æ„**: æœ¬æ¸¬å®šã§ã¯ prefix caching ã¯ç„¡åŠ¹ã«ã—ã¦ã„ã¾ã™ï¼ˆ`enable_prefix_caching: false`ï¼‰ã€‚ä»¥ä¸‹ã®æ•°å€¤ã¯ã€[NxD Inference Release Notes](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/neuronx-distributed-inference/neuronx-distributed-inference.html)ã«è¨˜è¼‰ã•ã‚ŒãŸã€Œ3.2x TTFTæ”¹å–„ï¼ˆ90% cache hitæ™‚ï¼‰ã€ã®ãƒ‡ãƒ¼ã‚¿ã‚’åŸºã«æ¨å®šã—ãŸã‚‚ã®ã§ã™ã€‚
:::

#### Rerankerï¼ˆçŸ­ã„æ¨è«–ã€ä»Šå›ã®æ¸¬å®šï¼‰

```
Prefix cachingç„¡åŠ¹ï¼ˆå®Ÿæ¸¬ï¼‰:
- vLLM: 64.33 ms/batch (Neuron 11.97ms + ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ 52.36ms)
- NxDç›´æ¥: 11.97 ms/batch

Prefix cachingæœ‰åŠ¹ï¼ˆæ¨å®šã€90% cache hitæ™‚ï¼‰:
- vLLM: ~56 ms/batch (Neuron 3.74ms + ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ 52.36ms)
  â†’ 13%æ”¹å–„ï¼ˆåŠ¹æœã¯é™å®šçš„ï¼‰
- NxDç›´æ¥: ~3.74 ms/batch
  â†’ 69%æ”¹å–„ï¼ˆå¤§ããªåŠ¹æœï¼‰

æ¨å®šæ ¹æ‹ : 11.97ms Ã· 3.2 â‰ˆ 3.74ms
```

**çµè«–**: **çŸ­ã„æ¨è«–ã§ã¯ã€vLLM ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒæ”¯é…çš„**ãªãŸã‚ã€prefix caching ã®åŠ¹æœã¯é™å®šçš„ã€‚

#### é€šå¸¸ã®ç”Ÿæˆï¼ˆé•·ã„æ¨è«–ï¼‰

```
ä¾‹: 100ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ @ 12 ms/token = 1200 ms

Prefix cachingç„¡åŠ¹:
- vLLM: 1252 ms (1200 + 52)
- NxD: 1200 ms

Prefix cachingæœ‰åŠ¹ï¼ˆ90% cache hitï¼‰:
- vLLM: 427 ms (375 + 52) â†’ 2.9xé«˜é€ŸåŒ–
- NxD: 375 ms â†’ 3.2xé«˜é€ŸåŒ–
```

**çµè«–**: **é•·ã„æ¨è«–ã§ã¯ã€prefix caching ã®åŠ¹æœãŒæ”¯é…çš„**ã€‚vLLM ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã®å‰²åˆãŒå°ã•ããªã‚‹ã€‚

## Phase 4: vLLM Bucketing ON/OFF ã®è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°

### 4.1 ãªãœã•ã‚‰ã«èª¿æŸ»ãŒå¿…è¦ã ã£ãŸã®ã‹

Phase 3 ã¾ã§ã®èª¿æŸ»ã§ã€ä»¥ä¸‹ã®ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã‚Šã¾ã—ãŸï¼š

- vLLM ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰: **+52.36 ms (81%)**
- NxD Inference ã® Python ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰: **~0 ms**
- vLLM ã® bucketing ON ã§æ€§èƒ½å‘ä¸Šã€NxD Inference ã§ã¯é€†ã«è‹¥å¹²æ‚ªåŒ–

ã—ã‹ã—ã€ã“ã“ã§ **é‡è¦ãªç–‘å•** ãŒæ®‹ã‚Šã¾ã—ãŸï¼š

:::message alert
**vLLM ã® bucketing OFF ï¼ˆ321.40 msï¼‰ãŒç•°å¸¸ã«é…ã„ç†ç”±ã¯ä½•ã‹ï¼Ÿ**

- NxD Inference bucketing OFF: 11.97 ms
- vLLM bucketing OFF: 321.40 ms
- **å·®åˆ†: 309.43 msï¼ˆ26å€ã®å·®ï¼‰**

é€šå¸¸ã® vLLM ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ï¼ˆ52msï¼‰ã§ã¯èª¬æ˜ã§ããªã„ã€‚
:::

ã“ã®ç–‘å•ã‚’è§£æ±ºã™ã‚‹ãŸã‚ã€`line_profiler` ã§ vLLM å†…éƒ¨ã‚’è©³ç´°ã«ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã—ã¾ã—ãŸã€‚

### 4.2 line_profiler ã«ã‚ˆã‚‹ vLLM å†…éƒ¨åˆ†æ

#### æ¸¬å®šæ¡ä»¶

```python
# Bucketing ON ã¨ OFF ã‚’ä¸¡æ–¹ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
@profile
def run_vllm_benchmark(model_path, bucketing, num_batches=30):
    vllm_config = {
        "tensor_parallel_size": 2,
        "max_num_seqs": 4,
        "enable_prefix_caching": False,  # bucketing åŠ¹æœã‚’åˆ†é›¢
        "additional_config": {
            "override_neuron_config": {
                "enable_bucketing": bucketing,  # True / False
            }
        }
    }
    llm = vllm.LLM(model=model_path, **vllm_config)
    # 30ãƒãƒƒãƒæ¸¬å®š
    for i in range(num_batches):
        outputs = llm.generate(batch_prompts, sampling_params)
```

#### æ¸¬å®šçµæœ

| è¨­å®š | å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· | ä¸­å¤®å€¤ | å€ç‡ |
|------|--------------|-------|------|
| **Bucketing ON** | **77.92 ms/batch** | 80.46 ms | 1.00x |
| **Bucketing OFF** | **328.73 ms/batch** | 294.71 ms | **4.22x** ğŸ”´ |

**Bucketing OFF ã¯ Bucketing ON ã® 4.2å€é…ã„ï¼ˆå›ºå®šé•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã‚‚ï¼ï¼‰**

:::message
**æ³¨æ„**: profiling overheadï¼ˆç´„+13msï¼‰ã‚’å«ã‚€ãŸã‚ã€ä»¥å‰ã®æ¸¬å®šå€¤ï¼ˆ64.33ms / 321.40msï¼‰ã¨ã¯è‹¥å¹²ç•°ãªã‚Šã¾ã™ã€‚
:::

### 4.3 line_profiler ãŒæ˜ã‚‰ã‹ã«ã—ãŸã“ã¨

#### Main Process ã®æ™‚é–“åˆ†å¸ƒ

```python
# line_profiler ã®çµæœï¼ˆBucketing ON + OFF ã®åˆè¨ˆï¼‰

Function: run_vllm_benchmark
Line 67  | llm = vllm.LLM(...)          | 619.5s | 97.8% | Workerèµ·å‹•+åˆæœŸåŒ–
Line 90  | llm.generate() (60å›)        | 12.2s  |  1.9% | æ¨è«–å®Ÿè¡Œ
Line 82  | Warmup                       |  0.6s  |  0.1% | Warmup
```

**é‡è¦ãªç™ºè¦‹:**
- åˆæœŸåŒ–æ™‚é–“ã® 99.9% ãŒ `MPClient.__init__` ã® `launch_core_engines()`
- æ¨è«–æ™‚é–“ã® 100% ãŒ `SyncMPClient.get_output()` ã® `outputs_queue.get()`

#### ã“ã‚ŒãŒæ„å‘³ã™ã‚‹ã“ã¨

**line_profiler ã§æ¸¬å®šã§ããŸã®ã¯ Main Process ã®ã¿ï¼š**

```
Main Process (line_profiler âœ…)     Worker Process (line_profiler âŒ)
â”‚                                   â”‚
â”œâ”€ LLMEngine                        â”œâ”€ ModelExecutor
â”‚  â””â”€ step()                        â”‚  â”œâ”€ execute_model()
â”‚     â””â”€ get_output() â±ï¸             â”‚  â”‚  â””â”€ NeuronCore å®Ÿè¡Œ âš¡
â”‚        â””â”€ queue.get()              â”‚  â”‚     â””â”€ NxD Inference
â”‚           [Wait for Worker]        â”‚  â””â”€ sample_tokens()
â”‚                                   â”‚
â””â”€ outputs_queue â† â† â† [ZMQ] â† â† â† â””â”€ çµæœé€ä¿¡
```

**Main Process ã§è¨ˆæ¸¬ã•ã‚Œã‚‹æ™‚é–“:**
- `get_output()` = Worker Process ã®å‡¦ç†å®Œäº†ã‚’å¾…ã¤æ™‚é–“ï¼ˆZMQ ã‚­ãƒ¥ãƒ¼ï¼‰

**Worker Process ã§å®Ÿè¡Œã•ã‚Œã‚‹å‡¦ç†ï¼ˆè¨ˆæ¸¬ä¸å¯ï¼‰:**
- ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°
- ãƒ¡ãƒ¢ãƒªç®¡ç†
- NeuronCore ã¸ã®å…¥åŠ›æº–å‚™
- NxD Inference å®Ÿè¡Œ
- å‡ºåŠ›å‡¦ç†

:::message alert
**é‡è¦:** `get_output()` ã§è¨ˆæ¸¬ã•ã‚Œã‚‹æ™‚é–“ã«ã¯ã€**Worker Process å†…ã®å…¨å‡¦ç†æ™‚é–“**ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚
:::

### 4.4 Bucketing ON/OFF ã®å†…éƒ¨å‹•ä½œã®é•ã„

#### åˆæœŸåŒ–ãƒ•ã‚§ãƒ¼ã‚ºã®é•ã„

**Bucketing ONï¼ˆç´„7åˆ†26ç§’ï¼‰:**
```
[16:38:51] åˆæœŸåŒ–é–‹å§‹
[16:39:20] HLOç”Ÿæˆé–‹å§‹ï¼ˆè¤‡æ•°ãƒã‚±ãƒƒãƒˆ: 128, 256, 512, 1024, 2048ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰
[16:40:20] HLOç”Ÿæˆå®Œäº†ï¼ˆ60.8ç§’ï¼‰
[16:40:20] ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«é–‹å§‹
[16:46:17] Warmupé–‹å§‹ï¼ˆã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚é–“: 357ç§’ï¼‰
```

**Bucketing OFFï¼ˆç´„2åˆ†54ç§’ï¼‰:**
```
[16:46:30] åˆæœŸåŒ–é–‹å§‹
[16:46:48] HLOç”Ÿæˆé–‹å§‹ï¼ˆå°‘ãªã„ãƒã‚±ãƒƒãƒˆ: 2048ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ï¼‰
[16:47:03] HLOç”Ÿæˆå®Œäº†ï¼ˆ15.8ç§’ï¼‰
[16:47:03] ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«é–‹å§‹
[16:49:24] Warmupé–‹å§‹ï¼ˆã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚é–“: 141ç§’ï¼‰
```

**è¦³å¯Ÿ:**
- Bucketing ON/OFF ã§ç•°ãªã‚‹ã‚¢ãƒ¼ãƒ†ã‚£ãƒ•ã‚¡ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½¿ç”¨
- Bucketing OFF ã®æ–¹ãŒåˆæœŸåŒ–ã¯é«˜é€Ÿï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹æœï¼Ÿï¼‰

#### æ¨è«–ãƒ•ã‚§ãƒ¼ã‚ºã®æ€§èƒ½å·®

**å®Ÿæ¸¬å€¤ã‹ã‚‰ã®é€†ç®—:**

å„ãƒãƒƒãƒã¯ 4ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ Ã— 1ãƒˆãƒ¼ã‚¯ãƒ³ = 4å›ã® `get_output()` å‘¼ã³å‡ºã—

| è¨­å®š | ãƒãƒƒãƒå¹³å‡ | get_output() 1å›ã‚ãŸã‚Š |
|------|-----------|----------------------|
| Bucketing ON | 77.92 ms | **19.5 ms** |
| Bucketing OFF | 328.73 ms | **82.2 ms** |
| **å·®åˆ†** | +250.81 ms | **+62.7 msï¼ˆ4.2å€ï¼‰** |

**ã“ã® 62.7ms ã®å·®ã¯ã©ã“ã§ç™ºç”Ÿã—ã¦ã„ã‚‹ã®ã‹ï¼Ÿ**

### 4.5 Worker Process å†…ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ï¼ˆæ¨æ¸¬ï¼‰

line_profiler ã§ã¯ Worker Process å†…éƒ¨ã‚’è¨ˆæ¸¬ã§ããªã„ãŸã‚ã€ãƒ­ã‚°ã¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‹ã‚‰æ¨æ¸¬ã—ã¾ã™ã€‚

#### Bucketing ON ã®å‹•ä½œï¼ˆé«˜é€Ÿãƒ‘ã‚¹ï¼‰

```python
# ãƒ—ãƒªã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ã‚°ãƒ©ãƒ•ã‚’åŠ¹ç‡çš„ã«ä½¿ç”¨
compiled_graphs = load_precompiled_neff()  # 32, 64, 96, 128ãƒˆãƒ¼ã‚¯ãƒ³
bucket = select_optimal_bucket(input_length)  # 97 â†’ 128
result = execute_on_neuroncore(compiled_graphs[bucket], input)
# æœ€é©åŒ–: ãƒ¡ãƒ¢ãƒªå›ºå®šã€ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°
```

- âœ… ãƒ—ãƒªã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿NEFFã‚’ãƒ­ãƒ¼ãƒ‰
- âœ… æœ€é©ãªãƒã‚±ãƒƒãƒˆã‚’åŠ¹ç‡çš„ã«é¸æŠ
- âœ… ãƒ¡ãƒ¢ãƒªç®¡ç†ã¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ãŒæœ€é©åŒ–

#### Bucketing OFF ã®å‹•ä½œï¼ˆä½é€Ÿãƒ‘ã‚¹ï¼‰

```python
# å‹•çš„ãªå‡¦ç†ãƒ‘ã‚¹ï¼ˆ"slow path"ï¼‰
graph = get_dynamic_graph(input_length)  # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ï¼Ÿ
result = execute_on_neuroncore_slow(graph, input)
# éæœ€é©åŒ–: å‹•çš„ãƒ¡ãƒ¢ãƒªã€åŠ¹ç‡æ‚ªã„ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°
```

- âŒ å‹•çš„ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆ97 â†’ 128ãƒˆãƒ¼ã‚¯ãƒ³ã€31ãƒˆãƒ¼ã‚¯ãƒ³ç„¡é§„ï¼‰
- âŒ ã‚°ãƒ©ãƒ•é¸æŠã®éåŠ¹ç‡æ€§ï¼ˆå®Ÿè¡Œæ™‚ãƒ‡ã‚£ã‚¹ãƒ‘ãƒƒãƒï¼‰
- âŒ ãƒ¡ãƒ¢ãƒªç®¡ç†ã®éåŠ¹ç‡æ€§ï¼ˆå‹•çš„å‰²ã‚Šå½“ã¦ã€ãƒ•ãƒ©ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
- âŒ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã®æœ€é©åŒ–ä¸è¶³
- âŒ NeuronCore åˆ©ç”¨åŠ¹ç‡ã®ä½ä¸‹

:::message
**vLLM ã®è¨­è¨ˆæ€æƒ³:** Bucketing ON ãŒå‰æã®å®Ÿè£…ã€‚Bucketing OFF ã¯äº’æ›æ€§ãƒ»ãƒ‡ãƒãƒƒã‚°ç”¨ã® fallback å®Ÿè£…ã§ã€ãƒ—ãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ç”¨é€”ã‚’æƒ³å®šã—ã¦ã„ãªã„ã€‚
:::

### 4.6 NxD Inference ã¨ã®æ¯”è¼ƒ

| ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ | Bucketing ON | Bucketing OFF | å·®åˆ† | å€ç‡ |
|---------------|-------------|--------------|------|------|
| **vLLM-Neuron** | 77.92 ms | 328.73 ms | +250.81 ms | **4.22x** ğŸ”´ |
| **NxD Inference** | 12.02 ms | 11.97 ms | -0.05 ms | **0.996x** âœ… |

**ãªãœã“ã®é•ã„ãŒç”Ÿã¾ã‚Œã‚‹ã®ã‹ï¼Ÿ**

#### NxD Inference ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```python
# Bucketing ON/OFF ã«é–¢ã‚ã‚‰ãšã€å¸¸ã«æœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ã‚¹
optimized_graph = prepare_graph(model, config)
result = execute_optimized(optimized_graph, input)
# å¸¸ã«æœ€é©åŒ–: ãƒ¡ãƒ¢ãƒªå›ºå®šã€åŠ¹ç‡çš„ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°
```

- NxD Inference ã¯ bucketing ã®æœ‰ç„¡ã«ä¾å­˜ã—ãªã„æœ€é©åŒ–
- å›ºå®šé•·ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã§ã¯ bucketing ã®åŠ¹æœã¯ã»ã¼ã‚¼ãƒ­

#### vLLM-Neuron ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```python
# Bucketing ON/OFF ã§ç•°ãªã‚‹å®Ÿè¡Œãƒ‘ã‚¹
if enable_bucketing:
    # é«˜é€Ÿãƒ‘ã‚¹: ãƒ—ãƒªã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ã‚°ãƒ©ãƒ•ã‚’ä½¿ç”¨
    result = execute_with_bucketing(compiled_graphs, input)
else:
    # ä½é€Ÿãƒ‘ã‚¹: Fallbackå®Ÿè£…
    result = execute_fallback(graph, input)
```

- vLLM ã¯ bucketing ON å‰æã®è¨­è¨ˆ
- Bucketing OFF ã¯ fallback å®Ÿè£…ã§æœªæœ€é©åŒ–

### 4.7 vLLM ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã®å†å®šç¾©

ã“ã‚Œã¾ã§ã®ç†è§£ï¼š

```
vLLM ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ = Main Process ã® Python å‡¦ç†
                     â‰ˆ 52ms
```

**æ–°ã—ã„ç†è§£ï¼ˆPhase 4 ã§åˆ¤æ˜ï¼‰:**

```
vLLM ã®è¨ˆæ¸¬æ™‚é–“ = Main Process (line_profiler âœ…)
                + Worker Process (line_profiler âŒ)
                + NxD Inference

Bucketing ON ã®å ´åˆ:
77.92ms = Main Process (å¾…æ©Ÿæ™‚é–“ã®ã¿)
        + Worker Process (æœ€é©åŒ–ãƒ‘ã‚¹)
        + NxD Inference (~12ms)

Bucketing OFF ã®å ´åˆ:
408.72ms = Main Process (å¾…æ©Ÿæ™‚é–“ã®ã¿)
         + Worker Process (ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ âš ï¸)
         + NxD Inference (~12ms?)
```

:::message alert
**Phase 5 ã§æ ¹æœ¬åŸå› ã‚’ç‰¹å®š:**
vLLM è©³ç´°ãƒ­ã‚°åˆ†æã«ã‚ˆã‚Šã€Bucketing OFF ãŒé…ã„çœŸã®åŸå› ãŒåˆ¤æ˜ã—ã¾ã—ãŸã€‚

- **Bucketing ON**: 10 HLOï¼ˆ128/256/512/1024/2048 ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ â†’ 97ãƒˆãƒ¼ã‚¯ãƒ³ã¯128ãƒã‚±ãƒƒãƒˆä½¿ç”¨
- **Bucketing OFF**: 2 HLOï¼ˆ2048 ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ï¼‰ â†’ 97ãƒˆãƒ¼ã‚¯ãƒ³ã‚’2048ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆ21å€ã®ç„¡é§„ï¼‰
- **å…¥åŠ›ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰**: 1951ãƒˆãƒ¼ã‚¯ãƒ³ã®ä¸è¦ãªè¨ˆç®—ãŒåŸå› 
- **åˆæœŸåŒ–æ™‚é–“ã®é€†è»¢**: Bucketing OFF ã¯ 152ç§’ã€ON ã¯ 385ç§’ï¼ˆHLOæ•°ã®é•ã„ï¼‰
:::

### 4.8 ã•ã‚‰ã«è©³ç´°ãªèª¿æŸ»ã®ãŸã‚ã«

line_profiler ã§ã¯ Worker Process å†…éƒ¨ã‚’è¨ˆæ¸¬ã§ããªã„ãŸã‚ã€ä»¥ä¸‹ã®3ã¤ã®æ‰‹æ³•ã‚’æ¤œè¨ï¼š

#### æ¯”è¼ƒè¡¨: Worker Process ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°æ‰‹æ³•

| æ‰‹æ³• | ãƒ¬ã‚¤ãƒ¤ãƒ¼ | ç²’åº¦ | å–å¾—æƒ…å ± | å„ªå…ˆåº¦ |
|------|----------|------|----------|--------|
| **vLLM è©³ç´°ãƒ­ã‚°** | ã‚·ã‚¹ãƒ†ãƒ  | HLO/ã‚³ãƒ³ãƒ‘ã‚¤ãƒ« | ãƒã‚±ãƒƒãƒˆé¸æŠã€HLOç”Ÿæˆæ™‚é–“ | â­â­â­ |
| **neuron-profile** | ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ | NeuronCore | ä½¿ç”¨ç‡ã€ãƒ¡ãƒ¢ãƒªè»¢é€ | â­â­ |
| **py-spy** | Pythoné–¢æ•° | é–¢æ•°å‘¼ã³å‡ºã— | execute_model()ã®å†…è¨³ | â­ |

#### 1. vLLM è©³ç´°ãƒ­ã‚°ï¼ˆæœ€å„ªå…ˆï¼‰âœ… Phase 5 ã§å®Ÿæ–½

```python
import logging
logging.basicConfig(level=logging.DEBUG)
vllm_config["disable_log_stats"] = False
```

**çµæœ:**
- âœ… Bucketing ON: 10 HLOç”Ÿæˆã€128ãƒˆãƒ¼ã‚¯ãƒ³ãƒã‚±ãƒƒãƒˆä½¿ç”¨
- âœ… Bucketing OFF: 2 HLOç”Ÿæˆã€2048ãƒˆãƒ¼ã‚¯ãƒ³ãƒã‚±ãƒƒãƒˆä½¿ç”¨
- âœ… **æ ¹æœ¬åŸå› ã‚’ç‰¹å®š**: å…¥åŠ›ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰

#### 2. neuron-profileï¼ˆNeuronCoreãƒ¬ãƒ™ãƒ«ï¼‰ğŸ”„ å®Ÿè¡Œä¸­

```bash
neuron-profile python /tmp/profile_neuron_simple.py
```

æœŸå¾…ã•ã‚Œã‚‹çµæœ:
- NeuronCore ã® utilizationï¼ˆBucketing ON vs OFFï¼‰
- å„æ“ä½œã®å®Ÿè¡Œæ™‚é–“ï¼ˆTensoræ¼”ç®—ã€ãƒ¡ãƒ¢ãƒªè»¢é€ï¼‰
- ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã®å®Ÿéš›ã®è¨ˆç®—ã‚³ã‚¹ãƒˆ

#### 3. py-spyï¼ˆPythoné–¢æ•°ãƒ¬ãƒ™ãƒ«ï¼‰ğŸ”„ å®Ÿè¡Œä¸­

```bash
# Worker Process å…¨ä½“ã‚’ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ï¼ˆ--subprocesses ã§å…¨ãƒ—ãƒ­ã‚»ã‚¹ã‚’è¿½è·¡ï¼‰
py-spy record -o /tmp/worker_profile_bucketing_on.svg --subprocesses -- \
  python3 /tmp/profile_vllm_pyspy.py
```

**å®Ÿè¡ŒçŠ¶æ³:**
- âœ… ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æˆåŠŸï¼ˆsudo pip installï¼‰
- ğŸ”„ ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­ï¼ˆvLLMåˆæœŸåŒ– + 10ãƒãƒƒãƒæ¨è«–ï¼‰
- ğŸ“Š FlameGraph ç”Ÿæˆäºˆå®š: `/tmp/worker_profile_bucketing_on.svg`

**æœŸå¾…ã•ã‚Œã‚‹çµæœ:**
- Worker Process ã®é–¢æ•°å‘¼ã³å‡ºã—ã‚°ãƒ©ãƒ•ï¼ˆflamegraphï¼‰
- execute_model() å†…ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯
- Python ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¨ NxD Inference ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®å¢ƒç•Œ
- å„é–¢æ•°ã®å®Ÿè¡Œæ™‚é–“ã®å‰²åˆï¼ˆ%ï¼‰

### 4.9 Phase 4 ã®ã¾ã¨ã‚

**ä¸»è¦ãªç™ºè¦‹:**

1. âœ… **Bucketing OFF ã¯ 5.0å€é…ã„**: å›ºå®šé•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã‚‚åŠ‡çš„ãªæ€§èƒ½å·®
2. âœ… **line_profiler ã®é™ç•Œ**: Worker Process å†…éƒ¨ã¯è¨ˆæ¸¬ä¸å¯
3. âœ… **ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¯ Worker Process å†…**: Main Process ã¯å¾…æ©Ÿã—ã¦ã„ã‚‹ã ã‘
4. âš ï¸ **æ ¹æœ¬åŸå› ã¯ä¸æ˜**: Worker Process å†…ã®è©³ç´°èª¿æŸ»ãŒå¿…è¦ â†’ Phase 5 ã¸

**æŠ€è¡“çš„çµè«–ï¼ˆPhase 4æ™‚ç‚¹ï¼‰:**

```
vLLM å…¨ä½“ã®å‡¦ç†æ™‚é–“
= Main Process (ã»ã¼ã‚¼ãƒ­ã€å¾…æ©Ÿã®ã¿)
+ Worker Process (å¤§ããªã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ âš ï¸ åŸå› ä¸æ˜)
  â”œâ”€ Bucketing ON: ~70ms
  â””â”€ Bucketing OFF: ~397msï¼ˆ5.0å€é…ã„ï¼‰
+ NxD Inference (~12ms)
```

**æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:**
- neuron-profile: NeuronCore ãƒ¬ãƒ™ãƒ«ã®è¨ˆæ¸¬
- vLLM è©³ç´°ãƒ­ã‚°: HLO ç”Ÿæˆã¨ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã®åˆ†æ â† **Phase 5 ã§å®Ÿæ–½**
- py-spy: Worker Process ã®é–¢æ•°å‘¼ã³å‡ºã—åˆ†æ

---
## Phase 5: vLLM è©³ç´°ãƒ­ã‚°ã«ã‚ˆã‚‹æ ¹æœ¬åŸå› ã®ç‰¹å®š

### 5.1 ç›®çš„

Phase 4 ã§ã€ŒBucketing OFF ãŒ 5.0å€é…ã„ã€ã“ã¨ã¯åˆ¤æ˜ã—ã¾ã—ãŸãŒã€**ãªãœé…ã„ã®ã‹**ã¯ä¸æ˜ã§ã—ãŸã€‚Worker Process å†…éƒ¨ã‚’è©³ç´°ã«èª¿æŸ»ã™ã‚‹ãŸã‚ã€vLLM ã®è©³ç´°ãƒ­ã‚°ã‚’æœ‰åŠ¹åŒ–ã—ã¦ HLO ç”Ÿæˆã¨ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ãƒ—ãƒ­ã‚»ã‚¹ã‚’åˆ†æã—ã¾ã™ã€‚

### 5.2 å®Ÿé¨“æ–¹æ³•

**è¨­å®š:**
```python
vllm_config = {
    "tensor_parallel_size": 2,
    "max_num_seqs": 8,  # â† v003b æœ€é©è¨­å®š
    "max_num_batched_tokens": 1024,  # â† v003b æœ€é©è¨­å®š
    "block_size": 32,
    "max_model_len": 2048,
    "num_gpu_blocks_override": 512,
    "enable_prefix_caching": False,
    "disable_log_stats": False,  # çµ±è¨ˆæƒ…å ±ã‚’æœ‰åŠ¹åŒ–
    "additional_config": {
        "override_neuron_config": {
            "enable_bucketing": bucketing  # True/False
        }
    }
}
```

:::message
**Phase 5 ã®è¨­å®šã«ã¤ã„ã¦:**
Phase 5 ã§ã¯ã€éå»ã®å®Ÿé¨“ï¼ˆv003bï¼‰ã§æœ€é©åŒ–ã•ã‚ŒãŸè¨­å®šï¼ˆ`max_num_seqs=8`, `max_num_batched_tokens=1024`ï¼‰ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚Phase 1-4 ãŠã‚ˆã³ Phase 6b ã§ã¯ã€ã‚ˆã‚Šã‚·ãƒ³ãƒ—ãƒ«ãªè¨­å®šï¼ˆ`max_num_seqs=4`, `max_num_batched_tokens=256`ï¼‰ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ãŸã‚ã€çµ¶å¯¾å€¤ãŒç•°ãªã‚Šã¾ã™ã€‚

ãŸã ã—ã€**Bucketing ã®åŠ¹æœï¼ˆæ¯”ç‡ï¼‰** ã¯ä¸€è²«ã—ã¦ã„ã¾ã™:
- Phase 5: 4.98x (82.13ms vs 408.72ms)
- Phase 6b: 4.28x (59.20ms vs 253.57ms)
:::

å®Ÿè¡Œ: `/tmp/profile_vllm_with_debug_logs.py`

### 5.3 å®Ÿé¨“çµæœ

#### åˆæœŸåŒ–æ™‚é–“ã¨HLOç”Ÿæˆ

| é …ç›® | Bucketing ON | Bucketing OFF | å·®åˆ† |
|------|--------------|---------------|------|
| **åˆæœŸåŒ–æ™‚é–“** | 385.5ç§’ | 152.5ç§’ | **-60%ï¼ˆOFFã®æ–¹ãŒé«˜é€Ÿï¼ï¼‰** |
| **HLOæ•°** | 10å€‹ | 2å€‹ | -80% |
| **context_encoding HLO** | 5å€‹ï¼ˆ128/256/512/1024/2048ï¼‰ | 1å€‹ï¼ˆ2048ã®ã¿ï¼‰ | -80% |
| **token_generation HLO** | 5å€‹ | 1å€‹ | -80% |
| **å„ªå…ˆHLOã‚³ãƒ³ãƒ‘ã‚¤ãƒ«** | 82.0ç§’ | 96.8ç§’ | +18% |
| **å…¨HLOã‚³ãƒ³ãƒ‘ã‚¤ãƒ«** | 249.1ç§’ | 18.5ç§’ | **-93%** |

#### æ¨è«–æ€§èƒ½

| é …ç›® | Bucketing ON | Bucketing OFF | å·®åˆ† |
|------|--------------|---------------|------|
| **å¹³å‡æ¨è«–æ™‚é–“** | 82.13 ms | 408.72 ms | **+398% (5.0å€é…ã„)** |
| **ä¸­å¤®å€¤** | 81.62 ms | 410.38 ms | +402% |
| **95ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«** | 84.59 ms | 476.50 ms | +463% |
| **1stãƒãƒƒãƒ** | 81ms | 475ms | +588% |
| **10thãƒãƒƒãƒ** | 80ms | 321ms | +301% |
| **ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—åŠ¹æœ** | ãªã—ï¼ˆå®‰å®šï¼‰ | ã‚ã‚Šï¼ˆ-32%æ”¹å–„ï¼‰ | - |

#### ãƒ­ã‚°ã‹ã‚‰åˆ¤æ˜ã—ãŸè©³ç´°

**Bucketing ON:**
```
Generating 5 hlos for key: context_encoding_model
- torch.Size([1, 128]) in 1.71s
- torch.Size([1, 256]) in 1.62s
- torch.Size([1, 512]) in 1.74s
- torch.Size([1, 1024]) in 1.97s
- torch.Size([1, 2048]) in 2.41s

Generating 5 hlos for key: token_generation_model
- 5 Ã— torch.Size([4, 1]) in ~1.8s each
```

**Bucketing OFF:**
```
Generating 1 hlos for key: context_encoding_model
- torch.Size([1, 2048]) in 2.56s

Generating 1 hlos for key: token_generation_model
- 1 Ã— torch.Size([4, 1]) in ~2.0s
```

### 5.4 æ ¹æœ¬åŸå› ã®ç‰¹å®š

#### å•é¡Œã®æ ¸å¿ƒ: å…¥åŠ›ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰

**Bucketing OFF ã®æŒ™å‹•:**
```python
# 97ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å‡¦ç†ã™ã‚‹å ´åˆ
buckets = [2048]  # 2048ãƒˆãƒ¼ã‚¯ãƒ³ãƒã‚±ãƒƒãƒˆã®ã¿

# 97ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ 2048 ãƒˆãƒ¼ã‚¯ãƒ³ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
padded_input = pad(input_97_tokens, target_length=2048)
# â†’ 1951ãƒˆãƒ¼ã‚¯ãƒ³ã®ç„¡é§„ãªãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
# â†’ NeuronCoreã¯2048ãƒˆãƒ¼ã‚¯ãƒ³å…¨ã¦ã‚’è¨ˆç®—ï¼ˆ21å€ã®ç„¡é§„ï¼‰
```

**Bucketing ON ã®æŒ™å‹•:**
```python
# 97ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å‡¦ç†ã™ã‚‹å ´åˆ
buckets = [128, 256, 512, 1024, 2048]  # è¤‡æ•°ãƒã‚±ãƒƒãƒˆ

# æœ€å°ã®é©åˆãƒã‚±ãƒƒãƒˆ(128)ã‚’é¸æŠ
selected_bucket = 128
padded_input = pad(input_97_tokens, target_length=128)
# â†’ 31ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã®ã¿
# â†’ NeuronCoreã¯128ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨ˆç®—ï¼ˆæœ€å°é™ã®ç„¡é§„ï¼‰
```

#### è¨ˆç®—é‡ã®æ¯”è¼ƒ

| æŒ‡æ¨™ | Bucketing ON | Bucketing OFF | æ¯”ç‡ |
|------|--------------|---------------|------|
| **å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³æ•°** | 97 | 97 | - |
| **ä½¿ç”¨ãƒã‚±ãƒƒãƒˆ** | 128 | 2048 | - |
| **ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°** | +31 (32%) | +1951 (2009%) | **63å€** |
| **NeuronCoreè¨ˆç®—é‡** | 128ãƒˆãƒ¼ã‚¯ãƒ³ | 2048ãƒˆãƒ¼ã‚¯ãƒ³ | **16å€** |
| **å®Ÿæ¸¬æ¨è«–æ™‚é–“** | 82ms | 409ms | **5.0å€** |

:::message
**ãªãœè¨ˆç®—é‡ã¯16å€ãªã®ã«å®Ÿæ¸¬ã¯5å€ãªã®ã‹ï¼Ÿ**

1. **å›ºå®šã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰**: ãƒãƒƒãƒå‡¦ç†ã€ãƒ¡ãƒ¢ãƒªè»¢é€ã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ï¼ˆ~40msï¼‰
2. **NeuronCoreã®æœ€é©åŒ–**: ã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«å‡¦ç†ã‚ˆã‚Šä¸¦åˆ—å‡¦ç†ã®æ–¹ãŒåŠ¹ç‡çš„
3. **ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—åŠ¹æœ**: Bucketing OFFã¯å®Ÿè¡Œå›æ•°ã§æ”¹å–„ï¼ˆ475ms â†’ 321msï¼‰

è¨ˆç®—å¼:
```
å®Ÿæ¸¬æ™‚é–“ = å›ºå®šã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ + (ãƒˆãƒ¼ã‚¯ãƒ³æ•° Ã— ä¿‚æ•°)
Bucketing ON:  82ms = 40ms + (128 Ã— 0.33ms)
Bucketing OFF: 409ms = 40ms + (2048 Ã— 0.18ms)
```

Bucketing OFFã¯å¤§é‡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¸¦åˆ—å‡¦ç†ã™ã‚‹ãŸã‚ã€ãƒˆãƒ¼ã‚¯ãƒ³ã‚ãŸã‚Šã®ä¿‚æ•°ã¯å°ã•ããªã‚‹ï¼ˆ0.18ms < 0.33msï¼‰ãŒã€ç·é‡ãŒå¤§ãã™ãã¦é…ã„ã€‚
:::

### 5.5 åˆæœŸåŒ–æ™‚é–“ã®é€†è»¢ç¾è±¡

**Bucketing OFFã®æ–¹ãŒåˆæœŸåŒ–ãŒé«˜é€Ÿãªç†ç”±:**

| é …ç›® | Bucketing ON | Bucketing OFF |
|------|--------------|---------------|
| **HLOç”Ÿæˆæ™‚é–“** | ~19.7ç§’ (10 HLO Ã— ~2ç§’) | ~5.0ç§’ (2 HLO Ã— ~2.5ç§’) |
| **å„ªå…ˆHLOã‚³ãƒ³ãƒ‘ã‚¤ãƒ«** | 82.0ç§’ | 96.8ç§’ |
| **æ®‹ã‚ŠHLOã‚³ãƒ³ãƒ‘ã‚¤ãƒ«** | 249.1ç§’ (9 HLOä¸¦åˆ—) | 18.5ç§’ (1 HLO) |
| **åˆè¨ˆ** | 385.5ç§’ | 152.5ç§’ |

**ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•:**
- Bucketing ON: åˆæœŸåŒ–ã¯é…ã„ï¼ˆè¤‡æ•°HLOã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ï¼‰ãŒæ¨è«–ã¯é«˜é€Ÿï¼ˆæœ€é©ãƒã‚±ãƒƒãƒˆé¸æŠï¼‰
- Bucketing OFF: åˆæœŸåŒ–ã¯é«˜é€Ÿï¼ˆ1HLOã®ã¿ï¼‰ãŒæ¨è«–ã¯ä½é€Ÿï¼ˆéå‰°ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼‰

**æ¨å¥¨:**
- **æœ¬ç•ªç’°å¢ƒ**: Bucketing ONï¼ˆåˆæœŸåŒ–ã‚³ã‚¹ãƒˆã‚’1å›æ‰•ãˆã°ã€æ¨è«–ã¯5å€é«˜é€Ÿï¼‰
- **ãƒ‡ãƒãƒƒã‚°ç’°å¢ƒ**: Bucketing OFFï¼ˆèµ·å‹•ãŒé«˜é€Ÿã ãŒæ¨è«–ã¯é…ã„ï¼‰

### 5.6 ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—åŠ¹æœã®è¬

Bucketing OFFã§è¦³æ¸¬ã•ã‚ŒãŸæ¨è«–æ™‚é–“ã®æ”¹å–„:
```
1stãƒãƒƒãƒ:  475.73ms
2ndãƒãƒƒãƒ:  447.45ms
...
10thãƒãƒƒãƒ: 321.24ms
æ”¹å–„ç‡: -32%
```

**è€ƒãˆã‚‰ã‚Œã‚‹åŸå› :**
1. **NeuronCore JITã‚³ãƒ³ãƒ‘ã‚¤ãƒ«**: å®Ÿè¡Œæ™‚ã®è¿½åŠ æœ€é©åŒ–
2. **ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—**: ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æœ€é©åŒ–
3. **ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æœ€é©åŒ–**: ãƒ‡ãƒ¼ã‚¿è»¢é€ã®ä¸¦åˆ—åŒ–

Bucketing ONã§ã¯ã“ã®åŠ¹æœãŒè¦‹ã‚‰ã‚Œãªã„ï¼ˆã™ã§ã«æœ€é©åŒ–æ¸ˆã¿ï¼‰ã€‚

### 5.7 Phase 5 ã®ã¾ã¨ã‚

**æ ¹æœ¬åŸå› ã®ç¢ºå®š:**

âœ… **Bucketing OFF ãŒ 5.0å€é…ã„ç†ç”± = å…¥åŠ›ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰**
- 97ãƒˆãƒ¼ã‚¯ãƒ³ â†’ 2048ãƒˆãƒ¼ã‚¯ãƒ³ã¸ã®éå‰°ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
- 1951ãƒˆãƒ¼ã‚¯ãƒ³ã®ç„¡é§„ãªè¨ˆç®—ï¼ˆ21å€ã®è¨ˆç®—é‡ï¼‰
- NeuronCoreãŒ2048ãƒˆãƒ¼ã‚¯ãƒ³å…¨ã¦ã‚’å‡¦ç†

**æŠ€è¡“çš„çµè«–:**

```
Bucketing ONï¼ˆæ¨å¥¨ï¼‰:
â”œâ”€ åˆæœŸåŒ–: 385ç§’ï¼ˆ10 HLO ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ï¼‰
â”œâ”€ æ¨è«–: 82msï¼ˆ128ãƒˆãƒ¼ã‚¯ãƒ³ãƒã‚±ãƒƒãƒˆä½¿ç”¨ï¼‰
â””â”€ ãƒˆãƒ¼ã‚¿ãƒ«: 1000æ¨è«–ã§åˆæœŸåŒ–ã‚³ã‚¹ãƒˆå›å

Bucketing OFF:
â”œâ”€ åˆæœŸåŒ–: 152ç§’ï¼ˆ2 HLO ã®ã¿ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ï¼‰
â”œâ”€ æ¨è«–: 409msï¼ˆ2048ãƒˆãƒ¼ã‚¯ãƒ³ãƒã‚±ãƒƒãƒˆå¼·åˆ¶ä½¿ç”¨ï¼‰
â””â”€ ãƒˆãƒ¼ã‚¿ãƒ«: åˆæœŸåŒ–ã¯é«˜é€Ÿã ãŒæ¨è«–ãŒé…ã™ãã¦éå®Ÿç”¨çš„
```

**æ¨å¥¨è¨­å®š:**

```python
# vLLM-Neuron ã§å¿…ãšä½¿ç”¨ã™ã¹ãè¨­å®š
vllm_config = {
    "additional_config": {
        "override_neuron_config": {
            "enable_bucketing": True,  # å¿…é ˆï¼ï¼ˆOFF ã¯å…¥åŠ›ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã§5å€é…ã„ï¼‰
        }
    }
}
```

**è¨­è¨ˆæ€æƒ³:**
- vLLM-Neuron ã¯ Bucketing ON ãŒå‰æã®è¨­è¨ˆ
- Bucketing OFF ã¯**ãƒ‡ãƒãƒƒã‚°ç”¨é€”ã®fallback**ã§ã‚ã‚Šã€æœ¬ç•ªåˆ©ç”¨ã¯éæ¨å¥¨
- å…¥åŠ›é•·ãŒå›ºå®šã§ã‚‚ Bucketing ON ã‚’ä½¿ç”¨ã™ã¹ãï¼ˆãƒ‘ãƒ‡ã‚£ãƒ³ã‚°æœ€é©åŒ–ã®ãŸã‚ï¼‰

### 5.8 Phase 6 ã¸ã®å±•é–‹ï¼ˆè¿½åŠ ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ï¼‰

Phase 5 ã§æ ¹æœ¬åŸå› ã¯ç‰¹å®šã§ãã¾ã—ãŸãŒã€ã•ã‚‰ã«æ·±ã„ç†è§£ã®ãŸã‚ã«2ã¤ã®æ‰‹æ³•ã‚’å®Ÿè¡Œä¸­ï¼š

#### ğŸ”„ neuron-profileï¼ˆå®Ÿè¡Œä¸­ï¼‰

**ç›®çš„:** NeuronCore ãƒ¬ãƒ™ãƒ«ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’å®šé‡åŒ–

```bash
neuron-profile python /tmp/profile_neuron_simple.py
```

**æœŸå¾…ã•ã‚Œã‚‹ç™ºè¦‹:**
- 128ãƒˆãƒ¼ã‚¯ãƒ³ vs 2048ãƒˆãƒ¼ã‚¯ãƒ³ã®å®Ÿéš›ã®è¨ˆç®—æ™‚é–“ã®å·®
- NeuronCore ã® utilizationï¼ˆä½¿ç”¨ç‡ï¼‰
- ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ãŒ NeuronCore ã§ã©ã†å‡¦ç†ã•ã‚Œã¦ã„ã‚‹ã‹

#### âœ… py-spyï¼ˆå®Œäº†ï¼‰

**ç›®çš„:** Worker Process ã® Python ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’å¯è¦–åŒ–

```bash
py-spy record -o /tmp/worker_profile_bucketing_on.svg --subprocesses -- \
  python3 /tmp/profile_vllm_pyspy.py
```

**å®Ÿè¡Œçµæœ:**
- âœ… å®Ÿè¡Œå®Œäº†ï¼ˆç´„8åˆ†ï¼‰
- âœ… FlameGraphç”ŸæˆæˆåŠŸ: 286KB
- ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«æ•°: 89,744ï¼ˆã‚¨ãƒ©ãƒ¼: 37ï¼‰
- â±ï¸ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å¹³å‡: **71.07 ms/batch**
- âš ï¸ è­¦å‘Š: 5.87ç§’ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é…å»¶ï¼ˆé«˜è² è·ã«ã‚ˆã‚Šï¼‰

**Phase 5 ã¨ã®æ¯”è¼ƒ:**
- Phase 5 (vLLMè©³ç´°ãƒ­ã‚°): 82.13 ms/batch
- py-spy: 71.07 ms/batch
- å·®åˆ†: -13%ï¼ˆæ¸¬å®šèª¤å·®ã®ç¯„å›²å†…ã€py-spyã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã¯è¨±å®¹ç¯„å›²ï¼‰

**FlameGraph è©³ç´°åˆ†æçµæœ:**

```
ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: 89,744
ç·å®Ÿè¡Œæ™‚é–“: 480ç§’ (~8åˆ†)
ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ: 187 samples/sec
```

| ãƒ•ã‚§ãƒ¼ã‚º | ã‚µãƒ³ãƒ—ãƒ«æ•° | å‰²åˆ | æ¨å®šæ™‚é–“ |
|---------|-----------|------|---------|
| **ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«** | 242,838 | 270% | 1299ç§’ (ä¸¦åˆ—) |
| **åˆæœŸåŒ–** | 170,772 | 190% | 913ç§’ |
| **HLOç”Ÿæˆ** | 10,821 | 12% | 58ç§’ |
| **æ¨è«–å®Ÿè¡Œ** | 7,652 | 8.5% | 41ç§’ (10ãƒãƒƒãƒ) |

**æ³¨**: å‰²åˆãŒ100%ã‚’è¶…ãˆã‚‹ã®ã¯ã€è¤‡æ•°ãƒ—ãƒ­ã‚»ã‚¹ãŒä¸¦åˆ—å®Ÿè¡Œã•ã‚Œã¦ã„ã‚‹ãŸã‚ï¼ˆ25å€‹ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ãƒ—ãƒ­ã‚»ã‚¹ï¼‰

**Worker Process ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ï¼ˆãƒˆãƒƒãƒ—5ï¼‰:**

:::message alert
**æ³¨æ„**: ã“ã‚Œã‚‰ã¯**åˆæœŸåŒ–ãƒ•ã‚§ãƒ¼ã‚º**ã®æ™‚é–“ã§ã™ã€‚Phase 5 ã§ç‰¹å®šã—ãŸ**æ¨è«–ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼**ï¼ˆ82ms vs 409msï¼‰ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å•é¡Œã¨ã¯**ç•°ãªã‚‹æ™‚é–“è»¸**ã§ã™ã€‚

- **åˆæœŸåŒ–æ™‚é–“**: ç§’å˜ä½ï¼ˆ28.6ç§’ãªã©ï¼‰â† py-spyã§è¨ˆæ¸¬
- **æ¨è«–ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼**: ãƒŸãƒªç§’å˜ä½ï¼ˆ82ms vs 409msï¼‰â† Phase 5ã§ç‰¹å®š

py-spy ã¯å…¨ä½“å®Ÿè¡Œæ™‚é–“ï¼ˆ480ç§’ï¼‰ã‚’ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã—ãŸãŸã‚ã€åˆæœŸåŒ–ï¼ˆ385ç§’ã€80%ï¼‰ãŒå¤§éƒ¨åˆ†ã‚’å ã‚ã€æ¨è«–ï¼ˆ0.71ç§’ã€0.15%ï¼‰ã®è©³ç´°ã¯ã‚­ãƒ£ãƒ—ãƒãƒ£ã§ãã¦ã„ã¾ã›ã‚“ã€‚
:::

1. `load_model` (neuron_worker.py): 5,354 samples (28.6ç§’) - **åˆæœŸåŒ–**: ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
2. `load_model` (model_runner.py): 5,353 samples (28.6ç§’) - **åˆæœŸåŒ–**: ãƒ©ãƒ³ãƒŠãƒ¼åˆæœŸåŒ–
3. `get_neuron_model` (model_loader.py): 5,353 samples (28.6ç§’) - **åˆæœŸåŒ–**: Neuronãƒ¢ãƒ‡ãƒ«å–å¾—
4. `load_weights` (model_loader.py): 5,352 samples (28.6ç§’) - **åˆæœŸåŒ–**: é‡ã¿ãƒ­ãƒ¼ãƒ‰
5. `_compile_and_load_model`: 3,924 samples (21ç§’) - **åˆæœŸåŒ–**: ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ï¼†ãƒ­ãƒ¼ãƒ‰

**æ¨è«–æ™‚é–“ã®æ¤œè¨¼:**
- py-spyã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°: 7,652 samples â†’ æ¨å®š 41ç§’ / 10ãƒãƒƒãƒ
- å®Ÿæ¸¬ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯: 71.07 ms/batch Ã— 10 = 0.71ç§’
- å·®åˆ†åŸå› : py-spyã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°é…å»¶ï¼ˆ5.87ç§’è­¦å‘Šï¼‰ã«ã‚ˆã‚Šã€æ¨è«–ãƒ•ã‚§ãƒ¼ã‚ºã®å¤§éƒ¨åˆ†ãŒè¨˜éŒ²ã•ã‚Œãš

**Phase 5 ã¨ã®æ•´åˆæ€§:**
- Phase 5: 82.13 ms/batch (vLLMè©³ç´°ãƒ­ã‚°)
- py-spy: 71.07 ms/batch (å®Ÿæ¸¬)
- å·®åˆ†: -13.5% (æ¸¬å®šèª¤å·®ã®ç¯„å›²å†…)

---

#### âœ… py-spy æ¨è«–å°‚ç”¨ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚° v2ï¼ˆå®Œäº†ï¼‰

åˆå›å®Ÿè¡Œã§ã¯åˆæœŸåŒ–ãƒ•ã‚§ãƒ¼ã‚ºï¼ˆ80%ï¼‰ãŒå¤§éƒ¨åˆ†ã‚’å ã‚ãŸãŸã‚ã€**æ¨è«–ãƒ•ã‚§ãƒ¼ã‚ºã®ã¿**ã‚’ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã™ã‚‹å°‚ç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆã—ã¾ã—ãŸã€‚

**ã‚¹ã‚¯ãƒªãƒ—ãƒˆè¨­è¨ˆ:**
```python
# 1. vLLMåˆæœŸåŒ–ï¼ˆ6åˆ†ï¼‰
llm = vllm.LLM(model=model_path, **vllm_config)

# 2. PIDãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
with open('/tmp/inference_python.pid', 'w') as f:
    f.write(str(os.getpid()))

# 3. 10ç§’å¾…æ©Ÿï¼ˆpy-spyèµ·å‹•å¾…ã¡ï¼‰
time.sleep(10)

# 4. æ¨è«–å®Ÿè¡Œï¼ˆ500ãƒãƒƒãƒã€~30ç§’ï¼‰
for i in range(500):
    outputs = llm.generate(batch_prompts, sampling_params, use_tqdm=False)

# åˆ¥ãƒ—ãƒ­ã‚»ã‚¹ã§sudo py-spyãŒã‚¢ã‚¿ãƒƒãƒã—ã¦40ç§’ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
```

**å®Ÿè¡Œçµæœ:**
- âœ… å®Ÿè¡Œå®Œäº†ï¼ˆåˆæœŸåŒ–6åˆ† + æ¨è«–30ç§’ï¼‰
- âœ… FlameGraphç”ŸæˆæˆåŠŸ: 35KB
- ğŸ“Š ã‚µãƒ³ãƒ—ãƒ«æ•°: **89 samples** (æ¨è«–ãƒ•ã‚§ãƒ¼ã‚ºã®ã¿)
- â±ï¸ å¹³å‡: **59.20 ms/batch** (Bucketing ON)

**Worker Process Pythonå±¤ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰å†…è¨³:**

| å‡¦ç† | ã‚µãƒ³ãƒ—ãƒ«æ•° | å‰²åˆ | æ¨å®šæ™‚é–“ | èª¬æ˜ |
|------|-----------|------|---------|------|
| **ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³** | 56 | 62.92% | 37ms | `_batch_encode_plus`ã€`_tokenize_prompt` |
| **SamplingParams ã‚³ãƒ”ãƒ¼** | 13 | 14.61% | 8.6ms | `clone()` ã§ã® deepcopy å‡¦ç† |
| **ãƒªã‚¯ã‚¨ã‚¹ãƒˆç®¡ç†** | 8-12 | 13% | 7.7ms | `add_request`ã€ZMQ é€šä¿¡ |
| **ãã®ä»–** | 9 | 10% | 5.9ms | ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã€ãƒ‡ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ç­‰ |

**è©³ç´°ãªé–¢æ•°ãƒ¬ãƒ™ãƒ«åˆ†æ:**

```
process_inputs (vllm/v1/engine/input_processor.py:448)
  â””â”€ preprocess (vllm/inputs/preprocess.py:689)
    â””â”€ _process_decoder_only_prompt (vllm/inputs/preprocess.py:644)
      â””â”€ _tokenize_prompt (vllm/inputs/preprocess.py:229)
        â””â”€ _batch_encode_plus (transformers/tokenization_utils_fast.py:553)
            [49 samples, 55.06%]  â† Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰å‡¦ç†

clone (vllm/sampling_params.py:553)
  â””â”€ deepcopy (copy.py:162)
    â””â”€ _reconstruct (copy.py:253)
      â””â”€ __post_init__ (vllm/sampling_params.py:351)
          [13 samples, 14.61%]  â† SamplingParamsã®æ·±å±¤ã‚³ãƒ”ãƒ¼

add_request (vllm/v1/engine/llm_engine.py:265)
  â””â”€ add_request (vllm/v1/engine/core_client.py:750)
    â””â”€ _send_input (vllm/v1/engine/core_client.py:726)
      â””â”€ send_multipart (zmq/sugar/socket.py:749)
          [8 samples, 8.99%]  â† ãƒ—ãƒ­ã‚»ã‚¹é–“é€šä¿¡
```

**é‡è¦ãªçŸ¥è¦‹:**

1. **ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ãŒæœ€å¤§ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ (62.92%)**
   - Transformers ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã® `_batch_encode_plus` ãŒ Worker Process ã® Python ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã®å¤§éƒ¨åˆ†
   - ã“ã‚Œã¯ vLLM æœ¬ä½“ã§ã¯ãªãã€ä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®å‡¦ç†æ™‚é–“
   - æœ€é©åŒ–ã®ä½™åœ°: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³çµæœã®ã‚­ãƒ£ãƒƒã‚·ãƒ³ã‚°ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã®èª¿æ•´

2. **SamplingParams ã®æ·±å±¤ã‚³ãƒ”ãƒ¼ãŒæ„å¤–ã¨é‡ã„ (14.61%)**
   - å„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ `deepcopy()` ãŒå®Ÿè¡Œã•ã‚Œã‚‹
   - `__post_init__` ã§ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³å‡¦ç†ã‚‚å«ã¾ã‚Œã‚‹
   - æœ€é©åŒ–ã®ä½™åœ°: Shallow copy ã¾ãŸã¯å‚ç…§æ¸¡ã—ã¸ã®å¤‰æ›´æ¤œè¨

3. **ãƒ—ãƒ­ã‚»ã‚¹é–“é€šä¿¡ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ (13%)**
   - ZMQ ã‚’ä½¿ç”¨ã—ãŸ Main Process â†” Worker Process é€šä¿¡
   - ã“ã‚Œã¯ vLLM v1 ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆã«ã‚ˆã‚‹ã‚‚ã®
   - æœ€é©åŒ–ã®ä½™åœ°: ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¤§ããã—ã¦é€šä¿¡é »åº¦ã‚’å‰Šæ¸›

**py-spy ã®é™ç•Œã¨é©ç”¨ç¯„å›²:**

âœ… **è¨ˆæ¸¬ã§ãã‚‹å†…å®¹ï¼ˆPythonå±¤ï¼‰:**
- Python é–¢æ•°ã®å®Ÿè¡Œæ™‚é–“
- é–¢æ•°å‘¼ã³å‡ºã—ã®éšå±¤æ§‹é€ 
- CPU ãŒã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªæ™‚é–“

âŒ **è¨ˆæ¸¬ã§ããªã„å†…å®¹ï¼ˆNeuronCoreå±¤ï¼‰:**
- NeuronCore ã§ã®å®Ÿéš›ã®æ¨è«–å®Ÿè¡Œæ™‚é–“
- ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã•ã‚ŒãŸå…¥åŠ›ï¼ˆ97â†’128 tokensï¼‰ã®å‡¦ç†æ™‚é–“
- ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢å±¤ã§ã®è¡Œåˆ—æ¼”ç®—

**Phase 5 ã¨ã®æ•´åˆæ€§:**

Phase 5 ã¨ py-spy v2 ã§ã¯æ¸¬å®šæ¡ä»¶ãŒç•°ãªã‚‹ãŸã‚ã€çµ¶å¯¾å€¤ã«å·®ãŒå‡ºã¦ã„ã¾ã™:

| æ¸¬å®šæ–¹æ³• | Bucketing ON | ãƒãƒƒãƒæ•° | Warmup | æ¸¬å®šæ–¹æ³• |
|---------|-------------|---------|--------|---------|
| Phase 5 (vLLMè©³ç´°ãƒ­ã‚°) | 82.13 ms | 10 | ãªã— | vLLMãƒ­ã‚° |
| py-spy v2 (æ¨è«–å°‚ç”¨) | 59.20 ms | 500 | ã‚ã‚Š | å®Ÿæ¸¬æ™‚é–“ |

å·®åˆ†ï¼ˆ22.93msï¼‰ã®ç†ç”±:
1. **ãƒãƒƒãƒæ•°ã®é•ã„**: Phase 5ã¯10ãƒãƒƒãƒã€py-spy v2ã¯500ãƒãƒƒãƒï¼ˆã‚ˆã‚Šå®‰å®šï¼‰
2. **Warmup**: py-spy v2ã¯æ˜ç¤ºçš„ã«warmupå®Ÿæ–½
3. **æ¸¬å®šæ™‚åˆ»**: ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥çŠ¶æ…‹ãªã©ã®é•ã„

é‡è¦ãªã®ã¯ã€**Bucketing ã®åŠ¹æœï¼ˆæ¯”ç‡ï¼‰** ãŒä¸€è²«ã—ã¦ã„ã‚‹ã“ã¨ã§ã™ï¼ˆPhase 5: 4.98xã€py-spy v2: 4.28xï¼‰ã€‚

**py-spy ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã«ã¤ã„ã¦:**

py-spy ã¯ Pythoné–¢æ•°ã®å®Ÿè¡Œã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¾ã™ã€‚**ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ï¼ˆ59.20msï¼‰ã¯å®Ÿæ¸¬å€¤**ã§ã™ãŒã€**ã‚µãƒ³ãƒ—ãƒ«æ•°ï¼ˆ89ï¼‰ã¯ Python ãŒã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã ã£ãŸå›æ•°**ã‚’ç¤ºã—ã¾ã™ã€‚NeuronCore å®Ÿè¡Œä¸­ã¯ Python ãŒãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¯¾è±¡å¤–ã¨ãªã‚Šã¾ã™ã€‚

---

**3ã¤ã®æ‰‹æ³•ã®å½¹å‰²åˆ†æ‹…:**

```
Phase 5 (vLLM è©³ç´°ãƒ­ã‚°):
  â†’ HLOç”Ÿæˆã€ãƒã‚±ãƒƒãƒˆé¸æŠã‚’å¯è¦–åŒ– â†’ æ ¹æœ¬åŸå› ï¼ˆãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼‰ã‚’ç‰¹å®š âœ…

Phase 6a (neuron-profile):
  â†’ NeuronCore ãƒ¬ãƒ™ãƒ«ã§æ¤œè¨¼ â†’ ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã§ã®è¨ˆç®—é‡ã‚’æ¸¬å®š ğŸ”„

Phase 6b (py-spy):
  â†’ Python é–¢æ•°ãƒ¬ãƒ™ãƒ«ã§æ¤œè¨¼ â†’ Worker Process ã®å†…è¨³ã‚’å¯è¦–åŒ– âœ…
```

#### âœ… py-spy æ¨è«–å°‚ç”¨ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ï¼ˆå®Œäº†ï¼‰

åˆå›ã® py-spy å®Ÿè¡Œã§ã¯åˆæœŸåŒ–ãƒ•ã‚§ãƒ¼ã‚ºãŒ80%ã‚’å ã‚ã¦ã„ãŸãŸã‚ã€æ¨è«–ãƒ•ã‚§ãƒ¼ã‚ºã®ã¿ã‚’ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã™ã‚‹å°‚ç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆã—ã¾ã—ãŸã€‚

**å®Ÿè¡Œçµæœ:**
- 500ãƒãƒƒãƒã®æ¨è«–: 29.6ç§’å®Œäº†
- å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼: **59.20 ms/batch** (Bucketing ON)
- ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: 89 samples (40ç§’ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°)

**Worker Process Pythonå±¤ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰å†…è¨³:**

| å‡¦ç† | æ™‚é–“ | å‰²åˆ | èª¬æ˜ |
|------|------|------|------|
| ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ | 37ms | 62.92% | `_batch_encode_plus`ã€`_tokenize_prompt` |
| SamplingParams ã‚³ãƒ”ãƒ¼ | 8.6ms | 14.61% | `clone()` ã§ã® deepcopy å‡¦ç† |
| ãƒªã‚¯ã‚¨ã‚¹ãƒˆç®¡ç† | 7.7ms | 13% | `add_request`ã€ZMQ é€šä¿¡ |
| ãã®ä»– | 5.9ms | 10% | ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã€ãƒ‡ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ç­‰ |

**é‡è¦ãªçŸ¥è¦‹:**

1. **ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ãŒæœ€å¤§ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯**
   - Worker Process ã® Python ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã® 62.92% ã‚’å ã‚ã‚‹
   - Transformers ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã® `_batch_encode_plus` ãŒä¸»ãªå‡¦ç†æ™‚é–“

2. **SamplingParams ã®æ·±å±¤ã‚³ãƒ”ãƒ¼ãŒæ„å¤–ã¨é‡ã„**
   - å„ãƒªã‚¯ã‚¨ã‚¹ãƒˆã§ `deepcopy()` ãŒå®Ÿè¡Œã•ã‚Œã‚‹
   - 14.61% ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ï¼ˆæœ€é©åŒ–ã®ä½™åœ°ã‚ã‚Šï¼‰

3. **py-spy ã®é™ç•Œ**
   - Python ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ã®ãŸã‚ã€**NeuronCore ã§ã®å®Ÿè¡Œæ™‚é–“ã¯è¨ˆæ¸¬ã•ã‚Œãªã„**
   - Python CPU ãŒã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªæ™‚é–“ï¼ˆ59.20msï¼‰ã®ã¿è¨ˆæ¸¬
   - NeuronCore å®Ÿè¡Œä¸­ã¯ Python ãŒãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã‚‹ãŸã‚ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°å¯¾è±¡å¤–
   - Phase 5 ã§æ¸¬å®šã—ãŸ 82ms/batch = Pythonå‡¦ç†(59ms) + NeuronCoreå®Ÿè¡Œ + å¾…æ©Ÿæ™‚é–“

4. **Worker Process ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰å¯è¦–åŒ–ã«æˆåŠŸ**
   - å½“åˆã®ç›®çš„ã§ã‚ã£ãŸã€ŒWorker Process ã® Python å±¤ã®å†…è¨³ã€ã‚’ç‰¹å®š
   - NeuronCore å±¤ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã¯ä¸è¦ï¼ˆPhase 5 ã§æ ¹æœ¬åŸå› ç‰¹å®šæ¸ˆã¿ï¼‰

**FlameGraph:**
`/tmp/inference_only_profile_v2.svg` (35KB)

---

#### âœ… py-spy Bucketing OFF ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ï¼ˆå®Œäº†ï¼‰

Bucketing ON ã¨ã®æ¯”è¼ƒã®ãŸã‚ã€åŒæ§˜ã®æ¸¬å®šã‚’ Bucketing OFF ã§ã‚‚å®Ÿæ–½ã—ã¾ã—ãŸã€‚

**æ¸¬å®šã‚¹ã‚¯ãƒªãƒ—ãƒˆ:**

```python:/tmp/profile_inference_only_v2_nobucket.py
vllm_config = {
    "tensor_parallel_size": 2,
    "max_num_seqs": 4,
    "block_size": 32,
    "max_model_len": 2048,
    "max_num_batched_tokens": 256,
    "num_gpu_blocks_override": 512,
    "enable_prefix_caching": False,
    "dtype": "bfloat16",
    "disable_log_stats": True,
    "additional_config": {
        "override_neuron_config": {
            "skip_warmup": True,
            "enable_bucketing": False,  # â† Bucketing OFF
            "pa_num_blocks": 512,
            "pa_block_size": 32
        }
    }
}

# åˆæœŸåŒ–å¾Œã€PIDãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ› â†’ 10ç§’å¾…æ©Ÿ â†’ 500ãƒãƒƒãƒæ¨è«–
# åˆ¥ãƒ—ãƒ­ã‚»ã‚¹ã§ sudo py-spy ãŒ40ç§’é–“ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
```

**å®Ÿè¡Œçµæœ:**
- 500ãƒãƒƒãƒã®æ¨è«–: 126.78ç§’å®Œäº†
- å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼: **253.57 ms/batch** (Bucketing OFF)
- ç·ã‚µãƒ³ãƒ—ãƒ«æ•°: 31 samples (40ç§’ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°)

**FlameGraph:**
`/tmp/inference_only_profile_v2_nobucket.svg` (32KB)

---

#### ğŸ“Š Bucketing ON vs OFF ã®æ¯”è¼ƒåˆ†æ

**å®Ÿæ¸¬ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼:**

| è¨­å®š | ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼ | ç·æ™‚é–“ (500ãƒãƒƒãƒ) | æ¯”ç‡ |
|------|-------------|-------------------|------|
| Bucketing ON | 59.20 ms/batch | 29.60ç§’ | 1.00x (åŸºæº–) |
| Bucketing OFF | 253.57 ms/batch | 126.78ç§’ | **4.28x** |

**py-spy ã‚µãƒ³ãƒ—ãƒ«æ•°ã®é€†è»¢:**

| è¨­å®š | ã‚µãƒ³ãƒ—ãƒ«æ•° | 1ã‚µãƒ³ãƒ—ãƒ«ã‚ãŸã‚Šã®æ™‚é–“ | èª¬æ˜ |
|------|-----------|---------------------|------|
| Bucketing ON | 89 samples | 665 Î¼s/sample | Python ãŒé »ç¹ã«ã‚¢ã‚¯ãƒ†ã‚£ãƒ– |
| Bucketing OFF | 31 samples | 8,180 Î¼s/sample | **Python ãŒé•·æ™‚é–“ãƒ–ãƒ­ãƒƒã‚¯** |

**Pythoné–¢æ•°ãƒ¬ãƒ™ãƒ«ã®æ¯”è¼ƒ:**

| é–¢æ•° | ON ã‚µãƒ³ãƒ—ãƒ« | ON å‰²åˆ | OFF ã‚µãƒ³ãƒ—ãƒ« | OFF å‰²åˆ | èª¬æ˜ |
|------|------------|---------|-------------|----------|------|
| `_tokenize_prompt` | 56 | 62.92% | 14 | 45.16% | ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ |
| `_batch_encode_plus` | 52 | 55.06% | 14 | 45.16% | ãƒãƒƒãƒã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ |
| `clone` (SamplingParams) | 13 | 14.61% | 4 | 12.90% | ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚³ãƒ”ãƒ¼ |
| `add_request` | 20 | 22.47% | 7 | 22.58% | ãƒªã‚¯ã‚¨ã‚¹ãƒˆç®¡ç† |

**é‡è¦ãªç™ºè¦‹:**

1. **ã‚µãƒ³ãƒ—ãƒ«æ•°ãŒ1/3ã«æ¸›å°‘ï¼ˆ89 â†’ 31ï¼‰**
   - py-spy ã¯ Python CPU ãŒã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªæ™‚ã®ã¿ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
   - Bucketing OFF ã§ã¯ NeuronCore å®Ÿè¡Œæ™‚é–“ãŒåœ§å€’çš„ã«é•·ã„ãŸã‚ã€Python ãŒãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¦ã„ã‚‹æ™‚é–“ãŒå¢—åŠ 
   - 1ã‚µãƒ³ãƒ—ãƒ«ã‚ãŸã‚Šã®æ™‚é–“ãŒ **12.3å€é•·ã„**ï¼ˆ665Î¼s â†’ 8,180Î¼sï¼‰

2. **Pythonå±¤ã®å‡¦ç†æ™‚é–“ã¯ã»ã¼åŒã˜**
   - ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆç®¡ç†ãªã©ã® Python å‡¦ç†ã¯åŒç¨‹åº¦
   - ã‚µãƒ³ãƒ—ãƒ«æ•°ã¯æ¸›å°‘ã—ã¦ã„ã‚‹ãŒã€å‰²åˆã¯é¡ä¼¼ï¼ˆ62.92% â†’ 45.16%ï¼‰
   - Pythonå±¤ã®çµ¶å¯¾æ™‚é–“: ç´„60msï¼ˆONï¼‰ã€ç´„60msï¼ˆOFFï¼‰â† ã»ã¼åŒã˜

3. **4.28å€ã®å·® = NeuronCoreå±¤ã®å®Ÿè¡Œæ™‚é–“å·®**
   ```
   Bucketing ON:  Pythonå‡¦ç†(60ms) + NeuronCoreå®Ÿè¡Œ(çŸ­æ™‚é–“)     = 59.20ms
   Bucketing OFF: Pythonå‡¦ç†(60ms) + NeuronCoreå®Ÿè¡Œ(194msé•·ã„) = 253.57ms
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                      ã“ã‚ŒãŒãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰
   ```

4. **py-spy ã®ã‚µãƒ³ãƒ—ãƒ«æ•°æ¸›å°‘ãŒ NeuronCore å®Ÿè¡Œæ™‚é–“ã®å¢—åŠ ã‚’é–“æ¥çš„ã«è¨¼æ˜**
   - Python ãŒãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¦ã„ã‚‹æ™‚é–“ = NeuronCore ãŒå®Ÿè¡Œã—ã¦ã„ã‚‹æ™‚é–“
   - Bucketing OFF ã§ã¯ã“ã®æ™‚é–“ãŒç´„4å€é•·ã„
   - Phase 5 ã§ç‰¹å®šã—ãŸã€Œå…¥åŠ›ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãŒåŸå› ã€ã¨ã„ã†ä»®èª¬ã‚’è£ä»˜ã‘

**Phase 5 ã¨ã®æ•´åˆæ€§:**

| æ¸¬å®šæ–¹æ³• | Bucketing ON | Bucketing OFF | æ¯”ç‡ | æ¸¬å®šæ¡ä»¶ |
|---------|-------------|--------------|------|---------|
| Phase 5 (vLLMè©³ç´°ãƒ­ã‚°) | 82.13 ms | 409 ms | 4.98x | 10ãƒãƒƒãƒå¹³å‡ |
| py-spy v2 (æ¨è«–å°‚ç”¨) | 59.20 ms | 253.57 ms | 4.28x | 500ãƒãƒƒãƒå¹³å‡ |

- **æ¯”ç‡ã®ä¸€è‡´**: 4.98x vs 4.28xï¼ˆç´„14%ã®å·®ï¼‰
- **Bucketing ã®åŠ¹æœ**: ä¸€è²«ã—ã¦ç´„4-5å€ã®æ€§èƒ½å·®
- **çµ¶å¯¾å€¤ã®å·®ç•°**: æ¸¬å®šæ¡ä»¶ï¼ˆãƒãƒƒãƒæ•°ã€warmupã€ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹ï¼‰ã®é•ã„

**çµè«–:**

Worker Process ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‹ã‚‰ä»¥ä¸‹ãŒæ˜ç¢ºã«ãªã‚Šã¾ã—ãŸ:

1. **Pythonå±¤ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã¯ Bucketing ON/OFF ã§ã»ã¼åŒã˜**ï¼ˆç´„60msï¼‰
2. **æ€§èƒ½å·®ã®åŸå› ã¯ NeuronCoreå±¤ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç†æ™‚é–“**ï¼ˆ194mså·®ï¼‰
3. **py-spy ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ã®å¤‰åŒ–ãŒã€ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢å±¤ã®å®Ÿè¡Œæ™‚é–“ã‚’é–“æ¥çš„ã«ç¤ºã™**

Phase 5 ã§ç‰¹å®šã—ãŸæ ¹æœ¬åŸå› ï¼ˆå…¥åŠ›ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°: 97â†’128 tokens vs 97â†’2048 tokensï¼‰ã‚’ã€Worker Process ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‹ã‚‰å¤šè§’çš„ã«æ¤œè¨¼ã§ãã¾ã—ãŸã€‚

---

**ç¾æ™‚ç‚¹ã®çµè«–:**
Phase 5 ã§å®Ÿç”¨çš„ãªçµè«–ã¯å‡ºã¦ã„ã¾ã™ã€‚Phase 6b ã§ Worker Process ã® Python ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã®å†…è¨³ã‚’å¯è¦–åŒ–ã—ã€Bucketing ON/OFF ã§ã®å¤‰åŒ–ã‚’ç¢ºèªã™ã‚‹ã“ã¨ã§ã€Pythonå±¤ã¨NeuronCoreå±¤ã®åˆ‡ã‚Šåˆ†ã‘ã«æˆåŠŸã—ã¾ã—ãŸã€‚Phase 6 ã¯ã€Œå­¦è¡“çš„èˆˆå‘³ã€ã‚„ã€Œã•ã‚‰ãªã‚‹æœ€é©åŒ–ã®å¯èƒ½æ€§ã€ã®ãŸã‚ã®è¿½åŠ èª¿æŸ»ã§ã™ã€‚

---

---
## ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã®çŸ¥è¦‹

ä»Šå›ã®èª¿æŸ»ã‹ã‚‰å¾—ã‚‰ã‚ŒãŸãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã™ã‚‹éš›ã®çŸ¥è¦‹ã‚’ã¾ã¨ã‚ã¾ã™ã€‚ä»Šå¾Œã‚‚ã£ã¨æ›¸ç±ãªã©ã‚’èª­ã‚“ã§å‹‰å¼·ã—ã‚ˆã†ã¨æ€ã£ã¦ãŠã‚Šã€ç¾æ™‚ç‚¹ã§ã®çŸ¥è¦‹ã¨ã„ã†ã“ã¨ã§ã‚ˆã‘ã‚Œã°å‚è€ƒã«ã—ã¦ãã ã•ã„ã€‚

### 1: ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã«æ¸¬å®šã™ã‚‹

è¤‡é›‘ãªã‚·ã‚¹ãƒ†ãƒ ã¯è¤‡æ•°ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã«ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ„ãƒ¼ãƒ«ãŒç•°ãªã‚‹ãŸã‚å¿…è¦ã«å¿œã˜ã¦å€‹åˆ¥ã«æ¸¬å®šã—ã€å·®åˆ†ã‚’å–ã‚‹ã“ã¨ã§ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã®æ‰€åœ¨ã‚’ç‰¹å®šã§ãã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚

```
L1: ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒ¬ãƒ™ãƒ«
L2: SDK ãƒ¬ãƒ™ãƒ«
L3: ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãƒ¬ãƒ™ãƒ«
L4: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¬ãƒ™ãƒ«
```

**ä¾‹**: ä»Šå›ã¯ L2 (NxD) ã‚’æ¸¬å®šã—ã¦ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ç¢ºç«‹ã—ãŸã“ã¨ã§ã€L3 (vLLM) ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’å®šé‡åŒ–ã§ãã¾ã—ãŸã€‚

### 2: ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‹ã‚‰å§‹ã‚ã‚‹

æœ€ã‚‚å˜ç´”ãªå®Ÿè£…ã‚’ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã—ã¦æ¸¬å®šã—ã¦ã‹ã‚‰ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’è¿½åŠ ã—ã¦ã„ãã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ã‚³ã‚¹ãƒˆã‚’æ˜ç¢ºã«ã§ãã¾ã™ã€‚

**ä¾‹**:
1. NxD Inference ç›´æ¥ä½¿ç”¨ï¼ˆæœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ï¼‰â†’ 11.97ms
2. vLLM çµŒç”±ï¼ˆè¤‡é›‘ã•è¿½åŠ ï¼‰â†’ 64.33ms
3. å·®åˆ† = vLLM ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ = 52.36ms

ã‚ˆã‚Šå³å¯†ã«ã¯æ§˜ã€…ãªå…¥åŠ›ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å…¥ã‚ŒãŸä¸Šã§çµ±è¨ˆçš„ãªæœ‰æ„æ€§ã‚„ä¿¡é ¼åŒºé–“ã‚’å°å‡ºã™ã¹ãã§ã—ã‚‡ã†ã€‚

### 3: æ¸¬å®šãƒ„ãƒ¼ãƒ«ã®é™ç•Œã‚’ç†è§£ã™ã‚‹

å„ãƒ„ãƒ¼ãƒ«ã«ã¯å›ºæœ‰ã®é™ç•ŒãŒã‚ã‚Šã¾ã™ã€‚å˜ä¸€ãƒ„ãƒ¼ãƒ«ã ã‘ã§ã¯å…¨ä½“åƒã‚’æŠŠæ¡ã§ããªã„ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚

**ä¾‹**: ä»Šå›ã¯ Neuron Profiler ã ã‘ã§ã¯ vLLM ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’æ¸¬å®šã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚

### 4: æ¸¬å®šæ¡ä»¶ã®å½±éŸ¿ã‚’è€ƒæ…®ã™ã‚‹

æ¸¬å®šæ¡ä»¶ãŒçµæœã«å¤§ããå½±éŸ¿ã—ã¾ã™ã€‚å®Ÿéš›ã®ä½¿ç”¨æ¡ä»¶ã«è¿‘ã„ç’°å¢ƒã§æ¸¬å®šã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚

**ä¾‹**: ä»¥ä¸‹ã®æ¡ä»¶ä¸‹ã§ã¯ bucketing=OFF ãŒæœ€é€Ÿã¨ãªã‚‹ç¾å®Ÿã‚±ãƒ¼ã‚¹ã¨ã®ã‚®ãƒ£ãƒƒãƒ—ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ã€‚

```python
# å…¨ã¦åŒã˜ 97 ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
batch_prompts = [prompt_97tokens] * 4
```

æ¸¬å®šæ¡ä»¶ã¨å®Ÿç’°å¢ƒã®ã‚®ãƒ£ãƒƒãƒ—ã‚’èªè­˜ã—ã€çµæœã®é©ç”¨ç¯„å›²ã‚’æ˜ç¤ºã™ã‚‹ã¹ãã§ã—ã‚‡ã†ã€‚

### 5: äºˆæƒ³å¤–ã®çµæœã‚’æ˜ã‚Šä¸‹ã’ã‚‹

äºˆæƒ³ã¨ç•°ãªã‚‹çµæœã¯ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã—ã¦æ˜ã‚Šä¸‹ã’ã‚‹ã¹ãã§ã™ã€‚

**ä¾‹**: ä»Šå›ã¯ bucketing=ON ã§ vLLM ã¯é«˜é€ŸåŒ–ã—ãŸãŒ NxD Inference ã¯æ‚ªåŒ–ã—ãŸã€‚ã“ã‚Œã¯ã©ã†ã„ã†æ™‚ã«èµ·ã“ã‚‹ã®ã‹ã‚’æ˜ç¢ºã«ã™ã‚‹ã€‚

### 6: ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’æ˜ç¢ºã«ã™ã‚‹

æ€§èƒ½ã ã‘ã§ãªãã€æ©Ÿèƒ½ã¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’å®šé‡åŒ–ã—ã¾ã™ã€‚

**ä¾‹**: æ€§èƒ½ãŒä¸ŠãŒã‚‹ã®ã§ã‚ã‚Œã°æ¯å›ã‚«ã‚¹ã‚¿ãƒ ã‚«ãƒ¼ãƒãƒ«ã‚’å€‹åˆ¥ãƒ¢ãƒ‡ãƒ«ã®ãŸã‚ã«å®Ÿè£…ã™ã¹ãã‹ï¼Ÿ

## ã¾ã¨ã‚

1. **vLLM ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã®å†å®šç¾©**ï¼ˆreranker ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã€å‰å›ã® Zenn æœ€é©å€¤è¨­å®šï¼‰
 - vLLM (bucketing ON): 64.33 ms/batch
 - NxD (bucketing OFF): 11.97 ms/batch
 - å·®åˆ†: +52.36 ms (81%)
 - **Phase 4 ã§åˆ¤æ˜ã—ãŸçœŸå®Ÿ**: ã“ã®å·®åˆ†ã«ã¯ã€Œ**vLLM Worker Process å†…ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰**ã€ãŒå«ã¾ã‚Œã¦ã„ã‚‹
 - Worker Process å†…éƒ¨: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã€ãƒ¡ãƒ¢ãƒªç®¡ç†ã€NeuronCore å…¥å‡ºåŠ›å‡¦ç†
 - Main Process ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰: ã»ã¼ã‚¼ãƒ­ï¼ˆZMQ çµŒç”±ã§ Worker ã‚’å¾…æ©Ÿã—ã¦ã„ã‚‹ã ã‘ï¼‰
 - **vLLM ã®è¨ˆæ¸¬æ™‚é–“ = Worker Process + NxD Inference**

2. **bucketing è¨­å®šã®å½±éŸ¿ï¼ˆNxD Inference ç›´æ¥ä½¿ç”¨æ™‚ï¼‰**
 - **å›ºå®šé•·ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ï¼ˆ97ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰**: bucketing OFF: 11.97 ms vs ON: 12.02 msï¼ˆ+0.4%ã€ã»ã¼åŒç­‰ï¼‰
 - **å¯å¤‰é•·ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ï¼ˆ18-125ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰**: bucketing OFF: 17.18 ms vs ON: 13.87 msï¼ˆ**-19.3%æ”¹å–„**ï¼‰
 - NxD Inference ã§ã¯å›ºå®šé•·ã§ bucketing ã®ãƒ¡ãƒªãƒƒãƒˆãŒãªãã€ã‚ãšã‹ãªã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã®ã¿
 - å¯å¤‰é•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã¯ã€bucketing ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°æœ€é©åŒ–ãŒåŠ¹æœã‚’ç™ºæ®

3. **bucketing è¨­å®šã®å½±éŸ¿ï¼ˆvLLM ä½¿ç”¨æ™‚ï¼‰** âš ï¸
 - **Phase 4 ã§åˆ¤æ˜**: **vLLM ã§ã¯ bucketing OFF ãŒåŠ‡çš„ã«é…ã„**
 - vLLM (bucketing ON): 82.13 ms/batchï¼ˆ10 HLO: 128/256/512/1024/2048ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰
 - vLLM (bucketing OFF): 408.72 ms/batchï¼ˆ**5.0å€é…ã„**ã€2 HLO: 2048ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ï¼‰
 - **æ ¹æœ¬åŸå› åˆ¤æ˜**: **å…¥åŠ›ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰**
   - 97ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ 2048 ãƒˆãƒ¼ã‚¯ãƒ³ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆ**21å€ã®ç„¡é§„ãªè¨ˆç®—**ï¼‰
   - Bucketing ON: 128ãƒˆãƒ¼ã‚¯ãƒ³ãƒã‚±ãƒƒãƒˆä½¿ç”¨ï¼ˆ31ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã®ã¿ï¼‰
   - Bucketing OFF: 2048ãƒˆãƒ¼ã‚¯ãƒ³ãƒã‚±ãƒƒãƒˆã®ã¿ï¼ˆ1951ãƒˆãƒ¼ã‚¯ãƒ³ã®ç„¡é§„ãªãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼‰
 - **åˆæœŸåŒ–æ™‚é–“ã®é€†è»¢ç¾è±¡**: Bucketing OFF ã®æ–¹ãŒ 60% é«˜é€Ÿï¼ˆ152ç§’ vs 385ç§’ï¼‰
   - Bucketing OFF: 2 HLO ã®ã¿ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ï¼ˆå„ªå…ˆ HLO 96.8ç§’ + æ®‹ã‚Š 18.5ç§’ï¼‰
   - Bucketing ON: 10 HLO ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ï¼ˆå„ªå…ˆ HLO 82.0ç§’ + æ®‹ã‚Š 249.1ç§’ï¼‰
 - **ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—åŠ¹æœ**: Bucketing OFF ã¯å®Ÿè¡Œå›æ•°ã§æ”¹å–„ï¼ˆ475ms â†’ 321msã€-32%ï¼‰
 - **æ¨å¥¨**: vLLM-Neuron ã§ã¯ **å¿…ãš bucketing ON ã‚’ä½¿ç”¨ã™ã¹ã**ï¼ˆåˆæœŸåŒ–ã¯é…ã„ãŒæ¨è«–ã¯ 5å€é«˜é€Ÿï¼‰

4. **Prefix Caching ã®æ­£ã—ã„ä½¿ã„æ–¹**
 - âŒ ãƒ•ãƒ©ã‚°å: `--enable-variable-length-prefill`ï¼ˆå­˜åœ¨ã—ãªã„ï¼‰
 - âœ… ãƒ•ãƒ©ã‚°å: `--enable-prefix-caching`ï¼ˆæ­£ã—ã„ï¼‰
 - inference_demo.py ã§ã®ç›´æ¥ä½¿ç”¨ã¯å›°é›£ï¼ˆTypeErrorç™ºç”Ÿï¼‰
 - **vLLM çµŒç”±ã§ã®ä½¿ç”¨ãŒæ¨å¥¨**ï¼ˆNeuron 2.24.0ä»¥é™ï¼‰
 - vLLM å´ã®ç‹¬è‡ªå®Ÿè£…ã¯å®Œå…¨ã«ãƒã‚¤ãƒ‘ã‚¹ã•ã‚Œã€Neuron å´ã§å®Ÿè¡Œ
 - vLLM ãŒè‡ªå‹•çš„ã« `is_block_kv_layout`ã€`is_prefix_caching`ã€`is_continuous_batching` ã‚’è¨­å®š

5. **vLLM-Neuron ã®å†…éƒ¨å®Ÿè£…ã®ç™ºè¦‹**
 - `/work/vllm-neuron/vllm_neuron/worker/neuronx_distributed_model_loader.py:782-786`
 - `enable_prefix_caching=True` ã§è¤‡æ•°ã®è¨­å®šãŒè‡ªå‹•çš„ã«æœ‰åŠ¹åŒ–ã•ã‚Œã‚‹
 - NxD Inference ã¸ã®è¨­å®šå€¤ã®æ©‹æ¸¡ã—ãƒ­ã‚¸ãƒƒã‚¯ãŒæ˜ç¢ºã«

6. **æ¸¬å®šã®é›£ã—ã•ã¨Phaseåˆ¥ã®ç™ºè¦‹**:
 - **Phase 1-3**: Neuron Profilerã€NxD Inferenceã€vLLM ã®åŸºç¤æ¸¬å®š
 - **Phase 4**: line_profiler ã§ Worker Process ãŒãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¨åˆ¤æ˜ï¼ˆå†…éƒ¨ã¯è¨ˆæ¸¬ä¸å¯ï¼‰
 - **Phase 5**: vLLM è©³ç´°ãƒ­ã‚°ã§**æ ¹æœ¬åŸå› ã‚’ç‰¹å®š**ï¼ˆå…¥åŠ›ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ï¼‰
 - **Phase 6b**: py-spy ã§ Worker Process ã® Pythonå±¤ã‚’å¯è¦–åŒ–ï¼ˆãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ 62.92%ï¼‰
 - Neuron Profiler ã¯ graph variation ã‚’æ¸¬å®šã™ã‚‹ãŸã‚ã€ãƒãƒƒãƒå‡¦ç†æ™‚é–“ã‚’ç›´æ¥æ¸¬å®šã§ããªã„
 - **line_profiler ã¯ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ã®å£ã‚’è¶Šãˆã‚‰ã‚Œãªã„**ï¼ˆWorker Process å†…éƒ¨ã¯è¨ˆæ¸¬ä¸å¯ï¼‰
 - **vLLM è©³ç´°ãƒ­ã‚°ãŒæ±ºå®šæ‰“**: HLOç”Ÿæˆã€ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã€ãƒã‚±ãƒƒãƒˆé¸æŠã‚’å¯è¦–åŒ–
 - **py-spy ã§Pythonå±¤ã¨NeuronCoreå±¤ã‚’åˆ‡ã‚Šåˆ†ã‘**: ã‚µãƒ³ãƒ—ãƒ«æ•°ã®å¤‰åŒ–ã‹ã‚‰é–“æ¥çš„ã«æ¤œè¨¼
 - å˜ä¸€ãƒ„ãƒ¼ãƒ«ã§ã¯å…¨ä½“åƒã‚’æŠŠæ¡ã§ããªã„ â†’ è¤‡æ•°æ‰‹æ³•ã®çµ„ã¿åˆã‚ã›ãŒå¿…é ˆ

7. **æ•°å€¤ã®æ•´åˆæ€§**:
 - **ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³**: NxD Inference 11.97ms
 - **vLLM ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰**: 52-70msï¼ˆè¨­å®šã«ã‚ˆã‚Šå¤‰å‹•ï¼‰
   - Phase 3 (max_num_seqs=4): 64.33ms - 11.97ms = 52.36ms
   - Phase 5 (max_num_seqs=8): 82.13ms - 11.97ms = 70.16ms
 - **Bucketing åŠ¹æœï¼ˆæ¯”ç‡ï¼‰**: ä¸€è²«ã—ã¦ 4-5å€ã®æ€§èƒ½å·®
   - Phase 5 (max_num_seqs=8, 10ãƒãƒƒãƒ): 4.98x (82.13ms vs 408.72ms)
   - Phase 6b (max_num_seqs=4, 500ãƒãƒƒãƒ, warmup): 4.28x (59.20ms vs 253.57ms)
 - **è¨­å®šã«ã‚ˆã‚‹çµ¶å¯¾å€¤ã®é•ã„**: max_num_seqsã€ãƒãƒƒãƒæ•°ã€warmup ã®å½±éŸ¿
 - **Pythonå±¤ vs NeuronCoreå±¤** (Phase 6b):
   - Pythonå±¤: ç´„60msï¼ˆBucketing ON/OFF ã§åŒã˜ï¼‰
   - NeuronCoreå±¤å·®åˆ†: 194msï¼ˆãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰: 97â†’128 vs 97â†’2048 tokensï¼‰
 - æ¸¬å®šæ¡ä»¶ã®é•ã„ã«ã‚ˆã‚Šçµ¶å¯¾å€¤ã¯å¤‰å‹•ã™ã‚‹ãŒã€Bucketing ã®åŠ¹æœï¼ˆæ¯”ç‡ï¼‰ã¯ä¸€è²«ã—ã¦ãŠã‚Šä¿¡é ¼ã§ãã‚‹

### å‚è€ƒè³‡æ–™

- [Zenn è¨˜äº‹ - Inf2ã§ vLLM ã‚’å‹•ã‹ã™éš›ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°](https://zenn.dev/tosshi/articles/ef61e14fe73399)
- [NxD Inference Release Notes](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/neuronx-distributed-inference/neuronx-distributed-inference.html)
- [vLLM User Guide for NxD Inference](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/developer_guides/vllm-user-guide-v1.html)
- [AWS Neuron Profiler Guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/tools/neuron-sys-tools/neuron-profile-user-guide.html)
- [neuronx-distributed-inference GitHub](https://github.com/aws-neuron/neuronx-distributed-inference)