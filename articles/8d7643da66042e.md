---
title: "vllm-neuron ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ç·´ç¿’ã§å¾—ãŸçŸ¥è¦‹"
emoji: "ğŸ”"
type: "tech"
topics: ["vllm", "AWSNeuron", "Profiler", "Python", "æ€§èƒ½æœ€é©åŒ–"]
published: false
---

## ã¯ã˜ã‚ã«

[å‰å›ã®è¨˜äº‹](https://zenn.dev/tosshi/articles/d68bd091d1934d) ã§ã¯åˆ¥ã«ãªãã¦ã‚‚ä»Šå›ã®è¨˜äº‹è‡ªä½“ã¯æ›¸ã‘ã‚‹ã«ã‚‚é–¢ã‚ã‚‰ãšãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ„ãƒ¼ãƒ«ã®é–‹ç™ºã«ã»ã¼è¶£å‘³ã§é›†ä¸­ã—ã¦ã—ã¾ã—ãŸãŒã€ä»Šå›ã¯ï¼ˆçœŸé¢ç›®ã«ï¼‰ AWS Inferentia2 ä¸Šã§ vllm-neuron ã‚’ä½¿ç”¨ã—ãŸãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°åˆ†æã«ãƒãƒ£ãƒ¬ãƒ³ã‚¸ã—ã¾ã™ã€‚

æœ¬è¨˜äº‹ã§ã¯ã€3 ã¤ã®èª¿æŸ»ãƒ•ã‚§ãƒ¼ã‚ºã«åˆ†ã‘ã¦èª¬æ˜ã—ã¾ã™ã€‚ã¾ãš Phase 1 ã§ã¯ AWS Neuron Profiler ã«ã‚ˆã‚‹ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒ¬ãƒ™ãƒ«ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‚’è©¦è¡ŒéŒ¯èª¤ã—ã¾ã—ãŸã€‚æ¬¡ã« Phase 2 ã§ã¯ line_profiler ã«ã‚ˆã‚‹ Python ãƒ¬ãƒ™ãƒ«ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‚’å®Ÿæ–½ã—ã¾ã—ãŸã€‚æœ€å¾Œã« Phase 3 ã§ã¯ NxD Inference ã‚’ç›´æ¥ä½¿ç”¨ã—ãŸæ¸¬å®šã‚’è¡Œã„ã€vLLM ã¨ã®è©³ç´°æ¯”è¼ƒã‚’å®Ÿæ–½ã—ã¾ã—ãŸã€‚

æœ€çµ‚çš„ã«ã€vllm-neuronã€NeuronCore å´ã®å‡¦ç†ã®å®Ÿè¡Œæ™‚é–“ã€bucketing è¨­å®šã‚„ prefix caching ã®æŒ™å‹•ã«ã¤ã„ã¦ç†è§£ã‚’æ·±ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã—ãŸã€‚

:::message
**ä»Šå›ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã®é€²ã‚æ–¹ã¯çµæœçš„ã«ã‹ãªã‚Šé–“é•ã£ã¦ã„ã¾ã—ãŸï¼** è‰²ã€…è©¦è¡ŒéŒ¯èª¤ã—ãŸã‚“ã ãªã€ã¨æ€ã„ãªãŒã‚‰æœ¬è¨˜äº‹ã‚’èª­ã‚“ã§ãã ã•ã„ã€‚æœ€å¾Œã«ã©ã†ã„ã†æµã‚Œã«ã™ã¹ãã ã£ãŸã¨æ€ã†ã®ã‹æ•´ç†ã—ã¾ã—ãŸã€‚
:::

:::message alert
æœ¬è¨˜äº‹ã¯åˆå­¦è€…å‘ã‘ã§ã¯ãªã„ãŸã‚ã‚ã‚‹ç¨‹åº¦ LLM æ¨è«–ã®åŸºç¤çŸ¥è­˜ã€vLLM ã®åŸºç¤çŸ¥è­˜ã€AWS Neuron ã®åŸºç¤çŸ¥è­˜ãŒã‚ã‚‹ã“ã¨ãŒå‰æã§ã™ã€‚
:::

---

## Phase 1: AWS Neuron Profiler ã§ã®è©¦è¡ŒéŒ¯èª¤

### 1.1 ãªãœãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‹ã‚‰å§‹ã‚ã‚‹ã®ã‹

æ€§èƒ½æœ€é©åŒ–ã‚’è¡Œã†éš›ã€ã¾ãšç¾çŠ¶ã‚’æŠŠæ¡ã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯æ¸¬å®šã§ã¯æ€§èƒ½ã®çµæœã¯åˆ†ã‹ã‚Šã¾ã™ãŒã€æ€§èƒ½ã®ç†ç”±ã€ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®å ´æ‰€ã€ãã—ã¦æ”¹å–„ã®ä½™åœ°ã¯åˆ†ã‹ã‚Šã¾ã›ã‚“ã€‚

ä»¥ä¸‹ã«å®Ÿé¨“ç’°å¢ƒã¨è¨­å®šæƒ…å ±ã‚’ã¾ã¨ã‚ã¦ãŠãã¾ã™ã€‚ä»¥å‰ã® Zenn è¨˜äº‹ã® P50 ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ãƒ¼æœ€é©å€¤ã‹ã‚‰ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‚’é–‹å§‹ã—ã¾ã™ã€‚

::::details å®Ÿé¨“ç’°å¢ƒã¨è¨­å®šæƒ…å ±

æœ¬èª¿æŸ»ã§ä½¿ç”¨ã—ãŸå®Ÿé¨“ç’°å¢ƒã¨è¨­å®šã®è©³ç´°ã‚’è¨˜è¼‰ã—ã¾ã™ã€‚

**ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ç’°å¢ƒ**:
- ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—: `inf2.xlarge`

**ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ãƒãƒ¼ã‚¸ãƒ§ãƒ³**:
- Neuron SDK: 2.27.x
- vLLM: 0.13.0ï¼ˆNeuron å¯¾å¿œç‰ˆï¼‰
- neuronx-distributed-inference (NxD Inference): 0.7.0
- Python: 3.12

**ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿**:
- ãƒ¢ãƒ‡ãƒ«: Qwen3-0.6B-Reranker
- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·: 97 ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆå›ºå®šé•·ï¼‰
- ã‚¿ã‚¹ã‚¯: Rerankerï¼ˆæ–‡æ›¸ãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼‰
- ãƒãƒƒãƒã‚µã‚¤ã‚º: 4

**vLLM è¨­å®šï¼ˆéå»ã® Zenn è¨˜äº‹ã®å®Ÿé¨“ã§ã®æœ€é©å€¤ï¼‰**:
```yaml
vllm:
  tensor_parallel_size: 2           # 2 NeuronCore ä½¿ç”¨
  max_num_seqs: 4                   # åŒæ™‚å‡¦ç†æ•°
  block_size: 32                    # KV cache block size
  max_model_len: 2048
  max_num_batched_tokens: 256
  num_gpu_blocks_override: 512
  enable_prefix_caching: false      # Phase 1-5 ã§ã¯ç„¡åŠ¹
  dtype: "bfloat16"

  additional_config:
    override_neuron_config:
      skip_warmup: True
      enable_bucketing: true        # å‹•çš„ãƒãƒƒãƒãƒ³ã‚°æœ‰åŠ¹
      pa_num_blocks: 512
      pa_block_size: 32
```

ã“ã‚Œã‚‰ã®è¨­å®šã¯ã€[å‰å›ã® Zenn è¨˜äº‹](https://zenn.dev/tosshi/articles/ef61e14fe73399) ã§æœ€é©åŒ–ã—ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚

**æ¸¬å®šã®ç„¦ç‚¹**:
æœ¬èª¿æŸ»ã§ã¯ã€ã“ã®ç‰¹å®šã®è¨­å®šã«ãŠã‘ã‚‹ vllm-neuron ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã€å¯å¤‰é•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€ã‚ˆã‚Šå¤§è¦æ¨¡ãªãƒãƒƒãƒã‚µã‚¤ã‚ºã§ã¯ã€ç•°ãªã‚‹æ€§èƒ½ç‰¹æ€§ã‚’ç¤ºã™å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

:::message
**NxD Inference ã¯ vllm-neuron ã§å†…éƒ¨çš„ã«æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã¨ã—ã¦åˆ©ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚å®Ÿæ…‹ã¨ã—ã¦ã¯ `override_neuron_config` ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒ NxD Inference ã«æ¸¡ã•ã‚Œã¦ã„ã‚‹å½¢ã§ã™ã€‚**
:::
::::

### 1.2 Perfetto ãƒˆãƒ¬ãƒ¼ã‚¹ã®åŸºæœ¬åˆ†æ

å‰å›ã‚‚å°‘ã—ç´¹ä»‹ã—ãŸ Perfetto ã«ã¤ã„ã¦ç´¹ä»‹ã—ã¾ã™ã€‚Perfetto ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã¯ SQLite ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦æ‰±ãˆã¾ã™ã€‚ã¾ãšä»¥ä¸‹ã®ã‚ˆã†ãªåˆ†æã‚¯ã‚¨ãƒªã‚’å®Ÿè¡Œã—ã¾ã—ãŸã€‚

:::details Perfetto ãƒˆãƒ¬ãƒ¼ã‚¹åˆ†æã‚³ãƒ¼ãƒ‰

```python
from perfetto.trace_processor import TraceProcessor
tp = TraceProcessor(trace='profile_output/trace.perfetto-trace')

# Operation ã”ã¨ã®é›†è¨ˆ
sql = """
SELECT name, COUNT(*) as count,
       SUM(dur) / 1e9 as total_seconds,
       AVG(dur) / 1e9 as avg_seconds
FROM slice WHERE dur > 0
GROUP BY name ORDER BY total_seconds DESC LIMIT 10
"""
```

**çµæœã®ä¸€éƒ¨**:
```
                  name   count total_seconds avg_seconds
0              unknown  156427      0.038387         0.0
1               MATMUL   21582      0.010941    0.000001
2 custom_call.17_sg0002      36      0.007028    0.000195
3            LDWEIGHTS   21212      0.004914         0.0
```

**ã‚¯ã‚¨ãƒªã®è¦‹æ–¹**:
`slice` ãƒ†ãƒ¼ãƒ–ãƒ«ã«ã¯å„ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®å®Ÿè¡Œè¨˜éŒ²ãŒæ ¼ç´ã•ã‚Œã¦ã„ã¾ã™ã€‚ã“ã®ã‚¯ã‚¨ãƒªã¯ä»¥ä¸‹ã‚’å–å¾—ã—ã¾ã™ã€‚
- `name`: ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³åï¼ˆMATMUL ãªã©ã€Neuron ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©ãŒç”Ÿæˆã—ãŸæ¼”ç®—ã®ç¨®é¡ï¼‰
- `count`: ãã®ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒå®Ÿè¡Œã•ã‚ŒãŸå›æ•°
- `dur`: å„å®Ÿè¡Œã®ç¶™ç¶šæ™‚é–“ï¼ˆãƒŠãƒç§’å˜ä½ã§è¨˜éŒ²ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€1e9 ã§å‰²ã£ã¦ç§’ã«å¤‰æ›ï¼‰
- `total_seconds`: ãã®ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆè¨ˆå®Ÿè¡Œæ™‚é–“ï¼ˆç§’å˜ä½ï¼‰
- `avg_seconds`: 1 å›ã‚ãŸã‚Šã®å¹³å‡å®Ÿè¡Œæ™‚é–“ï¼ˆç§’å˜ä½ï¼‰
:::

çµæœã¨ã—ã¦ã€ã¾ãšã€`custom_call.17_sg0002` ã¨ã„ã†æ“ä½œãŒãŸã£ãŸ 36 å›ã®å®Ÿè¡Œã§ 7ms ã‚‚æ¶ˆè²»ã—ã¦ã„ã‚‹ã“ã¨ãŒåˆ¤æ˜ã—ã¾ã—ãŸã€‚æ¬¡ã«ã€MATMUL ã¨ LDWEIGHTS ãŒã»ã¼åŒã˜å›æ•°å®Ÿè¡Œã•ã‚Œã¦ã„ã¾ã™ã€‚ã•ã‚‰ã«ã€`unknown` ã¨ã„ã†åˆ†é¡ã®æ“ä½œãŒ 38ms ã§æœ€å¤§ã®æ™‚é–“ã‚’æ¶ˆè²»ã—ã¦ã„ã¾ã—ãŸã€‚

`custom_call.17_sg0002`ã€‚ã€‚ã€‚ä½•ã“ã‚Œã€‚ã€‚ã€‚

:::details [ç™ºå±•çš„å†…å®¹] NEFF åˆ†æã«ã‚ˆã‚‹ custom_call ã®èª¿æŸ»

**ç–‘å•**: `custom_call.17_sg0002` ã¨ã¯ä½•ã‹ï¼ŸRoPEï¼Ÿæ´»æ€§åŒ–é–¢æ•°ï¼Ÿä½•ã‚‰ã‹ã®ã‚«ã‚¹ã‚¿ãƒ ã‚«ãƒ¼ãƒãƒ«ï¼Ÿ

Perfetto ãƒˆãƒ¬ãƒ¼ã‚¹ã§ã¯å®Ÿè¡Œå›æ•°ã¨æ™‚é–“ã—ã‹åˆ†ã‹ã‚‰ãªã„ãŸã‚ã€NEFF (Neuron Executable File Format) ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ [unpacking](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-runtime/explore/work-with-neff-files.html) ã—ã¦é™çš„ãªæ§‹é€ ã‚’èª¿æŸ»ã—ã¾ã—ãŸã€‚(uppack ã«ã¯ `neuron-packager unpack` ã‚³ãƒãƒ³ãƒ‰ã‚’åˆ©ç”¨ã—ã¦ã‚‚è‰¯ã„ã§ã™)

**NEFF ã‹ã‚‰åˆ¤æ˜ã—ãŸã“ã¨**:

```bash
# NEFF ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ unpacking
$ dd if=neff_322059935237836.neff of=neff.tar.gz bs=1024 skip=1
$ tar -xzf neff.tar.gz

# tensor_map.json ã‚’ç¢ºèª
$ cat sg00/tensor_map.json | jq '.["custom_call.17_sg0002"]'
{
  "dtype": "float32",
  "sim_shape": [256, 1, 1],
  "kind": null,
  "is_const": false,
  "layer_name": "custom_call.17"
}
```

**åˆ†ã‹ã‚‹ã“ã¨**:
- ãƒ‡ãƒ¼ã‚¿å‹: `float32`ï¼ˆç²¾åº¦é‡è¦–ã®æ¼”ç®—ï¼‰
- ãƒ†ãƒ³ã‚½ãƒ«å½¢çŠ¶: `[256, 1, 1]`ï¼ˆæ¯”è¼ƒçš„å°ã•ã„ï¼‰
- ã‚µãƒ–ã‚°ãƒ©ãƒ•: `sg0002`
- å‹•çš„ã«è¨ˆç®—ã•ã‚Œã‚‹ä¸­é–“ãƒ†ãƒ³ã‚½ãƒ«
- `custom_call.14` ï½ `17` ã®é€£ç¶šã—ãŸæ¼”ç®—ã‚·ãƒ¼ã‚±ãƒ³ã‚¹

**Qwen3 ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‹ã‚‰æ¨æ¸¬**:

å½¢çŠ¶ `[256, 1, 1]` ã¨å‘¨è¾ºã® `dot` (MATMUL) æ“ä½œã‹ã‚‰ã€ä»¥ä¸‹ã®ã‚ˆã†ãªæ“ä½œã¨æ¨æ¸¬
- **RoPE (Rotary Position Embedding)**: ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è¨ˆç®—
- **RMSNorm**: æ­£è¦åŒ–å±¤ã®çµ±è¨ˆå€¤è¨ˆç®—
- **ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ©Ÿæ§‹**: Softmax å‰ã®ä¸­é–“è¨ˆç®—

NEFF ã¯ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ã®é™çš„ãªæƒ…å ±ï¼ˆã‚°ãƒ©ãƒ•æ§‹é€ ã€ãƒ†ãƒ³ã‚½ãƒ«å½¢çŠ¶ã€ãƒ‡ãƒ¼ã‚¿å‹ï¼‰ã‚’å«ã¿ã¾ã™ãŒã€ä»¥ä¸‹ã¯åˆ¤æ˜ã—ãªã„ã‚ˆã†ã§ã™ã€‚
- å…·ä½“çš„ãªæ¼”ç®—ãƒ­ã‚¸ãƒƒã‚¯ï¼ˆã©ã®ã‚«ãƒ¼ãƒãƒ«ãŒå®Ÿè¡Œã•ã‚Œã‚‹ã‹ï¼‰
- å®Ÿè¡Œå›æ•°: 36 å›ï¼ˆPerfetto ãƒˆãƒ¬ãƒ¼ã‚¹ã§æ¸¬å®šå¯èƒ½ï¼‰
- å®Ÿè¡Œæ™‚é–“: 7msï¼ˆPerfetto ãƒˆãƒ¬ãƒ¼ã‚¹ã§æ¸¬å®šå¯èƒ½ï¼‰
- åˆå›å®Ÿè¡Œæ™‚ã®é…å»¶ï¼ˆskip_warmup ã«ã‚ˆã‚‹å‹•ä½œã®é•ã„ï¼‰

NEFF åˆ†æã‹ã‚‰ã¯ã€ä½•ãŒä½¿ã‚ã‚Œã¦ã„ã‚‹ã‹ã¯åˆ†ã‹ã‚Šã¾ã™ãŒã€ã©ã†å‹•ãã‹ã¯ Perfetto ãƒˆãƒ¬ãƒ¼ã‚¹ã§å®Ÿè¡Œæ™‚ã«æ¸¬å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ãã—ã¦ä¸Šè¨˜ä»¥ä¸Šã®è©³ç´°ãªç‰¹å®šã¯ç¾æ™‚ç‚¹ã§ã¯ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã€‚
:::

### 1.3 skip_warmup è¨­å®šã®å½±éŸ¿

è©³ç´°ã«è©¦è¡ŒéŒ¯èª¤ã—ãªãŒã‚‰å®Ÿè¡Œã—ãŸã‚¯ã‚¨ãƒªã‚’å…¨ã¦ç´¹ä»‹ã—ã¦ã„ã‚‹ã¨è†¨å¤§ã«ãªã£ã¦ã—ã¾ã†ãŸã‚å‰²æ„›ã—ã¾ã™ãŒ Phase 1 ã®æ™‚ç³»åˆ—ã®å‘½ä»¤å®Ÿè¡Œã«é–¢ã™ã‚‹èª¿æŸ»çµæœã‹ã‚‰ã€custom_call ãŒåˆå›å®Ÿè¡Œæ™‚ã«å¤§ããªé…å»¶ã‚’èµ·ã“ã—ã¦ã„ã‚‹ã“ã¨ãŒåˆ†ã‹ã£ãŸãŸã‚ã€NxD Inference ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã‚ã‚‹ `skip_warmup=False` ã‚’è©¦ã—ã¾ã—ãŸã€‚ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã¯ `False` ãªã®ã§ã™ãŒä»¥å‰ã®å®Ÿé¨“ã®è©¦è¡ŒéŒ¯èª¤ã§ `True` ã«ã—ã¦ã„ã¾ã—ãŸã€‚ã“ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å¾Œã« 1 å›ã® forward å®Ÿè¡Œã‚’è¡Œã„ã€é…å»¶åˆæœŸåŒ–ã‚’å®Œäº†ã•ã›ã‚‹ã¨ã„ã†ã‚‚ã®ã§ã™ã€‚
                                                                                                                  
| è¨­å®š | å¹³å‡æ™‚é–“ |
|------|---------|
| Baseline (skip_warmup=True) | 2.992ç§’ |
| Warmup (skip_warmup=False) | 3.110ç§’ (+3.9%) |

ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã™ã‚Œã°é€Ÿããªã‚‹ã¨äºˆæƒ³ã—ã¦ã„ã¾ã—ãŸãŒã€å®Ÿéš›ã«ã¯ç´„ 4% é…ããªã‚Šã¾ã—ãŸã€‚ï¼ˆæ¸¬å®šã®ãŸã³ã«çµæœã¯å¤šå°‘å¤‰å‹•ã—ã¾ã™ï¼‰å†åº¦ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã—ã¦ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®å¤‰åŒ–ã‚’ç¢ºèªã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚

:::details Operation æ•°ã®å¤‰åŒ–

```
Baseline (skip_warmup=True):
  MATMUL: 21,582å›, 10.94ms
  LDWEIGHTS: 21,212å›, 4.91ms
  ACTIVATE: 4,702å›, 1.65ms
  COPY: 83å›, 0.03ms

Warmup (skip_warmup=False):
  MATMUL: 13,497å› (-37%), 2.64ms (-76%)
  LDWEIGHTS: 13,497å› (-36%), 1.17ms (-76%)
  ACTIVATE: 4,207å› (-11%), 2.94ms (+78%)
  COPY: 554å› (+567%), 1.01ms (+3,267%)
```
:::

`skip_warmup=False` ã§ MATMUL/LDWEIGHTS ã®ä¸»è¦ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¯å®Ÿè¡Œæ™‚é–“ãŒ 76% æ¸›å°‘ã—ãŸã‚‚ã®ã®ã€ACTIVATE ã®å®Ÿè¡Œæ™‚é–“ãŒ +78%ã€COPY ã®å®Ÿè¡Œæ™‚é–“ãŒ +3,267% å¢—åŠ ã—ã€ãƒˆãƒ¼ã‚¿ãƒ«ã§ã¯é…ããªã‚Šã¾ã—ãŸã€‚


### 1.4 Neuron Profiler ã®æ¸¬å®šç¯„å›²ã®é™ç•Œ

:::message alert
ã“ã“ã§é‡è¦ãªæ°—ã¥ãï¼šNeuron Profiler ã®ãƒˆãƒ¬ãƒ¼ã‚¹æ™‚é–“ã¯ 16-17ms ãªã®ã«ã€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å…¨ä½“ã¯ç´„ 3 ç§’ã‹ã‹ã£ã¦ã„ã‚‹ã€‚**ã“ã® 16-17ms ã£ã¦ã©ã“ã‹ã‚‰ã©ã“ã¾ã§ã®ãªã‚“ã®å€¤ï¼Ÿ**
:::

èª¿æŸ»ã®çµæœã€Neuron Profiler ã®æ¸¬å®šç¯„å›²ã«é–¢ã™ã‚‹é‡è¦ãªç‰¹æ€§ãŒåˆ¤æ˜ã—ã¾ã—ãŸã€‚Neuron Profiler ã¯å®Ÿè¡Œæ™‚ã« NTFF (Neuron Trace File Format) ã¨ã„ã†ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã—ã€ãã‚Œã‚’ Perfetto ãƒˆãƒ¬ãƒ¼ã‚¹ã«å¤‰æ›ã—ã¾ã™ã€‚å„ NTFF ãƒ•ã‚¡ã‚¤ãƒ«ã¯ 1 ã¤ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ã‚°ãƒ©ãƒ•ã®å®Ÿè¡Œè¨˜éŒ²ã‚’è¡¨ã—ã¦ãŠã‚Šã€ç•°ãªã‚‹ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚„ sequence length ç”¨ã®è¤‡æ•°ã‚°ãƒ©ãƒ•ãŒå­˜åœ¨ã—ã¾ã™ã€‚

```bash
$ find profile_output -name "*.ntff" | wc -l
22  # 11ç¨®é¡ã®ã‚°ãƒ©ãƒ• Ã— 2ã‚³ã‚¢(tensor_parallel_size=2)

# NTFF ã¯ Perfetto ã«å¤‰æ›ã•ã‚Œã‚‹
$ ls profile_output/trace.perfetto-trace
trace.perfetto-trace  # ã“ã‚Œã‚’ TraceProcessor ã‚„ Perfetto UI ã§åˆ†æ
```

::::details [ç™ºå±•çš„å†…å®¹] NEFF ãƒ•ã‚¡ã‚¤ãƒ«ã¨ bucketing ã®é–¢ä¿‚

**NEFF (Neuron Executable File Format)** ã¯ã€NeuronCore ä¸Šã§å®Ÿè¡Œã•ã‚Œã‚‹ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ã‚°ãƒ©ãƒ•ã®å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«ã§ã™ã€‚bucketing ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã¨ã€è¤‡æ•°ã® (batch_size, sequence_length) ã®çµ„ã¿åˆã‚ã›ã«å¯¾å¿œã™ã‚‹è¤‡æ•°ã®ã‚°ãƒ©ãƒ•ãŒäº‹å‰ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã•ã‚Œã¾ã™ã€‚

```bash
# NEFF ãƒ•ã‚¡ã‚¤ãƒ«ã®åˆ†æ
$ find profile_output -name "*.neff" | wc -l
77  # è¤‡æ•°ã® PID ã‹ã‚‰ 11 ç¨®é¡ã®ã‚°ãƒ©ãƒ• Ã— è¤‡æ•°å›ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«

$ ls -lh profile_output/*/neff_*.neff | awk '{print $5}' | sort -u
801K   # Graph 1: æœ€å°ãƒã‚±ãƒƒãƒˆ
881K   # Graph 2
991K   # Graph 3
1.1M   # Graph 4
1.3M   # Graph 5
2.1M   # Graph 6
2.3M   # Graph 7
2.4M   # Graph 8
2.6M   # Graph 9
3.0M   # Graph 10
       # (åˆè¨ˆ 11 ç¨®é¡ã€124 MB)
```

**11 ã‚°ãƒ©ãƒ•ã®å®Ÿè¡Œæ™‚é–“åˆ†å¸ƒ**
```
Graph 1:  10.81 ms   Graph 2:  69.79 ms   Graph 3:  11.61 ms
Graph 4:  25.84 ms   Graph 5:  23.02 ms   Graph 6:  17.62 ms
Graph 7:  19.75 ms   Graph 8:  42.67 ms   Graph 9:  12.20 ms
Graph 10: 21.70 ms   Graph 11:  8.51 ms
```

**bucketing ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰**
1. **ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰**: 11 ã‚°ãƒ©ãƒ•ã‚’äº‹å‰ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ï¼ˆåˆå›èµ·å‹•æ™‚ï¼‰
2. **ãƒ¡ãƒ¢ãƒªã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰**: 124 MB ã® NEFF ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚°ãƒ©ãƒ•æ•°ãŒå¤šã„ã®ã§ãƒ¡ãƒ¢ãƒªã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒå¢—ãˆã‚‹ï¼‰
3. **é¸æŠã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰**: å®Ÿè¡Œæ™‚ã«å…¥åŠ›ã‚µã‚¤ã‚ºã«å¿œã˜ãŸæœ€é©ã‚°ãƒ©ãƒ•ã‚’é¸æŠ

**å›ºå®šé•·ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ï¼ˆ97 ãƒˆãƒ¼ã‚¯ãƒ³ Rerankerï¼‰ã®å ´åˆ**
- å®Ÿéš›ã«ä½¿ç”¨ã™ã‚‹ã®ã¯ 11 ã‚°ãƒ©ãƒ•ã®ã†ã¡ 1 ã¤ã ã‘
- æ®‹ã‚Š 10 ã‚°ãƒ©ãƒ•ã¯ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã‚‹ãŒä½¿ç”¨ã•ã‚Œãªã„
- bucketing=OFF ãªã‚‰ 1 ã‚°ãƒ©ãƒ•ã®ã¿ã‚³ãƒ³ãƒ‘ã‚¤ãƒ« â†’ ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãªã—

**å¯å¤‰é•·ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ï¼ˆä¸€èˆ¬çš„ãª LLM ã®ç”Ÿæˆï¼‰ã®å ´åˆ**
- ç•°ãªã‚‹é•·ã•ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒ 11 ã‚°ãƒ©ãƒ•ã«åˆ†æ•£
- å†ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ä¸è¦ã§åŠ¹ç‡çš„ãªãƒãƒƒãƒãƒ³ã‚°
- bucketing=ON ãŒé«˜é€ŸåŒ–ã«è²¢çŒ®

:::message
**ã“ã®ã‚ˆã†ã«ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã«ã‚ˆã£ã¦ bucketing ã®æ€§èƒ½ã¯ ON/OFF ã§ã©ã¡ã‚‰ãŒè‰¯ã„ã‹å¤‰å‹•ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã“ã¨ã‚’è¦šãˆã¦ãŠã„ã¦ãã ã•ã„ï¼**
:::
::::

:::message alert
**ä»Šå›ã®ç¬¬ä¸€ã®éã¡**: Neuron Profiler ã¯ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒ¬ãƒ™ãƒ«ã®åˆ†æã«ã¯æœ‰ç”¨ã ãŒã€vllm-neuron å…¨ä½“ã®æœ€é©åŒ–ã«ãŠã„ã¦åˆæ‰‹ã§ä½¿ã†ã‚‚ã®ã§ã¯ãªã„ã€‚
:::

ã‚«ã‚¹ã‚¿ãƒ ã‚«ãƒ¼ãƒãƒ«ã‚’å®Ÿè£…ã™ã‚‹ã‚ˆã†ãªã‚±ãƒ¼ã‚¹ã§ã¯å¿…é ˆã¨è¨€ãˆã¾ã™ãŒã€æœ€é©ãªè¨­å®šã‚’æ¢ã™éš›ã®åˆæ‰‹ã§å®Ÿæ–½ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã‚‚ã®ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ãã—ã¦ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒ¬ãƒ™ãƒ«ã®æ”¹å–„ã‚’ã™ã‚‹å‰ã« vllm-neuron å…¨ä½“ã®å®Ÿè¡Œæ™‚é–“ã®å†…ã®ã©ã®ç¨‹åº¦ã‚’ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢å´ã®æ¨è«–å‡¦ç†ãŒå ã‚ã¦ã„ã‚‹ã®ã‹ã«ã‚ˆã£ã¦æ”¹å–„ã®å„ªå…ˆåº¦ãŒå¤‰ã‚ã£ã¦ãã‚‹ã®ã§ vllm-neuron å…¨ä½“ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‚’ã—ãªã„ã¨æ„å‘³ãªã„ãã€ã¨æ€ã„ã¾ã—ãŸã€‚ã€‚ã¨ã¯ã„ãˆã€ä»Šå›å¾—ãŸ Neuron Profiler ã«é–¢ã™ã‚‹çŸ¥è¦‹ã¯æœ‰ç”¨ãªãŸã‚ã‚·ã‚§ã‚¢ã®æ„å‘³ã‚’è¾¼ã‚ã¦ Phase 1 ã‚’æ¶ˆã•ãšã«ãã®ã¾ã¾å…¬é–‹ã—ã¾ã™ã€‚

### 1.5 NEFFã€Perfetto ã¨ã¯

Phase 1 ã§ç™»å ´ã—ãŸãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¤ã„ã¦æ•´ç†ã—ã¾ã™ã€‚

:::message
NEFFï¼ˆã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚ï¼‰ â†’ NTFFï¼ˆãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°å®Ÿè¡Œæ™‚ã®ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰ â†’ **Perfetto ãƒˆãƒ¬ãƒ¼ã‚¹**ï¼ˆåˆ†æã«ä½¿ç”¨ï¼‰
:::

#### NEFF (Neuron Executable File Format)

[å‚è€ƒ: Work with NEFF Files](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-runtime/explore/work-with-neff-files.html)

**å½¹å‰²**: ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ã‚°ãƒ©ãƒ•ã®å®Ÿè¡Œãƒ•ã‚¡ã‚¤ãƒ«

```bash
# NEFF ã®æ§‹é€ 
neff_322059935237836.neff (801KB)
â”œâ”€â”€ [1024 byte header]
â””â”€â”€ [tar.gz archive]
    â”œâ”€â”€ info.json              # ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æƒ…å ±
    â”œâ”€â”€ hlo_stats.json         # æ¼”ç®—çµ±è¨ˆï¼ˆHloMacCount: 29.2B ãªã©ï¼‰
    â”œâ”€â”€ metrics.json           # æ¨å®šãƒ¬ã‚¤ãƒ†ãƒ³ã‚·
    â”œâ”€â”€ neff.json             # ã‚°ãƒ©ãƒ•å®šç¾©ï¼ˆ373 ãƒãƒ¼ãƒ‰ï¼‰
    â””â”€â”€ sg00/                  # ã‚µãƒ–ã‚°ãƒ©ãƒ• 0
        â”œâ”€â”€ tensor_map.json   # ãƒ†ãƒ³ã‚½ãƒ«æƒ…å ±ï¼ˆ458 ãƒ†ãƒ³ã‚½ãƒ«ï¼‰
        â”œâ”€â”€ PE.bin            # Processing Element å‘½ä»¤
        â”œâ”€â”€ Activation.bin    # æ´»æ€§åŒ–é–¢æ•°å‘½ä»¤
        â”œâ”€â”€ DVE.bin           # Data Vector Engine å‘½ä»¤
        â””â”€â”€ debug_info_*.dbg  # ãƒ‡ãƒãƒƒã‚°æƒ…å ±
```

#### NTFF (Neuron Trace File Format) - ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«

**å½¹å‰²**: Perfetto ãƒˆãƒ¬ãƒ¼ã‚¹ã«å¤‰æ›ã•ã‚Œã‚‹å‰ã®ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«

```bash
# NTFF ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¾‹ï¼ˆNeuron Profiler ãŒç”Ÿæˆï¼‰
profile_output/i-0049acfde6046f237_pid_520024/
â”œâ”€â”€ 322059935237836_instid_0_vnc_0.ntff  # Graph 1, Core 0
â”œâ”€â”€ 322059935237836_instid_0_vnc_1.ntff  # Graph 1, Core 1
â”œâ”€â”€ 729292360268366_instid_0_vnc_0.ntff  # Graph 4, Core 0
â”œâ”€â”€ 729292360268366_instid_0_vnc_1.ntff  # Graph 4, Core 1
...
â””â”€â”€ (22 files = 11 graphs Ã— 2 cores)

# Neuron Profiler ã§ Perfetto ã«å¤‰æ›
$ neuron-profile view --output-format perfetto profile_output
```

#### Perfetto ãƒˆãƒ¬ãƒ¼ã‚¹

**å½¹å‰²**: NeuronCore ä¸Šã®ä½ãƒ¬ãƒ™ãƒ«å®Ÿè¡Œãƒˆãƒ¬ãƒ¼ã‚¹

```bash
# Perfetto ãƒˆãƒ¬ãƒ¼ã‚¹
trace.perfetto-trace (110 MB)
â””â”€â”€ SQLite ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹
    â”œâ”€â”€ slice ãƒ†ãƒ¼ãƒ–ãƒ«          # ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œè¨˜éŒ²
    â”‚   â””â”€â”€ MATMUL: 21,582å›, 10.94ms
    â”‚       COPY: 83å›, 0.03ms
    â”‚       custom_call.17: 36å›, 7ms
    â”œâ”€â”€ thread ãƒ†ãƒ¼ãƒ–ãƒ«         # ã‚¹ãƒ¬ãƒƒãƒ‰æƒ…å ±
    â””â”€â”€ process ãƒ†ãƒ¼ãƒ–ãƒ«        # ãƒ—ãƒ­ã‚»ã‚¹æƒ…å ±
```

::::details NEFF ã¨ Perfetto ã®æ¯”è¼ƒ

| æƒ…å ± | NEFF | Perfetto | å‚™è€ƒ |
|------|------|----------|------|
| **é™çš„æ§‹é€ ** | | | |
| ã‚°ãƒ©ãƒ•æ§‹é€ ï¼ˆãƒãƒ¼ãƒ‰ã€ãƒ†ãƒ³ã‚½ãƒ«æ•°ï¼‰ | âœ… | âŒ | NEFF unpacking ã§å–å¾— |
| ãƒ†ãƒ³ã‚½ãƒ«å½¢çŠ¶ãƒ»ãƒ‡ãƒ¼ã‚¿å‹ | âœ… | âŒ | tensor_map.json |
| æ¼”ç®—é‡ï¼ˆç†è«–å€¤ï¼‰ | âœ… | âŒ | hlo_stats.json |
| ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ï¼ˆç†è«–å€¤ï¼‰ | âœ… | âŒ | IfmapSizeã€OfmapSize |
| DMA ã‚­ãƒ¥ãƒ¼æ§‹æˆ | âœ… | âŒ | def.json |
| **ã‚°ãƒ©ãƒ•ãƒ¬ãƒ™ãƒ«å®Ÿè¡Œ** | | | |
| ã‚°ãƒ©ãƒ•ã”ã¨ã®å®Ÿè¡Œæ™‚é–“ | âŒ | âš ï¸ | SQL é›†è¨ˆã§è¨ˆç®—å¯èƒ½ |
| NeuronCore ã”ã¨ã®å†…è¨³ | âŒ | âš ï¸ | ã‚¹ãƒ¬ãƒƒãƒ‰åˆ¥ã«é›†è¨ˆ |
| ä½¿ç”¨ã•ã‚ŒãŸã‚°ãƒ©ãƒ•ã®è­˜åˆ¥ | âŒ | âš ï¸ | slice åã‹ã‚‰æ¨å®š |
| ã‚°ãƒ©ãƒ•é–“ã®é·ç§»æ™‚é–“ | âŒ | âš ï¸ | ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‹ã‚‰æ¨å®š |
| **ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¬ãƒ™ãƒ«å®Ÿè¡Œ** | | | |
| å®Ÿè¡Œæ™‚é–“ï¼ˆå®Ÿæ¸¬å€¤ï¼‰ | âŒ | âœ… | slice.dur |
| å®Ÿè¡Œå›æ•° | âŒ | âœ… | COUNT(*) |
| ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³è©³ç´°ï¼ˆMATMULã€COPY ãªã©ï¼‰ | âŒ | âœ… | slice.name |
| ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã¨å®Ÿè¡Œé †åº | âŒ | âœ… | slice.ts |
| ä¸¦åˆ—å®Ÿè¡Œã®å¯è¦–åŒ– | âŒ | âœ… | Perfetto UI |
| åˆæœŸåŒ–é…å»¶ï¼ˆskip_warmup åŠ¹æœï¼‰ | âŒ | âœ… | åˆå›å®Ÿè¡Œæ™‚é–“ã®æ¯”è¼ƒ |
| **é«˜ãƒ¬ãƒ™ãƒ«æƒ…å ±** | | | |
| Python ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ | âŒ | âŒ | line_profiler ãªã©ãŒå¿…è¦ |
| æ¼”ç®—å†…å®¹ã®æ„å‘³ï¼ˆRoPEã€RMSNorm ãªã©ï¼‰ | âš ï¸ | âŒ | å½¢çŠ¶ã‹ã‚‰æ¨æ¸¬ã®ã¿ |

**å‡¡ä¾‹**: âœ… ç›´æ¥å–å¾—å¯èƒ½ã€âš ï¸ æ¨æ¸¬ãƒ»è¨ˆç®—ãŒå¿…è¦ã€âŒ å–å¾—ä¸å¯èƒ½
::::

---

## Phase 2: line_profiler ã«ã‚ˆã‚‹ Python ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°

Phase 1 ã§ã¯ Neuron Profiler ã«ã‚ˆã‚Š NeuronCore ãƒ¬ãƒ™ãƒ«ã®è©³ç´°ãªåˆ†æã‚’è¡Œã„ã¾ã—ãŸãŒã€Python ãƒ¬ãƒ™ãƒ«ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ï¼ˆãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã€ãƒ‡ãƒ¼ã‚¿æº–å‚™ãªã©ï¼‰ã®æ¸¬å®šã«ã¯åˆ¥ã®ãƒ„ãƒ¼ãƒ«ãŒå¿…è¦ã§ã™ã€‚ãã“ã§ line_profiler ã‚’ä½¿ç”¨ã—ã¦ Python ã‚³ãƒ¼ãƒ‰ã®è¡Œã”ã¨ã®å®Ÿè¡Œæ™‚é–“ã‚’æ¸¬å®šã—ã¾ã™ã€‚

### æ¸¬å®šã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æº–å‚™

Phase 1 ã§ä½¿ç”¨ã—ãŸ `test_reranker.py` ã¯ pytest + benchmark_capture ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ãŒã€line_profiler ã¨çµ„ã¿åˆã‚ã›ã‚‹ã¨å‡ºåŠ›ãŒè¤‡é›‘ã«ãªã‚Šã¾ã™ã€‚ãã“ã§ã€line_profiler å°‚ç”¨ã®ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¹ã‚¯ãƒªãƒ—ãƒˆ `profile_line.py` ã‚’ä½œæˆã—ã¾ã—ãŸã€‚ï¼ˆã“ã®è¾ºã‚Šã‚‚ vllm-neuron ã® Python ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã®ãŸã‚ã«ã†ã¾ãå–ã‚Œã‚‹ã‚ˆã†ã«ä»Šå¾Œ benchmark_capture ã®å®Ÿè£…ã‚’æ”¹å–„ã—ã¾ã™ï¼‰

::::details ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æ§‹é€ 

```python:profile_line.py
try:
    profile
except NameError:
    def profile(func):
        return func

# config.yaml ã‹ã‚‰è¨­å®šã‚’èª­ã¿è¾¼ã¿ï¼ˆtest_reranker.py ã¨åŒã˜ï¼‰
config_path = Path(__file__).parent / 'config.yaml'
with open(config_path, 'r') as f:
    config = yaml.safe_load(f)

# æ¸¬å®šå¯¾è±¡ã®é–¢æ•°ã« @profile ãƒ‡ã‚³ãƒ¬ãƒ¼ã‚¿ãƒ¼ã‚’è¿½åŠ 
@profile
def build_prompts_for_vllm(pairs, tokenizer, prefix_tokens, suffix_tokens):
    """ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰"""
    # ... (test_reranker.py ã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯)

@profile
def run_reranker(llm, tokenizer, token_true_id, token_false_id,
                 prefix_tokens, suffix_tokens):
    """ãƒªãƒ©ãƒ³ã‚«ãƒ¼ã®ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # ... (test_reranker.py ã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯)

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼ˆpytest éä¾å­˜ï¼‰"""
    llm = vllm.LLM(model=model_path, **vllm_config)
    # ... åˆæœŸåŒ–ã¨ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°å®Ÿè¡Œ

if __name__ == "__main__":
    main()
```

::::

::::details ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Œå…¨ç‰ˆ
```python:profile_line.py
"""
Line profiler script for vLLM-Neuron Reranker

Run with:
    kernprof -l -v profile_line.py

Or for more detailed output:
    kernprof -l profile_line.py
    python -m line_profiler profile_line.py.lprof
"""

# line_profiler compatibility: make @profile decorator optional
try:
    profile
except NameError:
    # If not running under kernprof, @profile is a no-op
    def profile(func):
        return func

import csv
import gc
import logging
import os
import sys
from pathlib import Path

import yaml

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load configuration
config_path = Path(__file__).parent / 'config.yaml'
with open(config_path, 'r') as f:
    config = yaml.safe_load(f)

# Get model path
model_path = config['model']['path']

# Get vLLM config
vllm_config = {
    "tensor_parallel_size": config['vllm']['tensor_parallel_size'],
    "max_num_seqs": config['vllm']['max_num_seqs'],
    "block_size": config['vllm']['block_size'],
    "max_model_len": config['vllm']['max_model_len'],
    "max_num_batched_tokens": config['vllm']['max_num_batched_tokens'],
    "num_gpu_blocks_override": config['vllm']['num_gpu_blocks_override'],
    "enable_prefix_caching": config['vllm']['enable_prefix_caching'],
    "dtype": config['vllm']['dtype'],
    "disable_log_stats": config['vllm'].get('disable_log_stats', False),
}

# Add additional_config if present (Zenn article optimal settings)
if 'additional_config' in config['vllm']:
    vllm_config['additional_config'] = config['vllm']['additional_config']

# Get reranker config
reranker_config = config['reranker']
benchmark_config = config['benchmark']

# Reranker prompts
reranker_prompts = {
    'instruction': reranker_config['instruction'],
    'prefix': reranker_config['prefix'],
    'suffix': reranker_config['suffix']
}

# Token IDs
token_ids = {
    'true': reranker_config['token_true'],
    'false': reranker_config['token_false']
}

# Load CSV data
csv_file = Path(__file__).parent / reranker_config['input_file']
with open(csv_file, 'r', encoding='utf-8') as f:
    reader = csv.DictReader(f)
    rows = list(reader)

num_queries = min(len(rows), benchmark_config['num_test_queries'])
search_num = reranker_config['search_num']
batch_size = reranker_config['batch_size']
max_length = reranker_config['max_length']

logger.info(f"Loaded {len(rows)} queries from {csv_file}")
logger.info(f"Testing with first {num_queries} queries")


def format_instruction(query: str, doc: str) -> str:
    """Format instruction for reranker"""
    instruction = reranker_prompts['instruction']
    output = f"<Instruct>: {instruction}\n<Query>: {query}\n<Document>: {doc}"
    # Truncate if too long
    if len(output) >= 2000:
        output = output[:2000]
    return output


@profile
def build_prompts_for_vllm(pairs, tokenizer, prefix_tokens, suffix_tokens):
    """Build prompts with proper tokenization - PROFILING TARGET"""
    prompts = []
    budget = max_length - len(prefix_tokens) - len(suffix_tokens)

    # Tokenize pairs
    enc = tokenizer(
        list(pairs),
        padding=False,
        truncation="longest_first",
        return_attention_mask=False,
        add_special_tokens=False,
        max_length=max(8, budget),
    )

    # Build final prompts: prefix + content + suffix
    for ids in enc["input_ids"]:
        final_ids = prefix_tokens + ids + suffix_tokens
        text = tokenizer.decode(final_ids, skip_special_tokens=False)
        prompts.append(text)

    return prompts


@profile
def run_reranker(llm, tokenizer, token_true_id, token_false_id, prefix_tokens, suffix_tokens):
    """Run reranker on queries - MAIN PROFILING TARGET"""

    import vllm
    from vllm import SamplingParams

    # Get use_tqdm setting from benchmark config
    use_tqdm = benchmark_config.get('use_tqdm', True)

    # Create SamplingParams
    sampling_params = SamplingParams(
        max_tokens=1,
        temperature=0.0,
        logprobs=20,
        detokenize=True,
        allowed_token_ids=[token_true_id, token_false_id]
    )

    logger.info(f"SamplingParams configured: max_tokens=1, "
                f"allowed_tokens=[{token_ids['true']}, {token_ids['false']}]")

    # Process each query
    total_processed = 0
    for query_idx, row in enumerate(rows[:num_queries]):
        query = row["query"]

        # Get candidates
        candidates = [
            row[f"answer_{i}"]
            for i in range(search_num)
            if f"answer_{i}" in row
        ]

        # Format query-document pairs
        pairs = [format_instruction(query, doc) for doc in candidates[:search_num]]

        # Build prompts with tokenization
        prompts = build_prompts_for_vllm(pairs, tokenizer, prefix_tokens, suffix_tokens)

        # Process in batches
        query_outputs = []
        for s in range(0, len(prompts), batch_size):
            batch_prompts = prompts[s:s + batch_size]
            outputs = llm.generate(batch_prompts, sampling_params, use_tqdm=use_tqdm)
            query_outputs.extend(outputs)

        total_processed += len(query_outputs)

        if query_idx == 0:
            # Show first result for verification
            logger.info(f"Query 1: {query[:80]}...")
            logger.info(f"Generated {len(query_outputs)} scores for "
                       f"{len(candidates[:search_num])} candidates")
            if query_outputs:
                first_output = query_outputs[0]
                logger.info(f"First output: {first_output.outputs[0].text} "
                           f"(token_ids={first_output.outputs[0].token_ids})")

    logger.info(f"Profiling completed: processed {total_processed} reranker pairs")
    return total_processed


def main():
    """Main profiling function"""
    import vllm

    logger.info("Initializing vLLM-Neuron reranker...")
    logger.info(f"Model: {model_path}")
    logger.info(f"Config: block_size={vllm_config['block_size']}, "
               f"max_num_seqs={vllm_config['max_num_seqs']}, "
               f"tensor_parallel_size={vllm_config['tensor_parallel_size']}")

    # Initialize vLLM
    llm = vllm.LLM(model=model_path, **vllm_config)

    # Get tokenizer and token IDs
    tokenizer = llm.get_tokenizer()
    token_false_id = tokenizer.convert_tokens_to_ids(token_ids['false'])
    token_true_id = tokenizer.convert_tokens_to_ids(token_ids['true'])

    logger.info(f"Token IDs: {token_ids['true']}={token_true_id}, "
               f"{token_ids['false']}={token_false_id}")

    # Encode prompt templates
    prefix_tokens = tokenizer.encode(
        reranker_prompts['prefix'], add_special_tokens=False
    )
    suffix_tokens = tokenizer.encode(
        reranker_prompts['suffix'], add_special_tokens=False
    )

    logger.info(f"Prefix tokens: {len(prefix_tokens)}, Suffix tokens: {len(suffix_tokens)}")

    # Run profiling
    logger.info("Starting profiling run...")
    total = run_reranker(llm, tokenizer, token_true_id, token_false_id, prefix_tokens, suffix_tokens)

    logger.info(f"Profiling complete. Processed {total} pairs.")

    # Cleanup
    del llm
    gc.collect()


if __name__ == "__main__":
    main()
````

```yaml:config.yaml
# vLLM-Neuron Reranker Benchmark Configuration

# Model configuration
model:
  # Path to the reranker model
  # Example: "/path/to/models/Qwen3-0.6B-Reranker"
  # Use environment variable: export RERANKER_MODEL_PATH="/your/model/path"
  path: "/home/coder/data-science/investigations/inf2-vllm-performance/models/Qwen3-0.6B-Reranker"

# vLLM-Neuron engine settings
vllm:
  tensor_parallel_size: 2           # Number of NeuronCores
  max_num_seqs: 4                   # Batch size
  block_size: 32                    # KV cache block size (32 for Zenn best case, 128 for stability)
  max_model_len: 2048               # Maximum sequence length
  max_num_batched_tokens: 256       # Performance optimization
  num_gpu_blocks_override: 512      # pa_num_blocks equivalent
  enable_prefix_caching: false      # Explicit disable
  dtype: "bfloat16"                 # Data type

  # Neuron-specific overrides (Zenn article optimal settings)
  additional_config:
    override_neuron_config:
      skip_warmup: true             # Phase 1-5 ã®è¨­å®šï¼ˆè¨˜äº‹ã¨ä¸€è‡´ï¼‰
      enable_bucketing: true        # å‹•çš„ãƒãƒƒãƒãƒ³ã‚°æœ‰åŠ¹
      pa_num_blocks: 512
      pa_block_size: 32

# Reranker-specific settings
reranker:
  # Input data
  input_file: "input_sample.csv"    # CSV file with queries and candidates

  # Processing parameters
  search_num: 20                    # Number of candidates per query to process
  batch_size: 8                     # Batch size for processing prompts
  max_length: 1500                  # Maximum prompt length

  # Model-specific tokens (for Qwen3-Reranker)
  # Change these for other reranker models
  token_true: "yes"
  token_false: "no"

  # Prompt templates (for Qwen3-Reranker)
  # Customize these for your model
  prefix: |
    <|im_start|>system
    Judge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be "yes" or "no".<|im_end|>
    <|im_start|>user

  # Note: "assitant" typo is intentional for Qwen3-Reranker compatibility
  suffix: |
    <|im_end|>
    <|im_start|>assitant
    <think>

    </think>


  instruction: "Given a web search query, retrieve relevant passages that answer the query"

# Benchmark settings
benchmark:
  rounds: 5                         # Number of benchmark rounds
  warmup_rounds: 1                  # Number of warmup rounds
  num_test_queries: 10              # Number of queries to use for testing (è¨˜äº‹ã¨åŒã˜æ¡ä»¶)

# Profiler settings (optional)
profiler:
  # Clear Neuron compilation cache before benchmark
  # WARNING: First run after clearing will recompile (10-15 minutes)
  # Useful when:
  # - Model configuration changed (batch size, sequence length, etc.)
  # - Neuron SDK version changed
  # - Testing clean compilation performance
  clear_cache_before: false

  # Clear cache after benchmark (useful for CI/CD to save disk space)
  clear_cache_after: false
```
::::

ã“ã‚Œã«ã‚ˆã‚Šã€**Phase 1 ã¨åŒã˜æ¸¬å®šæ¡ä»¶**ï¼ˆåŒã˜ config.yamlã€åŒã˜å‡¦ç†ãƒ­ã‚¸ãƒƒã‚¯ï¼‰ã‚’ç¶­æŒã—ãªãŒã‚‰ã€line_profiler ã«ã‚ˆã‚‹è©³ç´°ãª Python ãƒ¬ãƒ™ãƒ«ã®åˆ†æãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

### 2.1 æ¸¬å®šå¯¾è±¡ã®ç†è§£

**æ¸¬å®šå¯¾è±¡**: 1 ã‚¯ã‚¨ãƒªï¼ˆ20 å€™è£œæ–‡æ›¸ã®ãƒªãƒ©ãƒ³ã‚­ãƒ³ã‚°ï¼‰ã‚’å‡¦ç†ã™ã‚‹æ™‚é–“

```yaml
reranker:
  search_num: 20        # 1 ã‚¯ã‚¨ãƒªã‚ãŸã‚Š 20 å€™è£œæ–‡æ›¸
  batch_size: 8         # 8 ãƒšã‚¢ãšã¤ãƒãƒƒãƒå‡¦ç†

vllm:
  max_num_seqs: 4       # vLLM ã®åŒæ™‚å‡¦ç†æ•°
```

```
1  ã‚¯ã‚¨ãƒª = 20 ãƒšã‚¢ Ã· batch_size=8 = 3 ãƒãƒƒãƒ
10 ã‚¯ã‚¨ãƒª = 30 ãƒãƒƒãƒ
åˆè¨ˆæ™‚é–“ = 2,992ms â†’ 1 ã‚¯ã‚¨ãƒªã‚ãŸã‚Šç´„ 300ms
```

### 2.2 line_profiler æ¸¬å®šçµæœ

::::details line_profiler ã®å®Ÿè¡Œ

**å®Ÿè¡Œç’°å¢ƒã®æº–å‚™**:

```bash
# vLLM-Neuron ç’°å¢ƒã‚’ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ãƒˆ
source /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/bin/activate

# PATH ã« Neuron SDK ã®ãƒ„ãƒ¼ãƒ«ã‚’è¿½åŠ 
export PATH="/opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/bin:$PATH"

# line_profiler ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ãªã„å ´åˆ
pip install line-profiler
```

**ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°å®Ÿè¡Œ**:

```bash
cd /path/to/my-reranker
kernprof -l -v -p vllm.v1.engine profile_line.py
```

:::message
**kernprof ã‚ªãƒ—ã‚·ãƒ§ãƒ³èª¬æ˜**:
- `-l` (--line-by-line): è¡Œã”ã¨ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‚’æœ‰åŠ¹åŒ–
- `-v` (--view): çµæœã‚’å³åº§ã«è¡¨ç¤º
- `-p vllm.v1.engine` (--prof-mod): **vllm.v1.engine ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’è‡ªå‹•ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°å¯¾è±¡ã«æŒ‡å®š**ï¼ˆã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å†…ã®å…¨é–¢æ•°ã‚’è‡ªå‹•çš„ã«ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ï¼‰
:::

å®Ÿè¡Œå¾Œã€`profile_line.py.lprof` ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã€ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«è©³ç´°ãªè¡Œã”ã¨ã®å®Ÿè¡Œæ™‚é–“ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚
::::

ä»¥ä¸‹ã«å®Ÿéš›ã« line_profiler ã®çµæœã‹ã‚‰å¾—ã‚‰ã‚ŒãŸæƒ…å ±ã‚’ç¤ºã—ã¾ã™ã€‚

```python
# llm.generate() - 30ãƒãƒƒãƒå‡¦ç†
Line 157: outputs = llm.generate(batch_prompts, sampling_params)
  - Hits: 30 batches
  - Time: 3781.560 ms (3.78ç§’)
  - Per Hit: 126.052 ms/batch
  - % Time: 99.1%

# LLMEngine.step() ã®å†…è¨³
Line 293: outputs = self.engine_core.get_output()
  - Hits: 229 steps (7.6 steps/batch)
  - Time: 3197.372 ms
  - Per Hit: 13.962 ms/step
  - % Time: 95.3%
```

line_profiler ã«ã‚ˆã‚‹æ¸¬å®šã®çµæœã€10 ã‚¯ã‚¨ãƒªï¼ˆ30 ãƒãƒƒãƒï¼‰ã®å‡¦ç†ã«åˆè¨ˆ 3.78 ç§’ã‹ã‹ã‚Šã€ãã®ã†ã¡ `llm.generate()` ã®å‘¼ã³å‡ºã—ã ã‘ã§ **99.1%ï¼ˆ3.78 ç§’ï¼‰** ã‚’å ã‚ã‚‹ã“ã¨ãŒåˆ¤æ˜ã—ã¾ã—ãŸã€‚ç´„ 3 ç§’ã‹ã‚‰æ™‚é–“ãŒå¢—ãˆã¦ã„ã‚‹ã®ã¯ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã§ã™ã€‚

ã•ã‚‰ã«é‡è¦ãªç™ºè¦‹ã¨ã—ã¦ã€**1 ãƒãƒƒãƒã‚ãŸã‚Šã®å‡¦ç†æ™‚é–“ãŒ 126.052ms** ã¨ã„ã†æ¸¬å®šå€¤ãŒå¾—ã‚‰ã‚Œã¾ã—ãŸã€‚ã“ã®å€¤ã¯ Phase 3 ã§ NxD Inference ã¨ç›´æ¥æ¯”è¼ƒã™ã‚‹éš›ã®åŸºæº–å€¤ã¨ãªã‚Šã¾ã™ã€‚ã¾ãŸã€vLLM ã®å†…éƒ¨å‡¦ç†ã‚’è©³ã—ãè¦‹ã‚‹ã¨ã€`LLMEngine.step()` ãŒ 229 å›å‘¼ã°ã‚Œã¦ãŠã‚Šã€30 ãƒãƒƒãƒã«å¯¾ã—ã¦ **å¹³å‡ 7.6 steps/batch** ã¨ã„ã†è¬ã®å€¤ãŒè¦³æ¸¬ã•ã‚Œã¾ã—ãŸã€‚ãªãœ 1 ãƒãƒƒãƒã®å‡¦ç†ã« 7.6 å›ã‚‚ã®ã‚¹ãƒ†ãƒƒãƒ—ãŒå¿…è¦ãªã®ã‹ã€ã“ã®æ™‚ç‚¹ã§ã¯ç†è§£ã§ãã¦ã„ã¾ã›ã‚“ã§ã—ãŸã€‚

### 2.3 7.6 steps/batch ã®ç†ç”±ã‚’è¿½ã†

ã“ã®è¬ã‚’è§£æ˜ã™ã‚‹ãŸã‚ã€`LLMEngine.step()` ã®ä¸­èº«ã‚’ã•ã‚‰ã«è©³ã—ãèª¿ã¹ã‚‹ã“ã¨ã«ã—ã¾ã—ãŸã€‚line_profiler ã® `-p vllm.v1.engine` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚Šã€vLLM å†…éƒ¨ã®ã‚³ãƒ¼ãƒ‰ã‚‚è‡ªå‹•çš„ã«ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã•ã‚Œã¦ã„ã¾ã™ã€‚

`LLMEngine.step()` ã®å‡¦ç†æ™‚é–“ã®ã»ã¼å…¨ã¦ï¼ˆ95.3%ï¼‰ãŒ `engine_core.get_output()` ã¨ã„ã†å˜ä¸€ã®é–¢æ•°å‘¼ã³å‡ºã—ã§è²»ã‚„ã•ã‚Œã¦ã„ã¾ã—ãŸã€‚ã•ã‚‰ã«ãã® `get_output()` ã®ä¸­èº«ã‚’è¦‹ã‚‹ã¨ã€**100% ãŒ `outputs_queue.get()` ã¨ã„ã†ã‚­ãƒ¥ãƒ¼ã‹ã‚‰ã®ãƒ‡ãƒ¼ã‚¿å–å¾—å‡¦ç†**ã§ã—ãŸã€‚

```python
# LLMEngine.step() ã®ä¸­èº«
Line 293: outputs = self.engine_core.get_output()
  - Time: 3197.372 ms (95.3% of step())

# get_output() ã®ä¸­èº«
Line 715: outputs = self.outputs_queue.get()
  - Time: 3194.6 ms
  - % Time: 100.0% of get_output()
```

ã¤ã¾ã‚Šã€ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã¯ `outputs_queue.get()` ã§ã‚­ãƒ¥ãƒ¼ã‹ã‚‰çµæœãŒé€ã‚‰ã‚Œã¦ãã‚‹ã®ã‚’ãŸã **å¾…ã£ã¦ã„ã‚‹ã ã‘**ã§ã—ãŸã€‚ã“ã‚Œã¯å®Ÿéš›ã®æ¨è«–å‡¦ç†ãŒåˆ¥ãƒ—ãƒ­ã‚»ã‚¹ã§è¡Œã‚ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚ã“ã“ã§ vLLM v1 ã®ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®å­˜åœ¨ã‚’æ€ã„å‡ºã—ã¾ã—ãŸã€‚

ï¼ˆä»¥ä¸‹ã®è¨˜äº‹ã«å†…éƒ¨ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è§£èª¬ãŒã‚ã‚Šã¾ã™ï¼‰

https://zenn.dev/tosshi/articles/f64ba0b86e330b

vLLM v1 ã§ã¯ã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’å—ã‘å–ã‚‹ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã¨ã€å®Ÿéš›ã«æ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹ Worker ãƒ—ãƒ­ã‚»ã‚¹ãŒåˆ†é›¢ã•ã‚Œã¦ã„ã¾ã™ã€‚ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã¯ `llm.generate()` ã‚’å‘¼ã³å‡ºã™ã¨ã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ Worker ãƒ—ãƒ­ã‚»ã‚¹ã«é€ä¿¡ã—ã€`outputs_queue.get()` ã§ãƒ–ãƒ­ãƒƒã‚¯ã—ã¦çµæœã‚’å¾…ã¡ã¾ã™ã€‚ä¸€æ–¹ã€Worker ãƒ—ãƒ­ã‚»ã‚¹ã¯ NeuronCore ã§ã®æ¨è«–å®Ÿè¡Œã€çµæœã®ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã€ãã—ã¦ãƒ—ãƒ­ã‚»ã‚¹é–“é€šä¿¡ã‚’é€šã˜ã¦ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã«çµæœã‚’è¿”ã—ã¾ã™ã€‚ã“ã®æ§‹é€ ã‚’å›³ç¤ºã™ã‚‹ã¨ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

**vLLM v1 ã®ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ (vllm-neuron)**

```mermaid
%%{init: {'theme':'dark'}}%%
graph LR
    subgraph MainProcess["Main Process"]
        A[LLMEngine.step] --> B[get_output]
        B --> C[outputs_queue.get<br/>13.962 ms/step]
    end

    subgraph WorkerProcess["Worker Process"]
        D[EngineCore] --> E[execute_model]
        E --> F[Neuron å®Ÿè¡Œ]
        F --> G[ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³]
        G --> H[ZMQ é€ä¿¡]
    end

    H -->|IPC| C

    style MainProcess fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style WorkerProcess fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style C fill:#0f3460,stroke:#16213e,color:#fff
    style F fill:#0f3460,stroke:#16213e,color:#fff
```

line_profiler ã¯ Python ã®æ¨™æº–çš„ãªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ã¨åŒæ§˜ã«ã€**å®Ÿè¡Œä¸­ã®ãƒ—ãƒ­ã‚»ã‚¹ã®ã‚³ãƒ¼ãƒ‰ã—ã‹æ¸¬å®šã§ãã¾ã›ã‚“**ã€‚ã¤ã¾ã‚Šã€Worker ãƒ—ãƒ­ã‚»ã‚¹ã§å®Ÿè¡Œã•ã‚Œã‚‹ `execute_model()` ã‚„ NeuronCore ã§ã®æ¨è«–å‡¦ç†ã¯ã€ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ã‹ã‚‰è¦‹ã‚‹ã¨ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã§ã™ã€‚

æ¸¬å®šã§ããŸã®ã¯ `outputs_queue.get()` ã§å¾…æ©Ÿã—ã¦ã„ã‚‹æ™‚é–“ï¼ˆ13.962ms/stepï¼‰ã ã‘ã§ã‚ã‚Šã€ã“ã®æ™‚é–“ã«ã¯æ¨è«–ã€IPC ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãªã©ã®ã™ã¹ã¦ã®æ™‚é–“ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚

ã§ã¯ã€ãªãœ 1 ãƒãƒƒãƒã®å‡¦ç†ã«å¹³å‡ 7.6 å›ã‚‚ `step()` ãŒå‘¼ã°ã‚Œã‚‹ã®ã§ã—ã‚‡ã†ã‹ã€‚ã“ã‚Œã¯ vLLM v1 ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã®å‹•ä½œæ–¹æ³•ã¨ã—ã¦ãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚»ã‚¹ãŒä½•åº¦ã‚‚ `step()` ã‚’ç¢ºèªã—ã¦ã‚­ãƒ¥ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯ã—ç¶šã‘ã¦ã„ã‚‹ã‹ã‚‰ã§ã™ã€‚1 ãƒãƒƒãƒã‚ãŸã‚Šå¹³å‡ã—ã¦ 7.6 å›ã‚­ãƒ¥ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã„ãŸã¨ã„ã†ã“ã¨ã§ã™ã€‚

è¬ã¯è§£ã‘ã¾ã—ãŸãŒã€è‚å¿ƒã® **Worker ãƒ—ãƒ­ã‚»ã‚¹å†…ã§ã® NeuronCore ã®æ¨è«–å‡¦ç†æ™‚é–“**ã‚’åˆ†è§£ã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚`outputs_queue.get()` ã® 13.962 ms ã«ã¯ã€æ¨è«–å®Ÿè¡Œã€ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã€IPC é€šä¿¡ã®ã™ã¹ã¦ãŒå«ã¾ã‚Œã¦ãŠã‚Šã€line_profiler ã§ã¯ã“ã‚Œä»¥ä¸Šåˆ†è§£ã‚’ã™ã‚‹ã®ã¯å¤§å¤‰ãã†ã§ã™ã€‚

### 2.4 æ¸¬å®šã®é™ç•Œã¨ä»Šå¾Œã®æ–¹å‘æ€§

line_profiler ã«ã‚ˆã‚‹æ¸¬å®šã§åˆ¤æ˜ã—ãŸã“ã¨ã‚’æ•´ç†ã™ã‚‹ã¨ã€ä»¥ä¸‹ã®ã‚ˆã†ãªæ§‹é€ ã«ãªã‚Šã¾ã™ã€‚å…¨ä½“ã¨ã—ã¦ 126.052 ms/batch ã¨ã„ã†å‡¦ç†æ™‚é–“ã¯æ¸¬å®šã§ãã¾ã—ãŸãŒã€ãã®å†…è¨³ã®å¤§éƒ¨åˆ†ï¼ˆ84.2%ï¼‰ã®è©³ç´°ãŒä¸æ˜ã¨ã„ã†çŠ¶æ³ã§ã™ã€‚

ã“ã®çŠ¶æ³ã‚’æ‰“é–‹ã™ã‚‹ãŸã‚ã€NxD Inference ã‚’ç›´æ¥ä½¿ã£ãŸæ¸¬å®šã‚’è©¦ã¿ã¾ã—ãŸã€‚

## Phase 3: NxD Inference ç›´æ¥æ¸¬å®š

Worker ãƒ—ãƒ­ã‚»ã‚¹ã®ç›´æ¥æ¸¬å®šãŒå›°é›£ï¼ˆé¢å€’ï¼‰ãªãŸã‚ã€**vLLM ã‚’ä½¿ã‚ãšã« NxD Inference ã‚’ç›´æ¥ä½¿ç”¨**ã—ã¦ç´”ç²‹ãªæ¨è«–å®Ÿè¡Œæ™‚é–“ã‚’æ¸¬å®šã™ã‚‹ã“ã¨ã«ã—ã¾ã—ãŸã€‚è¨­å®šã¯ã“ã‚Œã¾ã§ã¨åŒã˜ã«ã—ã¦ãŠãã¾ã™ã€‚

### 3.1 vLLM æ¸¬å®šï¼ˆ30ãƒãƒƒãƒï¼‰

::::details ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æ§‹é€ 
```python:benchmark_30batches.py
import time
from vllm import LLM, SamplingParams

llm = LLM(model=model_path, **vllm_config)
sampling_params = SamplingParams(max_tokens=1, temperature=0.0, logprobs=20)

# 97ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ Ã— 4 = ãƒãƒƒãƒ
batch_prompts = [prompt_template] * 4

# Warmup
llm.generate(batch_prompts, sampling_params, use_tqdm=False)

# æ¸¬å®š: 30ãƒãƒƒãƒ
batch_times = []
for i in range(30):
    start = time.perf_counter()
    outputs = llm.generate(batch_prompts, sampling_params, use_tqdm=False)
    elapsed = (time.perf_counter() - start) * 1000
    batch_times.append(elapsed)
```
::::

::::details ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Œå…¨ç‰ˆ
```python:benchmark_30batches.py
"""
30ãƒãƒƒãƒã®çµ±ä¸€æ¸¬å®šï¼ˆvLLM bucketing=Trueï¼‰
"""
import logging
import time
import yaml
import vllm
from vllm import SamplingParams

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load config
with open('config.yaml', 'r') as f:
    config = yaml.safe_load(f)

model_path = config['model']['path']

# vLLM config with bucketing=True
vllm_config = {
    "tensor_parallel_size": config['vllm']['tensor_parallel_size'],
    "max_num_seqs": config['vllm']['max_num_seqs'],
    "block_size": config['vllm']['block_size'],
    "max_model_len": config['vllm']['max_model_len'],
    "max_num_batched_tokens": config['vllm']['max_num_batched_tokens'],
    "num_gpu_blocks_override": config['vllm']['num_gpu_blocks_override'],
    "enable_prefix_caching": config['vllm']['enable_prefix_caching'],
    "dtype": config['vllm']['dtype'],
}

# Add additional_config (bucketing=True)
if 'additional_config' in config['vllm']:
    vllm_config['additional_config'] = config['vllm']['additional_config']

logger.info("Initializing vLLM with bucketing=True...")
logger.info(f"Config: {vllm_config}")

llm = vllm.LLM(model=model_path, **vllm_config)
tokenizer = llm.get_tokenizer()

# Reranker prompt template (97 tokens avg)
prompt_template = """<|im_start|>system
Judge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be "yes" or "no".<|im_end|>
<|im_start|>user
<Instruct>: Given a web search query, retrieve relevant passages that answer the query
<Query>: ãƒ—ãƒ­ã‚»ã‚¹ã«ã¤ã„ã¦æ•™ãˆã¦
<Document>: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®çµ±åˆã•ã‚ŒãŸæ©Ÿèƒ½ã‚’åˆ†æã™ã‚‹æ–¹æ³•<|im_end|>
<|im_start|>assitant
<think>

</think>
"""

# Create 30 batches of 4 prompts each (max_num_seqs=4)
batch_prompts = [prompt_template] * 4

sampling_params = SamplingParams(
    max_tokens=1,
    temperature=0.0,
    logprobs=20,
    detokenize=True,
)

logger.info("Running 30-batch benchmark...")
batch_times = []

for i in range(30):
    start = time.perf_counter()
    outputs = llm.generate(batch_prompts, sampling_params, use_tqdm=False)
    elapsed = (time.perf_counter() - start) * 1000
    batch_times.append(elapsed)
    
    if (i + 1) % 10 == 0:
        logger.info(f"  Batch {i+1}/30: {elapsed:.2f} ms")

logger.info(f"\n=== Results (30 batches, bucketing=True) ===")
logger.info(f"Average: {sum(batch_times)/len(batch_times):.2f} ms/batch")
logger.info(f"Min: {min(batch_times):.2f} ms")
logger.info(f"Max: {max(batch_times):.2f} ms")
logger.info(f"Total: {sum(batch_times):.2f} ms")
```
::::

çµæœã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚

```
Average: 64.33 ms/batch
Min: 60.78 ms
Max: 85.76 ms
```

### 3.2 NxD Inference æ¸¬å®š

:::details NxD Inference æ¸¬å®šã‚³ãƒãƒ³ãƒ‰ã¨çµæœ

```bash
# neuronx-distributed-inference ãƒªãƒã‚¸ãƒˆãƒªã‚¯ãƒ­ãƒ¼ãƒ³
git clone https://github.com/aws-neuron/neuronx-distributed-inference.git
cd neuronx-distributed-inference

# æ¸¬å®šå®Ÿè¡Œ
python src/neuronx_distributed_inference/inference_demo.py \
  --model-type qwen3 \
  --task-type causal-lm \
  run \
  --model-path "/path/to/Qwen3-0.6B-Reranker" \
  --prompt "$PROMPT" \
  --tp-degree 2 \
  --batch-size 4 \
  --seq-len 128 \
  --max-context-length 128 \
  --max-length 129 \
  --enable-bucketing \
  --benchmark \
  --benchmark-report-path /tmp/benchmark_nxd.json
```

**çµæœ**:
```json
{
  "context_encoding_model": {
    "latency_ms_avg": 12.05,
    "latency_ms_p50": 12.03,
    "latency_ms_p90": 12.11,
    "throughput": 42475.60
  }
}
```

:::

### 3.6 ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰æ¯”è¼ƒ

| æ¸¬å®šå¯¾è±¡ | å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· (ms/batch) | Min (ms) | Max (ms) |
|---------|-------------------------|----------|----------|
| **vLLM** | **64.33** | 60.78 | 85.76 |
| **NxD Inference** | **12.05** | - | - |

**ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰å†…è¨³**:

```mermaid
%%{init: {'theme':'dark'}}%%
graph LR
    subgraph NxD["NxD Inference (12.05 ms)"]
        A[Neuron å®Ÿè¡Œ<br/>12.05 ms<br/>100%]
    end

    subgraph vLLM["vLLM (64.33 ms)"]
        B[Neuron å®Ÿè¡Œ<br/>12.05 ms<br/>19%]
        C[Python ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰<br/>52.28 ms<br/>81%]
        B -.-> C
    end

    style NxD fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style vLLM fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style A fill:#0f3460,stroke:#16213e,color:#fff
    style B fill:#0f3460,stroke:#16213e,color:#fff
    style C fill:#8b4513,stroke:#16213e,color:#fff
```

ç´”ç²‹ãª Neuron å®Ÿè¡Œæ™‚é–“ã¯ 12.05 ms (19%) ã§ã‚ã‚‹ã®ã«å¯¾ã—ã€vLLM ã® Python ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒ 52.28 ms (81%) ã‚’å ã‚ã¦ã„ã¾ã™ã€‚

**vLLM ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ = 4.33å€ (+52ms)**

---

## bucketing è¨­å®šã®äºˆæƒ³å¤–ã®å½±éŸ¿

### 4ãƒ‘ã‚¿ãƒ¼ãƒ³æ¸¬å®šã®å®Ÿæ–½

NxD Inference ã§ä»¥ä¸‹ã®4ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¸¬å®šã—ã¾ã—ãŸï¼š

| ãƒ‘ã‚¿ãƒ¼ãƒ³ | Prefix Caching | Bucketing | çµæœ | å¹³å‡ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· (ms) |
|----------|----------------|-----------|------|---------------------|
| **A** | OFF | OFF | âœ… æˆåŠŸ | **12.06** |
| **B** | OFF | ON | âœ… æˆåŠŸ | **23.60** |
| **C** | ON | OFF | âŒ å¤±æ•—* | - |
| **D** | ON | ON | âŒ å¤±æ•—* | - |

\* Prefix caching ã‚¨ãƒ©ãƒ¼ï¼ˆå¾Œè¿°ï¼‰

### ãƒ‘ã‚¿ãƒ¼ãƒ³è©³ç´°

:::details è©³ç´°ãªæ¸¬å®šçµæœ

**ãƒ‘ã‚¿ãƒ¼ãƒ³A: bucketing=OFFï¼ˆæœ€é€Ÿï¼‰**
```json
{
  "context_encoding_model": {
    "latency_ms_avg": 12.059056758880615,
    "latency_ms_p50": 12.01629638671875,
    "latency_ms_p90": 12.193155288696289,
    "throughput": 42457.71541152664
  }
}
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³B: bucketing=ONï¼ˆ1.96å€é…ã„ï¼‰**
```json
{
  "context_encoding_model": {
    "latency_ms_avg": 23.59837293624878,
    "latency_ms_p50": 23.57304096221924,
    "latency_ms_p90": 23.70884418487549,
    "throughput": 21696.411078135458
  }
}
```

:::

### ãªãœ bucketing ã§é€†è»¢ã—ãŸã®ã‹ï¼Ÿ

```mermaid
%%{init: {'theme':'dark'}}%%
graph TD
    subgraph vLLM["vLLM ã§ã®çµæœ"]
        A1["bucketing=False<br/>126.05 ms"] -.->|2å€é«˜é€ŸåŒ–| A2["bucketing=True<br/>64.33 ms âœ…"]
    end

    subgraph NxD["NxD Inference ç›´æ¥"]
        B1["bucketing=OFF<br/>12.06 ms âœ…"] -.->|1.96å€ä½é€ŸåŒ–| B2["bucketing=ON<br/>23.60 ms âš ï¸"]
    end

    style vLLM fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style NxD fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style A1 fill:#8b4513,stroke:#16213e,color:#fff
    style A2 fill:#0f3460,stroke:#16213e,color:#fff
    style B1 fill:#0f3460,stroke:#16213e,color:#fff
    style B2 fill:#8b4513,stroke:#16213e,color:#fff
```

vLLM ã§ã¯ bucketing ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã“ã¨ã§ 126.05 ms ã‹ã‚‰ 64.33 ms ã¸ã¨ 2 å€é«˜é€ŸåŒ–ã•ã‚Œã¾ã—ãŸã€‚ä¸€æ–¹ã€NxD Inference ã‚’ç›´æ¥ä½¿ç”¨ã—ãŸå ´åˆã€bucketing ã‚’æœ‰åŠ¹ã«ã™ã‚‹ã¨ 12.06 ms ã‹ã‚‰ 23.60 ms ã¸ã¨ 1.96 å€ä½é€ŸåŒ–ã™ã‚‹çµæœã¨ãªã‚Šã¾ã—ãŸã€‚

### è€ƒãˆã‚‰ã‚Œã‚‹ç†ç”±

**1. æ¸¬å®šæ¡ä»¶ã®é•ã„: åŒä¸€é•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ**

ä»Šå›ã®æ¸¬å®šã§ã¯ã€å…¨ãåŒã˜97ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’4å›ä½¿ç”¨ï¼š

```python
# ã™ã¹ã¦åŒã˜é•·ã•
batch_prompts = [prompt_97tokens] * 4
```

- **bucketing=OFF**: æ­£ç¢ºã«97ãƒˆãƒ¼ã‚¯ãƒ³ã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ« â†’ å®Œç’§ã«ãƒãƒƒãƒ â†’ ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãªã—
- **bucketing=ON**: è¤‡æ•°ãƒã‚±ãƒƒãƒˆï¼ˆä¾‹: 64, 128, 256...ï¼‰å¯¾å¿œ â†’ 128ãƒã‚±ãƒƒãƒˆã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚° â†’ ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ç™ºç”Ÿ

**2. vLLM ã¨ NxD ç›´æ¥ä½¿ç”¨ã®é•ã„**

| é …ç›® | vLLM | NxD Inference ç›´æ¥ |
|------|------|-------------------|
| **ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰** | å¯å¤‰é•·ãƒãƒƒãƒï¼ˆå‹•çš„ï¼‰ | å›ºå®šé•·ãƒãƒƒãƒï¼ˆé™çš„ï¼‰ |
| **bucketing ã®å½¹å‰²** | å¿…é ˆï¼ˆåŠ¹ç‡çš„ãªå‹•çš„ãƒãƒƒãƒãƒ³ã‚°ï¼‰ | ä¸è¦ï¼ˆå›ºå®šé•·ãªã‚‰å®Œå…¨ä¸€è‡´ï¼‰ |
| **æœ€é©è¨­å®š** | bucketing=ON | bucketing=OFFï¼ˆå›ºå®šé•·ã®å ´åˆï¼‰ |

### çµè«–: bucketing è¨­å®šã¯ä½¿ç”¨ã‚±ãƒ¼ã‚¹ã«ä¾å­˜

| ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ | æœ€é©è¨­å®š | ç†ç”± |
|-------------|---------|------|
| **Rerankerï¼ˆå›ºå®šé•·ï¼‰** | bucketing=OFF | å®Œå…¨ä¸€è‡´ã€ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãªã— |
| **ä¸€èˆ¬çš„ãªç”Ÿæˆï¼ˆå¯å¤‰é•·ï¼‰** | bucketing=ON | åŠ¹ç‡çš„ãªå‹•çš„ãƒãƒƒãƒãƒ³ã‚° |
| **vLLM ä½¿ç”¨æ™‚** | bucketing=ON | vLLM ã®å‹•çš„ãƒãƒƒãƒãƒ³ã‚°ã«å¿…é ˆ |

---

## NxD Inference å†…éƒ¨ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°

NxD Inference ã®å†…éƒ¨å‡¦ç†ã‚’è©³ç´°ã«æ¸¬å®šã—ã¾ã—ãŸã€‚

### æ¸¬å®šã‚³ãƒ¼ãƒ‰ã¨çµæœ

:::details NxD Inference å†…éƒ¨ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‚³ãƒ¼ãƒ‰

```python
# profile_nxd_detailed.py (æŠœç²‹)
import time
import torch
from neuronx_distributed_inference.models.config import NeuronConfig
from neuronx_distributed_inference.models.qwen3.modeling_qwen3 import NeuronQwen3ForCausalLM

# Configuration
neuron_config = NeuronConfig(
    tp_degree=2,
    batch_size=4,
    max_context_length=128,
    seq_len=128,
    enable_bucketing=True,
)

# Load model
model = NeuronQwen3ForCausalLM(compiled_model_path)
model.load(compiled_model_path)

# Warmup
with torch.no_grad():
    _ = model(inputs.input_ids, attention_mask=inputs.attention_mask, position_ids=position_ids)

# Benchmark (30 iterations)
times_total = []
times_forward = []
times_output = []

for i in range(30):
    # Total time
    t_start = time.perf_counter()

    # Forward pass
    t_forward_start = time.perf_counter()
    with torch.no_grad():
        outputs = model(inputs.input_ids,
                       attention_mask=inputs.attention_mask,
                       position_ids=position_ids)
    t_forward_end = time.perf_counter()

    # Output processing (minimal)
    t_output_start = time.perf_counter()
    # (outputs processing would go here)
    t_output_end = time.perf_counter()

    t_end = time.perf_counter()

    times_total.append((t_end - t_start) * 1000)
    times_forward.append((t_forward_end - t_forward_start) * 1000)
    times_output.append((t_output_end - t_output_start) * 1000)
```

**æ¸¬å®šçµæœ**:
```
================================================================================
NxD Inference è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
================================================================================

Configuration: tp_degree=2, batch_size=4, bucketing=True

Total time:
  å¹³å‡: 12.70 ms
  æœ€å°: 12.60 ms
  æœ€å¤§: 13.28 ms

Forward pass:
  å¹³å‡: 12.70 ms  (100.0% of total)
  æœ€å°: 12.60 ms
  æœ€å¤§: 13.28 ms

Output processing:
  å¹³å‡: 0.00 ms   (0.0% of total)
  æœ€å°: 0.00 ms
  æœ€å¤§: 0.00 ms

================================================================================
å†…è¨³
================================================================================
Forward pass:        12.70 ms (100.0%)  â† Neuronå®Ÿè¡Œ
Output processing:   0.00 ms (0.0%)
Other overhead:      0.00 ms (0.0%)     â† Pythonã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãªã—
```

:::

### é‡è¦ãªç™ºè¦‹: Python ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã¯ã‚¼ãƒ­

NxD Inference ã®æ¸¬å®šçµæœã‹ã‚‰ã€æ¥µã‚ã¦é‡è¦ãªç‰¹æ€§ãŒæ˜ã‚‰ã‹ã«ãªã‚Šã¾ã—ãŸã€‚Python ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã¯ã»ã¼ 0 ms ã§ã‚ã‚Šã€å…¨å®Ÿè¡Œæ™‚é–“ãŒ Neuron å®Ÿè¡Œã«ä½¿ã‚ã‚Œã¦ã„ã¾ã™ã€‚ã“ã‚Œã¯ã‚·ãƒ³ã‚°ãƒ«ãƒ—ãƒ­ã‚»ã‚¹ã®ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«ã‚ˆã‚‹ã‚‚ã®ã§ã™ã€‚

ã“ã®çµæœã«ã‚ˆã‚Šã€**vLLM ã® 52ms ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã¯å®Œå…¨ã« vLLM å´ã®å®Ÿè£…ã‚³ã‚¹ãƒˆ**ã§ã‚ã‚‹ã“ã¨ãŒç¢ºèªã§ãã¾ã—ãŸã€‚

---

## Prefix Cachingã®çœŸå®Ÿ

### ãƒ†ã‚¹ãƒˆçµæœ

Prefix caching æœ‰åŠ¹æ™‚ã€ä»¥ä¸‹ã®ã‚¨ãƒ©ãƒ¼ã§å¤±æ•—ï¼š

```python
TypeError: unsupported operand type(s) for -: 'NoneType' and 'NoneType'

File: neuronx_distributed_inference/models/model_base.py, line 3679
  num_queries = full_context_lens - computed_context_lens
```

### ã‚¨ãƒ©ãƒ¼ã®åŸå› 

**inference_demo.pyç›´æ¥å®Ÿè¡Œã§ã¯ã€`full_context_lens`ã¨`computed_context_lens`ãŒæ¸¡ã•ã‚Œãªã„**

```python
# model_base.py:3679ä»˜è¿‘
if self.neuron_config.is_prefix_caching:
    num_queries = full_context_lens - computed_context_lens  # â† ä¸¡æ–¹None
    # ...
```

### Prefix Cachingã®ã‚µãƒãƒ¼ãƒˆçŠ¶æ³

| å®Ÿè£… | ã‚µãƒãƒ¼ãƒˆçŠ¶æ³ | è©³ç´° |
|------|-------------|------|
| **vLLM çµŒç”±** | âœ… ã‚µãƒãƒ¼ãƒˆ | vLLM ãŒå¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æä¾›ï¼ˆNeuron 2.24.0+ï¼‰ |
| **inference_demo.pyç›´æ¥** | âŒ æœªã‚µãƒãƒ¼ãƒˆ | å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ¸¡ã•ãªã„è¨­è¨ˆ |
| **æœ¬æ¸¬å®šã§ã®è¨­å®š** | `enable_prefix_caching: false` | Neuron å´ã®æ©Ÿèƒ½ã‚’ç„¡åŠ¹åŒ– |

### ãƒªãƒªãƒ¼ã‚¹ãƒãƒ¼ãƒˆç¢ºèª

[NxD Inference Release Notes](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/neuronx-distributed-inference/neuronx-distributed-inference.html)ã‚ˆã‚Šï¼š

**Neuron 2.24.0**
- **Automatic Prefix Caching (APC) ã‚µãƒãƒ¼ãƒˆé–‹å§‹**
- **vLLM çµŒç”±ã§ã®ã¿å‹•ä½œ**
- 3.2x TTFTæ”¹å–„ï¼ˆ90%ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆæ™‚ã€Llama3.3 70Bï¼‰

### vllm-neuron ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```mermaid
%%{init: {'theme':'dark'}}%%
graph TD
    subgraph GPU["é€šå¸¸ã® vLLM (GPU ãªã©)"]
        A1[Scheduler<br/>ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°] --> A2[KVCacheCoordinator]
        A2 --> A3[Hash-based prefix caching<br/>PagedAttention<br/>Block allocation/eviction<br/>LRU eviction policy]
        A3 --> A4[GPU Worker<br/>å®Ÿè¡Œ]
    end

    subgraph Neuron["vllm-neuron"]
        B1[Scheduler<br/>full_context_lens è¨ˆç®—<br/>computed_context_lens è¨ˆç®—] --> B2["âŒ KVCacheCoordinator<br/>(ä½¿ç”¨ã•ã‚Œãªã„)"]
        B2 -.->|bypass| B3[vllm-neuron Plugin]
        B3 --> B4[is_prefix_caching è¨­å®š<br/>ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ Neuron ã«æ¸¡ã™]
        B4 --> B5[NxD Inference<br/>Neuron ãƒ¬ãƒ™ãƒ«]
        B5 --> B6[Prefix caching å®Ÿè£…<br/>KV ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†<br/>ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³]
    end

    style GPU fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style Neuron fill:#1a1a2e,stroke:#16213e,stroke-width:2px,color:#fff
    style A2 fill:#0f3460,stroke:#16213e,color:#fff
    style B2 fill:#8b0000,stroke:#16213e,color:#fff
    style B6 fill:#0f3460,stroke:#16213e,color:#fff
```

### é‡è¦ãªç™ºè¦‹

**vLLM ä½¿ç”¨æ™‚ã€prefix caching ã¯ Neuron å´ã§å®Ÿè¡Œã•ã‚Œã‚‹**ã“ã¨ãŒåˆ¤æ˜ã—ã¾ã—ãŸã€‚

ã¾ãšã€vLLM å´ã®ç‹¬è‡ª prefix caching å®Ÿè£…ï¼ˆ`KVCacheCoordinator`ï¼‰ã¯å®Œå…¨ã«ãƒã‚¤ãƒ‘ã‚¹ã•ã‚Œã¾ã™ã€‚é€šå¸¸ã® vLLM ã§ã¯ hash-based ã‚„ PagedAttention-based ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ãŒè¡Œã‚ã‚Œã¾ã™ãŒã€vllm-neuron ã§ã¯ã“ã®æ©Ÿæ§‹ã¯ä½¿ç”¨ã•ã‚Œã¾ã›ã‚“ã€‚

ä»£ã‚ã‚Šã«ã€Neuron å´ã§ prefix caching ãŒå®Œå…¨ã«å®Ÿè£…ã•ã‚Œã¾ã™ã€‚vLLM å´ã¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¨ˆç®—ï¼ˆ`full_context_lens`, `computed_context_lens`ï¼‰ã®ã¿ã‚’æ‹…å½“ã—ã€å®Ÿéš›ã® prefix caching å®Ÿè¡Œã¯ Neuron å´ã§ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒ¬ãƒ™ãƒ«ã§è¡Œã‚ã‚Œã¾ã™ã€‚

ã“ã®è¨­è¨ˆã«ã‚ˆã‚Šã€vLLM å´ã®è¿½åŠ ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã¯æœ€å°é™ã«æŠ‘ãˆã‚‰ã‚Œã¦ã„ã¾ã™ã€‚ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¨ˆç®—ã®ã¿ã§æ¸ˆã‚€ãŸã‚ã€æ—¢å­˜ã® 52ms ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã«æ¯”ã¹ã¦ç„¡è¦–ã§ãã‚‹ç¨‹åº¦ã®å½±éŸ¿ã—ã‹ã‚ã‚Šã¾ã›ã‚“ã€‚

### ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å½±éŸ¿ï¼ˆæ¨å®šï¼‰

:::message alert
**æ³¨æ„**: æœ¬æ¸¬å®šã§ã¯ prefix caching ã¯ç„¡åŠ¹ã«ã—ã¦ã„ã¾ã™ï¼ˆ`enable_prefix_caching: false`ï¼‰ã€‚ä»¥ä¸‹ã®æ•°å€¤ã¯ã€[NxD Inference Release Notes](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/neuronx-distributed-inference/neuronx-distributed-inference.html)ã«è¨˜è¼‰ã•ã‚ŒãŸã€Œ3.2x TTFTæ”¹å–„ï¼ˆ90% cache hitæ™‚ï¼‰ã€ã®ãƒ‡ãƒ¼ã‚¿ã‚’åŸºã«æ¨å®šã—ãŸã‚‚ã®ã§ã™ã€‚
:::

#### Rerankerï¼ˆçŸ­ã„æ¨è«–ã€ä»Šå›ã®æ¸¬å®šï¼‰

```
Prefix cachingç„¡åŠ¹ï¼ˆå®Ÿæ¸¬ï¼‰:
- vLLM: 64.33 ms/batch (Neuron 12ms + ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ 52ms)
- NxDç›´æ¥: 12.06 ms/batch

Prefix cachingæœ‰åŠ¹ï¼ˆæ¨å®šã€90% cache hitæ™‚ï¼‰:
- vLLM: ~56 ms/batch (Neuron 4ms + ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ 52ms)
  â†’ 12%æ”¹å–„ï¼ˆåŠ¹æœã¯é™å®šçš„ï¼‰
- NxDç›´æ¥: ~3.8 ms/batch
  â†’ 68%æ”¹å–„ï¼ˆå¤§ããªåŠ¹æœï¼‰

æ¨å®šæ ¹æ‹ : 12.06ms Ã· 3.2 â‰ˆ 3.8ms
```

**çµè«–**: **çŸ­ã„æ¨è«–ã§ã¯ã€vLLM ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒæ”¯é…çš„**ãªãŸã‚ã€prefix caching ã®åŠ¹æœã¯é™å®šçš„ã€‚

#### é€šå¸¸ã®ç”Ÿæˆï¼ˆé•·ã„æ¨è«–ï¼‰

```
ä¾‹: 100ãƒˆãƒ¼ã‚¯ãƒ³ç”Ÿæˆ @ 12 ms/token = 1200 ms

Prefix cachingç„¡åŠ¹:
- vLLM: 1252 ms (1200 + 52)
- NxD: 1200 ms

Prefix cachingæœ‰åŠ¹ï¼ˆ90% cache hitï¼‰:
- vLLM: 427 ms (375 + 52) â†’ 2.9xé«˜é€ŸåŒ–
- NxD: 375 ms â†’ 3.2xé«˜é€ŸåŒ–
```

**çµè«–**: **é•·ã„æ¨è«–ã§ã¯ã€prefix caching ã®åŠ¹æœãŒæ”¯é…çš„**ã€‚vLLM ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã®å‰²åˆãŒå°ã•ããªã‚‹ã€‚

---

## æ¸¬å®šæ–¹æ³•ã¨å†ç¾æ‰‹é †

### å‰ææ¡ä»¶

```bash
# ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èµ·å‹•
aws ec2 run-instances \
  --instance-type inf2.xlarge \
  --image-id ami-xxxxx  # Neuron SDK pre-installed AMI
```

### ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

```bash
# 1. Neuron SDKç¢ºèª
neuron-ls --version
# å‡ºåŠ›ä¾‹: neuron-ls 2.27.33.0

# 2. vLLMç’°å¢ƒã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ãƒˆ
source /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/bin/activate

# 3. ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç¢ºèª
python -c "import vllm; print('vLLM:', vllm.__version__)"
python -c "import neuronx_distributed_inference as nxdi; print('NxD:', nxdi.__version__)"

# å‡ºåŠ›ä¾‹:
# vLLM: 0.13.0
# NxD: 0.7.0

# 4. ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
mkdir -p ~/data-science/investigations
cd ~/data-science/investigations
```

### ãƒ¢ãƒ‡ãƒ«æº–å‚™

```bash
# Hugging Faceã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
mkdir -p inf2-vllm-performance/models
cd inf2-vllm-performance/models

huggingface-cli download tosshitsu/Qwen3-0.6B-Reranker \
  --local-dir Qwen3-0.6B-Reranker \
  --local-dir-use-symlinks False
```

### æ¸¬å®š1: vLLMï¼ˆ30ãƒãƒƒãƒæ¸¬å®šï¼‰

:::details vLLM ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```bash
cd ~/test2/my-reranker

# config.yamlä½œæˆ
cat > config.yaml << 'EOF'
model:
  path: "/home/coder/data-science/investigations/inf2-vllm-performance/models/Qwen3-0.6B-Reranker"

vllm:
  tensor_parallel_size: 2
  max_num_seqs: 4
  block_size: 32
  max_model_len: 2048
  max_num_batched_tokens: 256
  num_gpu_blocks_override: 512
  enable_prefix_caching: false
  dtype: "bfloat16"

  additional_config:
    override_neuron_config:
      enable_bucketing: true
      pa_num_blocks: 512
      pa_block_size: 32
EOF

# æ¸¬å®šã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ
cat > benchmark_30batches.py << 'EOF'
import time
import yaml
from vllm import LLM, SamplingParams

# Load config
with open('config.yaml', 'r') as f:
    config = yaml.safe_load(f)

model_path = config['model']['path']
vllm_config = config['vllm']

# Initialize LLM
print("Initializing vLLM...")
llm = LLM(model=model_path, **vllm_config)

# Sampling parameters
sampling_params = SamplingParams(
    max_tokens=1,
    temperature=0.0,
    logprobs=20,
)

# Prepare batch prompts (97 tokens)
prompt_template = """<|im_start|>system
Judge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be "yes" or "no".<|im_end|>
<|im_start|>user
<Instruct>: Given a web search query, retrieve relevant passages that answer the query
<Query>: ãƒ—ãƒ­ã‚»ã‚¹ã«ã¤ã„ã¦æ•™ãˆã¦
<Document>: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®çµ±åˆã•ã‚ŒãŸæ©Ÿèƒ½ã‚’åˆ†æã™ã‚‹æ–¹æ³•<|im_end|>
<|im_start|>assitant
<think>

</think>
"""

batch_prompts = [prompt_template] * 4

# Warmup
print("Warmup...")
llm.generate(batch_prompts, sampling_params, use_tqdm=False)

# Benchmark: 30 batches
print("\nBenchmarking 30 batches...")
batch_times = []

for i in range(30):
    start = time.perf_counter()
    outputs = llm.generate(batch_prompts, sampling_params, use_tqdm=False)
    elapsed = (time.perf_counter() - start) * 1000
    batch_times.append(elapsed)

    if (i + 1) % 10 == 0:
        print(f"  Batch {i+1}: {elapsed:.2f} ms")

# Results
print(f"\n{'='*50}")
print("Results (30 batches)")
print(f"{'='*50}")
print(f"Average: {sum(batch_times)/len(batch_times):.2f} ms/batch")
print(f"Min: {min(batch_times):.2f} ms")
print(f"Max: {max(batch_times):.2f} ms")
EOF

# å®Ÿè¡Œ
python benchmark_30batches.py

# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
# Average: 64.33 ms/batch
# Min: 60.78 ms
# Max: 85.76 ms
```

:::

### æ¸¬å®š2: NxD Inference 4ãƒ‘ã‚¿ãƒ¼ãƒ³

:::details NxD Inference 4ãƒ‘ã‚¿ãƒ¼ãƒ³æ¸¬å®šã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```bash
cd ~/data-science/investigations

# neuronx-distributed-inference ãƒªãƒã‚¸ãƒˆãƒªã‚¯ãƒ­ãƒ¼ãƒ³
git clone https://github.com/aws-neuron/neuronx-distributed-inference.git
cd neuronx-distributed-inference

# 4ãƒ‘ã‚¿ãƒ¼ãƒ³æ¸¬å®šã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ
cat > benchmark_4patterns.sh << 'EOF'
#!/bin/bash
cd /home/coder/data-science/investigations/neuronx-distributed-inference
source /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/bin/activate

# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
PROMPT_FILE="/tmp/nxd_benchmark_prompt.txt"
cat > "$PROMPT_FILE" << 'PROMPT_EOF'
<|im_start|>system
Judge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be "yes" or "no".<|im_end|>
<|im_start|>user
<Instruct>: Given a web search query, retrieve relevant passages that answer the query
<Query>: ãƒ—ãƒ­ã‚»ã‚¹ã«ã¤ã„ã¦æ•™ãˆã¦
<Document>: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®çµ±åˆã•ã‚ŒãŸæ©Ÿèƒ½ã‚’åˆ†æã™ã‚‹æ–¹æ³•<|im_end|>
<|im_start|>assitant
<think>

</think>
PROMPT_EOF

PROMPT=$(cat "$PROMPT_FILE")
MODEL_PATH="/home/coder/data-science/investigations/inf2-vllm-performance/models/Qwen3-0.6B-Reranker"

echo "=========================================="
echo "NxD Inference 4ãƒ‘ã‚¿ãƒ¼ãƒ³æ¸¬å®š"
echo "=========================================="

# ãƒ‘ã‚¿ãƒ¼ãƒ³A: caching=OFF, bucketing=OFF
echo "=== ãƒ‘ã‚¿ãƒ¼ãƒ³A: caching=OFF, bucketing=OFF ==="
rm -rf /tmp/nxd-compiled-A
python src/neuronx_distributed_inference/inference_demo.py \
  --model-type qwen3 \
  --task-type causal-lm \
  run \
  --model-path "$MODEL_PATH" \
  --prompt "$PROMPT" \
  --tp-degree 2 \
  --batch-size 4 \
  --seq-len 128 \
  --max-context-length 128 \
  --max-length 129 \
  --benchmark \
  --compiled-model-path /tmp/nxd-compiled-A \
  --benchmark-report-path /tmp/benchmark_A_cache_off_bucket_off.json

# ãƒ‘ã‚¿ãƒ¼ãƒ³B: caching=OFF, bucketing=ON
echo "=== ãƒ‘ã‚¿ãƒ¼ãƒ³B: caching=OFF, bucketing=ON ==="
rm -rf /tmp/nxd-compiled-B
python src/neuronx_distributed_inference/inference_demo.py \
  --model-type qwen3 \
  --task-type causal-lm \
  run \
  --model-path "$MODEL_PATH" \
  --prompt "$PROMPT" \
  --tp-degree 2 \
  --batch-size 4 \
  --seq-len 128 \
  --max-context-length 128 \
  --max-length 129 \
  --benchmark \
  --enable-bucketing \
  --compiled-model-path /tmp/nxd-compiled-B \
  --benchmark-report-path /tmp/benchmark_B_cache_off_bucket_on.json

# çµæœè¡¨ç¤º
echo ""
echo "çµæœã‚µãƒãƒªãƒ¼:"
python -c "
import json

patterns = {
    'A (bucketing=OFF)': '/tmp/benchmark_A_cache_off_bucket_off.json',
    'B (bucketing=ON)': '/tmp/benchmark_B_cache_off_bucket_on.json',
}

for name, path in patterns.items():
    try:
        with open(path) as f:
            data = json.load(f)
            latency = data['context_encoding_model']['latency_ms_avg']
            print(f'{name}: {latency:.2f} ms')
    except:
        print(f'{name}: Failed')
"
EOF

chmod +x benchmark_4patterns.sh

# å®Ÿè¡Œï¼ˆç´„10-15åˆ†: ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«å«ã‚€ï¼‰
./benchmark_4patterns.sh

# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
# ãƒ‘ã‚¿ãƒ¼ãƒ³A (bucketing=OFF): 12.06 ms
# ãƒ‘ã‚¿ãƒ¼ãƒ³B (bucketing=ON): 23.60 ms
```

:::

### æ¸¬å®š3: NxD Inference å†…éƒ¨ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°

:::details NxD Inference å†…éƒ¨ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆå†ç¾æ‰‹é †ï¼‰

```bash
cd ~/data-science/investigations/neuronx-distributed-inference

# è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ
cat > profile_nxd_detailed.py << 'EOF'
"""
NxD Inferenceè©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
inference_demo.pyã®å†…éƒ¨å‡¦ç†ã‚’æ™‚é–“æ¸¬å®š
"""
import time
import torch
from transformers import AutoTokenizer

from neuronx_distributed_inference.models.config import NeuronConfig
from neuronx_distributed_inference.models.qwen3.modeling_qwen3 import (
    NeuronQwen3ForCausalLM,
    Qwen3InferenceConfig,
)
from neuronx_distributed_inference.utils.hf_adapter import load_pretrained_config

# Model paths
model_path = "/home/coder/data-science/investigations/inf2-vllm-performance/models/Qwen3-0.6B-Reranker"
compiled_model_path = "/home/coder/data-science/investigations/inf2-vllm-performance/models/Qwen3-0.6B-Reranker-nxd"

print("=" * 80)
print("NxD Inference è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°")
print("=" * 80)

# Configuration (bucketing=True)
neuron_config = NeuronConfig(
    tp_degree=2,
    batch_size=4,
    max_context_length=128,
    seq_len=128,
    enable_bucketing=True,
)

config = Qwen3InferenceConfig(
    neuron_config,
    load_config=load_pretrained_config(model_path),
)

print(f"Configuration: tp_degree={neuron_config.tp_degree}, batch_size={neuron_config.batch_size}, bucketing={neuron_config.enable_bucketing}")

# Load model
print("\nLoading model...")
model = NeuronQwen3ForCausalLM(compiled_model_path)
model.load(compiled_model_path)
print("âœ“ Model loaded")

# Prepare input
prompt = """<|im_start|>system
Judge whether the Document meets the requirements based on the Query and the Instruct provided. Note that the answer can only be "yes" or "no".<|im_end|>
<|im_start|>user
<Instruct>: Given a web search query, retrieve relevant passages that answer the query
<Query>: ãƒ—ãƒ­ã‚»ã‚¹ã«ã¤ã„ã¦æ•™ãˆã¦
<Document>: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã®çµ±åˆã•ã‚ŒãŸæ©Ÿèƒ½ã‚’åˆ†æã™ã‚‹æ–¹æ³•<|im_end|>
<|im_start|>assitant
<think>

</think>
"""

prompts = [prompt] * 4  # batch_size=4

tokenizer = AutoTokenizer.from_pretrained(model_path)
inputs = tokenizer(prompts, padding=True, return_tensors="pt", truncation=False)

batch_size, seq_len = inputs.input_ids.shape
position_ids = torch.arange(seq_len, dtype=torch.long).unsqueeze(0).expand(batch_size, -1)

# Warmup
print("\nWarmup...")
with torch.no_grad():
    _ = model(inputs.input_ids, attention_mask=inputs.attention_mask, position_ids=position_ids)

# Detailed benchmarking
print("\n" + "=" * 80)
print("è©³ç´°ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼ˆ30å›æ¸¬å®šï¼‰")
print("=" * 80)

num_iterations = 30
times_total = []
times_forward = []

for i in range(num_iterations):
    t_start = time.perf_counter()

    t_forward_start = time.perf_counter()
    with torch.no_grad():
        outputs = model(inputs.input_ids, attention_mask=inputs.attention_mask, position_ids=position_ids)
    t_forward_end = time.perf_counter()

    t_end = time.perf_counter()

    times_total.append((t_end - t_start) * 1000)
    times_forward.append((t_forward_end - t_forward_start) * 1000)

    if (i + 1) % 10 == 0:
        print(f"  Iteration {i+1}: total={times_total[-1]:.2f} ms")

print("\n" + "=" * 80)
print("çµæœ")
print("=" * 80)

avg_total = sum(times_total) / len(times_total)
avg_forward = sum(times_forward) / len(times_forward)

print(f"\nTotal time:")
print(f"  å¹³å‡: {avg_total:.2f} ms")
print(f"  æœ€å°: {min(times_total):.2f} ms")
print(f"  æœ€å¤§: {max(times_total):.2f} ms")

print(f"\nForward pass:")
print(f"  å¹³å‡: {avg_forward:.2f} ms  ({avg_forward/avg_total*100:.1f}% of total)")

print(f"\n" + "=" * 80)
print("å†…è¨³")
print("=" * 80)
print(f"Forward pass:        {avg_forward:.2f} ms ({avg_forward/avg_total*100:.1f}%)")
print(f"Python overhead:     {(avg_total-avg_forward):.2f} ms ({(avg_total-avg_forward)/avg_total*100:.1f}%)")
EOF

# å®Ÿè¡Œï¼ˆäº‹å‰ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ï¼‰
python profile_nxd_detailed.py

# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
# Forward pass:        12.70 ms (100.0%)
# Python overhead:     0.00 ms (0.0%)
```

:::

---

## ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã®7åŸå‰‡

ä»Šå›ã®èª¿æŸ»ã‹ã‚‰å¾—ã‚‰ã‚ŒãŸã€è¤‡é›‘ãªã‚·ã‚¹ãƒ†ãƒ ã‚’ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã™ã‚‹éš›ã®æ™®éçš„ãªåŸå‰‡ã‚’ã¾ã¨ã‚ã¾ã™ã€‚

### åŸå‰‡1: ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã«æ¸¬å®šã™ã‚‹ï¼ˆLayer-by-Layer Measurementï¼‰

è¤‡é›‘ãªã‚·ã‚¹ãƒ†ãƒ ã¯è¤‡æ•°ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’å€‹åˆ¥ã«æ¸¬å®šã—ã€å·®åˆ†ã‚’å–ã‚‹ã“ã¨ã§ã€ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã®æ‰€åœ¨ã‚’ç‰¹å®šã§ãã¾ã™ã€‚

```
L1: ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒ¬ãƒ™ãƒ«  (Neuron Profiler â†’ å€‹ã€…ã®æ¼”ç®—)
L2: SDK ãƒ¬ãƒ™ãƒ«           (NxD Inference â†’ 12.06ms)
L3: ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ãƒ¬ãƒ™ãƒ« (vLLM â†’ 64.33ms)
L4: ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¬ãƒ™ãƒ« (ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰)

ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ = L(n+1) - L(n)
```

**é©ç”¨ä¾‹**: ä»Šå›ã¯ L2 (NxD) ã‚’æ¸¬å®šã—ã¦ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’ç¢ºç«‹ã—ãŸã“ã¨ã§ã€L3 (vLLM) ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’å®šé‡åŒ–ã§ãã¾ã—ãŸã€‚

### åŸå‰‡2: ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‹ã‚‰å§‹ã‚ã‚‹ï¼ˆSimple Baseline Firstï¼‰

æœ€ã‚‚å˜ç´”ãªå®Ÿè£…ã‚’ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã—ã¦æ¸¬å®šã—ã¦ã‹ã‚‰ã€ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’è¿½åŠ ã—ã¦ã„ãã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ã‚³ã‚¹ãƒˆã‚’æ˜ç¢ºã«ã§ãã¾ã™ã€‚

**ä»Šå›ã®é©ç”¨**:
1. âœ… NxD Inference ç›´æ¥ä½¿ç”¨ï¼ˆæœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ï¼‰â†’ 12.06ms
2. âœ… vLLM çµŒç”±ï¼ˆè¤‡é›‘ã•è¿½åŠ ï¼‰â†’ 64.33ms
3. âœ… å·®åˆ† = vLLM ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ = 52.27ms

**ã‚¢ãƒ³ãƒãƒ‘ã‚¿ãƒ¼ãƒ³**: æœ€åˆã‹ã‚‰ vLLM ã§æ¸¬å®šã—ã€ã€Œé…ã„ã€ã¨ã„ã†çµè«–ã ã‘ã§çµ‚ã‚ã‚‹ã“ã¨ã€‚

### åŸå‰‡3: æ¸¬å®šãƒ„ãƒ¼ãƒ«ã®é™ç•Œã‚’ç†è§£ã™ã‚‹ï¼ˆKnow Your Tools' Limitsï¼‰

å„ãƒ„ãƒ¼ãƒ«ã«ã¯å›ºæœ‰ã®é™ç•ŒãŒã‚ã‚Šã¾ã™ã€‚å˜ä¸€ãƒ„ãƒ¼ãƒ«ã§ã¯å…¨ä½“åƒã‚’æŠŠæ¡ã§ãã¾ã›ã‚“ã€‚

| ãƒ„ãƒ¼ãƒ« | å¼·ã¿ | é™ç•Œ | ä»Šå›ã®ä½¿ç”¨ |
|--------|------|------|-----------|
| **Neuron Profiler** | ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒ¬ãƒ™ãƒ«ã®è©³ç´° | âŒ ãƒãƒƒãƒå‡¦ç†æ™‚é–“ã‚’ç›´æ¥æ¸¬å®šä¸å¯ | Phase 1-2 |
| **line_profiler** | Python ãƒ¬ãƒ™ãƒ«ã®è©³ç´° | âŒ ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ã®å£ | Phase 3-5 |
| **time.perf_counter()** | ã‚·ãƒ³ãƒ—ãƒ«ã§æ­£ç¢º | âŒ å†…éƒ¨ã®è©³ç´°ãŒè¦‹ãˆãªã„ | Phase 6 |

**æ•™è¨“**: è¤‡æ•°ãƒ„ãƒ¼ãƒ«ã‚’çµ„ã¿åˆã‚ã›ã€ç›¸äº’ã«æ¤œè¨¼ã™ã‚‹ã€‚

### åŸå‰‡4: æ¸¬å®šæ¡ä»¶ã®å½±éŸ¿ã‚’è€ƒæ…®ã™ã‚‹ï¼ˆContext Mattersï¼‰

æ¸¬å®šæ¡ä»¶ãŒçµæœã«å¤§ããå½±éŸ¿ã—ã¾ã™ã€‚å®Ÿéš›ã®ä½¿ç”¨æ¡ä»¶ã«è¿‘ã„ç’°å¢ƒã§æ¸¬å®šã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚

**ä»Šå›ã®è½ã¨ã—ç©´**:
```python
# å…¨ã¦åŒã˜97ãƒˆãƒ¼ã‚¯ãƒ³ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
batch_prompts = [prompt_97tokens] * 4
```

ã“ã®æ¡ä»¶ä¸‹ã§ã¯ï¼š
- âœ… bucketing=OFF ãŒæœ€é€Ÿï¼ˆå®Œå…¨ä¸€è‡´ã€ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ä¸è¦ï¼‰
- âš ï¸ bucketing=ON ãŒé…ã„ï¼ˆä¸è¦ãªãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼‰

**ã—ã‹ã—å®Ÿéš›ã® vLLM ä½¿ç”¨ã§ã¯**:
- å¯å¤‰é•·ãƒãƒƒãƒãŒæ¨™æº–
- bucketing=ON ãŒå¿…é ˆï¼ˆå‹•çš„ãƒãƒƒãƒãƒ³ã‚°ï¼‰

**æ•™è¨“**: æ¸¬å®šæ¡ä»¶ã¨å®Ÿç’°å¢ƒã®ã‚®ãƒ£ãƒƒãƒ—ã‚’èªè­˜ã—ã€çµæœã®é©ç”¨ç¯„å›²ã‚’æ˜ç¤ºã™ã‚‹ã€‚

### åŸå‰‡5: äºˆæƒ³å¤–ã®çµæœã‚’æ˜ã‚Šä¸‹ã’ã‚‹ï¼ˆInvestigate Surprisesï¼‰

äºˆæƒ³ã¨ç•°ãªã‚‹çµæœã¯ã€ç†è§£ã®æ¬ å¦‚ã‚’ç¤ºã™ã‚·ã‚°ãƒŠãƒ«ã§ã™ã€‚

**ä»Šå›ã®é©šã**:
```
äºˆæƒ³: bucketing=ON â†’ é«˜é€ŸåŒ–ï¼ˆvLLM ã§ã®å®Ÿç¸¾ï¼‰
çµæœ: bucketing=ON â†’ 1.96å€ä½é€ŸåŒ–ï¼ˆNxDç›´æ¥ï¼‰
```

**æ˜ã‚Šä¸‹ã’ãŸçµæœ**:
- vLLM: å¯å¤‰é•·ãƒãƒƒãƒ â†’ bucketing ãŒåŠ¹ç‡çš„
- NxDç›´æ¥ï¼ˆä»Šå›ï¼‰: å›ºå®šé•·ãƒãƒƒãƒ â†’ bucketing ã¯ä¸è¦

**æ•™è¨“**: äºˆæƒ³å¤–ã®çµæœã‚’ã€Œãƒã‚°ã€ã¨æ±ºã‚ã¤ã‘ãšã€æ¸¬å®šæ¡ä»¶ã‚„å‰æã‚’å†æ¤œè¨ã™ã‚‹ã€‚

### åŸå‰‡6: ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’æ˜ç¢ºã«ã™ã‚‹ï¼ˆQuantify Trade-offsï¼‰

æ€§èƒ½ã ã‘ã§ãªãã€æ©Ÿèƒ½ã¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’å®šé‡åŒ–ã—ã¾ã™ã€‚

| é …ç›® | NxD ç›´æ¥ | vLLM |
|------|----------|------|
| **ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·** | âœ… 12.06ms (5.3å€é«˜é€Ÿ) | âš ï¸ 64.33ms |
| **ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ** | âœ… 42,400 tok/s | âš ï¸ ç´„8,000 tok/s |
| **Continuous batching** | âŒ éå¯¾å¿œ | âœ… å¯¾å¿œ |
| **å¯å¤‰é•·ãƒãƒƒãƒ** | âŒ å›ºå®šã®ã¿ | âœ… å‹•çš„ |
| **OpenAI API äº’æ›** | âŒ éå¯¾å¿œ | âœ… å¯¾å¿œ |
| **å®Ÿè£…ã®è¤‡é›‘ã•** | âœ… ã‚·ãƒ³ãƒ—ãƒ« | âš ï¸ è¤‡é›‘ |

**é©ç”¨ã‚¬ã‚¤ãƒ‰**:
- **Rerankerï¼ˆå›ºå®šé•·ã€çŸ­ã„æ¨è«–ï¼‰**: NxD ç›´æ¥ã‚’æ¨å¥¨
- **é€šå¸¸ã®ç”Ÿæˆï¼ˆå¯å¤‰é•·ã€é•·ã„æ¨è«–ï¼‰**: vLLM ã‚’æ¨å¥¨ï¼ˆã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰å‰²åˆãŒä½ä¸‹ï¼‰

### åŸå‰‡7: ã€Œåˆ†ã‹ã‚‰ãªã„ã€ã¨èªã‚ã‚‹å‹‡æ°—ï¼ˆAdmit Unknownsï¼‰

å®Œç’§ãªæ¸¬å®šã¯ä¸å¯èƒ½ãªå ´åˆãŒã‚ã‚Šã¾ã™ã€‚ä¸ç¢ºå®Ÿæ€§ã‚’æ˜ç¤ºã™ã‚‹ã“ã¨ãŒã€èª¤ã£ãŸæœ€é©åŒ–ã‚’é¿ã‘ã‚‹ãŸã‚ã®ç¬¬ä¸€æ­©ã§ã™ã€‚

**ä»Šå›ã®ä¾‹**:

```
Phase 3-5 æ™‚ç‚¹ï¼ˆline_profiler ã®ã¿ï¼‰:
â”œâ”€ outputs_queue.get() = 13.962 ms/step
â”‚  â”œâ”€ Neuron å®Ÿè¡Œ: ??? ms          â† ä¸æ˜
â”‚  â”œâ”€ ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³: ??? ms  â† ä¸æ˜
â”‚  â””â”€ Ray IPC: ??? ms              â† ä¸æ˜

Phase 6ï¼ˆNxD ç›´æ¥æ¸¬å®šå¾Œï¼‰:
â”œâ”€ Neuron å®Ÿè¡Œ: 12.06 ms           â† ç¢ºå®šâœ…
â””â”€ vLLM ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰: 52.27 ms   â† ç¢ºå®šâœ…
```

**æ•™è¨“**:
- âŒ ä¸æ˜ãªå€¤ã‚’æ¨æ¸¬ã§åŸ‹ã‚ãªã„
- âœ… ã€Œæ¸¬å®šã§ããªã„ã€ã¨æ˜è¨˜ã—ã€ä»£æ›¿æ‰‹æ³•ã‚’æ¢ã™
- âœ… æ–°ã—ã„æ¸¬å®šæ‰‹æ³•ã§ç¢ºå®šã§ããŸæ™‚ç‚¹ã§æ›´æ–°ã™ã‚‹

### ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã®ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‚’å§‹ã‚ã‚‹å‰ã«ã€ä»¥ä¸‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼š

- [ ] **åŸå‰‡1**: å„ãƒ¬ã‚¤ãƒ¤ãƒ¼ï¼ˆHW/SDK/FW/Appï¼‰ã‚’å€‹åˆ¥ã«æ¸¬å®šã™ã‚‹è¨ˆç”»ãŒã‚ã‚‹ã‹ï¼Ÿ
- [ ] **åŸå‰‡2**: ã‚·ãƒ³ãƒ—ãƒ«ãªãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³æ¸¬å®šã‹ã‚‰å§‹ã‚ã¦ã„ã‚‹ã‹ï¼Ÿ
- [ ] **åŸå‰‡3**: ä½¿ç”¨ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã®é™ç•Œã‚’ç†è§£ã—ã¦ã„ã‚‹ã‹ï¼Ÿ
- [ ] **åŸå‰‡4**: æ¸¬å®šæ¡ä»¶ã¯å®Ÿç’°å¢ƒã‚’ä»£è¡¨ã—ã¦ã„ã‚‹ã‹ï¼Ÿ
- [ ] **åŸå‰‡5**: äºˆæƒ³å¤–ã®çµæœã‚’æ˜ã‚Šä¸‹ã’ã‚‹æº–å‚™ãŒã‚ã‚‹ã‹ï¼Ÿ
- [ ] **åŸå‰‡6**: æ€§èƒ½ã¨ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã™ã‚‹æ©Ÿèƒ½ã‚’ç‰¹å®šã—ã¦ã„ã‚‹ã‹ï¼Ÿ
- [ ] **åŸå‰‡7**: æ¸¬å®šã§ããªã„é …ç›®ã‚’æ˜ç¤ºã—ã¦ã„ã‚‹ã‹ï¼Ÿ

### ãƒ¡ã‚¿åŸå‰‡: æ¸¬å®šã‚‚ç§‘å­¦ã§ã‚ã‚‹

æœ€ã‚‚é‡è¦ãªæ•™è¨“ã¯ã€**ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã¯ç§‘å­¦çš„ãƒ—ãƒ­ã‚»ã‚¹ã§ã‚ã‚‹**ã¨ã„ã†ã“ã¨ã§ã™ï¼š

1. **ä»®èª¬ã‚’ç«‹ã¦ã‚‹**: ã€ŒvLLM ã¯é…ã„ã¯ãšã€
2. **æ¸¬å®šã‚’è¨­è¨ˆã™ã‚‹**: ãƒ¬ã‚¤ãƒ¤ãƒ¼ã”ã¨ã€ãƒ„ãƒ¼ãƒ«ã”ã¨ã«è¨ˆç”»
3. **ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã™ã‚‹**: è¤‡æ•°ã®æ–¹æ³•ã§æ¸¬å®š
4. **ä»®èª¬ã‚’æ¤œè¨¼ã™ã‚‹**: äºˆæƒ³ã¨çµæœã‚’æ¯”è¼ƒ
5. **äºˆæƒ³å¤–ã‚’æ¢æ±‚ã™ã‚‹**: bucketing ã®é€†è»¢ç¾è±¡
6. **çµè«–ã‚’æ–‡æ›¸åŒ–ã™ã‚‹**: æˆåŠŸã‚‚å¤±æ•—ã‚‚è¨˜éŒ²
7. **å†ç¾æ€§ã‚’ç¢ºä¿ã™ã‚‹**: è©³ç´°ãªæ‰‹é †ã‚’æä¾›

**ã€Œåˆ†ã‹ã‚‰ãªã„ã€ã¨èªã‚ã‚‹ã“ã¨ãŒã€é–“é•ã£ãŸæœ€é©åŒ–ã‚’é¿ã‘ã‚‹ç¬¬ä¸€æ­©ã§ã™ã€‚**

---

## çµè«–ã¨æ¨å¥¨äº‹é …

### ä¸»è¦ãªç™ºè¦‹ã®ã¾ã¨ã‚

1. **vLLM ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã¯4.3å€**ï¼ˆrerankerãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã€Zenn æœ€é©å€¤è¨­å®šï¼‰
 - vLLM: 64.33 ms/batch
 - NxD: 12.06 ms/batch
 - ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰: +52 ms (81%)

2. **ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã¯å®Œå…¨ã« vLLM å´**
 - NxD Inference ã® Python ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰: ~0 ms
 - vLLM ã®ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹æ§‹é€ ã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã€é€šä¿¡ã‚³ã‚¹ãƒˆãŒè¦å› 

3. **bucketing è¨­å®šã¯ä½¿ç”¨ã‚±ãƒ¼ã‚¹ã«ä¾å­˜**
 - **å›ºå®šé•·ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰**: bucketing=OFF ãŒæœ€é€Ÿï¼ˆ12.06 msï¼‰
 - **å¯å¤‰é•·ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰**: bucketing=ON ãŒæ¨å¥¨ï¼ˆvLLM ä½¿ç”¨æ™‚ã¯å¿…é ˆï¼‰
 - åŒä¸€é•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®æ¸¬å®šã§ã¯ã€bucketing ã¯é€†åŠ¹æœ

4. **çŸ­ã„æ¨è«–ã»ã©ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã®å½±éŸ¿ãŒå¤§ãã„**
 - Reranker (12mså®Ÿè¡Œ): 81%ãŒã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰
 - é€šå¸¸ã®ç”Ÿæˆ (500mså®Ÿè¡Œ): 9%ãŒã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰
 - é•·æ–‡ç”Ÿæˆ (5000mså®Ÿè¡Œ): 1%ãŒã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰

5. **Prefix Caching ã¯ vllm çµŒç”±ã§ã®ã¿å‹•ä½œ**
 - Neuron 2.24.0 ä»¥é™ã§ã‚µãƒãƒ¼ãƒˆé–‹å§‹
 - vLLM å´ã®ç‹¬è‡ªå®Ÿè£…ã¯å®Œå…¨ã«ãƒã‚¤ãƒ‘ã‚¹ã•ã‚Œã€Neuron å´ã§å®Ÿè¡Œ
 - inference_demo.py ç›´æ¥å®Ÿè¡Œã§ã¯æœªã‚µãƒãƒ¼ãƒˆ

### ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰åˆ¥ã®æ¨å¥¨äº‹é …

#### Reranker / Embeddingãƒ¢ãƒ‡ãƒ«ï¼ˆçŸ­ã„æ¨è«–ï¼‰

**NxD Inference ç›´æ¥ä½¿ç”¨ã‚’æ¨å¥¨** âœ…

```python
# æ¨å¥¨è¨­å®š
neuron_config = NeuronConfig(
    tp_degree=2,
    batch_size=4,
    seq_len=128,              # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·ã«æœ€é©åŒ–
    max_context_length=128,
    enable_bucketing=False,   # å›ºå®šé•·ãªã‚‰OFF
    enable_prefix_caching=False,
)

# æœŸå¾…ã•ã‚Œã‚‹æ€§èƒ½
latency = 12.06 ms/batch
throughput = 42,400 tokens/s
```

**ãƒ¡ãƒªãƒƒãƒˆ**:
- âœ… 5å€ä»¥ä¸Šé«˜é€Ÿ
- âœ… ã‚·ãƒ³ãƒ—ãƒ«ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- âœ… ä½ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰

**ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ**:
- âŒ å›ºå®šãƒãƒƒãƒã‚µã‚¤ã‚º
- âŒ Continuous batchingä¸å¯
- âŒ OpenAI APIäº’æ›ãªã—

#### é€šå¸¸ã®ç”Ÿæˆã‚¿ã‚¹ã‚¯ï¼ˆä¸­ã€œé•·ã„æ¨è«–ï¼‰

**vLLM ä½¿ç”¨ã‚’æ¨å¥¨** âœ…

```yaml
# æ¨å¥¨è¨­å®š
vllm:
  tensor_parallel_size: 2
  max_num_seqs: 4
  block_size: 32
  enable_prefix_caching: false  # ã¾ãŸã¯ trueï¼ˆNeuron 2.24.0+ï¼‰

  additional_config:
    override_neuron_config:
      enable_bucketing: true     # å¿…é ˆ
```

**ãƒ¡ãƒªãƒƒãƒˆ**:
- âœ… Continuous batching
- âœ… å¯å¤‰é•·ãƒãƒƒãƒ
- âœ… OpenAI APIäº’æ›
- âœ… è¤‡é›‘ãªã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°
- âœ… ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰å‰²åˆã¯ä½ã„ï¼ˆæ¨è«–æ™‚é–“ãŒé•·ã„ãŸã‚ï¼‰

**ãƒ‡ãƒ¡ãƒªãƒƒãƒˆ**:
- âŒ 52msã®å›ºå®šã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰

### è¨­å®šãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ

#### vLLM ä½¿ç”¨æ™‚

- [ ] `enable_bucketing: true` ã‚’è¨­å®šï¼ˆ**å¿…é ˆ**ï¼‰
- [ ] `block_size: 32`ï¼ˆZenn æœ€é©å€¤ï¼‰
- [ ] `pa_num_blocks: 512`
- [ ] `tensor_parallel_size`ãŒ NeuronCore æ•°ã¨ä¸€è‡´
- [ ] `max_num_seqs`ãŒãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã«é©åˆ‡
- [ ] Prefix caching ã®æœ‰ç„¡ã‚’æ˜ç¤ºçš„ã«è¨­å®š

#### NxD Inference ç›´æ¥ä½¿ç”¨æ™‚

- [ ] `tp_degree`ãŒ NeuronCore æ•°ã¨ä¸€è‡´
- [ ] `batch_size`ãŒãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã«é©åˆ‡
- [ ] `seq_len`ãŒå®Ÿéš›ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·ã«æœ€é©åŒ–
- [ ] å›ºå®šé•·ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ãªã‚‰ `enable_bucketing=False`
- [ ] `enable_prefix_caching=False`ï¼ˆvLLM çµŒç”±ä»¥å¤–ã§ã¯ä½¿ç”¨ä¸å¯ï¼‰

### æ¸¬å®šæ™‚ã®æ³¨æ„ç‚¹

1. **bucketing ã®åŠ¹æœæ¸¬å®šã«ã¯å¯å¤‰é•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒå¿…è¦**
 - åŒä¸€é•·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ã¯çœŸã®åŠ¹æœã‚’è©•ä¾¡ã§ããªã„
 - ç•°ãªã‚‹é•·ã•ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ··ãœã¦æ¸¬å®šã™ã¹ã

2. **ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¡ä»¶ã‚’çµ±ä¸€**
 - æ–°è¦ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã¨æ—¢å­˜ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã§æ€§èƒ½ãŒç•°ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹

3. **ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã®é‡è¦æ€§**
 - åˆå›å®Ÿè¡Œã¯é…ã„ï¼ˆJITã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒŸã‚¹ï¼‰
 - æœ€ä½10å›ã¯ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—ã—ã¦ã‹ã‚‰æ¸¬å®š

4. **æ¸¬å®šå›æ•°**
 - vLLM: 30ãƒãƒƒãƒä»¥ä¸Šæ¨å¥¨
 - NxD: 20å›ä»¥ä¸Šæ¨å¥¨ï¼ˆinference_demo.pyãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰

---

## æœ€çµ‚ã¾ã¨ã‚

### åˆ†ã‹ã£ãŸã“ã¨

1. **æ¸¬å®šã®é›£ã—ã•**:
 - Neuron Profiler ã¯ graph variation ã‚’æ¸¬å®šã™ã‚‹ãŸã‚ã€ãƒãƒƒãƒå‡¦ç†æ™‚é–“ã‚’ç›´æ¥æ¸¬å®šã§ããªã„
 - line_profiler ã¯ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ã®å£ã‚’è¶Šãˆã‚‰ã‚Œãªã„
 - å˜ä¸€ãƒ„ãƒ¼ãƒ«ã§ã¯å…¨ä½“åƒã‚’æŠŠæ¡ã§ããªã„

2. **å‡¦ç†æ™‚é–“ã®å†…è¨³ï¼ˆç¢ºå®šï¼‰**:
 ```
   vLLM = 64.33 ms/batch
   â”œâ”€ Neuronå®Ÿè¡Œ: 12.06 ms (19%)  â† NxDç›´æ¥æ¸¬å®šã§ç¢ºå®š
   â””â”€ vLLMã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰: 52.27 ms (81%)
   ```

3. **æœ€é©åŒ–ã®ç¾å®Ÿ**:
 - vLLM ã®è¨­å®šèª¿æ•´: 10-15% æ”¹å–„
 - ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰å‰Šæ¸›: 20-30% æ”¹å–„
 - SDK ç›´æ¥ä½¿ç”¨: **5.3å€é«˜é€ŸåŒ–**ï¼ˆæ©Ÿèƒ½å–ªå¤±ã¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ï¼‰

### æ•™è¨“: è¤‡é›‘ãªã‚·ã‚¹ãƒ†ãƒ ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°

ä»Šå›ã®èª¿æŸ»ã‹ã‚‰å¾—ã‚‰ã‚ŒãŸé‡è¦ãªæ•™è¨“ã‚’ä»¥ä¸‹ã«ã¾ã¨ã‚ã¾ã™ã€‚

**1. å˜ä¸€ãƒ„ãƒ¼ãƒ«ã§ã¯ä¸ååˆ†**

Neuron Profiler ã¯ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã®è©³ç´°ã¯åˆ†ã‹ã‚Šã¾ã™ãŒã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¬ãƒ™ãƒ«ã®æ™‚é–“ã¯æ¸¬å®šã§ãã¾ã›ã‚“ã€‚line_profiler ã¯ Python ãƒ¬ãƒ™ãƒ«ã®æƒ…å ±ã¯å–å¾—ã§ãã¾ã™ãŒã€åˆ¥ãƒ—ãƒ­ã‚»ã‚¹ã®å‹•ä½œã¯è¦‹ãˆã¾ã›ã‚“ã€‚ã“ã®ã‚ˆã†ã«ã€å„ãƒ„ãƒ¼ãƒ«ã«ã¯å›ºæœ‰ã®é™ç•ŒãŒã‚ã‚‹ãŸã‚ã€è¤‡æ•°ã®ãƒ„ãƒ¼ãƒ«ã‚’çµ„ã¿åˆã‚ã›ã¦ç›¸äº’ã«æ¤œè¨¼ã™ã‚‹ã“ã¨ãŒå¿…é ˆã¨ãªã‚Šã¾ã™ã€‚

**2. æ¸¬å®šã®é™ç•Œã‚’èªè­˜ã™ã‚‹**

å®Œç’§ãªæ¸¬å®šã¯ä¸å¯èƒ½ãªå ´åˆãŒã‚ã‚Šã¾ã™ã€‚ãã®ã‚ˆã†ãªçŠ¶æ³ã§ã¯ã€åˆç†çš„ãªæ¨å®šã‚’è¡Œã„ã€ãã®ä¿¡é ¼åŒºé–“ã‚’æ˜ç¤ºã™ã‚‹ã“ã¨ãŒé‡è¦ã§ã™ã€‚ã¾ãŸã€ã€Œåˆ†ã‹ã‚‰ãªã„ã€ã¨èªã‚ã‚‹ã“ã¨ã‚‚ã€èª¤ã£ãŸæœ€é©åŒ–ã‚’é¿ã‘ã‚‹ãŸã‚ã®é‡è¦ãªçµè«–ã¨ãªã‚Šã¾ã™ã€‚

**3. ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ç†è§£ãŒéµ**

vLLM ã®ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ç†è§£ã—ã¦ã„ãªã‘ã‚Œã°ã€é©åˆ‡ãªæ¸¬å®šæˆ¦ç•¥ã¯ç«‹ã¦ã‚‰ã‚Œã¾ã›ã‚“ã€‚ã‚½ãƒ¼ã‚¹ã‚³ãƒ¼ãƒ‰ã®èª­è§£ã¨å®Ÿé¨“ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ã‚·ã‚¹ãƒ†ãƒ ã®æŒ™å‹•ã‚’æ­£ã—ãç†è§£ã—ã€åŠ¹æœçš„ãªæ¸¬å®šæ‰‹æ³•ã‚’é¸æŠã§ãã¾ã™ã€‚

**4. æ¸¬å®šæ‰‹æ³•ã®æ–‡æ›¸åŒ–**

æ¸¬å®šæ–¹æ³•ã€ä»®å®šã€é™ç•Œã‚’æ˜ç¢ºã«æ–‡æ›¸åŒ–ã™ã‚‹ã“ã¨ã§ã€ä»–ã®ç ”ç©¶è€…ãŒçµæœã‚’æ¤œè¨¼ã—ã€æ”¹å–„ã™ã‚‹ã“ã¨ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚ã¾ãŸã€ã€Œé–“é•ã£ãŸæ¸¬å®šæ–¹æ³•ã€ã®è¨˜éŒ²ã‚‚ã€åŒã˜å¤±æ•—ã‚’ç¹°ã‚Šè¿”ã•ãªã„ãŸã‚ã®è²´é‡ãªçŸ¥è¦‹ã¨ãªã‚Šã¾ã™ã€‚

### å‚è€ƒè³‡æ–™

- [Zenn è¨˜äº‹ - Inf2ã§ vLLM ã‚’å‹•ã‹ã™éš›ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°](https://zenn.dev/tosshi/articles/ef61e14fe73399)
- [NxD Inference Release Notes](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/release-notes/neuronx-distributed-inference/neuronx-distributed-inference.html)
- [vLLM User Guide for NxD Inference](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/libraries/nxd-inference/developer_guides/vllm-user-guide-v1.html)
- [AWS Neuron Profiler Guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/tools/neuron-sys-tools/neuron-profile-user-guide.html)
- [neuronx-distributed-inference GitHub](https://github.com/aws-neuron/neuronx-distributed-inference)

---

**æœ€ã‚‚é‡è¦ãªæ•™è¨“**: æ¸¬å®šã¯ç§‘å­¦ã§ã‚ã‚Šã€ã€Œåˆ†ã‹ã‚‰ãªã„ã€ã¨èªã‚ã‚‹ã“ã¨ãŒã€é–“é•ã£ãŸæœ€é©åŒ–ã‚’é¿ã‘ã‚‹ç¬¬ä¸€æ­©ã§ã™ã€‚

---

## OSS ã¸ã®è²¢çŒ®æ©Ÿä¼š

æœ¬èª¿æŸ»ã‚’é€šã˜ã¦æ˜ã‚‰ã‹ã«ãªã£ãŸ vllm-neuron ã®æ€§èƒ½ç‰¹æ€§ã¯ã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¸ã®è²¢çŒ®æ©Ÿä¼šã‚’ç¤ºå”†ã—ã¦ã„ã¾ã™ã€‚

### vLLM ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æœ€é©åŒ–

vLLM ã® 52ms ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ï¼ˆå…¨ä½“ã® 81%ï¼‰ã¯ã€ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«èµ·å› ã™ã‚‹æœ¬è³ªçš„ãªåˆ¶ç´„ã§ã™ã€‚Main Process ã¨ Worker Process é–“ã®é€šä¿¡ã«ã¯ ZMQ ã‚’ä»‹ã—ãŸã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ãŒå¿…è¦ã§ã‚ã‚Šã€ã“ã®å‡¦ç†ãŒçŸ­ã„æ¨è«–ã‚¿ã‚¹ã‚¯ã«ãŠã„ã¦æ”¯é…çš„ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¨ãªã£ã¦ã„ã¾ã™ã€‚ã“ã®å•é¡Œã«å¯¾ã—ã¦ã¯ã€shared memory ã‚’æ´»ç”¨ã—ãŸ zero-copy é€šä¿¡æ©Ÿæ§‹ã®å°å…¥ãŒæœ‰åŠ¹ãªæ”¹å–„ç­–ã¨ãªã‚Šå¾—ã¾ã™ã€‚ç‰¹ã« AWS Inferentia2 ã®ã‚ˆã†ãªå°‚ç”¨ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã§ã¯ã€ãƒ—ãƒ­ã‚»ã‚¹é–“ã§ Neuron ã®å‡ºåŠ›ãƒãƒƒãƒ•ã‚¡ã‚’ç›´æ¥å…±æœ‰ã™ã‚‹ã“ã¨ã§ã€ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã‚³ã‚¹ãƒˆã‚’å¤§å¹…ã«å‰Šæ¸›ã§ãã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

ã•ã‚‰ã«ã€çŸ­ã„æ¨è«–ã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã—ãŸ lightweight ãƒ¢ãƒ¼ãƒ‰ã®å®Ÿè£…ã‚‚æ¤œè¨ã«å€¤ã—ã¾ã™ã€‚ç¾åœ¨ã® vLLM ã¯ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°ã‚„ continuous batching ã¨ã„ã£ãŸé«˜åº¦ãªæ©Ÿèƒ½ã‚’å‰æã¨ã—ã¦ã„ã¾ã™ãŒã€reranker ã‚„ embedding ãƒ¢ãƒ‡ãƒ«ã®ã‚ˆã†ãªå›ºå®šé•·ãƒ»åŒæœŸçš„ãªãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã§ã¯ã€ã“ã‚Œã‚‰ã®æ©Ÿèƒ½ã¯ä¸è¦ã§ã™ã€‚ã‚·ãƒ³ãƒ—ãƒ«ãªåŒæœŸå®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã‚’æä¾›ã™ã‚‹ã“ã¨ã§ã€ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’æœ€å°é™ã«æŠ‘ãˆã¤ã¤ã€vLLM ã®åˆ©ä¾¿æ€§ï¼ˆOpenAI API äº’æ›æ€§ãªã©ï¼‰ã‚’ç¶­æŒã§ãã¾ã™ã€‚

### ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ„ãƒ¼ãƒ«ã®æ‹¡å¼µ

Neuron Profiler ã¯ç¾åœ¨ã€å€‹ã€…ã®ã‚°ãƒ©ãƒ•å®Ÿè¡Œã®è©³ç´°ã‚’æ‰ãˆã‚‹ã“ã¨ã«ç‰¹åŒ–ã—ã¦ã„ã¾ã™ãŒã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¬ãƒ™ãƒ«ã®ãƒãƒƒãƒå‡¦ç†æ™‚é–“ã‚’ç›´æ¥æ¸¬å®šã™ã‚‹æ©Ÿèƒ½ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚è¤‡æ•°ã® NTFF ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¾ãŸãŒã‚‹å®Ÿè¡Œã‚’çµ±åˆã—ã€ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚’å¯è¦–åŒ–ã™ã‚‹æ©Ÿèƒ½ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã§ã€é–‹ç™ºè€…ã¯ã‚ˆã‚Šå®Ÿç”¨çš„ãªæ€§èƒ½åˆ†æãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚ã“ã‚Œã¯ Neuron SDK ãƒãƒ¼ãƒ ã¸ã® feature request ã¨ã—ã¦ææ¡ˆã™ã‚‹ä¾¡å€¤ãŒã‚ã‚Šã¾ã™ã€‚

line_profiler ã«ã¤ã„ã¦ã‚‚ã€ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ã‚¹ç’°å¢ƒã§ã®æ¸¬å®šã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹æ‹¡å¼µæ©Ÿèƒ½ãŒæ±‚ã‚ã‚‰ã‚Œã¾ã™ã€‚vLLM v1 ã®ã‚ˆã†ãªåˆ†æ•£ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã¯ã€Worker ãƒ—ãƒ­ã‚»ã‚¹å†…ã®å‡¦ç†ã‚’å¯è¦–åŒ–ã™ã‚‹ã“ã¨ãŒä¸å¯æ¬ ã§ã™ã€‚ã“ã‚Œã¯ line_profiler æœ¬ä½“ã¸ã®è²¢çŒ®ã€ã‚ã‚‹ã„ã¯ vLLM å°‚ç”¨ã®ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã¨ã—ã¦å®Ÿè£…ã™ã‚‹ã“ã¨ãŒè€ƒãˆã‚‰ã‚Œã¾ã™ã€‚

### ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

æœ¬èª¿æŸ»ã§å¾—ã‚‰ã‚ŒãŸ bucketing è¨­å®šã®æŒ™å‹•ï¼ˆå›ºå®šé•·ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã§ã¯ OFF ãŒæœ€é©ã€å¯å¤‰é•·ã§ã¯ ON ãŒå¿…é ˆï¼‰ã¯ã€å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã¯æ˜è¨˜ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚vLLM-Neuron ãŠã‚ˆã³ NxD Inference ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«ã€ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ç‰¹æ€§ã«å¿œã˜ãŸè¨­å®šã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã¯ã€ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£å…¨ä½“ã«ã¨ã£ã¦æœ‰ç›Šã§ã™ã€‚ç‰¹ã«ã€reranker ã‚„ embedding ã¨ã„ã£ãŸç‰¹æ®Šãªãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã«é–¢ã™ã‚‹ performance tuning guide ã®æ•´å‚™ã¯ã€åŒæ§˜ã®èª²é¡Œã«ç›´é¢ã—ã¦ã„ã‚‹é–‹ç™ºè€…ã®åŠ©ã‘ã¨ãªã‚‹ã§ã—ã‚‡ã†ã€‚

ã¾ãŸã€prefix caching ã®å‹•ä½œåŸç†ï¼ˆvLLM å´ã®å®Ÿè£…ãŒãƒã‚¤ãƒ‘ã‚¹ã•ã‚Œã€Neuron å´ã§å®Ÿè¡Œã•ã‚Œã‚‹ï¼‰ã¯ã€ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£å›³ã‚’å«ã‚€è©³ç´°ãªèª¬æ˜ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚ã“ã®ç‹¬ç‰¹ãªè¨­è¨ˆã®èƒŒæ™¯ã¨åˆ©ç”¨æ–¹æ³•ã‚’æ˜ç¢ºã«æ–‡æ›¸åŒ–ã™ã‚‹ã“ã¨ã§ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒé©åˆ‡ã«æ©Ÿèƒ½ã‚’æ´»ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

### ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆã®æ§‹ç¯‰

ç¾åœ¨ã€vllm-neuron ã®æ€§èƒ½ã‚’ä½“ç³»çš„ã«è©•ä¾¡ã™ã‚‹ãŸã‚ã®æ¨™æº–çš„ãªãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¹ã‚¤ãƒ¼ãƒˆã¯å­˜åœ¨ã—ã¾ã›ã‚“ã€‚æœ¬èª¿æŸ»ã§é–‹ç™ºã—ãŸæ¸¬å®šæ‰‹æ³•ã‚’æ±ç”¨åŒ–ã—ã€æ§˜ã€…ãªãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã€sequence length ã®çµ„ã¿åˆã‚ã›ã§è‡ªå‹•çš„ã«æ€§èƒ½ã‚’æ¸¬å®šã™ã‚‹ãƒ„ãƒ¼ãƒ«ã‚’ OSS ã¨ã—ã¦å…¬é–‹ã™ã‚‹ã“ã¨ãŒè€ƒãˆã‚‰ã‚Œã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€vLLM ã‚„ Neuron SDK ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚¢ãƒƒãƒ—æ™‚ã®æ€§èƒ½é€€è¡Œã‚’æ¤œå‡ºã—ã€ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£å…¨ä½“ã§æœ€é©åŒ–ã®é€²æ—ã‚’å…±æœ‰ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

### ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¸ã®çŸ¥è¦‹ã®å…±æœ‰

æœ¬èª¿æŸ»ã§å¾—ã‚‰ã‚ŒãŸæ¸¬å®šæ–¹æ³•ã€å¤±æ•—äº‹ä¾‹ã€è§£æ±ºã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€GitHub ã® discussion ã‚„ issue ã¨ã—ã¦å…¬é–‹ã™ã‚‹ã“ã¨ã§ã€åŒæ§˜ã®èª²é¡Œã«ç›´é¢ã—ã¦ã„ã‚‹é–‹ç™ºè€…ã®å‚è€ƒã¨ãªã‚Šã¾ã™ã€‚ç‰¹ã«ã€ã€Œãªãœã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒã†ã¾ãã„ã‹ãªã‹ã£ãŸã®ã‹ã€ã¨ã„ã†å¤±æ•—ã®è¨˜éŒ²ã¯ã€è©¦è¡ŒéŒ¯èª¤ã®ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›ã™ã‚‹è²´é‡ãªæƒ…å ±ã§ã™ã€‚vLLM ãŠã‚ˆã³ Neuron SDK ã®ãƒªãƒã‚¸ãƒˆãƒªã«ã€æœ¬èª¿æŸ»ã®è¦ç´„ã¨æ¨å¥¨äº‹é …ã‚’ issue ã¾ãŸã¯ RFCï¼ˆRequest for Commentsï¼‰ã¨ã—ã¦ææ¡ˆã™ã‚‹ã“ã¨ã‚’æ¤œè¨ã—ã¦ã„ã¾ã™ã€‚

ã“ã‚Œã‚‰ã®è²¢çŒ®ã¯ã€å˜ãªã‚‹æ€§èƒ½æ”¹å–„ã«ã¨ã©ã¾ã‚‰ãšã€AWS Inferentia2 ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã®æˆç†Ÿåº¦ã‚’é«˜ã‚ã€ã‚ˆã‚Šå¤šãã®é–‹ç™ºè€…ãŒå°‚ç”¨ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã‚’åŠ¹æœçš„ã«æ´»ç”¨ã§ãã‚‹ç’°å¢ƒã‚’æ§‹ç¯‰ã™ã‚‹ã“ã¨ã«ã¤ãªãŒã‚Šã¾ã™ã€‚
