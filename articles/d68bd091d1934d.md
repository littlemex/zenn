---
title: "vllm-neuron „Çí 1 Ë°å„Éá„Ç≥„É¨„Éº„Çø„Éº„Åß„Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞"
emoji: "üìå"
type: "tech" # tech: ÊäÄË°ìË®ò‰∫ã / idea: „Ç¢„Ç§„Éá„Ç¢
topics: ["vllm", "aws", "inferentia", "oss", "neuron"]
published: true
---

## „ÅØ„Åò„ÇÅ„Å´

[ÂâçÂõû„ÅÆË®ò‰∫ã„Äåvllm-neuron „ÇíÁî®„ÅÑ„Åü LLM Êé®Ë´ñ„Å®„Åù„ÅÆÊ∏¨ÂÆöÊñπÊ≥ï„ÅÆË™øÊüª„Äç](https://zenn.dev/tosshi/articles/ef61e14fe73399)„Åß„ÅØ„ÄÅAWS Inferentia2 ‰∏ä„Åß vllm-neuron „ÅÆ Bucketing „ÇÑ Prefix Caching Ë®≠ÂÆö„ÇíÊØîËºÉ„Åó„ÄÅÊÄßËÉΩÂ∑Æ„ÇíÂÆöÈáèÂåñ„Åó„Åæ„Åó„Åü„ÄÇ„Åù„ÅÆÈöõ„ÄÅ`vllm bench` „Åå‰Ωø„Åà„Å™„Åã„Å£„Åü„Åü„ÇÅ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Çπ„ÇØ„É™„Éó„Éà„Çí‰Ωú„Çä„Åæ„Åó„Åü„ÄÇ„Åì„Çå„Çí„Éô„Éº„Çπ„Å´ Neuron Profiler „Çí‰Ωø„Å£„Åü„Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞„ÇíÂÆüÊñΩ„Åô„Çã„Åì„Å®„Åå‰ªäÂõû„ÅÆÁõÆÁöÑ„Åß„Åô„ÄÇ

:::message alert
vllm-neuron „Å´„Çà„Çâ„Åö AWS Neuron „Éô„Éº„Çπ„ÅÆ„Ç≥„Éº„Éâ„Åß„ÅÇ„Çå„Å∞„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Å®„Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞„Åå„Åß„Åç„Çã„ÅØ„Åö„Åß„Åô„ÅåÁèæÁä∂ vllm-neuron ‰ª•Â§ñ„Åß„ÅØÂãï‰ΩúÊú™Ê§úË®º„Åß„Åô„ÄÇ
:::

Êú¨Ë®ò‰∫ã„Åß„ÅØÊúÄÁµÇÁöÑ„Å´„ÄÄ**AWS Neuron „Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞ OSS „Åß„ÅÇ„Çã [`benchmark-capture`](https://test.pypi.org/project/benchmark-capture/) „ÇíÈñãÁô∫„Åó„ÅüÁµåÁ∑Ø**„Å®„ÄÅ„Åù„ÅÆË®≠Ë®àÊÄùÊÉ≥„ÄÅÂÆüË£Ö„ÅÆÊ¶ÇË¶Å„ÄÅÂæó„Çâ„Çå„ÅüÁü•Ë¶ã„ÄÅ„Å´„Å§„ÅÑ„Å¶Á¥π‰ªã„Åó„Åæ„Åô„ÄÇ

:::message
**ÂÖ¨ÈñãÁä∂Ê≥ÅÔºà2026 Âπ¥ 1 ÊúàÊôÇÁÇπÔºâ:**
ÁèæÂú®„ÅØ TestPyPI „ÅßÂÖ¨Èñã‰∏≠„Åß„Åô„ÄÇAWS Inferentia2ÔºàNeuronÔºâÁí∞Â¢É„Åß„ÅÆÂãï‰ΩúÊ§úË®º„ÅØÂÆå‰∫Ü„Åó„Å¶„ÅÑ„Åæ„Åô„Åå„ÄÅ**CI/CD ËøΩÂä†„ÄÅNVIDIA GPU Áí∞Â¢É„Åß„ÅÆÊ§úË®ºÂæå„Å´Ê≠£Âºè PyPI „Å∏ÂÖ¨Èñã‰∫àÂÆö**„Åß„Åô„ÄÇ
:::

## 1. ÂâçÂõû„ÅÆ„Åä„Åï„Çâ„ÅÑ

ÂâçÂõû„ÄÅBucketing „ÇÑ Prefix Caching Ë®≠ÂÆö„ÅÆÊØîËºÉ„ÅÆ„Åü„ÇÅ„Å´‰ΩúÊàê„Åó„Åü„Çπ„ÇØ„É™„Éó„Éà„ÅØÈùûÂ∏∏„Å´ÂçòÁ¥î„Å™‰Ωú„Çä„Åß„Åó„Åü„ÄÇ„Éô„Éº„ÇπË®≠ÂÆö„Å®„ÉÜ„Çπ„Éà„Ç±„Éº„Çπ„Å®„Åó„Å¶ Bucketing ON/OFF „Å™„Å©„ÅÆË®≠ÂÆö„ÇíÂ§â„Åà„Åü `test_configs` „ÅßË®≠ÂÆö„Çí„Çπ„Ç§„Éº„Éó„Åô„Çã„Åü„ÇÅ„ÅÆË®≠ÂÆö json „Éï„Ç°„Ç§„É´„ÄÅ„Åù„ÅÆË®≠ÂÆö„ÇíË™≠„ÅøËæº„Çì„Åß vllm-neuron „ÅÆ„Ç™„Éï„É©„Ç§„É≥Êé®Ë´ñ„Ç≥„Éû„É≥„Éâ„Å´Ë®≠ÂÆöÂÄ§„ÅÆ„Éë„Çø„Éº„É≥ÂàÜË®≠ÂÆö„ÇíÊ∏°„Åó„ÄÅË®≠ÂÆö„Åî„Å®„ÅÆÁµêÊûú„Çí„Éá„Ç£„É¨„ÇØ„Éà„É™„ÇíÂàÜ„Åë„Å¶Âá∫Âäõ„Åô„Çã„Çπ„ÇØ„É™„Éó„Éà„Åã„ÇâÊßãÊàê„Åï„Çå„Åæ„Åô„ÄÇ

::::details ÂâçÂõû„ÅÆË®≠ÂÆö„Éï„Ç°„Ç§„É´
```json:sample.json
{
  "model": "./models/Qwen3-0.6B-Reranker",
  "tensor_parallel_size": 2,
  "base_config": {
    "max_model_len": 2048,
    "block_size": 32,
    "input_file": "./sample.csv",
    "search_num": 20,
    "top_n": 6,
    "batch_size": 8,
    "max_length": 1500
  },
  "test_configs": [
    {
      "name": "cache_ON_bucketing_ON",
      "max_num_seqs": 4,
      "max_num_batched_tokens": 256,
      "pa_num_blocks": 512,
      "enable_prefix_caching": true,
      "enable_bucketing": true
    },
...
  ]
}
```
::::

„Åì„ÅÆÊßãÊàê„Åß„ÇÇ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Åô„Çã„Å†„Åë„Åß„ÅÇ„Çå„Å∞ÂÖ®„ÅèÂïèÈ°å„Åå„ÅÇ„Çä„Åæ„Åõ„Çì„Åß„Åó„Åü„Åå„ÄÅ„ÅÑ„Åñ„Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞„Çí„ÇÑ„Çç„ÅÜ„Å®„Åô„Çã„Å®„ÅÑ„Åè„Å§„ÅãÈù¢ÂÄí„Å™ÁÇπ„ÅåÂá∫„Å¶„Åç„Åæ„Åó„Åü„ÄÇ‚ë† AWS Neuron „Çí„Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞„Çí„Åô„ÇãÈöõ„Å´„ÅØ„ÅÑ„Åè„Å§„Åã„ÅÆ„Éó„É≠„Éï„Ç°„Ç§„É´„ÅÆ„Åü„ÇÅ„ÅÆÁí∞Â¢ÉÂ§âÊï∞„ÇíË®≠ÂÆö„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ„Åì„Çå„Çâ„ÅÆÁí∞Â¢ÉÂ§âÊï∞„ÇíÊØéÂõûÊåáÂÆö„ÅóÂøò„Çå„Å¶ÊôÇÈñì„Åã„Åë„Å¶ÁµêÊûú„ÇíÂæÖ„Å£„Åü„ÅÆ„Å´„Éó„É≠„Éï„Ç°„Ç§„É´„ÅÆÊàêÊûúÁâ©„Åå„Å™„ÅÑ„Å®„ÅÑ„ÅÜÁµ∂Êúõ„ÇíÂë≥„Çè„ÅÑ„Åæ„Åó„ÅüÔºàËá™ÂàÜ„ÅÆ„Åõ„ÅÑÔºâ„ÄÇ‚ë° ÁîüÊàê„Åï„Çå„Åü ntff „Éï„Ç°„Ç§„É´„ÅåÊï£‰π±„Åó„Å¶„Åó„Åæ„ÅÑÂÆüÈ®ìË®≠ÂÆö„Å®ÁµêÊûú„ÅåÁ¥ê„Å•„Åã„Å™„Åè„Å™„Çä„Åæ„Åó„Åü„ÄÇJupyter Notebook „ÅßÂÆüÈ®ì„Åó„Å¶„ÅÑ„Çã„Å®Ë®≥„Çè„Åã„Çâ„Å™„Åè„Å™„Çã„Ç¢„É¨„Å´Ëøë„ÅÑ„Åß„Åô„Å≠„ÄÇ‚ë¢ „É¢„Éá„É´„ÅåÊúüÂæÖ„Åô„ÇãË®≠ÂÆöÂÄ§„Å® vllm-neuron „ÅåÊúüÂæÖ„Åô„ÇãË®≠ÂÆöÂÄ§„ÅåÂêå„ÅòË®≠ÂÆö„Éï„Ç°„Ç§„É´ÂÜÖ„Å´„Åî„Å°„ÇÉ„Å£„Å®Ê∑∑Âú®„Åó„Å¶„Åä„ÇäÊ∞óÊåÅ„Å°„ÅåÊÇ™„ÅÑ„Åß„Åô„ÄÇ‚ë£ NVIDIA GPU „Å® AWS Inferentia „ÇíÊØîËºÉ„Åó„Çà„ÅÜ„Å®„Åô„Çã„Å®„Åù„Çå„Åû„Çå„Åß„Çπ„ÇØ„É™„Éó„Éà„Çí‰Ωú„Çâ„Å™„Åë„Çå„Å∞„Å™„Çâ„Å™„ÅÑ„ÅÆ„ÅßÂ§ßÂ§â„Åß„Åô„ÄÇ‚ë§ Ë®àÊ∏¨„Å®ÁµêÊûúÈõÜË®à„Çí‰∏ÄÁ∑í„Å´‰∏Ä„Å§„ÅÆ„Çπ„ÇØ„É™„Éó„Éà„ÅßÂÆüÁèæ„Åó„Å¶„ÅÑ„Åæ„Åó„Åü„ÅåÂØÜÁµêÂêà„Åô„Åé„Çã„ÅÆ„ÅßË®àÊ∏¨„ÅÆÂ§âÊõ¥„Åß„Åô„Åê„Å´ÁµêÊûúÈõÜË®àÂÅ¥„ÅåÂãï„Åã„Å™„Åè„Å™„Çä„Åæ„Åô„ÄÇ

## 2. ËâØ„Åï„Åí„Å™Ë®≠Ë®à„ÇíËÄÉ„Åà„Çã

ÂÆüË£Ö„ÇíËÄÉ„Åà„ÇãÈöõ„Å´ÊúÄ„ÇÇÈáçË¶ñ„Åó„ÅüÁÇπ„ÅØ„ÄÅ„É°„Ç§„É≥„ÅÆÊé®Ë´ñÁî®„ÅÆ„Ç≥„Éº„Éâ„Å´„ÅØÂâçÂá¶ÁêÜ„ÇÑÂæåÂá¶ÁêÜ„Å™„Å©„ÅÆÊú¨Áï™Áî®„ÅÆÂá¶ÁêÜ„ÇíÂê´„ÇÄ„Ç≥„Éº„Éâ„ÅåÊúÄÁµÇÁöÑ„Å´„ÅØÁµÑ„ÅøËæº„Åæ„Çå„Çã„Åì„Å®„ÇÇ„ÅÇ„Çã„Åü„ÇÅ„Åì„Çå„Çâ„ÅÆ„É°„Ç§„É≥„Ç≥„Éº„Éâ„Å´ÂΩ±Èüø„Çí„Åß„Åç„Çã„Å†„Åë‰∏é„Åà„Åö„Å´„Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞„Çí„Åô„Çã„ÄÅ„Å®„ÅÑ„ÅÜ„Åì„Å®„Åß„Åô„ÄÇPython „ÅÆÂ†¥Âêà„ÄÅÂ§ö„Åè„ÅÆ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„ÇÑ„Éó„É≠„Éï„Ç°„Ç§„É´„ÉÑ„Éº„É´„ÄÅNumba „ÇÑ JAX „ÅÆ„Çà„ÅÜ„Å™„É©„Ç§„Éñ„É©„É™„ÅØ„É°„Ç§„É≥„Ç≥„Éº„Éâ„Å´„Åß„Åç„Çã„Å†„ÅëÂΩ±Èüø„Çí‰∏é„Åà„Å™„ÅÑ„Çà„ÅÜ„Å´„Åô„Çã„Åü„ÇÅ„Å´„Éá„Ç≥„É¨„Éº„Çø„Éë„Çø„Éº„É≥„ÇíÊé°Áî®„Åó„Å¶„ÅÑ„Çã„Åì„Å®„ÅåÂ§ö„ÅÑË™çË≠ò„Åß„ÅÇ„Çã„Åü„ÇÅ„ÄÅ„Éá„Ç≥„É¨„Éº„Çø„Éº `@profile()` „Å†„Åë„Åß **Neuron „Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞ÔºàNTFF „Éï„Ç°„Ç§„É´ÁîüÊàêÔºâ„ÇíÊúâÂäπ„Å´„Åß„Åç„Çã**„Å®ËâØ„Åï„Åù„ÅÜ„Åß„ÅÇ„Çã„ÄÅ„Å®ËÄÉ„Åà„Åæ„Åó„Åü„ÄÇ„Åì„ÅÜ„Åô„Çã„Åì„Å®„Åß„É¢„Éá„É´„ÅåÊúüÂæÖ„Åô„ÇãË®≠ÂÆöÂÄ§„Å´Èñ¢„Åó„Å¶„ÅØ„É¢„Éá„É´ÂÅ¥„ÅÆ„Ç≥„Éº„Éâ„Åß„Çà„Åó„Å™„Å´Êâ±„Å£„Å¶„Åè„Çå„Çå„Å∞ËâØ„ÅÑ„Åü„ÇÅ„ÄÅ„Åì„ÅÆÊôÇÁÇπ„Åß„É¢„Éá„É´ÂÅ¥„ÅÆË®≠ÂÆö„Å®„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„ÅÆ„Çπ„Ç§„Éº„Éó„Éë„Çø„Éº„É≥„ÅÆË®≠ÂÆö„ÅØË≤¨Âãô„ÅåÂàÜÈõ¢„Åï„Çå„Åæ„Åô„ÄÇ

:::message
**Neuron „Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞„ÅÆ‰ªïÁµÑ„Åø**

„Åì„ÅÆ„ÉÑ„Éº„É´„Å´Ë°å„Å£„Å¶„Åª„Åó„ÅÑ„ÅÆ„ÅØ„ÄÅ**NTFFÔºàNeuron Trace File FormatÔºâ„Éï„Ç°„Ç§„É´„ÅÆËá™ÂãïÁîüÊàê**„Åß„Åô„ÄÇ

ÂÖ∑‰ΩìÁöÑ„Å´„ÅØ„ÄÅ

1. **Áí∞Â¢ÉÂ§âÊï∞„ÅÆËá™ÂãïË®≠ÂÆö**
   - Standard „É¢„Éº„Éâ: `NEURON_PROFILE` ‚Üí Neuron „É©„É≥„Çø„Ç§„É†„Åå NTFF ÁîüÊàê
   - Perfetto „É¢„Éº„Éâ: `NEURON_RT_INSPECT_*` ‚Üí Perfetto ‰∫íÊèõ NTFF ÁîüÊàê

2. **ÁîüÊàê„Åï„Çå„Åü NTFF „Éï„Ç°„Ç§„É´„ÅÆËß£Êûê**ÔºàÂà•„É©„Ç§„Éñ„É©„É™„Å´Ë≤¨Âãô„ÇíÂàÜÈõ¢„Åó„Åü„ÅÑÔºâ
   - `neuron-profile view/convert` „Ç≥„Éû„É≥„Éâ
   - Perfetto UIÔºàPerfetto „É¢„Éº„ÉâÔºâ
   - neuron-workflow „Å™„Å©„ÅÆËß£Êûê„ÉÑ„Éº„É´

`@profile()` „ÅØ„ÄåNTFF ÁîüÊàê„ÅÆËá™ÂãïÂåñ„Äç„ÇíÊãÖÂΩì„Åó„ÄÅÂÆüÈöõ„ÅÆ„Éó„É≠„Éï„Ç°„Ç§„É´Ëß£Êûê„ÅØ AWS Neuron „ÅåÊèê‰æõ„Åô„ÇãÊó¢Â≠ò„ÉÑ„Éº„É´„ÅßË°å„ÅÑ„ÄÅ„Åù„Çå„Çí„É©„ÉÉ„Éó„Åó„ÅüËß£Êûê„É©„Ç§„Éñ„É©„É™„ÇÇ‰Ωú„Çä„Åü„ÅÑ„ÄÇ
:::

Ê¨°„Å´ËÄÉ„Åà„Åü„ÅÆ„ÅØÂâçÂõû„ÅÆË®ò‰∫ã„ÅßÂÆüË£Ö„Åó„Åü„Çà„ÅÜ„Å™ vLLM „ÅÆË®≠ÂÆöÂÄ§„ÇíË§áÊï∞„Éë„Çø„Éº„É≥Ë®≠ÂÆö„Åó„Å¶„Çπ„Ç§„Éº„Éó„Åô„Çã„Çà„ÅÜ„Å™‰ªïÁµÑ„Åø„ÅØÂºï„ÅçÁ∂ö„Åç„Åª„Åó„ÅÑ„ÄÅ„Å®„ÅÑ„ÅÜ„Åì„Å®„Åß„Åô„ÄÇ„Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞„ÅåÁõÆÁöÑ„Åß„ÅØ„ÅÇ„Çã„ÇÇ„ÅÆ„ÅÆ„ÄÅÂçò„Å´Áí∞Â¢ÉÂ§âÊï∞„ÇíË®≠ÂÆö„Åô„Çã„Å†„Åë„Åß„ÅØ„ÅÇ„Åæ„ÇäÂ≠òÂú®‰æ°ÂÄ§„Åå„Å™„ÅÑ„Åü„ÇÅ„ÄÅ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„ÅÆ„Éá„Ç≥„É¨„Éº„Çø„Éº„Å´Âêà„Çè„Åõ„Å¶„Éó„É≠„Éï„Ç°„Ç§„É´„Éá„Ç≥„É¨„Éº„Çø„Éº„ÇíË®≠ÂÆö„Åô„Çã„Å®„ÅÑ„ÅÑÊÑü„Åò„Å´„É°„Ç§„É≥„Ç≥„Éº„ÉâÊ±öÊüì„ÇíÊ∏õ„Çâ„Åõ„Çã„Çì„Åò„ÇÉ„Å™„ÅÑ„Åã„Å®ËÄÉ„Åà„Åæ„Åó„Åü„ÄÇ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Çí„Éá„Ç≥„É¨„Éº„Çø„Éº„Éë„Çø„Éº„É≥„ÅßÂÆüÁèæ„Åô„Çã„É©„Ç§„Éñ„É©„É™„ÇíË™øÊüª„Åó„Åü„Å®„Åì„Çç„ÄÅStar Êï∞„Åå 1k Ë∂Ö„Åà„ÅßÊúÄËøë„ÇÇÊ¥ªÁô∫„Å´ÈñãÁô∫„Åï„Çå„Å¶„ÅÑ„Çã OSS „Åå„ÅÑ„Åè„Å§„ÅãË¶ã„Å§„Åã„Çä„ÄÅÂØæÂøúÊ©üËÉΩ„ÇíÊØîËºÉ„Åó„ÅüÁµêÊûú„ÄÅpytest „Éô„Éº„Çπ„ÅÆ [`pytest-benchmark`](https://github.com/ionelmc/pytest-benchmark) „Å®„ÅÑ„ÅÜ„É©„Ç§„Éñ„É©„É™„ÇíÈÅ∏„Å≥„Åæ„Åó„Åü„ÄÇÂü∫Êú¨ÁöÑ„Å™ÈÅ∏ÂÆöÂü∫Ê∫ñ„Å®„Åó„Å¶„ÅØ„ÄÅ„Éá„Ç≥„É¨„Éº„Çø„Éº„Éë„Çø„Éº„É≥„Åß„ÅÇ„Çã„Åì„Å®„ÄÅËªΩÈáè„Åß„ÅÇ„Çã„Åì„Å®„ÄÅ„ÇØ„É©„Çπ„Éô„Éº„Çπ„ÇíÂº∑Âà∂„Åó„Å¶„Åì„Å™„ÅÑ„Åì„Å®„ÄÅ„ÇíÈáçË¶ñ„Åó„Åæ„Åó„Åü„ÄÇ

`pytest-benchmark` „ÅåÊèê‰æõ„Åô„Çã‰∏ª„Å™Ê©üËÉΩ„Å®„Åó„Å¶„ÄÅËá™Âãï„Ç≠„É£„É™„Éñ„É¨„Éº„Ç∑„Éß„É≥ÔºàÊúÄÈÅ©„Å™ÂÆüË°åÂõûÊï∞Ôºâ„ÄÅwarmup ÂÆüË°å„ÄÅGC Âà∂Âæ°„ÄÅÊ≠£Á¢∫„Å™„Çø„Ç§„Éû„ÉºÔºàperf_counterÔºâ„ÄÅÁµ±Ë®àË®àÁÆóÔºàmean, std, min, max, median, IQR, outliersÔºâ„ÄÅÂ§ñ„ÇåÂÄ§Ê§úÂá∫„ÄÅÂêÑ„Éë„É©„É°„Éº„ÇøÁµÑ„ÅøÂêà„Çè„Åõ„Åî„Å®„Å´Áµ±Ë®à„ÄÅ„Ç∞„É´„Éº„ÉóÂåñ„Å®„ÇΩ„Éº„Éà„ÄÅÊØîËºÉ„É¢„Éº„Éâ„Å™„Å©„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇÁßÅ„ÅåÊ¨≤„Åó„Åã„Å£„ÅüÊ©üËÉΩ„ÅåÂ§ß‰ΩìÁ∂≤ÁæÖ„Åß„Åç„Åù„ÅÜ„Åß„ÅÇ„Çã„Åì„Å®„Åå„Çè„Åã„Å£„Åü„ÅÆ„Åß„Åì„ÅÆ„É©„Ç§„Éñ„É©„É™„ÇíÊé°Áî®„Åó„Å¶„Åø„Çã„Åì„Å®„Å´„Åó„Åæ„Åó„Åü„ÄÇ

‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å´„Éë„É©„É°„Éº„Çø„Éº„Çπ„Ç§„Éº„Éó„ÅÆË®≠ÂÆö„Çí„Éá„Ç≥„É¨„Éº„Çø„Éº„ÅßÊåáÂÆö„Åß„Åç„Åæ„Åô„ÄÇ

```python
@pytest.mark.parametrize("max_tokens", [512, 1024, 2048])
@pytest.mark.parametrize("batch_size", [1, 2, 4, 8])
def test_sweep(benchmark, batch_size, max_tokens):
    # Import vLLM inside test function
    import vllm

    # setup Èñ¢Êï∞„Åß LLM „ÇíÂàùÊúüÂåñÔºàÊ∏¨ÂÆöÂØæË±°Â§ñÔºâ
    def setup():
        llm = vllm.LLM(
            max_num_seqs=batch_size,
            max_num_batched_tokens=max_tokens,
        )
        return (llm,), {}  # args, kwargs „ÇíËøî„Åô

    # ÂÆüÈöõ„ÅÆÊé®Ë´ñ„ÅÆ„Åø„ÇíÊ∏¨ÂÆö
    def inference(llm):
        return llm.generate(prompts)

    # pedantic „Åß setup „Å® inference „ÇíÂàÜÈõ¢
    result = benchmark.pedantic(inference, setup=setup, rounds=5)
```

## 3. Ë®≠Ë®àÊÄùÊÉ≥

ÊúÄÂàù„Å´ÁêÜÊÉ≥ÁöÑ„Å™„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ„ÅÆË®≠Ë®à„ÇíËÄÉ„Åà„Åæ„Åó„Åü„ÄÇ„Ç≥„Ç¢ÂéüÂâá„Å®„Åó„Å¶‰ª•‰∏ã„ÅÆ 3 „Å§„ÇíÂèñ„ÇäÂÖ•„Çå„Åæ„Åó„Åü„ÄÇ

**Simple things should be simple, complex things should be possible**

Âü∫Êú¨ÁöÑ„Å™„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„ÅØ 1-2 Ë°å„ÅßÊõ∏„Åë„Çã„Åπ„Åç„Åß„ÅÇ„Çä„ÄÅÂêåÊôÇ„Å´Ë§áÈõë„Å™Ë¶Å‰ª∂„Å´„ÇÇÂØæÂøúÂèØËÉΩ„Å™Êã°Âºµ„Éù„Ç§„É≥„Éà„ÇíÁî®ÊÑè„Åó„Åü„ÅÑ„Åß„Åô„ÄÇ

**Convention over Configuration**

Ë≥¢„ÅÑ„Éá„Éï„Ç©„É´„ÉàÂÄ§„ÇíÊèê‰æõ„Åó„ÄÅÂøÖË¶Å„Å™ÊôÇ„Å†„Åë„Ç™„Éº„Éê„Éº„É©„Ç§„Éâ„Åß„Åç„Çã„Çà„ÅÜ„Å´„Åó„Å¶„Åæ„Åö„ÅØÂãï„Åã„Åô„Åæ„Åß„ÅÆÊôÇÈñì„ÇíÂâäÊ∏õ„Åó„Åü„ÅÑ„Åß„Åô„ÄÇ

**Separation of Concerns**

„Éô„É≥„ÉÅ„Éû„Éº„ÇØÂÆöÁæ©„ÄÅÂÆüË°åÂà∂Âæ°„ÄÅÁµêÊûúÂàÜÊûê„ÄÅ„Çí„Åù„Çå„Åû„ÇåÁã¨Á´ã„Åï„Åõ„Åæ„Åô„ÄÇ

„Åì„Çå„Çâ„ÅÆÂéüÂâá„Å´Ââá„Å£„Å¶ÁêÜÊÉ≥ÁöÑ„Å™Ë®≠Ë®à„ÇíËÄÉ„Åà„Åü‰∏ä„Åß„ÄÅ**ÂÆüÈöõ„Å´Âãï„Åè„ÇÇ„ÅÆ„ÇíÊúÄÁü≠„Åß‰Ωú„Çã**„Åì„Å®„ÇíÂÑ™ÂÖà„Åó„Åæ„Åó„Åü„ÄÇ„Éï„É´„Çπ„Çø„ÉÉ„ÇØ„ÅÆ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Éï„É¨„Éº„É†„ÉØ„Éº„ÇØ„Çí‰Ωú„Çã„ÅÆ„Åß„ÅØ„Å™„Åè„ÄÅÊó¢Â≠ò„ÅÆÂÑ™„Çå„Åü„ÉÑ„Éº„É´„ÇíÊ¥ªÁî®„Åô„ÇãÊñπÈáù„Å´„Åó„Åæ„Åó„Åü„ÄÇ„Éô„É≥„ÉÅ„Éû„Éº„ÇØÂÆüË°å„ÅØÊó¢Â≠ò„ÅÆÂÑ™„Çå„Åü `pytest-benchmark` „Å´‰ªª„Åõ„ÄÅÁµ±Ë®àË®àÁÆó„ÇÇ `pytest-benchmark` „Å´‰ªª„Åõ„Çã„Åì„Å®„Å´„Åó„Åæ„Åó„Åü„ÄÇ„Åù„Åó„Å¶„ÄÅ„Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞„ÅÆÁí∞Â¢ÉÂ§âÊï∞Ë®≠ÂÆö„ÅÆËá™ÂãïÂåñ„Å´Ê≥®Âäõ„Åó„ÄÅ„Éè„Éº„Éâ„Ç¶„Çß„Ç¢Ëá™ÂãïÊ§úÂá∫„Åß„Éû„É´„ÉÅ„Éá„Éê„Ç§„Çπ„Å∏„ÅÆÂØæÂøú„ÇíÂÆüË£Ö„Åó„ÄÅ„Éá„Ç≥„É¨„Éº„Çø„Éº 1 Ë°å„Åß„Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞ÊúâÂäπÂåñ„Åô„Çã„Åì„Å®„ÇíÊ±∫„ÇÅ„Åæ„Åó„Åü„ÄÇ

## 4. benchmark-capture „ÅÆÂÆüË£Ö

### „Ç≥„Ç¢Ê©üËÉΩ

ÈñãÁô∫„Åó„Åü [benchmark-capture](https://github.com/littlemex/benchmark-capture) „ÅÆ‰∏ªË¶ÅÊ©üËÉΩ„ÅØ‰ª•‰∏ã„ÅÆÈÄö„Çä„Åß„Åô„ÄÇ

„Éè„Éº„Éâ„Ç¶„Çß„Ç¢Ëá™ÂãïÊ§úÂá∫„Å´„Çà„Çã„Çº„É≠„Ç≥„É≥„Éï„Ç£„ÇÆ„É•„É¨„Éº„Ç∑„Éß„É≥ÂØæÂøú„Å®„Åó„Å¶„ÄÅAWS Neuron„ÄÅNVIDIA GPU„ÄÅCPU „ÇíËá™ÂãïÂà§Âà•„Åó„ÄÅÈÅ©Âàá„Å™„Éó„É≠„Éï„Ç°„Ç§„É©„Éº„ÇíÈÅ∏Êäû„Åó„Åæ„Åô„ÄÇpytest-benchmark „Å®„ÅÆÁµ±Âêà„Å´„Çà„Çä„ÄÅ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„ÉÑ„Éº„É´„ÅÆÊ©üËÉΩ„ÇíÁ∂ôÊâø„Åó„Å§„Å§„ÄÅ„Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞Ê©üËÉΩ„ÇíËøΩÂä†„Åó„Åæ„Åô„ÄÇÂü∫Êú¨ÁöÑ„Å´„ÅØ„Éá„Ç≥„É¨„Éº„Çø„Éº 1 Ë°å„Å´„Çà„ÇãÊúÄÂ∞èÈôê„ÅÆ„Ç≥„Éº„ÉâÂ§âÊõ¥„ÇíÂÆüÁèæ„Åó„ÄÅÊó¢Â≠ò„ÅÆ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Ç≥„Éº„Éâ„Å∏„ÅÆÂΩ±Èüø„ÇíÊúÄÂ∞èÂåñ„Åó„Åæ„Åô„ÄÇÂêå‰∏Ä„Ç≥„Éº„Éâ„ÅÆÁï∞„Å™„Çã„Éè„Éº„Éâ„Ç¶„Çß„Ç¢„Åß„ÅÆÂÆüË°å„ÇíÂèØËÉΩ„Å´„Åó„ÄÅÁßªÊ§çÊÄß„ÇíÁ¢∫‰øù„Åó„Åæ„Åô„ÄÇËªΩÈáè„Å™‰æùÂ≠òÈñ¢‰øÇ„Å´„Çà„Çä„ÄÅÂ∞éÂÖ•„Ç≥„Çπ„Éà„Çí‰ΩéÊ∏õ„Åó„Åæ„Åô„ÄÇvllm / vllm-neuron Êé®Ë´ñÁî®„ÉÜ„É≥„Éó„É¨„Éº„Éà„ÇíÊèê‰æõ„Åó„Å¶„Åô„Åê„Å´‰Ωø„ÅÑÂßã„ÇÅ„Çâ„Çå„ÇãÁí∞Â¢É„ÇíÁî®ÊÑè„Åó„Åæ„Åô„ÄÇ

### Âü∫Êú¨ÁöÑ„Å™‰Ωø„ÅÑÊñπ

ÊúÄ„ÇÇ„Ç∑„É≥„Éó„É´„Å™‰Ωø„ÅÑÊñπ„ÅØ„ÄÅÊó¢Â≠ò„ÅÆ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Ç≥„Éº„Éâ„Å´ `@profile()` „Çí 1 Ë°åËøΩÂä†„Åô„Çã„Å†„Åë„Åß„Åô„ÄÇ

```python:test_vllm_py
import pytest
from benchmark_capture import profile

@pytest.mark.benchmark
@profile()  # ‚Üê !!!!!! „Åì„ÅÆ 1 Ë°å„ÇíËøΩÂä†„Åô„Çã„Å†„Åë !!!!!!
def test_vllm_neuron_inference(benchmark, model_path):
    # CRITICAL: vLLM must be imported INSIDE test function
    # to ensure profiling env vars are set before Neuron Runtime initialization
    import vllm

    # vllm-neuron configuration
    llm = vllm.LLM(
        model=model_path,
        device="neuron",
        tensor_parallel_size=2
    )

    prompts = ["Hello, how are you?"] * 10

    # pytest-benchmark „ÅÆ fixture „Çí‰ΩøÁî®
    result = benchmark(llm.generate, prompts)

    return result
```

ÂÆüË°å„ÅØ‰ª•‰∏ã„ÅÆ„Ç≥„Éû„É≥„Éâ„ÅßË°å„ÅÑ„Åæ„Åô„ÄÇ

```bash
pytest test_vllm.py --benchmark-only -v
```

‰ªäÂõû„ÅÆ‰æã„Åß„ÅØ„Åì„Çå„Å†„Åë„Åß„ÄÅNeuron „Éè„Éº„Éâ„Ç¶„Çß„Ç¢„ÇíËá™ÂãïÊ§úÂá∫„Åó„ÄÅÈÅ©Âàá„Å™„Éó„É≠„Éï„Ç°„Ç§„É´Áí∞Â¢ÉÂ§âÊï∞„ÇíËá™ÂãïË®≠ÂÆö„Åó„ÄÅ„Éô„É≥„ÉÅ„Éû„Éº„ÇØÁµ±Ë®à„ÇíËá™ÂãïË®àÁÆóÔºàmean, std, min, max „Å™„Å©Ôºâ„Åó„ÄÅ„Éó„É≠„Éï„Ç°„Ç§„É´ÁµêÊûú„ÇíËá™Âãï‰øùÂ≠òÔºàNTFF „Éï„Ç°„Ç§„É´Ôºâ„Åó„ÄÅ„É°„Çø„Éá„Éº„Çø„Çí JSON „Åß‰øùÂ≠ò„Åó„Åæ„Åô„ÄÇ

### „Éè„Éº„Éâ„Ç¶„Çß„Ç¢Ëá™ÂãïÊ§úÂá∫

`@profile()` „ÇíÂºïÊï∞„Å™„Åó„ÅßÂëº„Å∂„Å®„ÄÅ‰ª•‰∏ã„ÅÆÂÑ™ÂÖàÈ†Ü‰Ωç„Åß„Éè„Éº„Éâ„Ç¶„Çß„Ç¢„ÇíÊ§úÂá∫„Åó„Åæ„Åô„ÄÇ

:::message alert
vllm-neuron „ÅÆ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Å®„Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞„Çí„Åô„Çã„Åü„ÇÅ„Å´‰Ωú„Å£„Å¶„ÅÑ„Çã„ÅÆ„Åß GPU „ÅÆÂØæÂøú„ÅØÂæåÂõû„Åó„Å´„Åó„Å¶„Åä„Çä„ÄÅ„Åæ„Å† GPU „Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞„ÅÆ‰ªïÁµÑ„Åø„ÅØÂÆåÊàê„Åó„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇ
:::

```mermaid
%%{init: {'theme':'dark', 'themeVariables': { 'primaryColor':'#1e3a5f','primaryTextColor':'#fff','primaryBorderColor':'#4a90e2','lineColor':'#4a90e2','secondaryColor':'#2c5282','tertiaryColor':'#1a202c','background':'#0d1117','mainBkg':'#1e3a5f','secondBkg':'#2c5282'}}}%%
flowchart TD
    A["@profile() Âëº„Å≥Âá∫„Åó"] --> B{AWS Neuron<br/>Ê§úÂá∫ÂèØËÉΩ?}
    B -->|Yes| C[NeuronProfiler ÈÅ∏Êäû]
    B -->|No| D{NVIDIA GPU<br/>Ê§úÂá∫ÂèØËÉΩ?}
    D -->|Yes| E[NSightProfiler ÈÅ∏Êäû]
    D -->|No| F[NoOpProfiler ÈÅ∏Êäû]

    C --> G[Áí∞Â¢ÉÂ§âÊï∞Ë®≠ÂÆö<br/>NEURON_PROFILE]
    E --> H[Áí∞Â¢ÉÂ§âÊï∞Ë®≠ÂÆö<br/>NSYS_OUTPUT_FILE]
    F --> I[„Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞„Å™„Åó]

    G --> J[„Éô„É≥„ÉÅ„Éû„Éº„ÇØÂÆüË°å]
    H --> J
    I --> J

    subgraph detection ["Ê§úÂá∫ÊñπÊ≥ï"]
        B2["neuron-ls „Ç≥„Éû„É≥„ÉâÂÆüË°å"]
        D1["torch.cuda.is_available()"]
        D2["nvidia-smi „Ç≥„Éû„É≥„ÉâÂÆüË°å"]
    end

    style A fill:#2c5282,stroke:#4a90e2,stroke-width:2px,color:#fff
    style C fill:#1e3a5f,stroke:#4a90e2,stroke-width:2px,color:#fff
    style E fill:#1e3a5f,stroke:#4a90e2,stroke-width:2px,color:#fff
    style F fill:#1e3a5f,stroke:#4a90e2,stroke-width:2px,color:#fff
    style J fill:#2c5282,stroke:#4a90e2,stroke-width:2px,color:#fff
```

„Åæ„Åö AWS Neuron „ÅÆÊ§úÂá∫„ÇíË©¶„Åø„Åæ„Åô„ÄÇ`neuron-ls` „Ç≥„Éû„É≥„Éâ„ÅÆÂÆüË°åÂèØÂê¶„ÅßÂà§ÂÆö„Åó„Åæ„Åô„ÄÇÊ¨°„Å´ NVIDIA GPU „ÅÆÊ§úÂá∫„ÇíË©¶„Åø„Åæ„Åô„ÄÇ`torch.cuda.is_available()` „Åæ„Åü„ÅØ `nvidia-smi` „Ç≥„Éû„É≥„Éâ„ÅßÂà§ÂÆö„Åó„Åæ„Åô„ÄÇ‰∏äË®ò„Å©„Å°„Çâ„ÇÇÊ§úÂá∫„Åß„Åç„Å™„ÅÑÂ†¥Âêà„ÄÅCPUÔºà„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÔºâ„Å®„Åó„Å¶ no-op „Éó„É≠„Éï„Ç°„Ç§„É©„Éº„ÇíÈÅ∏Êäû„Åó„Åæ„Åô„ÄÇ

„Åì„ÅÆËá™ÂãïÊ§úÂá∫„Å´„Çà„Çä„ÄÅÂêå„Åò„Ç≥„Éº„Éâ„ÇíÁï∞„Å™„ÇãÁí∞Â¢É„ÅßÂÆüË°å„Åó„Å¶„ÇÇËá™ÂãïÁöÑ„Å´ÈÅ©Âàá„Å™„Éó„É≠„Éï„Ç°„Ç§„É©„Éº„ÅåÈÅ∏Êäû„Åï„Çå„Åæ„Åô„ÄÇ

### ÊòéÁ§∫ÁöÑ„Å™„Éó„É≠„Éï„Ç°„Ç§„É©„ÉºÊåáÂÆö

ÁâπÂÆö„ÅÆ„Éó„É≠„Éï„Ç°„Ç§„É©„Éº„ÇíÊòéÁ§∫ÁöÑ„Å´ÊåáÂÆö„Åô„Çã„Åì„Å®„ÇÇ„Åß„Åç„Åæ„Åô„ÄÇ

```python
@profile("neuron")  # Neuron „ÇíÂº∑Âà∂
@profile("nsight")  # NVIDIA NSight Systems „ÇíÂº∑Âà∂
@profile("noop")    # „Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞ÁÑ°ÂäπÂåñ
```

„Åì„Çå„Å´„Çà„Çä„ÄÅCI/CD „Åß„Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞„ÇíÁÑ°ÂäπÂåñ„Åó„Åü„Çä„ÄÅÁâπÂÆöÁí∞Â¢É„Åß„ÅÆ„Åø„Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞„Åó„Åü„Çä„Åô„ÇãÂà∂Âæ°„ÅåÂèØËÉΩ„Åß„Åô„ÄÇ

### pytest-benchmark „Å®„ÅÆÁµ±Âêà

`pytest-benchmark` „ÅÆÂº∑Âäõ„Å™Ê©üËÉΩ„Çí„Åù„ÅÆ„Åæ„ÅæÊ¥ªÁî®„Åß„Åç„Åæ„Åô„ÄÇ

```python
@pytest.mark.parametrize("max_tokens", [512, 1024, 2048])
@pytest.mark.parametrize("batch_size", [1, 2, 4, 8])
@profile("neuron")
def test_parameter_sweep(benchmark, batch_size, max_tokens):
    # Import vLLM inside test function
    import vllm

    # setup Èñ¢Êï∞„Åß LLM „ÇíÂàùÊúüÂåñÔºàÊ∏¨ÂÆöÂØæË±°Â§ñÔºâ
    def setup():
        llm = vllm.LLM(
            model="model",
            max_num_seqs=batch_size,
            max_num_batched_tokens=max_tokens,
        )
        return (llm,), {}

    # ÂÆüÈöõ„ÅÆÊé®Ë´ñ„ÅÆ„Åø„ÇíÊ∏¨ÂÆö
    def inference(llm):
        prompts = ["Hello"] * batch_size
        return llm.generate(prompts)

    result = benchmark.pedantic(inference, setup=setup, rounds=5)
```

„Åì„Çå„Å´„Çà„Çä„ÄÅ3 √ó 4 = 12 ÈÄö„Çä„ÅÆ„Éë„É©„É°„Éº„ÇøÁµÑ„ÅøÂêà„Çè„Åõ„ÇíËá™ÂãïÂÆüË°å„Åó„ÄÅÂêÑÁµÑ„ÅøÂêà„Çè„Åõ„Åî„Å®„Å´Áµ±Ë®àË®àÁÆó„Åó„ÄÅÂàùÊúüÂåñÊôÇ„ÇíÊ∏¨ÂÆö„Åã„ÇâÈô§Â§ñ„Åô„Çã warm up„ÄÅÂêÑÂÆüË°å„Åî„Å®„Å´„Éó„É≠„Éï„Ç°„Ç§„É´ÁµêÊûú„Çí‰øùÂ≠ò„Åó„Åæ„Åô„ÄÇ

### Perfetto „É¢„Éº„Éâ

Perfetto „É¢„Éº„Éâ„Çí‰Ωø„ÅÜ„Å®„ÄÅ[Perfetto UI](https://ui.perfetto.dev/) „ÅßÂèØË¶ñÂåñ„Åß„Åç„ÇãÂΩ¢Âºè„ÅÆ NTFF „Éï„Ç°„Ç§„É´„ÇíÁîüÊàê„Åó„Åæ„Åô„ÄÇ

```python
@profile("neuron", perfetto=True, output_dir="/tmp/profiles")
def test_perfetto_profile(benchmark):
    # Import vLLM inside test function
    import vllm

    llm = vllm.LLM(model="model")
    result = benchmark(llm.generate, prompts)
```

ÂÆüË°åÂæå„ÅÆÁ¢∫Ë™ç„ÅØ‰ª•‰∏ã„ÅÆ„Ç≥„Éû„É≥„Éâ„ÅßË°å„ÅÑ„Åæ„Åô„ÄÇ

```bash
# „Çª„ÉÉ„Ç∑„Éß„É≥„Éá„Ç£„É¨„ÇØ„Éà„É™„Åå„Éá„Éï„Ç©„É´„Éà„Åß„ÅØ‰ª•‰∏ã„ÅÆ„Éë„Çπ„ÅßÁîüÊàê„Åï„Çå„Çã
ls -la /tmp/profiles/i-*_pid_*/

# „É°„Çø„Éá„Éº„Çø„ÅßÁ¢∫Ë™ç
cat /tmp/profiles/metadata.json | jq '.perfetto_mode'  # true
cat /tmp/profiles/metadata.json | jq '.session_dir'
cat /tmp/profiles/metadata.json | jq '.ntff_files'
```

Standard vs Perfetto „É¢„Éº„Éâ„ÅÆÈÅï„ÅÑ„ÅØ‰ª•‰∏ã„ÅÆÈÄö„Çä„Åß„Åô„ÄÇÔºà„Éë„ÇπÁ≠â„ÅØ„Éá„Éï„Ç©„É´„Éà„ÅÆÂ†¥ÂêàÔºâ

```mermaid
%%{init: {'theme':'dark', 'themeVariables': { 'primaryColor':'#1e3a5f','primaryTextColor':'#fff','primaryBorderColor':'#4a90e2','lineColor':'#4a90e2','secondaryColor':'#2c5282','tertiaryColor':'#1a202c','background':'#0d1117','mainBkg':'#1e3a5f','secondBkg':'#2c5282'}}}%%
graph LR
    subgraph standard ["Standard „É¢„Éº„Éâ"]
        A1["@profile"] --> B1[NEURON_PROFILE Ë®≠ÂÆö]
        B1 --> C1["/tmp/profiles/*.ntff"]
        C1 --> D1["metadata.json<br/>profile_files: [...]"]
        D1 --> E1[neuron-profile CLI]
    end

    subgraph perfetto ["Perfetto „É¢„Éº„Éâ"]
        A2["@profile"] --> B2[NEURON_RT_INSPECT_* Ë®≠ÂÆö]
        B2 --> C2["/tmp/profiles/i-xx_pid_yy/"]
        C2 --> D2["metadata.json<br/>session_dir, ntff_files"]
        D2 --> E2[Perfetto UI / neuron-workflow]
    end

    standard o-.-o perfetto
    style A1 fill:#2c5282,stroke:#4a90e2,stroke-width:2px,color:#fff
    style A2 fill:#2c5282,stroke:#4a90e2,stroke-width:2px,color:#fff
    style E1 fill:#1e3a5f,stroke:#4a90e2,stroke-width:2px,color:#fff
    style E2 fill:#1e3a5f,stroke:#4a90e2,stroke-width:2px,color:#fff
```

## 5. ÂÆüË£Ö„ÅÆÂ∑•Â§´ÁÇπ

### Áí∞Â¢ÉÂ§âÊï∞„ÅÆËá™ÂãïÁÆ°ÁêÜ

„Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞ÊôÇ„ÅÆÊúÄÂ§ß„ÅÆË™≤È°å„ÅØ„ÄÅÁí∞Â¢ÉÂ§âÊï∞„ÅÆË®≠ÂÆöÂøò„Çå„Åß„Åó„Åü„ÄÇÊôÇÈñì„Çí„Åã„Åë„Å¶„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„ÇíÂÆüË°å„Åó„Åü„ÅÆ„Å´„ÄÅ„Éó„É≠„Éï„Ç°„Ç§„É´ÁµêÊûú„Åå„Å™„ÅÑ„Å®„ÅÑ„ÅÜÁµ∂Êúõ„Çí‰ΩïÂ∫¶„ÇÇÁµåÈ®ì„Åó„Åæ„Åó„Åü„ÄÇ

`benchmark-capture` „Åß„ÅØ„ÄÅ‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å´Áí∞Â¢ÉÂ§âÊï∞„ÇíËá™ÂãïÁÆ°ÁêÜ„Åó„Åæ„Åô„ÄÇ

```python
class NeuronProfiler(Profiler):
    def setup(self, function_name: str) -> None:
        """„Éô„É≥„ÉÅ„Éû„Éº„ÇØÂÆüË°åÂâç„Å´Áí∞Â¢ÉÂ§âÊï∞„ÇíË®≠ÂÆö"""
        if self.options.get("perfetto", False):
            os.environ["NEURON_RT_INSPECT_ENABLE"] = "1"
            os.environ["NEURON_RT_INSPECT_SYSTEM_PROFILE"] = "1"
            os.environ["NEURON_RT_INSPECT_DEVICE_PROFILE"] = "1"
            os.environ["NEURON_RT_INSPECT_OUTPUT_DIR"] = str(self.output_dir)
        else:
            os.environ["NEURON_PROFILE"] = str(self.output_dir)

        os.environ["NEURON_RT_EXEC_TIMEOUT"] = str(self.options.get("timeout", 600))

    def teardown(self) -> None:
        """„Éô„É≥„ÉÅ„Éû„Éº„ÇØÂÆüË°åÂæå„Å´Áí∞Â¢ÉÂ§âÊï∞„Çí„ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó"""
        for var in ["NEURON_PROFILE", "NEURON_RT_EXEC_TIMEOUT",
                    "NEURON_RT_INSPECT_ENABLE", "NEURON_RT_INSPECT_OUTPUT_DIR"]:
            os.environ.pop(var, None)
```

„Éá„Ç≥„É¨„Éº„Çø„Éº„ÅÆÂá¶ÁêÜ„Éï„É≠„Éº„ÅØ‰ª•‰∏ã„ÅÆÈÄö„Çä„Åß„Åô„ÄÇ

```mermaid
%%{init: {'theme':'dark', 'themeVariables': { 'primaryColor':'#1e3a5f','primaryTextColor':'#fff','primaryBorderColor':'#4a90e2','lineColor':'#4a90e2','secondaryColor':'#2c5282','tertiaryColor':'#1a202c','background':'#0d1117','mainBkg':'#1e3a5f','secondBkg':'#2c5282'}}}%%
sequenceDiagram
    participant T as Test Function
    participant D as @profile Decorator
    participant P as Profiler
    participant E as Environment
    participant B as Benchmark

    T->>D: Èñ¢Êï∞Âëº„Å≥Âá∫„Åó
    D->>P: setup(function_name)
    P->>E: Áí∞Â¢ÉÂ§âÊï∞Ë®≠ÂÆö<br/>(NEURON_PROFILE „Å™„Å©)
    P->>P: output_dir ‰ΩúÊàê
    P-->>D: setup ÂÆå‰∫Ü

    D->>B: „Éô„É≥„ÉÅ„Éû„Éº„ÇØÂÆüË°å
    B->>B: warmup ÂÆüË°å
    B->>B: Ê∏¨ÂÆöÂÆüË°å
    B->>E: NTFF „Éï„Ç°„Ç§„É´ÁîüÊàê
    B-->>D: ÂÆüË°åÁµêÊûú

    D->>P: teardown()
    P->>E: Áí∞Â¢ÉÂ§âÊï∞„ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó
    P->>P: „É°„Çø„Éá„Éº„Çø‰øùÂ≠ò
    P-->>D: teardown ÂÆå‰∫Ü

    D-->>T: ÁµêÊûúËøîÂç¥

    Note over E: Áí∞Â¢ÉÂ§âÊï∞„ÅØËá™ÂãïÁöÑ„Å´<br/>Ë®≠ÂÆö„Éª„ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó

    rect rgba(30, 58, 95, 0.3)
        Note over P,E: „Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞ÊúüÈñì
    end
```

„Åì„ÅÆËá™ÂãïÁÆ°ÁêÜ„Å´„Çà„Çä„ÄÅÁí∞Â¢ÉÂ§âÊï∞Ë®≠ÂÆöÂøò„Çå„Åå„Çº„É≠„Å´„Å™„Çä„ÄÅÂÆüË°åÂæå„ÅÆËá™Âãï„ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó„ÅåË°å„Çè„Çå„ÄÅ„ÉÜ„Çπ„ÉàÈñì„ÅÆÁí∞Â¢ÉÊ±öÊüì„ÇíÈò≤Ê≠¢„Åó„Åæ„Åô„ÄÇ

### „É°„Çø„Éá„Éº„Çø„ÅÆËá™Âãï‰øùÂ≠ò

ÂêÑ„Éô„É≥„ÉÅ„Éû„Éº„ÇØÂÆüË°åÊôÇ„Å´„ÄÅ‰ª•‰∏ã„ÅÆÊÉÖÂ†±„ÇíÂê´„ÇÄ„É°„Çø„Éá„Éº„Çø„Çí JSON „ÅßËá™Âãï‰øùÂ≠ò„Åó„Åæ„Åô„ÄÇ

```json
{
  "function": "test_vllm_inference",
  "profiler": "NeuronProfiler",
  "output_dir": "/tmp/profiles",
  "profiler_type": "neuron",
  "perfetto_mode": true,
  "session_dir": "/tmp/profiles/i-0abc123_pid_45678",
  "ntff_files": [
    "i-0abc123_pid_45678/neff_001.ntff",
    "i-0abc123_pid_45678/neff_002.ntff"
  ],
  "timeout": 600,
  "framework_profile": false,
  "clear_cache_before": false,
  "clear_cache_after": false
}
```

„Åì„ÅÆ„É°„Çø„Éá„Éº„Çø„Å´„Çà„Çä„ÄÅÂÆüÈ®ìË®≠ÂÆö„Å®„Éó„É≠„Éï„Ç°„Ç§„É´ÁµêÊûú„ÅÆÁ¥ê‰ªò„Åë„ÅåÂèØËÉΩ„Å´„Å™„Çä„ÄÅÂæå„Åã„ÇâÂÆüÈ®ìÊù°‰ª∂„ÇíÁ¢∫Ë™ç„Åß„Åç„ÄÅÂàÜÊûê„Çπ„ÇØ„É™„Éó„Éà„Åß„ÅÆËá™ÂãïÂá¶ÁêÜ„ÅåÂÆπÊòì„Å´„Å™„Çä„Åæ„Åô„ÄÇ

### Neuron „Ç≠„É£„ÉÉ„Ç∑„É•ÁÆ°ÁêÜ

vllm-neuron „Åß„ÅØ„ÄÅÂàùÂõûÂÆüË°åÊôÇ„Å´„É¢„Éá„É´„Çí„Ç≥„É≥„Éë„Ç§„É´„Åó„Å¶„Ç≠„É£„ÉÉ„Ç∑„É•„Åó„Åæ„Åô„ÄÇ„Åì„ÅÆÊåôÂãï„ÇíÂà∂Âæ°„Åô„Çã„Åü„ÇÅ„ÄÅ„Ç≠„É£„ÉÉ„Ç∑„É•ÁÆ°ÁêÜÊ©üËÉΩ„ÇíÊèê‰æõ„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ

```python
@profile(
    "neuron",
    clear_cache_before=True,  # „Éô„É≥„ÉÅ„Éû„Éº„ÇØÂâç„Å´„Ç≠„É£„ÉÉ„Ç∑„É•„ÇØ„É™„Ç¢
    clear_cache_after=False,  # „Éô„É≥„ÉÅ„Éû„Éº„ÇØÂæå„ÅØ„Ç≠„É£„ÉÉ„Ç∑„É•‰øùÊåÅ
)
def test_with_cache_control(benchmark):
    # Import vLLM inside test function
    import vllm

    llm = vllm.LLM(model="model")
    result = benchmark(llm.generate, prompts)
```

‰Ωø„ÅÑÂàÜ„Åë„Å®„Åó„Å¶„ÄÅ„Ç≥„É≥„Éë„Ç§„É´ÊôÇÈñìËæº„Åø„ÅßÊ∏¨ÂÆö„Åó„Åü„ÅÑÂ†¥Âêà„ÅØ `clear_cache_before=True` „Çí‰ΩøÁî®„Åó„ÄÅ„Ç≥„É≥„Éë„Ç§„É´Ê∏à„Åø„ÅÆÁä∂ÊÖã„ÅßÊé®Ë´ñÊÄßËÉΩ„ÅÆ„ÅøÊ∏¨ÂÆö„Åó„Åü„ÅÑÂ†¥Âêà„ÅØ `clear_cache_before=False` „Çí‰ΩøÁî®„Åó„Åæ„Åô„ÄÇ

### „Éó„É≠„Ç∏„Çß„ÇØ„ÉàÂàùÊúüÂåñÊ©üËÉΩ

benchmark-capture „ÅØ„ÄÅ`benchmark-capture-init` „Ç≥„Éû„É≥„Éâ„Å´„Çà„Çã„Éó„É≠„Ç∏„Çß„ÇØ„ÉàÂàùÊúüÂåñÊ©üËÉΩ„ÇíÊèê‰æõ„Åó„Åæ„Åô„ÄÇ

```bash
# vLLM-Neuron Reranker „ÅÆÂÆåÂÖ®„Å™‰æã„Çí„Éá„Éó„É≠„Ç§
benchmark-capture-init ./my-reranker --example vllm-neuron-reranker
cd my-reranker

# ÂÜÖÂÆπÁ¢∫Ë™ç
ls -la
# config.yaml, test_reranker.py, input_sample.csv, conftest.py „Å™„Å©

# „É¢„Éá„É´„Éë„Çπ„ÇíË®≠ÂÆö
vim config.yaml  # MODEL_PATH „ÇíÂÆüÈöõ„ÅÆ„Éë„Çπ„Å´Â§âÊõ¥

# ÂÆüË°å
pytest test_reranker.py --benchmark-only -v
```

„Åì„ÅÆ example „Å´„ÅØ„ÄÅÂâçÂõû„ÅÆË®ò‰∫ã„ÅßÁ¥π‰ªã„Åó„Åü Reranker „ÅÆ„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÊ§úË®ºÁí∞Â¢É„Çí„Åô„Åê„Å´ÈÅ©Áî®„Åß„Åç„ÇãÊßãÊàê„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Çµ„É≥„Éó„É´ CSV „Éá„Éº„ÇøÔºà10 „ÇØ„Ç®„É™„ÄÅ200 „Éö„Ç¢Ôºâ„ÄÅË®≠ÂÆöÈßÜÂãïÂûã„ÅÆ„ÉÜ„Çπ„Éà„Ç≥„Éº„Éâ„ÄÅPerfetto „É¢„Éº„Éâ„Åå„Éá„Éï„Ç©„É´„Éà„ÅßÊúâÂäπ„ÄÅ„Åô„Åê„Å´Âãï‰Ωú„Åô„ÇãÊé®Ë´ñ„Ç≥„Éº„Éâ„ÅåÊèê‰æõ„Åï„Çå„Åæ„Åô„ÄÇ„Éë„É©„É°„Éº„Çø„Éº„ÅØÂâçÂõû„ÅÆË®ò‰∫ã„ÅßÊúÄËâØ„ÅÆ„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÇíÁô∫ÊèÆ„Åó„ÅüÂÄ§„Åå„Éá„Éï„Ç©„É´„Éà„ÅßË®≠ÂÆö„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ


### VLLMConfigHelper

„Éô„É≥„ÉÅ„Éû„Éº„ÇØÂÆüË°å„Å´„Åä„ÅÑ„Å¶„ÄÅNeuron Âõ∫Êúâ„ÅÆË®≠ÂÆö„Å® GPU ÂÖ±ÈÄö„ÅÆË®≠ÂÆö„ÇíÈÅ©Âàá„Å´ÁÆ°ÁêÜ„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ„Åì„ÅÆË™≤È°å„Å´ÂØæÂá¶„Åô„Çã„Åü„ÇÅ„ÄÅ`VLLMConfigHelper` „ÇØ„É©„Çπ„ÇíÂÆüË£Ö„Åó„Åæ„Åó„Åü„ÄÇVLLMConfigHelper „ÅÆË®≠Ë®à„Å´„Åä„ÅÑ„Å¶„ÅØ„ÄÅ‰ª•‰∏ã„ÅÆÂéüÂâá„ÇíÈáçË¶ñ„Åó„Åæ„Åó„Åü„ÄÇ

**Ë≤¨Âãô„ÅÆÊúÄÂ∞èÂåñ**

„Éè„Éº„Éâ„Ç¶„Çß„Ç¢Ê§úÂá∫„Å® GPU Ê§úÂá∫ÊôÇ„Å´ Neuron ÁâπÊúâË®≠ÂÆö„ÅÆË®≠ÂÆö„Éï„Ç£„É´„Çø„É™„É≥„Ç∞„ÅÆ„Åø„ÇíÊãÖÂΩì„Åó„ÄÅNeuron Ë®≠ÂÆö„ÅÆÂ§âÊèõ„ÇÑÁÆ°ÁêÜ„ÅØË°å„ÅÑ„Åæ„Åõ„Çì„ÄÇ„Åì„ÅÆÂéüÂâá„Å´„Çà„Çä„ÄÅNeuron Ë®≠ÂÆö„ÅÆ‰∏≠Ë∫´„Å´Èñ¢„Åô„ÇãË≤¨Âãô„ÇíÊåÅ„Åü„Åö„ÄÅ‰ªïÊßòÂ§âÊõ¥„Å´ÂØæ„Åô„Çã‰øùÂÆà„Ç≥„Çπ„Éà„ÇíÊúÄÂ∞èÂåñ„Åó„Åæ„Åô„ÄÇNeuron Ë®≠ÂÆöËá™‰Ωì„ÅØ„ÉÜ„Çπ„Éà„Ç≥„Éº„ÉâÂÅ¥„ÅåÊãÖ„ÅÜ„Åπ„ÅçË≤¨Âãô„Åß„ÅÇ„Çä„ÄÅ„Åù„Çå„Çâ„ÅÆ„Éô„Éº„ÇπË®≠ÂÆö„Å´ÂØæ„Åó„Å¶„Çπ„Ç§„Éº„Éó„Åô„ÇãÂá¶ÁêÜ„ÅØ `pytest` „ÅåÊãÖ„ÅÜ„Åπ„Åç„Åß„Åô„ÄÇ

**ÈÄèÊòéÊÄß„ÅÆÁ¢∫‰øù**

ÈÅ©Áî®„Åï„Çå„ÇãË®≠ÂÆö„Çí„É≠„Ç∞„Å´Âá∫Âäõ„Åó„ÄÅ„É¶„Éº„Ç∂„Éº„ÅåÂÆüÈöõ„Å´‰ΩøÁî®„Åï„Çå„ÇãË®≠ÂÆö„ÇíÁ¢∫Ë™ç„Åß„Åç„Çã„Çà„ÅÜ„Å´„Åó„Åæ„Åô„ÄÇ„Åì„ÅÆÂéüÂâá„Å´„Çà„Çä„ÄÅ„Éá„Éê„ÉÉ„Ç∞„ÅåÂÆπÊòì„Å´„Å™„Çä„ÄÅ‰∫àÊúü„Åó„Å™„ÅÑÂãï‰Ωú„ÇíÂõûÈÅø„Åß„Åç„Åæ„Åô„ÄÇ

**„É¶„Éº„Ç∂„Éº„Ç≥„É≥„Éà„É≠„Éº„É´**

`override_neuron_config` „Å®„ÅÑ„ÅÜ Neuron Âêë„ÅëË®≠ÂÆö„Çí„É¶„Éº„Ç∂„Éº„ÅåÁõ¥Êé•ÁÆ°ÁêÜ„Åó„ÄÅ„Éò„É´„Éë„Éº„ÇØ„É©„Çπ„ÅØ‰ªãÂÖ•„Åó„Åæ„Åõ„Çì„ÄÇ„Åì„ÅÆÂéüÂâá„Å´„Çà„Çä„ÄÅNeuron Ë®≠ÂÆö„ÅÆÊüîËªüÊÄß„ÇíÊúÄÂ§ßÈôê„Å´Á¢∫‰øù„Åó„Åæ„Åô„ÄÇ

**„Ç§„Éü„É•„Éº„Çø„Éì„É™„ÉÜ„Ç£**

ÂÖÉ„ÅÆË®≠ÂÆö„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„ÇíÊ±∫„Åó„Å¶Â§âÊõ¥„Åõ„Åö„ÄÅÂ∏∏„Å´Êñ∞„Åó„ÅÑ„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„ÇíËøî„Åó„Åæ„Åô„ÄÇ„Åì„ÅÆÂéüÂâá„Å´„Çà„Çä„ÄÅpytest „ÅÆ parametrize Ê©üËÉΩ„ÅßÂêå‰∏Ä„ÅÆË®≠ÂÆö„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„ÅåÂÜçÂà©Áî®„Åï„Çå„ÇãÂ†¥Âêà„Åß„ÇÇ„ÄÅÂâØ‰ΩúÁî®„ÇíÈò≤Ê≠¢„Åó„Åæ„Åô„ÄÇ

Âü∫Êú¨ÁöÑ„Å™‰ΩøÁî®ÊñπÊ≥ï„ÅØ‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å´„Å™„Çä„Åæ„Åô„ÄÇ

```python
from benchmark_capture.utils import VLLMConfigHelper

def test_vllm_benchmark(benchmark):
    # Import vLLM inside test function
    import vllm

    # „É¶„Éº„Ç∂„Éº„ÅåÂÖ®„Å¶„ÅÆË®≠ÂÆö„ÇíÂÆöÁæ©
    config = {
        "tensor_parallel_size": 2,
        "max_num_seqs": 4,
        "num_gpu_blocks_override": 512,

        # Neuron Âõ∫Êúâ„ÅÆË®≠ÂÆö
        "additional_config": {
            "override_neuron_config": {
                "pa_num_blocks": 512,
                "pa_block_size": 32,
                "enable_bucketing": True,
            }
        }
    }

    # „Éè„Éº„Éâ„Ç¶„Çß„Ç¢ÂØæÂøú„ÅÆ„Éì„É´„Éâ
    vllm_config = VLLMConfigHelper(config).build()

    # vLLM „Å´Áõ¥Êé•Ê∏°„Åô
    llm = vllm.LLM(model="model_path", **vllm_config)
```

Neuron „Éè„Éº„Éâ„Ç¶„Çß„Ç¢‰∏ä„ÅßÂÆüË°å„Åô„ÇãÂ†¥Âêà„ÄÅ`override_neuron_config` „Åå„Åù„ÅÆ„Åæ„Åæ‰øùÊåÅ„Åï„Çå„ÄÅ„Éï„É´Ë®≠ÂÆö„Åå„É≠„Ç∞„Å´Âá∫Âäõ„Åï„Çå„Åæ„Åô„ÄÇGPU „Éè„Éº„Éâ„Ç¶„Çß„Ç¢‰∏ä„ÅßÂÆüË°å„Åô„ÇãÂ†¥Âêà„ÄÅ`override_neuron_config` „ÅåËá™ÂãïÁöÑ„Å´Èô§Â§ñ„Åï„Çå„ÄÅ„ÇØ„É™„Éº„É≥„Å™ GPU Ë®≠ÂÆö„ÅÆ„Åø„Åå‰ΩøÁî®„Åï„Çå„Åæ„Åô„ÄÇ„Åä„Åù„Çâ„Åè vllm ÂÅ¥„ÅÆÂÆüË£Ö„Åß„ÇÇÂêå„Åò„Çà„ÅÜ„Å´ `override_neuron_config` „ÅåÈô§Â§ñ„Åï„Çå„Å¶„ÅÑ„ÇãÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„Åô„ÅåÊòéÁ§∫ÁöÑ„Å´Èô§Â§ñÂá¶ÁêÜ„Çí„Åô„Çã„Åü„ÇÅ„Å´ÂÆüË£Ö„Åó„Å¶„ÅÇ„Çä„Åæ„Åô„ÄÇ

**pytest parametrize „Å®„ÅÆÁµ±Âêà**

`VLLMConfigHelper` „ÅØ pytest „ÅÆ parametrize Ê©üËÉΩ„Å®„Ç∑„Éº„É†„É¨„Çπ„Å´Áµ±Âêà„Åß„Åç„Åæ„Åô„ÄÇ

```python
@pytest.mark.parametrize("pa_num_blocks", [256, 512, 1024])
@profile("neuron", perfetto=True)
def test_pa_blocks_sweep(benchmark, model_path, pa_num_blocks):
    """ÂêÑË®≠ÂÆö„Åî„Å®„Å´Âà•„ÄÖ„ÅÆ„Éó„É≠„Éï„Ç°„Ç§„É´„Éï„Ç°„Ç§„É´„ÅåÁîüÊàê„Åï„Çå„Çã"""
    # Import vLLM inside test function
    import vllm

    config = VLLMConfigHelper({
        "tensor_parallel_size": 2,
        "num_gpu_blocks_override": pa_num_blocks,
        "additional_config": {
            "override_neuron_config": {
                "pa_num_blocks": pa_num_blocks,
                "pa_block_size": 32,
            }
        }
    }).build()

    llm = vllm.LLM(model=model_path, **config)
    result = benchmark(llm.generate, prompts)
```

:::message
**ÈáçË¶Å„Å™Ê≥®ÊÑèÁÇπ**„Å®„Åó„Å¶„ÄÅ`pa_num_blocks` „Çí‰ΩøÁî®„Åô„ÇãÂ†¥Âêà„ÄÅ`num_gpu_blocks_override` „Å´„ÇÇÂêå„ÅòÂÄ§„ÇíË®≠ÂÆö„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ„Åì„Çå„ÅØ vLLM „Å® NxDIÔºàNeuron Distributed InferenceÔºâ„Åå„Éñ„É≠„ÉÉ„ÇØÊï∞„Çí‰∏ÄËá¥„Åï„Åõ„ÇãÂøÖË¶Å„Åå„ÅÇ„Çã„Åü„ÇÅ„Åß„Åô„ÄÇ„Åì„ÅÆË®≠ÂÆö„ÇíÊÄ†„Çã„Å®„ÄÅ‰ª•‰∏ã„ÅÆ„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åô„ÄÇË™øÊüª„Åó„Åç„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì„Åå vllm-neuron „ÅÆ Neuron „Åü„ÇÅ„Å´ÂøÖË¶Å„Å™„Éë„É©„É°„Éº„Çø„Å® vllm ÂÅ¥„ÅÆË®≠ÂÆö„ÅßÂêå„ÅòÊÑèÂë≥„ÇíÁ§∫„Åô„ÅåÂêçÁß∞„ÅåÁï∞„Å™„Çã„Éë„É©„É°„Éº„Çø„ÅØÂÜÖÈÉ®ÁöÑ„Å´Â§âÊèõ„Åï„Çå„Å¶„ÅÑ„ÇãÂèØËÉΩÊÄß„ÅåÈ´ò„ÅÑ„Åß„Åô„ÄÇ`v0.13.0` „ÅßÂãï„Åã„Åó„Åü„Å®„Åì„Çç vllm-neruon „ÅÆ `additional_config` „ÇíË®≠ÂÆö„Åó„Å™„Åè„Å¶„ÇÇÂü∫Êú¨ÁöÑ„Å™Êé®Ë´ñÂãï‰Ωú„Çí„Åô„Çã„Åì„Å®„Åã„ÇâÂÜÖÈÉ®ÁöÑ„Å´Â§âÊèõ„Åó„Å¶„ÅÑ„ÇãÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ

```
ValueError: When setting pa_num_blocks (512) in override_neuron_config,
you must also set --num-gpu-blocks-override to the same value
```
:::

## 6. ÈñãÁô∫ÈÅéÁ®ã„ÅßÈÅ≠ÈÅá„Åó„ÅüÈáçË¶Å„Å™Ë™≤È°å

### Áí∞Â¢ÉÂ§âÊï∞Ë®≠ÂÆö„ÅÆ„Çø„Ç§„Éü„É≥„Ç∞ÂïèÈ°å

„Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞ÂÆüË£Ö„ÅÆÂàùÊúüÊÆµÈöé„Åß„ÄÅÁï∞Â∏∏„Å™ÁµêÊûú„ÅåË¶≥Ê∏¨„Åï„Çå„Åæ„Åó„Åü„ÄÇ

```bash
$ ls -lh ./profiles/
total 8.2MB  # „Éó„É≠„Éï„Ç°„Ç§„É´„Éá„Éº„Çø„ÅØÁîüÊàê„Åï„Çå„Å¶„ÅÑ„Çã

$ neuron-profile view -d ./profiles/
‚ö†Ô∏è  Profile Coverage: 1.2% (46.4ms / 3,836ms)
```

„Éó„É≠„Éï„Ç°„Ç§„É´„Éï„Ç°„Ç§„É´„ÅØÁîüÊàê„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„Åå„ÄÅ„Ç´„Éê„É¨„ÉÉ„Ç∏„Åå 1.2% „Å®Ê•µÁ´Ø„Å´‰Ωé„Åè„ÄÅDMA Ëª¢ÈÄÅÊôÇÈñì„Åå 0 „ÅßÂÆüÈöõ„ÅÆ„Éá„Éº„ÇøÂá¶ÁêÜ„ÅåË®òÈå≤„Åï„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì„Åß„Åó„Åü„ÄÇ

vllm-neuron „ÅÆ„ÇΩ„Éº„Çπ„Ç≥„Éº„Éâ„ÇíË©≥Á¥∞„Å´Ë™øÊüª„Åó„ÅüÁµêÊûú„ÄÅ‰ª•‰∏ã„ÅÆ‰∫ãÂÆü„ÅåÂà§Êòé„Åó„Åæ„Åó„Åü„ÄÇ

[`vllm.worker.neuronx_distributed_model_runner`
](https://docs.vllm.ai/en/v0.9.1/api/vllm/worker/neuronx_distributed_model_runner.html#vllm.worker.neuronx_distributed_model_runner.NeuronxDistributedModelRunner)

**Module Level Import „Å´„Çà„ÇãÊó©ÊúüÂàùÊúüÂåñ**

```python
import torch
from neuronx_distributed_inference.modules.generation.sampling import \
    prepare_sampling_params
```

„Åì„Çå„Çâ„ÅÆ import Êñá„ÅØ„Éï„Ç°„Ç§„É´„ÅÆÂÖàÈ†≠„Å´ÈÖçÁΩÆ„Åï„Çå„Å¶„Åä„Çä„ÄÅPython „Åå„Åì„ÅÆ„Éï„Ç°„Ç§„É´„ÇíË™≠„ÅøËæº„Çì„Å†Áû¨Èñì„Å´ÂÆüË°å„Åï„Çå„Åæ„Åô„ÄÇ

**Import „ÉÅ„Çß„Éº„É≥„Å´„Çà„ÇãÂàùÊúüÂåñ**

```python
from vllm import LLM  # „É¶„Éº„Ç∂„Éº„Ç≥„Éº„Éâ
    ‚Üì
vllm/__init__.py „Åå„É≠„Éº„Éâ
    ‚Üì
vllm_neuron „Éó„É©„Ç∞„Ç§„É≥„ÅåÊ§úÂá∫„Åï„Çå„Çã
    ‚Üì
neuronx_distributed_model_runner.py „Åå„Ç§„É≥„Éù„Éº„Éà„Åï„Çå„Çã
    ‚Üì
neuronx_distributed_inference „Åå„Ç§„É≥„Éù„Éº„Éà„Åï„Çå„Çã
    ‚Üì
‚òÖ Neuron Runtime „ÅåÂàùÊúüÂåñ„Åï„Çå„Çã ‚òÖ
    ‚Üì ÔºàÁí∞Â¢ÉÂ§âÊï∞„ÇíË™≠„ÅøÂèñ„ÇãÔºâ
Ôºà„Åì„ÅÆÂæå„Åß„ÇØ„É©„Çπ„ÅÆ __init__ „ÅåÂÆüË°å„Åï„Çå„ÇãÔºâ
    ‚Üì
os.environ['NEURON_RT_INSPECT_*'] = ... ‚Üê ‚òÖ„Åì„ÅÆË®≠ÂÆö„ÅØ„Åô„Åß„Å´ Runtime „ÅßÁí∞Â¢ÉÂ§âÊï∞„ÅåË™≠„ÅøËæº„Åæ„Çå„ÅüÂæå„Å´Ë®≠ÂÆö„Åï„Çå„Çã‚òÖ
```

ÂïèÈ°å„ÅÆÊú¨Ë≥™„ÅØ„ÄÅ`from vllm import LLM` „ÅÆÊôÇÁÇπ„Åß„ÄÅ„Ç§„É≥„Éù„Éº„ÉàÂá¶ÁêÜ‰∏≠„Å´ Neuron Runtime „ÅåÂàùÊúüÂåñ„Åï„ÇåÁí∞Â¢ÉÂ§âÊï∞„ÇíË™≠„ÅøÂèñ„Çã„Åì„Å®„Åß„Åô„ÄÇ„Åù„ÅÆÂæå„Å´„ÇØ„É©„Çπ„ÅÆ `__init__` „É°„ÇΩ„ÉÉ„ÉâÂÜÖ„ÅßÁí∞Â¢ÉÂ§âÊï∞„ÇíË®≠ÂÆö„Åó„Å¶„ÇÇ„ÄÅ„Åô„Åß„Å´ÂàùÊúüÂåñÊ∏à„Åø„ÅÆ„É©„É≥„Çø„Ç§„É†„Å´„ÅØÂèçÊò†„Åï„Çå„Åæ„Åõ„Çì„ÄÇ

**Ëß£Ê±∫Á≠ñ**

„Å§„Åæ„ÇäËß£Ê±∫Á≠ñ„ÅØÊòéÁ¢∫„Åß„ÄÅ**vLLM „Çí„Ç§„É≥„Éù„Éº„Éà„Åô„ÇãÂâç„Å´Áí∞Â¢ÉÂ§âÊï∞„ÇíË®≠ÂÆö**„Åô„Çå„Å∞ËâØ„ÅÑ„Åß„Åô„ÄÇ

ÊúÄÁµÇÁöÑ„Å™ÂÆüË£Ö„Åß„ÅØ„ÄÅ**test function ÂÜÖ„Åß vLLM „Çí„Ç§„É≥„Éù„Éº„Éà„Åô„Çã**„Ç¢„Éó„É≠„Éº„ÉÅ„ÇíÊé°Áî®„Åó„Åæ„Åó„Åü„ÄÇ„Åì„Çå„Å´„Çà„Çä„ÄÅ`@profile` „Éá„Ç≥„É¨„Éº„Çø„Éº„ÅåÁí∞Â¢ÉÂ§âÊï∞„ÇíË®≠ÂÆö„Åó„ÅüÂæå„Å´ vLLM „Åå„Ç§„É≥„Éù„Éº„Éà„Åï„Çå„Çã„Åì„Å®„Çí‰øùË®º„Åó„Åæ„Åô„ÄÇ

```python
@profile("neuron", perfetto=True)
def test_vllm_benchmark(benchmark, model_path):
    # Import vLLM here to ensure profiling env vars are set first
    import vllm
    from vllm import SamplingParams

    # „Åì„ÅÆÊôÇÁÇπ„ÅßÁí∞Â¢ÉÂ§âÊï∞„ÅØÊó¢„Å´Ë®≠ÂÆöÊ∏à„Åø
    llm = vllm.LLM(model=model_path, **config)
    # ... „ÉÜ„Çπ„Éà„Ç≥„Éº„Éâ
```

„Åì„ÅÆÊñπÊ≥ï„Å´„Çà„Çä„ÄÅPython „ÅÆ„Ç§„É≥„Éù„Éº„Éà„É°„Ç´„Éã„Ç∫„É†„ÇíÂà©Áî®„Åó„Å¶„ÄÅÁ¢∫ÂÆü„Å´Áí∞Â¢ÉÂ§âÊï∞Ë®≠ÂÆö„ÅÆ„Çø„Ç§„Éü„É≥„Ç∞„ÇíÂà∂Âæ°„Åß„Åç„Åæ„Åô„ÄÇ

### PYTHONPATH Ë®≠ÂÆö

vllm-neuron Áí∞Â¢ÉÔºà`/opt/aws_neuronx_venv_*`Ôºâ„ÅØ externally-managed Áí∞Â¢É„Å®„Åó„Å¶ÁÆ°ÁêÜ„Åï„Çå„Å¶„Åä„Çä„ÄÅÈÄöÂ∏∏„ÅÆ `pip install` „ÅåË®±ÂèØ„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑ„Çà„ÅÜ„Åß„Åô„ÄÇ

```bash
pip install -e /path/to/benchmark-capture
# ERROR: externally-managed-environment

pip install --user -e /path/to/benchmark-capture
# ERROR: User site-packages are not visible in this virtualenv
```

„Åì„ÅÆÂà∂Á¥Ñ„Å´„Çà„Çä„ÄÅÈñãÁô∫Áâà„ÅÆ benchmark-capture „Çí‰ΩøÁî®„Åô„Çã„Åü„ÇÅ„Å´„ÅØ„ÄÅPYTHONPATH „ÇíË®≠ÂÆö„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ

```bash
# ÂøÖÈ†àË®≠ÂÆö
export PYTHONPATH=/path/to/benchmark-capture:$PYTHONPATH

# Á¢∫Ë™ç
python3 -c "from benchmark_capture.utils import VLLMConfigHelper; print('OK')"
```

„Åì„ÅÆË®≠ÂÆö„Å´„Çà„Çä„ÄÅpip install „Å™„Åó„Åß„Ç§„É≥„Éù„Éº„Éà„ÅåÂèØËÉΩ„Å´„Å™„Çä„ÄÅ„Ç≥„Éº„ÉâÂ§âÊõ¥„ÅåÂç≥Â∫ß„Å´ÂèçÊò†„Åï„Çå„ÄÅ„Ç∑„Çπ„ÉÜ„É†Áí∞Â¢É„ÇíÊ±öÊüì„Åó„Åæ„Åõ„Çì„ÄÇ‰ªäÂæåÈñãÁô∫„ÇÑ„Ç´„Çπ„Çø„Éû„Ç§„Ç∫„ÇíË°å„ÅÜ„É¶„Éº„Ç∂„Éº„Å´„Å®„Å£„Å¶„ÅØÊúâÁõä„Å™ÊÉÖÂ†±„Åß„ÅÇ„Çã„Åü„ÇÅÂÖ±Êúâ„Åó„Å¶„Åä„Åç„Åæ„Åô„ÄÇ

## 7. ÂÆüÈöõ„Å´„É©„Ç§„Éñ„É©„É™„ÇíË©¶„Åô

### 7.1 Áí∞Â¢ÉÊßãÁØâ

**ÂâçÊèêÊù°‰ª∂:**
- AWS Inferentia2 „Ç§„É≥„Çπ„Çø„É≥„ÇπÔºàinf2.xlarge ‰ª•‰∏äÔºâ
- AWS Neuron SDK Áí∞Â¢ÉÔºàDLAMI „Å´Âê´„Åæ„Çå„ÇãÔºâ
- Hugging Face „Åã„Çâ„É¢„Éá„É´„Çí„ÉÄ„Ç¶„É≥„É≠„Éº„ÉâÂèØËÉΩ

**„Çπ„ÉÜ„ÉÉ„Éó1: Neuron Áí∞Â¢É„ÅÆ„Ç¢„ÇØ„ÉÜ„Ç£„Éô„Éº„Éà**

AWS Neuron Áî®„ÅÆ DLAMI „Åß Amazon EC2 „ÇíÁ´ã„Å°‰∏ä„Åí„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÅØ‰ª•‰∏ã„ÅÆ activate „Åß Neuron Áí∞Â¢É„Çí„Ç¢„ÇØ„ÉÜ„Ç£„Éô„Éº„Éà„Åß„Åç„Åæ„Åô„ÄÇ„Éë„Çπ„ÅØ DLAMI „ÅÆ„Éê„Éº„Ç∏„Éß„É≥Á≠â„Å´„Çà„Å£„Å¶Â§â„Çè„Çã„ÅÆ„ÅßÈÅ©Âàá„Å™„ÇÇ„ÅÆ„ÇíÈÅ∏Êäû„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇÂâçÂõû„ÅÆË®ò‰∫ã„Åß„ÅØ vllm-neuron `v0.11.0` „Åß„Åó„Åü„Åå `v0.13.0` „Å´„Å™„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇ

```bash
source /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/bin/activate
```

:::message
**Ê≥®ÊÑè**: „Éë„Çπ„ÅØÁí∞Â¢É„Å´„Çà„Å£„Å¶Áï∞„Å™„ÇãÂ†¥Âêà„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ`/opt/` „Éá„Ç£„É¨„ÇØ„Éà„É™„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
:::

**„Çπ„ÉÜ„ÉÉ„Éó2: benchmark-capture „ÅÆ„Ç§„É≥„Çπ„Éà„Éº„É´**

```bash
sudo /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/bin/pip install --index-url https://test.pypi.org/simple/ --extra-index-url https://pypi.org/simple/ 'benchmark-capture[init]==0.2.4' pytest
```

### 7.2 „Éó„É≠„Ç∏„Çß„ÇØ„ÉàÂàùÊúüÂåñ„Å®„Ç´„Çπ„Çø„Éû„Ç§„Ç∫

**„Çπ„ÉÜ„ÉÉ„Éó3: Reranker Example „ÅÆ„Éá„Éó„É≠„Ç§**

Example Ê©üËÉΩ„Çí‰Ωø„ÅÜ„Å®„ÄÅ„Éá„Éº„ÇøËæº„Åø„ÅÆÂÆåÂÖ®„Å™„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Çí„Éá„Éó„É≠„Ç§„Åß„Åç„Åæ„Åô„ÄÇ

```bash
# vLLM-Neuron Reranker Example „Çí„Éá„Éó„É≠„Ç§
benchmark-capture-init ./my-reranker --example vllm-neuron-reranker

cd my-reranker

# „Éá„Éó„É≠„Ç§„Åï„Çå„Åü„Éï„Ç°„Ç§„É´Á¢∫Ë™ç
ls -la
# config.yaml          # Ë®≠ÂÆö„Éï„Ç°„Ç§„É´
# conftest.py          # pytest fixtures
# test_reranker.py     # „É°„Ç§„É≥„Éô„É≥„ÉÅ„Éû„Éº„ÇØ
# test_config_sweep.py # Ë®≠ÂÆö„Çπ„Ç§„Éº„Éó‰æã
# input_sample.csv     # „Çµ„É≥„Éó„É´„Éá„Éº„ÇøÔºà10„ÇØ„Ç®„É™„ÄÅ200„Éö„Ç¢Ôºâ
# requirements.txt     # ‰æùÂ≠òÈñ¢‰øÇ
# run_benchmark.sh     # ÂÆüË°å„Çπ„ÇØ„É™„Éó„Éà
```

**„Çπ„ÉÜ„ÉÉ„Éó4: Ë®≠ÂÆö„Çí„Ç´„Çπ„Çø„Éû„Ç§„Ç∫**

`config.yaml` „ÇíÁ∑®ÈõÜ:

:::details config.yaml
```yaml
model:
  path: "/path/to/your/Qwen3-Reranker-0.6B"  # „É¢„Éá„É´„Éë„Çπ„ÇíÊåáÂÆö

vllm:
  tensor_parallel_size: 2
  max_num_seqs: 4
  block_size: 32
  max_model_len: 2048
  max_num_batched_tokens: 256
  num_gpu_blocks_override: 512
  enable_prefix_caching: false
  dtype: "bfloat16"

reranker:
  input_file: "input_sample.csv"
  search_num: 20
  batch_size: 8
  max_length: 1500
  token_true: "yes"
  token_false: "no"

benchmark:
  rounds: 5
  warmup_rounds: 1
  num_test_queries: 10

profiler:
  clear_cache_before: false
  clear_cache_after: false
```
::::

### 7.3 „Éô„É≥„ÉÅ„Éû„Éº„ÇØÂÆüË°å„Å®„Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞

**„Çπ„ÉÜ„ÉÉ„Éó6: Âü∫Êú¨ÁöÑ„Å™„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„ÇíÂÆüË°å**

```bash
# Reranker„Éô„É≥„ÉÅ„Éû„Éº„ÇØÂÆüË°å
 bash run_benchmark.sh
```

::::details ÂÆüË°å„É≠„Ç∞
```bash
bash run_benchmark.sh 
================================================================
vLLM-Neuron Reranker Benchmark with Profiling
================================================================

Activating Neuron environment...
Verifying benchmark-capture import...
‚úì Import successful

Environment setup complete!
  - Neuron venv: /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13
  - benchmark-capture: /home

Running pytest...

============================================================== test session starts ===============================================================
platform linux -- Python 3.12.3, pytest-9.0.2, pluggy-1.6.0 -- /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/bin/python3
cachedir: .pytest_cache
benchmark: 5.2.3 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)
rootdir: /home/coder/test2/my-reranker
plugins: anyio-4.12.1, benchmark-5.2.3
collected 1 item                                                                                                                                 

test_reranker.py::test_vllm_neuron_reranker 
----------------------------------------------------------------- live log setup -----------------------------------------------------------------
INFO     torch_neuron:initializer.py:82 PJRT_DEVICE not set, defaulting to NEURON
INFO     benchmark_capture.utils.vllm_config:vllm_config.py:169 vLLM Configuration (Hardware: Neuron)
INFO     benchmark_capture.utils.vllm_config:vllm_config.py:170 {
  "tensor_parallel_size": 2,
  "max_num_seqs": 4,
  "block_size": 32,
  "max_model_len": 2048,
  "max_num_batched_tokens": 256,
  "num_gpu_blocks_override": 512,
  "enable_prefix_caching": false,
  "dtype": "bfloat16",
  "additional_config": {
    "override_neuron_config": {
      "skip_warmup": true,
      "pa_num_blocks": 512,
      "pa_block_size": 32,
      "enable_bucketing": true
    }
  }
}

================================================================================
vLLM Configuration
================================================================================
Hardware: Neuron
--------------------------------------------------------------------------------
{
  "tensor_parallel_size": 2,
  "max_num_seqs": 4,
  "block_size": 32,
  "max_model_len": 2048,
  "max_num_batched_tokens": 256,
  "num_gpu_blocks_override": 512,
  "enable_prefix_caching": false,
  "dtype": "bfloat16",
  "additional_config": {
    "override_neuron_config": {
      "skip_warmup": true,
      "pa_num_blocks": 512,
      "pa_block_size": 32,
      "enable_bucketing": true
    }
  }
}
================================================================================

----------------------------------------------------------------- live log call ------------------------------------------------------------------
INFO     benchmark_capture.profilers.neuron:neuron.py:103 Perfetto profiling enabled: profile_output
INFO 01-31 15:37:52 [__init__.py:43] Available plugins for group vllm.platform_plugins:
INFO 01-31 15:37:52 [__init__.py:45] - neuron -> vllm_neuron:register
INFO 01-31 15:37:52 [__init__.py:48] All plugins in this group will be loaded. Set `VLLM_PLUGINS` to control which plugins to load.
INFO 01-31 15:37:52 [__init__.py:217] Platform plugin neuron is activated
INFO 01-31 15:37:53 [importing.py:44] Triton is installed but 0 active driver(s) found (expected 1). Disabling Triton to prevent runtime errors.
INFO 01-31 15:37:53 [importing.py:68] Triton not installed or not compatible; certain GPU-related functions will not be available.
INFO     test_reranker:test_reranker.py:66 Loaded 10 queries from /home/coder/test2/my-reranker/input_sample.csv
INFO     test_reranker:test_reranker.py:67 Testing with first 10 queries
INFO     test_reranker:test_reranker.py:71 Initializing vLLM-Neuron reranker...
INFO     test_reranker:test_reranker.py:72 Model: /home/coder/data-science/investigations/inf2-vllm-performance/models/Qwen3-0.6B-Reranker
INFO     test_reranker:test_reranker.py:73 Config: block_size=32, max_num_seqs=4, tensor_parallel_size=2
INFO 01-31 15:37:55 [utils.py:253] non-default args: {'dtype': 'bfloat16', 'max_model_len': 2048, 'tensor_parallel_size': 2, 'block_size': 32, 'enable_prefix_caching': False, 'max_num_batched_tokens': 256, 'max_num_seqs': 4, 'disable_log_stats': True, 'num_gpu_blocks_override': 512, 'additional_config': {'override_neuron_config': {'skip_warmup': True, 'pa_num_blocks': 512, 'pa_block_size': 32, 'enable_bucketing': True}}, 'model': '/home/coder/data-science/investigations/inf2-vllm-performance/models/Qwen3-0.6B-Reranker'}
INFO     vllm_neuron.platform:platform.py:100 Applying Neuron config overrides
INFO     vllm_neuron.platform:platform.py:116 Neuron config overrides applied successfully
INFO 01-31 15:37:55 [model.py:514] Resolved architecture: Qwen3ForCausalLM
INFO 01-31 15:37:56 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=256.
INFO     vllm_neuron.platform_overrides:platform_overrides.py:22 Skipping attention head divisibility check for Neuron platform
WARNING 01-31 15:37:56 [cache.py:232] Possibly too large swap space. 8.00 GiB out of the 15.25 GiB total CPU memory is allocated for the swap space.
INFO     vllm_neuron.platform:platform.py:149 Neuron OpenAI serving overrides applied successfully
INFO     vllm_neuron.platform:platform.py:215 Adding 1 to num_gpu_blocks_override (512 -> 513) to account for null block allocation
INFO     vllm_neuron.platform:platform.py:241 The custom Neuron scheduler will disable chunked prefill and schedule requests using the continuous batching mechanism, prioritizing prefill over decode.
INFO     vllm_neuron.platform:platform.py:254 Neuron custom scheduler default: max_num_batched_tokens set to 131072. Override with --max-num-batched-tokens if needed.
WARNING  vllm_neuron.platform:platform.py:280 Pin memory is not supported on Neuron.
(EngineCore_DP0 pid=73078) INFO 01-31 15:37:57 [core.py:93] Initializing a V1 LLM engine (v0.13.0) with config: model='/home/coder/data-science/investigations/inf2-vllm-performance/models/Qwen3-0.6B-Reranker', speculative_config=None, tokenizer='/home/coder/data-science/investigations/inf2-vllm-performance/models/Qwen3-0.6B-Reranker', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cpu, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=0, served_model_name=/home/coder/data-science/investigations/inf2-vllm-performance/models/Qwen3-0.6B-Reranker, enable_prefix_caching=False, enable_chunked_prefill=False, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': [256], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}
(EngineCore_DP0 pid=73078) WARNING 01-31 15:37:58 [interface.py:221] Failed to import from vllm._C: ImportError('libcuda.so.1: cannot open shared object file: No such file or directory')
(EngineCore_DP0 pid=73078) INFO 01-31 15:38:00 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.31.41.208:49267 backend=gloo
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
(EngineCore_DP0 pid=73078) INFO 01-31 15:38:00 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=73078) WARNING 01-31 15:38:00 [vllm.py:1403] Current vLLM config is not set.
(EngineCore_DP0 pid=73078) INFO 01-31 15:38:00 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=2048.
(EngineCore_DP0 pid=73078) INFO     vllm_neuron.worker.neuronx_distributed_model_loader:neuronx_distributed_model_loader.py:792 Retrieved override_neuron_config from additional_config: {'skip_warmup': True, 'pa_num_blocks': 512, 'pa_block_size': 32, 'enable_bucketing': True}
(EngineCore_DP0 pid=73078) WARNING  vllm_neuron.worker.neuronx_distributed_model_loader:neuronx_distributed_model_loader.py:210 Exception: [Errno 2] No such file or directory: '/home/coder/data-science/investigations/inf2-vllm-performance/models/Qwen3-0.6B-Reranker/neuron-compiled-artifacts/3d41f03e2951559ef780ab04cc226691/neuron_config.json'
(EngineCore_DP0 pid=73078) WARNING  vllm_neuron.worker.neuronx_distributed_model_loader:neuronx_distributed_model_loader.py:211 Unable to find precompiled artifacts from /home/coder/data-science/investigations/inf2-vllm-performance/models/Qwen3-0.6B-Reranker/neuron-compiled-artifacts/3d41f03e2951559ef780ab04cc226691. Recompiling...
(EngineCore_DP0 pid=73078) INFO     root:model_wrapper.py:168 neuronx-cc compiler_args are: --auto-cast=none --model-type=transformer  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2 --vectorize-strided-dma ' --lnc=1 -O1  --internal-hlo2tensorizer-options=' --modular-flow-mac-threshold=10  --verify-hlo=true' 
(EngineCore_DP0 pid=73078) INFO     root:model_wrapper.py:168 neuronx-cc compiler_args are: --auto-cast=none --model-type=transformer  --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=1 --vectorize-strided-dma ' --lnc=1 -O2  --internal-hlo2tensorizer-options='--verify-hlo=true' 
(EngineCore_DP0 pid=73078) INFO     Neuron:application_base.py:300 Saving the neuron_config to /home/coder/data-science/investigations/inf2-vllm-performance/models/Qwen3-0.6B-Reranker/neuron-compiled-artifacts/3d41f03e2951559ef780ab04cc226691/
(EngineCore_DP0 pid=73078) INFO     Neuron:model_builder.py:549 Generating HLOs for the following models: ['context_encoding_model', 'token_generation_model']
(EngineCore_DP0 pid=73078) [2026-01-31 15:38:00.140: I neuronx_distributed/parallel_layers/parallel_state.py:630] > initializing tensor model parallel with size 2
(EngineCore_DP0 pid=73078) [2026-01-31 15:38:00.140: I neuronx_distributed/parallel_layers/parallel_state.py:631] > initializing pipeline model parallel with size 1
(EngineCore_DP0 pid=73078) [2026-01-31 15:38:00.140: I neuronx_distributed/parallel_layers/parallel_state.py:632] > initializing context model parallel with size 1
(EngineCore_DP0 pid=73078) [2026-01-31 15:38:00.140: I neuronx_distributed/parallel_layers/parallel_state.py:633] > initializing data parallel with size 1
(EngineCore_DP0 pid=73078) [2026-01-31 15:38:00.140: I neuronx_distributed/parallel_layers/parallel_state.py:634] > initializing world size to 2
(EngineCore_DP0 pid=73078) [2026-01-31 15:38:00.140: I neuronx_distributed/parallel_layers/parallel_state.py:379] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x7591667a1080>, 'Ascending Ring PG Group')>
(EngineCore_DP0 pid=73078) [2026-01-31 15:38:00.140: I neuronx_distributed/parallel_layers/parallel_state.py:658] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0, 1]]
(EngineCore_DP0 pid=73078) [2026-01-31 15:38:00.140: I neuronx_distributed/parallel_layers/parallel_state.py:659] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0], [1]]
(EngineCore_DP0 pid=73078) [2026-01-31 15:38:00.140: I neuronx_distributed/parallel_layers/parallel_state.py:660] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0], [1]]
(EngineCore_DP0 pid=73078) [2026-01-31 15:38:00.140: I neuronx_distributed/parallel_layers/parallel_state.py:661] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0], [1]]
(EngineCore_DP0 pid=73078) [2026-01-31 15:38:00.141: I neuronx_distributed/parallel_layers/parallel_state.py:662] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0], [1]]
(EngineCore_DP0 pid=73078) [2026-01-31 15:38:00.141: I neuronx_distributed/parallel_layers/parallel_state.py:663] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0], [1]]
(EngineCore_DP0 pid=73078) INFO     Neuron:model_builder.py:575 Generating 5 hlos for key: context_encoding_model
(EngineCore_DP0 pid=73078) INFO     Neuron:model_builder.py:929 Minimal metadata will be added to HLO
(EngineCore_DP0 pid=73078) INFO     Neuron:model_builder.py:858 Started loading module context_encoding_model
(EngineCore_DP0 pid=73078) WARNING  Neuron:gqa.py:94 TP degree (2) and KV heads (8) are not divisible. Overriding attention sharding strategy to GQA.CONVERT_TO_MHA!
(EngineCore_DP0 pid=73078) INFO     Neuron:model_builder.py:861 Finished loading module context_encoding_model in 0.18622732162475586 seconds
(EngineCore_DP0 pid=73078) INFO     Neuron:model_builder.py:886 generating HLO: context_encoding_model, input example shape = torch.Size([1, 128])
(EngineCore_DP0 pid=73078) INFO     Neuron:model_builder.py:900 Finished generating HLO for context_encoding_model in 2.070460319519043 seconds, input example shape = torch.Size([1, 128])
(EngineCore_DP0 pid=73078) INFO     Neuron:model_builder.py:886 generating HLO: context_encoding_model, input example shape = torch.Size([1, 256])
(EngineCore_DP0 pid=73078) INFO     Neuron:model_builder.py:900 Finished generating HLO for context_encoding_model in 1.9017930030822754 seconds, input example shape = torch.Size([1, 256])
(EngineCore_DP0 pid=73078) INFO     Neuron:model_builder.py:886 generating HLO: context_encoding_model, input example shape = torch.Size([1, 512])
(EngineCore_DP0 pid=73078) INFO     Neuron:model_builder.py:900 Finished generating HLO for context_encoding_model in 2.027911901473999 seconds, input example shape = torch.Size([1, 512])
(EngineCore_DP0 pid=73078) INFO     Neuron:model_builder.py:886 generating HLO: context_encoding_model, input example shape = torch.Size([1, 1024])
(EngineCore_DP0 pid=73078) INFO     Neuron:model_builder.py:900 Finished generating HLO for context_encoding_model in 2.683790922164917 seconds, input example shape = torch.Size([1, 1024])
(EngineCore_DP0 pid=73078) INFO     Neuron:model_builder.py:886 generating HLO: context_encoding_model, input example shape = torch.Size([1, 2048])
(EngineCore_DP0 pid=73078) INFO     Neuron:model_builder.py:900 Finished generating HLO for context_encoding_model in 2.6831250190734863 seconds, input example shape = torch.Size([1, 2048])
(EngineCore_DP0 pid=73078) INFO     Neuron:model_builder.py:575 Generating 5 hlos for key: token_generation_model
(EngineCore_DP0 pid=73078) INFO     Neuron:model_builder.py:929 Minimal metadata will be added to HLO
(EngineCore_DP0 pid=73078) INFO     Neuron:model_builder.py:858 Started loading module token_generation_model
(EngineCore_DP0 pid=73078) WARNING  Neuron:gqa.py:94 TP degree (2) and KV heads (8) are not divisible. Overriding attention sharding strategy to GQA.CONVERT_TO_MHA!
..
(EngineCore_DP0 pid=73078) INFO     Neuron:model_builder.py:861 Finished loading module token_generation_model in 0.21245217323303223 seconds
(EngineCore_DP0 pid=73078) INFO     Neuron:model_builder.py:886 generating HLO: token_generation_model, input example shape = torch.Size([4, 1])
(EngineCore_DP0 pid=73078) INFO     Neuron:model_builder.py:900 Finished generating HLO for token_generation_model in 2.2957053184509277 seconds, input example shape = torch.Size([4, 1])
(EngineCore_DP0 pid=73078) INFO     Neuron:model_builder.py:886 generating HLO: token_generation_model, input example shape = torch.Size([4, 1])
...
..Roundtrip rolls back... [rounds: 1; efficiency: 27]
  tiled_pf_transpose: Fix prefix () and permute (0,) with (1,) / latency=13,290; shape=(512, 4); dtype_size=4

..Completed run_backend_driver.

Compiler status PASS
(EngineCore_DP0 pid=73078) 2026-01-31 15:39:43.000488:  73078  [INFO]: Compilation Successfully Completed for model.MODULE_feedd414807570a03b22+97c2cc02.hlo_module.pb
(EngineCore_DP0 pid=73078) INFO     Neuron:model_builder.py:678 Done compilation for the priority HLO in 80.66311740875244 seconds
...
```
::::

::::details „Éô„É≥„ÉÅ„Éû„Éº„ÇØÁµêÊûú

ÂâçÂõû„ÅÆ zenn Ë®ò‰∫ã„ÅÆÂÆüÊ∏¨ÂÄ§„Å®„Åª„Å®„Çì„Å©Âêå„ÅòÂÄ§„Åå Per-Query Metrics „ÅßÂæó„Çâ„Çå„Å¶„ÅÑ„Åæ„Åô„Å≠ÔºÅÊï∞„Ç≥„Éû„É≥„Éâ„ÅßË®àÊ∏¨„Åß„Åç„Åü„ÅÆ„ÅØÊúÄÈ´ò„Åß„Åô„ÄÇ

```bash
================================================================================
‚úÖ Benchmark Results
================================================================================

üìä Overall Performance:
   Total time (mean): 2992.155 ms
   Min: 2987.091 ms
   Max: 3002.296 ms
   Median: 2989.672 ms
   StdDev: 6.143 ms

üìà Per-Query Metrics:
   Latency per query: 299.216 ms (0.2992 s)
   Throughput (QPS): 3.3421 queries/second

üî¢ Configuration:
   Total queries: 10
   Candidates per query: 20
   Total pairs: 200
   Batch size: 8
   Block size: 32
   Tensor parallel size: 2
================================================================================

PASSED
Wrote benchmark data in: <_io.BufferedWriter name='results.json'>
```
::::

**„Çπ„ÉÜ„ÉÉ„Éó7: „Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞„Éï„Ç°„Ç§„É´„ÇíÁ¢∫Ë™ç**

NTFF „Éï„Ç°„Ç§„É´„ÅåÁîüÊàê„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„Å≠ÔºÅ

```bash
# „Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞„Éá„Ç£„É¨„ÇØ„Éà„É™„ÇíÁ¢∫Ë™ç
ls -la profile_output/
total 16
drwxr-xr-x 3 coder coder 4096 Jan 31 15:43 .
drwxr-xr-x 6 coder coder 4096 Jan 31 15:43 ..
drwxr-xr-x 2 coder coder 4096 Jan 31 15:44 i-0049acfde6046f237_pid_73078
-rw-r--r-- 1 coder coder  653 Jan 31 15:44 metadata.json

ls -la profile_output/i-0049acfde6046f237_pid_73078/
total 96188
drwxr-xr-x 2 coder coder     4096 Jan 31 15:44 .
drwxr-xr-x 3 coder coder     4096 Jan 31 15:43 ..
-rw-r--r-- 1 coder coder 17245222 Jan 31 15:44 322059935237836_instid_0_vnc_0.ntff
-rw-r--r-- 1 coder coder 17243807 Jan 31 15:44 322059935237836_instid_0_vnc_1.ntff
-rw-r--r-- 1 coder coder 22672956 Jan 31 15:44 948584188481322_instid_0_vnc_0.ntff
-rw-r--r-- 1 coder coder 22672577 Jan 31 15:44 948584188481322_instid_0_vnc_1.ntff
-rw-r--r-- 1 coder coder  2192384 Jan 31 15:44 neff_1014068936860577.neff
...(10 „ÅÆ neff „Éï„Ç°„Ç§„É´)
```

„Åì„Çå„ÅßÊúÄÈÄü„Åã„Å§ÊúÄ‰ΩéÈôê„ÅÆ `benchmark-capture` „ÅÆÂãï‰ΩúÁ¢∫Ë™ç„ÅØÁµÇ‰∫Ü„Åß„Åô„ÄÇ„ÅÇ„Å®„ÅØÁîüÊàê„Åï„Çå„Åü„Éï„Ç°„Ç§„É´„Çí‰Ωø„Å£„Å¶ÂÆüÈöõ„Å´„Éó„É≠„Éï„Ç°„Ç§„É´ÂàÜÊûê„Åó„Åæ„Åô„ÄÇ‰ªñ„Å´ pytest „Åß„ÅÆË®≠ÂÆö„Çπ„Ç§„Éº„Éó„Å™„Å©„ÇÇ README.md „Çí„Åø„Å™„Åå„Çâ„Åú„Å≤Á¢∫Ë™ç„Åó„Å¶„Åø„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

## 8. „Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞ÁµêÊûú„ÅÆËß£Êûê

benchmark-capture „ÅßÁîüÊàê„Åï„Çå„Åü NTFF „Éï„Ç°„Ç§„É´„ÇíËß£Êûê„Åô„ÇãÊñπÊ≥ï„ÇíÁ¥π‰ªã„Åó„Åæ„Åô„ÄÇNTFFÔºàNeuron Trace File FormatÔºâ„ÅØ AWS Neuron „ÅåÁîüÊàê„Åô„Çã„Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞„Éá„Éº„Çø„ÅÆÂΩ¢Âºè„Åß„ÄÅÂÆüË°åÊôÇ„ÅÆ„Éè„Éº„Éâ„Ç¶„Çß„Ç¢„É¨„Éô„É´„ÅÆË©≥Á¥∞„Å™ÊÉÖÂ†±„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ

:::message
„Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞ÂàÜÊûê„Å´„Å§„ÅÑ„Å¶„ÇÇ„É©„Ç§„Éñ„É©„É™„Çí‰Ωú„Çç„ÅÜ„Å®ÊÄù„Å£„Å¶„ÅÑ„Åæ„Åô„ÅåÂõ∫ÂÆöÁöÑ„Å™ÂàÜÊûê„Åó„Åã„Åß„Åç„Å™„ÅÑ„Å®„ÅÇ„Åæ„ÇäÊúâÁî®„Åß„ÅØ„Å™„ÅÑ„Åü„ÇÅ„Å©„ÅÆ„Çà„ÅÜ„Å´ÂÆüË£Ö„Åô„Çã„ÅÆ„ÅãÈùûÂ∏∏„Å´ÊÇ©„Çì„Åß„ÅÑ„Åæ„Åô„ÄÇ„Åù„ÇÇ„Åù„ÇÇ„Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞„Å´Èñ¢„Åô„ÇãÁü•Ë¶ã„Åå‰∏çË∂≥„Åó„Å¶„ÅÑ„Çã„Åü„ÇÅ„Åæ„Åö„ÅØÁµåÈ®ì„ÇíÁ©ç„Çì„Åß„Åã„ÇâÂàÜÊûê„É©„Ç§„Éñ„É©„É™„Çí‰Ωú„Çç„ÅÜ„Å®ËÄÉ„Åà„Å¶„ÅÑ„Åæ„Åô„ÄÇÊòé„Çâ„Åã„Å´‰∫∫Èñì„Çà„Çä AI „Ç®„Éº„Ç∏„Çß„É≥„Éà„ÅÆÊñπ„ÅåÁµêÊûú„ÅÆÂàÜÊûê„ÅØÂæóÊÑè„Å™È†òÂüü„Å´„Å™„Çä„Å§„Å§„ÅÇ„Çã„Åü„ÇÅ„ÄÅAI „Å®„ÅÆÈÄ£Êê∫„ÇíÂäπÁéáÂåñ„Åô„Çã„Çà„ÅÜ„Å™„É©„Ç§„Éñ„É©„É™„ÅåËâØ„ÅÑ„Åã„ÇÇ„Åó„Çå„Åæ„Åõ„Çì„ÄÇ„ÅÇ„Å®ÂæåËø∞„Åô„Çã„Çà„ÅÜ„Å´ neuron-profile „Ç≥„Éû„É≥„Éâ„ÅåÈùûÂ∏∏„Å´ÂÑ™ÁßÄ„Å™„ÅÆ„ÅßÂõ∫ÂÆöÁöÑ„Å™ÁµêÊûú„ÅÆÈõÜË®à„ÉÑ„Éº„É´„ÅØ„Åù„Åì„Åæ„ÅßÂøÖË¶Å„Å™„ÅÑ„Å®ÊÄù„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇ„Åù„Åó„Å¶„Åæ„Å†Ë©¶„Åõ„Å¶„ÅÑ„Åæ„Åõ„Çì„Åå**‰ªäÂæåÂá∫„Å¶„Åè„Çã [Neuron Explorer](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/tools/neuron-explorer/index.html) „ÅåÈùûÂ∏∏„Å´ÂÑ™ÁßÄ„Åù„ÅÜ**„Å™„ÅÆ„Åß‰ªäÂàÜÊûêÂÅ¥„Çí‰Ωú„Çã„ÅÆ„ÅØ„ÅÇ„Åæ„ÇäÊ∞ó‰πó„Çä„Åó„Åæ„Åõ„Çì„ÄÇ
:::

### ÁîüÊàê„Åï„Çå„Åü„Éï„Ç°„Ç§„É´„ÅÆÁ¢∫Ë™ç

Perfetto „É¢„Éº„Éâ„ÅßÂÆüË°å„Åô„Çã„Å®„ÄÅ‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å™„Éï„Ç°„Ç§„É´ÊßãÈÄ†„ÅåÁîüÊàê„Åï„Çå„Åæ„Åô„ÄÇ

```bash
# „Éó„É≠„Éï„Ç°„Ç§„É´Âá∫Âäõ„Éá„Ç£„É¨„ÇØ„Éà„É™„ÅÆÁ¢∫Ë™ç
ls -la profile_output/

# Âá∫Âäõ‰æã:
# drwxr-xr-x  i-0049acfde6046f237_pid_16695/  # „Çª„ÉÉ„Ç∑„Éß„É≥„Éá„Ç£„É¨„ÇØ„Éà„É™
# -rw-r--r--  metadata.json                    # „É°„Çø„Éá„Éº„Çø

# „Çª„ÉÉ„Ç∑„Éß„É≥„Éá„Ç£„É¨„ÇØ„Éà„É™ÂÜÖ„ÅÆÁ¢∫Ë™ç
ls -lh profile_output/i-*/

# Âá∫Âäõ‰æãÔºàNTFF„Éï„Ç°„Ç§„É´Áæ§Ôºâ:
# -rw-r--r--  90860660587470_instid_0_vnc_0.ntff  (17M)
# -rw-r--r--  90860660587470_instid_0_vnc_1.ntff  (17M)
# -rw-r--r--  948584188481322_instid_0_vnc_0.ntff (22M)
# -rw-r--r--  948584188481322_instid_0_vnc_1.ntff (22M)
# -rw-r--r--  neff_*.neff                         (ÂêÑÁ®ÆNEFF„Éï„Ç°„Ç§„É´)
```

### ÊñπÊ≥ï1: neuron-profile „Ç≥„Éû„É≥„Éâ„ÅßËß£Êûê

AWS Neuron SDK „Å´Âê´„Åæ„Çå„Çã `neuron-profile` „Ç≥„Éû„É≥„Éâ„Çí‰Ωø„Å£„Å¶„ÄÅ„ÉÜ„Ç≠„Çπ„ÉàÂΩ¢Âºè„ÅÆ„Çµ„Éû„É™„Éº„ÇíÂèñÂæó„Åß„Åç„Åæ„Åô„ÄÇ

```bash
# „Çµ„Éû„É™„Éº„ÉÜ„Ç≠„Çπ„ÉàÂΩ¢Âºè„ÅßË°®Á§∫
neuron-profile view -d profile_output/i-*/  --output-format summary-text
```

**Âá∫Âäõ‰æãÔºàÊäúÁ≤ãÔºâ:**
```
n_e378c855a4f6486b3d3bb00d39fad58ee9b175_90860660587470
    total_time                                     0.014276642922
    total_active_time                              0.013842402239
    total_active_time_percent                      0.9695838380652608
    dma_active_time_percent                        0.2798723112170025
    tensor_engine_active_time_percent              0.27477470617094135
    hardware_flops                                 86660320050
    model_flops                                    58405804032
    mfu_estimated_percent                          0.044588405622639014
```

ÁµêÊûú„ÅÆÁ¢∫Ë™çÊñπÊ≥ï„Å´„Å§„ÅÑ„Å¶„ÅØ [Neuron Profiler User Guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/tools/profiler/neuron-profile-user-guide.html) „ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

‰∏ªË¶Å„Å™„É°„Éà„É™„ÇØ„Çπ:
- **total_time**: ÂÖ®‰Ωì„ÅÆÂÆüË°åÊôÇÈñì
- **total_active_time_percent**: „Ç¢„ÇØ„ÉÜ„Ç£„ÉñÊôÇÈñì„ÅÆÂâ≤ÂêàÔºàÈ´ò„ÅÑ„Åª„Å©ËâØ„ÅÑÔºâ
- **mfu_estimated_percent**: Model FLOPs UtilizationÔºà„É¢„Éá„É´ÊºîÁÆóÂäπÁéáÔºâ
- **dma_active_time_percent**: DMA Ëª¢ÈÄÅ„ÅÆÊ¥ªÊÄßÁéá
- **tensor_engine_active_time_percent**: Tensor Engine „ÅÆÊ¥ªÊÄßÁéá

### ÊñπÊ≥ï2: Perfetto UI „ÅßÂèØË¶ñÂåñ

„Çà„ÇäË©≥Á¥∞„Å™ÂàÜÊûê„Å´„ÅØ„ÄÅPerfetto UI „Çí‰ΩøÁî®„Åó„Åæ„Åô„ÄÇ

**„Çπ„ÉÜ„ÉÉ„Éó1: PerfettoÂΩ¢Âºè„Å´Â§âÊèõ**

```bash
# NTFF„Éï„Ç°„Ç§„É´„ÇíPerfettoÂΩ¢Âºè„Å´Â§âÊèõ
neuron-profile view \
  -d profile_output/i-*/ \
  --output-format perfetto \
  --output-file profile_output/trace.perfetto-trace
```

„Åì„Çå„Å´„Çà„Çä„ÄÅPerfetto UI „ÅßË™≠„ÅøËæº„ÇÅ„ÇãÂΩ¢Âºè„ÅÆ„Éà„É¨„Éº„Çπ„Éï„Ç°„Ç§„É´„ÅåÁîüÊàê„Åï„Çå„Åæ„Åô„ÄÇ

**„Çπ„ÉÜ„ÉÉ„Éó2: Perfetto UI „ÅßÈñã„Åè**

1. „Éñ„É©„Ç¶„Ç∂„Åß https://ui.perfetto.dev/ „ÇíÈñã„Åè
2. ÁîüÊàê„Åï„Çå„Åü `trace.perfetto-trace` „Çí„Éâ„É©„ÉÉ„Ç∞&„Éâ„É≠„ÉÉ„Éó

‰ª•‰∏ã„ÅÆ„Çà„ÅÜ„Å™„Éà„É¨„Éº„ÇπÁµêÊûú„ÅåÂæó„Çâ„Çå„Åæ„Åô„ÄÇPending DMA Count „ÅåÂçòË™øÂ¢óÂä†„Åó„Å¶„Åä„Çä DMA „Åå„Éú„Éà„É´„Éç„ÉÉ„ÇØ„Å´„Å™„Å£„Å¶„ÅÑ„Çã„Åì„Å®„Åå„Çè„Åã„Çä„Åæ„Åô„Å≠„ÄÇ

![](/images/d68bd091d1934d-000.png)

### ÊñπÊ≥ï3: Python „Çπ„ÇØ„É™„Éó„Éà„ÅßËá™ÂãïËß£Êûê

```bash
sudo /opt/aws_neuronx_venv_pytorch_inference_vllm_0_13/bin/pip install perfetto
```

SQL „ÇØ„Ç®„É™„Çí‰Ωø„Å£„Å¶„ÄÅ„Éó„É≠„Éï„Ç°„Ç§„É´ÁµêÊûú„Åã„ÇâÂøÖË¶Å„Å™„É°„Éà„É™„ÇØ„Çπ„ÇíÊäΩÂá∫„Åô„Çã Python „Çπ„ÇØ„É™„Éó„Éà„Çí‰ΩúÊàê„Åó„Åæ„Åó„Åü„ÄÇ

:::details NTFF „Éï„Ç°„Ç§„É´Ëß£Êûê‰æã
```python:analyze_ntff.py
"""
Analyze NTFF (Neuron Trace File Format) files using Perfetto TraceProcessor.

This script demonstrates how to extract performance metrics from NTFF files
generated by benchmark-capture with Perfetto mode enabled.

Usage:
    python analyze_ntff.py <ntff_file_path>

Example:
    python analyze_ntff.py profile_output/i-xxx_pid_yyy/*.ntff
"""

import sys
from pathlib import Path
from typing import Dict, Any, List


def trace_overview(tp) -> Dict[str, Any]:
    """Get high-level trace metrics."""
    sql = """
    SELECT
        (SELECT MAX(ts) FROM slice) / 1e9 AS duration_seconds,
        (SELECT COUNT(*) FROM slice) AS total_slices,
        (SELECT COUNT(DISTINCT name) FROM slice) AS unique_slice_names,
        (SELECT COUNT(*) FROM counter) AS total_counter_events
    """
    result = tp.query(sql)

    # Convert iterator to list
    rows = list(result)
    if rows:
        row = rows[0]
        return {
            'duration_seconds': row.duration_seconds,
            'total_slices': row.total_slices,
            'unique_slice_names': row.unique_slice_names,
            'total_counter_events': row.total_counter_events
        }
    return {}


def top_slices_by_duration(tp, top_n: int = 10) -> List[Dict[str, Any]]:
    """Find the longest running operations."""
    sql = f"""
    SELECT
        name,
        COUNT(*) as count,
        SUM(dur) / 1e9 as total_duration_seconds,
        AVG(dur) / 1e9 as avg_duration_seconds,
        MAX(dur) / 1e9 as max_duration_seconds
    FROM slice
    WHERE dur > 0
    GROUP BY name
    ORDER BY total_duration_seconds DESC
    LIMIT {top_n}
    """
    result = tp.query(sql)

    rows = []
    for row in result:
        rows.append({
            'name': row.name,
            'count': row.count,
            'total_duration_seconds': row.total_duration_seconds,
            'avg_duration_seconds': row.avg_duration_seconds,
            'max_duration_seconds': row.max_duration_seconds
        })
    return rows


def runtime_api_calls(tp, top_n: int = 10) -> List[Dict[str, Any]]:
    """Analyze Neuron Runtime API calls."""
    sql = f"""
    SELECT
        name,
        COUNT(*) as call_count,
        SUM(dur) / 1e9 as total_duration_seconds,
        AVG(dur) / 1e9 as avg_duration_seconds,
        MAX(dur) / 1e9 as max_duration_seconds
    FROM slice
    WHERE name LIKE 'nrt_%' OR name LIKE 'Neuron%'
    GROUP BY name
    ORDER BY total_duration_seconds DESC
    LIMIT {top_n}
    """
    result = tp.query(sql)

    rows = []
    for row in result:
        rows.append({
            'name': row.name,
            'call_count': row.call_count,
            'total_duration_seconds': row.total_duration_seconds,
            'avg_duration_seconds': row.avg_duration_seconds,
            'max_duration_seconds': row.max_duration_seconds
        })
    return rows


def execution_phases(tp) -> List[Dict[str, Any]]:
    """Identify major execution phases."""
    sql = """
    SELECT
        name,
        COUNT(*) as occurrences,
        SUM(dur) / 1e9 as total_duration_seconds,
        AVG(dur) / 1e9 as avg_duration_seconds
    FROM slice
    WHERE name IN ('compile', 'load', 'execute', 'inference', 'forward')
        OR name LIKE '%compile%'
        OR name LIKE '%execute%'
        OR name LIKE '%inference%'
    GROUP BY name
    ORDER BY total_duration_seconds DESC
    """
    result = tp.query(sql)

    rows = []
    for row in result:
        rows.append({
            'name': row.name,
            'occurrences': row.occurrences,
            'total_duration_seconds': row.total_duration_seconds,
            'avg_duration_seconds': row.avg_duration_seconds
        })
    return rows


def analyze_ntff_file(ntff_path: str):
    """Analyze a single NTFF file and print metrics."""
    try:
        from perfetto.trace_processor import TraceProcessor
    except ImportError:
        print("ERROR: perfetto package not installed")
        print("Install with: pip install perfetto")
        sys.exit(1)

    ntff_file = Path(ntff_path)
    if not ntff_file.exists():
        print(f"ERROR: File not found: {ntff_path}")
        sys.exit(1)

    print(f"\n{'='*80}")
    print(f"Analyzing NTFF file: {ntff_file.name}")
    print(f"{'='*80}\n")

    # Initialize TraceProcessor
    tp = TraceProcessor(trace=str(ntff_file))

    # 1. Trace Overview
    print("üìä Trace Overview")
    print("-" * 80)
    overview = trace_overview(tp)
    if overview:
        print(f"  Duration:              {overview['duration_seconds']:.3f} seconds")
        print(f"  Total Slices:          {overview['total_slices']:,}")
        print(f"  Unique Slice Names:    {overview['unique_slice_names']:,}")
        print(f"  Total Counter Events:  {overview['total_counter_events']:,}")
    else:
        print("  No data available")

    # 2. Top Operations by Duration
    print(f"\nüîù Top 10 Operations by Total Duration")
    print("-" * 80)
    top_ops = top_slices_by_duration(tp, top_n=10)
    if top_ops:
        print(f"{'Operation':<50} {'Count':>8} {'Total (s)':>12} {'Avg (ms)':>12} {'Max (ms)':>12}")
        print("-" * 80)
        for op in top_ops:
            name = op['name'][:48] if len(op['name']) > 48 else op['name']
            print(f"{name:<50} {op['count']:>8} {op['total_duration_seconds']:>12.3f} "
                  f"{op['avg_duration_seconds']*1000:>12.3f} {op['max_duration_seconds']*1000:>12.3f}")
    else:
        print("  No data available")

    # 3. Runtime API Calls
    print(f"\nüîß Neuron Runtime API Calls")
    print("-" * 80)
    api_calls = runtime_api_calls(tp, top_n=10)
    if api_calls:
        print(f"{'API Call':<50} {'Count':>8} {'Total (s)':>12} {'Avg (ms)':>12}")
        print("-" * 80)
        for call in api_calls:
            name = call['name'][:48] if len(call['name']) > 48 else call['name']
            print(f"{name:<50} {call['call_count']:>8} {call['total_duration_seconds']:>12.3f} "
                  f"{call['avg_duration_seconds']*1000:>12.3f}")
    else:
        print("  No Neuron Runtime API calls detected")

    # 4. Execution Phases
    print(f"\n‚ö° Execution Phases")
    print("-" * 80)
    phases = execution_phases(tp)
    if phases:
        print(f"{'Phase':<50} {'Count':>8} {'Total (s)':>12} {'Avg (ms)':>12}")
        print("-" * 80)
        for phase in phases:
            name = phase['name'][:48] if len(phase['name']) > 48 else phase['name']
            print(f"{name:<50} {phase['occurrences']:>8} {phase['total_duration_seconds']:>12.3f} "
                  f"{phase['avg_duration_seconds']*1000:>12.3f}")
    else:
        print("  No execution phases detected")

    print(f"\n{'='*80}\n")


def main():
    if len(sys.argv) < 2:
        print("Usage: python analyze_ntff.py <ntff_file_path>")
        print("\nExample:")
        print("  python analyze_ntff.py profile_output/i-xxx_pid_yyy/*.ntff")
        sys.exit(1)

    ntff_path = sys.argv[1]
    analyze_ntff_file(ntff_path)


if __name__ == "__main__":
    main()

```
::::

**ÂÆüË°åÁµêÊûúÔºàÂÆüÈöõ„ÅÆ„Éó„É≠„Éï„Ç°„Ç§„É´ÁµêÊûúÔºâ:**

```bash
python analyze_ntff.py profile_output/trace.perfetto-trace
```

```
================================================================================
Analyzing NTFF file: trace.perfetto-trace
================================================================================

üìä Trace Overview
--------------------------------------------------------------------------------
  Duration:              0.014 seconds
  Total Slices:          350,713
  Unique Slice Names:    10,510
  Total Counter Events:  238,160

üîù Top 10 Operations by Total Duration
--------------------------------------------------------------------------------
Operation                                             Count    Total (s)     Avg (ms)     Max (ms)
--------------------------------------------------------------------------------
unknown                                              156427        0.038        0.000        0.003
MATMUL                                                21582        0.008        0.000        0.002
custom_call.17_sg0002                                    36        0.007        0.196        7.028
EVENT_SEMAPHORE                                        2184        0.003        0.002        0.046
LDWEIGHTS                                             21212        0.003        0.000        0.001
FIND_INDEX8                                             224        0.002        0.010        0.014
MAX8                                                    224        0.002        0.010        0.014
MATCH_REPLACE8                                          217        0.002        0.010        0.014
ACTIVATE                                               4702        0.002        0.000        0.001
                                                       1276        0.002        0.001        0.025

üîß Neuron Runtime API Calls
--------------------------------------------------------------------------------
API Call                                              Count    Total (s)     Avg (ms)
--------------------------------------------------------------------------------
NeuronQwen3ForCausalLM[.1][0]/NeuronQwen3Model[.      30228        0.001        0.000

‚ö° Execution Phases
--------------------------------------------------------------------------------
  No execution phases detected

================================================================================
```

### „Éó„É≠„Éï„Ç°„Ç§„É´ÁµêÊûú„ÅÆË™≠„ÅøÊñπ„Å®ÊúÄÈÅ©Âåñ„Éí„É≥„Éà

**1. Top Operations „ÅÆÂàÜÊûê**

- **MATMUL**: Ë°åÂàó‰πóÁÆó„Åå 0.008 ÁßíÔºà„Éà„É¨„Éº„ÇπÂÖ®‰Ωì„ÅÆÁ¥Ñ57%„ÇíÂç†„ÇÅ„ÇãÔºâ
  - Ë®àÁÆó: 0.008Áßí / 0.014ÁßíÔºàÂÖ®‰Ωì durationÔºâ= 57%

- **custom_call.17_sg0002**: „Ç´„Çπ„Çø„É†Êìç‰Ωú„Åå 0.007 Áßí
  - Max „Åå 7.028ms „Å®Á™ÅÂá∫ ‚Üí „Éú„Éà„É´„Éç„ÉÉ„ÇØ„ÅÆÂèØËÉΩÊÄß
  - ***Perfetto UI „Åß„Çø„Ç§„É†„É©„Ç§„É≥„ÇíÁ¢∫Ë™ç„Åô„Åπ„Åç***

- **LDWEIGHTS**: Èáç„ÅøË™≠„ÅøËæº„Åø„ÅåÈ†ªÁπÅÔºà21,212 ÂõûÔºâ
  - Prefix Caching „ÇíÊúâÂäπÂåñ„Åô„Çã„Åì„Å®„ÅßÂâäÊ∏õÂèØËÉΩÔºü

**2. API Call Frequency „ÅÆÁ¢∫Ë™ç**

- 30,228 Âõû„ÅÆ API Âëº„Å≥Âá∫„Åó ‚Üí „Éê„ÉÉ„ÉÅ„Çµ„Ç§„Ç∫ÊúÄÈÅ©Âåñ„ÅßÂâäÊ∏õÂèØËÉΩÔºü

Ê¨°Âõû„ÅÆË®ò‰∫ã„Åß„ÅØ„ÄÅ„Åì„Çå„Çâ„ÅÆ„Éó„É≠„Éï„Ç°„Ç§„É´„Å´„Çà„Çã„Ç§„É≥„Çµ„Ç§„Éà„Åã„ÇâË®≠ÂÆö„ÅÆÂ§âÊõ¥„ÇíÂÆüÈöõ„Å´Ë©¶„Åó„Å¶„ÄÅ„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÂêë‰∏ä„ÇíË©¶„Åø„Åæ„Åô„ÄÇ

## 9. ÁèæÂú®„ÅÆÂà∂Èôê‰∫ãÈ†Ö

NVIDIA GPU Áí∞Â¢É„Åß„ÅÆÂãï‰ΩúÊ§úË®º„ÅåÊú™ÂÆå‰∫ÜÔºàNSight Systems „Éó„É≠„Éï„Ç°„Ç§„É©„ÉºÔºâ„Åß„ÅÇ„Çã„Åì„Å®„ÄÅCI/CD „Éë„Ç§„Éó„É©„Ç§„É≥„ÅåÊú™Êï¥ÂÇô„Åß„ÅÇ„Çã„Åì„Å®„ÄÅÂàÜÊï£Êé®Ë´ñ„Å∏„ÅÆÂØæÂøú„ÄÅvllm-neuron ‰ª•Â§ñ„Åß„ÅÆÂãï‰ΩúÁ¢∫Ë™ç„ÄÅ„Å™„Å©„Åå„Åß„Åç„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇ

## 10. „Åæ„Å®„ÇÅ

Êú¨Ë®ò‰∫ã„Åß„ÅØ„ÄÅvllm-neuron „ÅÆ„Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞„ÇíÁ∞°Âçò„Å´„Åô„Çã„Åü„ÇÅ„Å´ÈñãÁô∫„Åó„Åü benchmark-capture „Å´„Å§„ÅÑ„Å¶‰Ωï„ÇíËÄÉ„Åà„Å¶‰Ωú„Å£„Åü„ÅÆ„Åã„ÄÅ„Å©„ÅÜ„ÅÑ„ÅÜÊ©üËÉΩ„ÇíÂÖ•„Çå„Åü„ÅÆ„Åã„ÄÅ„Å©„ÅÜÂãï„Åã„Åô„ÅÆ„Åã„ÄÅ„Åä„Åæ„Åë„Å®„Åó„Å¶„Å©„ÅÜ„Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞„Åô„Çã„ÅÆ„Åã„ÄÅÁ¥π‰ªã„Åó„Åæ„Åó„Åü„ÄÇ‰ªäÂæåËá™„Çâ„Éâ„ÉÉ„Ç∞„Éï„Éº„Éá„Ç£„É≥„Ç∞„Åó„Å¶„Éñ„É©„ÉÉ„Ç∑„É•„Ç¢„ÉÉ„Éó„Åó„Å¶„ÅÑ„Åè‰∫àÂÆö„Åß„Åô„ÄÇ

ÈáçË¶Å„Å™Êé®„Åó„Éù„Ç§„É≥„Éà„Å®„Åó„Å¶„ÄÅ„Éá„Ç≥„É¨„Éº„Çø„Éº 1 Ë°å„Åß„Éó„É≠„Éï„Ç°„Ç§„É™„É≥„Ç∞Ôºà`@profile()` „ÇíËøΩÂä†„Åô„Çã„Å†„ÅëÔºâ„ÄÅ„Éè„Éº„Éâ„Ç¶„Çß„Ç¢Ëá™ÂãïÊ§úÂá∫ÔºàAWS Neuron, NVIDIA GPU, CPU „ÇíËá™ÂãïÂà§Âà•Ôºâ„ÄÅpytest-benchmark Áµ±ÂêàÔºàÊó¢Â≠ò„ÉÑ„Éº„É´„ÇíÊ¥ªÁî®Ôºâ„ÄÅÁí∞Â¢ÉÂ§âÊï∞„ÅÆËá™ÂãïÁÆ°ÁêÜÔºàË®≠ÂÆöÂøò„Çå„Çí„Çº„É≠„Å´Ôºâ„ÄÅPerfetto „É¢„Éº„ÉâÔºàPerfetto UI „Åß„ÅÆÂèØË¶ñÂåñ„Å´ÂØæÂøúÔºâ„ÇíÂÆüÁèæ„Åó„Åæ„Åó„Åü„ÄÇ

Ê¨°Âõû„ÅÆË®ò‰∫ã„Åß„ÅØ„ÄÅ„Åì„ÅÆ benchmark-capture „Çí‰Ωø„Å£„ÅüÂÆüÈöõ„ÅÆ vllm-neuron „Éë„É©„É°„Éº„Çø„Çπ„Ç§„Éº„ÉóÂÆüÈ®ì„Å®„ÄÅ„Åù„ÅÆÁµêÊûúÂàÜÊûê„Å´„Å§„ÅÑ„Å¶Á¥π‰ªã„Åô„Çã‰∫àÂÆö„Åß„Åô„ÄÇ

**„É™„É≥„ÇØÔºö**
- GitHub: https://github.com/littlemex/benchmark-capture
- TestPyPI: https://test.pypi.org/project/benchmark-capture/
- ÊúÄÊñ∞„É™„É™„Éº„Çπ: v0.2.4

:::message
AI Âêõ„ÇíÈßÜ‰Ωø„Åó„Å¶„ÇÇÊú¨ OSS „ÅÆ‰ΩúÊàê„ÅØ„ÇÅ„Å£„Å°„ÇÉ„Åè„Å°„ÇÉÂ§ßÂ§â„Åß„Åó„Åü„ÄÇ„ÄÇÂàùÊúü„ÅÆ‰∫àÂÇôÂÆüÈ®ì„Åã„ÇâÊú¨„Éñ„É≠„Ç∞„ÅÆÂÆåÊàê„Åæ„ÅßÂê´„ÇÅ„Çã„Å® 4 Êó•„Åã„Åã„Çä„Åæ„Åó„Åü„ÄÇ„ÄÇ„ÅÇ„Å® AI Âêõ„Åå `/compact` „Åô„Çã„Åü„Å≥„Å´Ëã±Ë™ûË©±ËÄÖ„Å´Ëª¢Áîü„Åó„Å¶ÂÖ®„Å¶„ÇíÂøòÂç¥„Åô„Çã„ÅÆ„Åß AI ÈñãÁô∫Áí∞Â¢É„Çí„Åù„Çç„Åù„Çç„ÇÇ„Å£„Å®Êï¥ÂÇô„Åó„Å™„ÅÑ„Å®„ÉÄ„É°„Åß„Åô„Å≠„ÄÇ„ÄÇ„ÅÇ„Å®„Ç®„É©„Éº„Åô„Çã„Å®„Åô„Åê„ÉÅ„Éß„É≥„Éú„Åó„Å¶„ÇÑ„Å£„Åü„Åì„Å®„Å´„Åô„Çã„ÅÆ„ÇÇÂõ∞„Çä„ÇÇ„ÅÆ„Åß„Åó„Åü„ÄÇ„ÄÇ
:::