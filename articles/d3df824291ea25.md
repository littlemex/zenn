---
title: "[翻訳] AWS Neuron ProfilerによるNKIカーネルパフォーマンスのデコード"
emoji: "🐥"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["AWS Neuron"]
published: true
---


# AWS Neuron ProfilerによるNKIカーネルパフォーマンスのデコード

このブログでは、NKI（Neuron Kernel Interface）でNeuron Profilerを使用してカスタムカーネルのパフォーマンスを分析する方法を実演し、実践的な例を通じて、プロファイラがNeuronCoreエンジン全体での最適化機会をどのように明らかにするかを示します。

![Sadaf Rasool](https://avatars.builderprofile.aws.dev/34RcM8QFE4L3VhuNbKrSVxSnNy2.webp)

[Sadaf Rasool](/community/@sadafrasool)

**公開日: 2025年10月23日**

https://builder.aws.com/content/34Ru44lIq9QrlPgr16BvDm78F3G/decoding-nki-kernel-performance-using-aws-neuron-profiler の翻訳

---

NKI（[Neuron Kernel Interface](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/index.html)）は、TrainiumとInferentiaインスタンス上のAWS NeuronCoreハードウェアへの直接アクセスを可能にする強力な低レベルプログラミングフレームワークです。このベアメタル言語とコンパイラシステムにより、機械学習開発者はカスタムオペレータを作成および最適化でき、NeuronCoreの計算とメモリ機能の潜在能力を最大限に引き出すことができます。NeuronCores v2（Trainium1）以降、NKIは開発者がハードウェア活用に対する新しいアプローチを独自に探求し実装することを可能にし、最終的にAWS MLアクセラレータで可能なことの限界を押し広げます。

Neuron Profilerは、NeuronデバイスでのNKIカーネル実行のパフォーマンス特性への窓口として機能します。専用のオンチッププロファイリングハードウェアを活用することで、ほぼゼロのオーバーヘッドで詳細なパフォーマンスメトリクスを提供します。このツールは、コンピュートエンジン全体の命令実行に関する包括的なデータをキャプチャし、DMAエンジンを通じたデータ移動を監視しながら、コンピュートエンジンの活用率やメモリ帯域幅効率などの重要なパフォーマンス指標を測定します。この詳細な可視性により、開発者はパフォーマンスのボトルネックと最適化の機会を正確に特定でき、アプリケーションの最大限のハードウェア効率を確保できます。

## NKIカーネルでのNeuron Profiler

この例では、NKIを使用して2Dテンソルに対するタイル化された要素ごとの`exp`操作を実装するカーネルを使用します。NKIカーネルをプロファイリングする方法は多数ありますが、この例では`@nki.benchmark`デコレータを使用します。

**ステップ1** - このPythonファイルを`exp.py`として保存します

```python
"""
Example kernel used to demmonstrate Neuron Profile with nki.benchmark.
"""
from neuronxcc import nki
from neuronxcc.nki.typing import tensor
import neuronxcc.nki.language as nl
import math

@nki.benchmark(save_neff_name='file.neff', save_trace_name='profile.ntff')
def tensor_exp_kernel_(in_tensor):
  """NKI kernel to compute elementwise exponential of an input tensor
  Args:
    in_tensor: an input tensor of ANY 2D shape (up to SBUF size)
  Returns:
    out_tensor: an output tensor of ANY 2D shape (up to SBUF size)
  """
  out_tensor = nl.ndarray(in_tensor.shape, dtype=in_tensor.dtype,
                          buffer=nl.shared_hbm)

  sz_p, sz_f = in_tensor.shape
  i_f = nl.arange(sz_f)[None, :]
  for p in nl.affine_range(math.ceil(sz_p / nl.tile_size.pmax)):
    # Generate tensor indices for the input/output tensors
    # pad index to pmax, for simplicity
    i_p = p * nl.tile_size.pmax + nl.arange(nl.tile_size.pmax)[:, None]
    # Load input data from external memory to on-chip memory
    # only read up to sz_p
    in_tile = nl.load(in_tensor[i_p, i_f], mask=(i_p<sz_p))
    # perform the computation
    out_tile = nl.exp(in_tile)
    # store the results back to external memory
    # only write up to sz_p
    nl.store(out_tensor[i_p, i_f], value=out_tile, mask=(i_p<sz_p))

  return out_tensor

if __name__ == "__main__":
  tensor_exp_kernel_(tensor[[250, 512], nl.float32])
```

**ステップ2** - このPythonファイルを実行します

```
python3 exp.py
```

実行が完了すると、NKIカーネルのプロファイルがキャプチャされたことがわかります。

```
+---+----+---------+---------+---------+---------+---------+------+-------+-------+--------+---------+---------+-------+
  B  NC  NC USED  WEIGHTS     MODE     INF/S   IRES/S   L(1)  L(50)  L(99)  NCL(1)  NCL(50)  NCL(99)  %USER
  1   1       1   dynamic  LIBMODE   5123.60  5123.60    53     80     93      20       20       21    N/A
+---+----+---------+---------+---------+---------+---------+------+-------+-------+--------+---------+---------+-------+
Profile results are written to profile.ntff
```

**ステップ3** - Neuron Profilerを使用してこのNKIカーネルのプロファイルを可視化します。

```
neuron-profile view -n file.neff -s profile.ntff
```

![](https://prod-assets.cosmic.aws.dev/a/34RunM0MSxwFL7yv42vneJgkSLS/blog.webp?imgSize=1000x241 "nki_profile_engines")

このプロファイルの中央セクションを見ると、重要なDMA（Direct Memory Access）アクティビティに気付くでしょう。この領域にカーソルを合わせることで、任意の特定のタイムスタンプでの正確なDMAスループット測定値を表示できます。このインタラクティブ機能により、メモリ転送パターンとパフォーマンス特性の詳細な分析が可能になります。

![](https://prod-assets.cosmic.aws.dev/a/34RutCu9prJcsopGXwVI7fAK3ac/blog.webp?imgSize=822x201 "dma_throughput")

異なるエンジンで実行されている特定の操作を識別するには、任意のタイムライン上にマウスを合わせるだけです。ツールチップが表示され、現在の操作のオペコードが表示され、各エンジンを占有しているタスクに関する詳細な洞察を提供します。このインタラクティブ機能により、システムのエンジン全体の実行フローをリアルタイムで簡単に追跡および分析できます。

![](https://prod-assets.cosmic.aws.dev/a/34RuxfHZG2EWZ27Yw9H4QpcTnUZ/blog.webp?imgSize=1000x476 "nki_code")

以下に示すように、トレースで`/./`パターンを検索することで、NKIソースコードの特定の行にリンクされているすべての命令を簡単に識別できます。この技術は、NKIカーネルを分析および最適化する際に特に有用で、潜在的なパフォーマンスのボトルネックを特定するのに役立ちます。

![](https://prod-assets.cosmic.aws.dev/a/34Rv3OdFjGNHXPaN8PW6BTDeDHs/blog.webp?imgSize=1000x172 "nki_search")

さらに、強調表示された命令バーにカーソルを合わせると、ツールは標準の命令の詳細を表示するだけでなく、対応するNKIソースコードの場所も明らかにします。このマッピングにより、どのNKIコードの正確な行が特定のエンジンで操作をトリガーしたかが明確に可視化され、低レベルの実行動作と高レベルのカーネルコードを接続することがはるかに容易になります。

![](https://prod-assets.cosmic.aws.dev/a/34Rv8cipwsu7d5FMlBTo9kD6Num/blog.webp?imgSize=1000x263 "nki_location")

グラフィカルインターフェースを補完するものとして、Neuron Profilerはパフォーマンスデータの詳細な表形式ビューを提供します。この代替表現は、同じメトリクスを構造化された形式で提示し、`total_time`、`event_count`などの特定の値を簡単に分析できるようにします。表形式は、パフォーマンスメトリクスの正確な分析と比較に特に有用で、上記のグラフが提供する視覚的な洞察に対する定量的な補完を提供します。

![](https://prod-assets.cosmic.aws.dev/a/34RvDNf8BMTkvOKQTTtEQyRz2Xl/blog.webp?imgSize=1000x248 "summary")

この例では、Neuron Profilerを使用してNKIカーネルをプロファイリングし、そのパフォーマンス特性に関する洞察を得ました。Neuron Profilerでプロファイリングすることで、カーネルが実行時にどのように動作するかの詳細なビューが提供されるだけでなく、潜在的なボトルネック、非効率性、最適化の機会を特定するのに役立ちます。プロファイリング結果から、このカーネルは主にベクトル、スカラー、GPSIMDの3つのNeuronコアエンジンを利用していることがわかります。しかし、それらの間にオーバーラップはありません。さらに、DMAと計算の間にもオーバーラップがありません—データは計算が始まる前に完全にロードされます。これは、タイリングを適用して異なる計算エンジン間で操作をより適切にパイプライン化および並列化し、全体的な効率を向上させることができることを示しています。

この情報は、カーネルのパフォーマンスを正しい方向にチューニングおよび改善するための貴重なガイドとして機能します。ベストプラクティスと最適化戦略のより深い理解については、[NKI Performance Guide](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/nki/nki_perf_guide.html)を参照してください。これは、より良いパフォーマンスのためにNKIカーネルをチューニングする際の優れた出発点となるパターンと最適化戦略の包括的なコレクションを提供しています。

---

このブログは、Esha Lakhotia、Scott Perry、Sadaf Rasoolによって執筆されました。

**Esha Lakhotia**は、AWSのAnnapurna Labsのプロダクトマネージャーです。彼女は、AWSのAIチップ用の開発者ツールを構築することで、顧客がMLパフォーマンス最適化目標を達成できるようにしています。AWS TrainiumとAWS Inferentia上で顧客が機械学習ワークロードをプロファイル、デバッグ、加速するのに役立つ直感的なツール体験の作成に注力しています。

**Scott Perry**は、AWSのAnnapurna MLアクセラレータチームのソリューションアーキテクトです。カナダを拠点とし、AWS InferentiaとAWS Trainiumを使用したディープラーニングのトレーニングと推論ワークロードの展開と最適化を顧客が行うのを支援しています。彼の関心分野には、大規模言語モデル、深層強化学習、IoT、ゲノミクスが含まれます。

**Sadaf Rasool**は、AWSのAnnapurna Labsのソリューションアーキテクトです。Sadafは顧客と協力して、重要なビジネス課題に対処する機械学習ソリューションを設計しています。顧客がAWS TrainiumまたはAWS Inferentiaチップを活用して機械学習モデルをトレーニングおよび展開し、イノベーションの旅を加速するのを支援しています。
