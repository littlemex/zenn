---
title: "OpenAI Whisper ãƒ¢ãƒ‡ãƒ«ã‚’ NxD Inference ã§å‹•ã‹ã™"
emoji: "ğŸ”¥"
type: "tech"
topics: ["AWSNeuron", "Whisper", "Trainium", "æ©Ÿæ¢°å­¦ç¿’", "éŸ³å£°èªè­˜"]
published: false
---

**å¯¾è±¡èª­è€…**: AWS Trainium/Inferentia2 ã§éŸ³å£°èªè­˜ (STT) ã‚’ã‚µãƒ¼ãƒ“ãƒ³ã‚°ã—ãŸã„æ–¹
**å‰æçŸ¥è­˜**: AWS Neuron é–¢é€£ã®åŸºç¤çŸ¥è­˜ã€Transformers ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®åŸºæœ¬çš„ãªä½¿ã„æ–¹

> ğŸ“š **éŸ³å£°èªè­˜ (Speech Recognition) ã«ã¤ã„ã¦å­¦ã¶**
> éŸ³å£°èªè­˜ã®åŸºç¤ã‹ã‚‰å®Ÿè£…ã¾ã§ä½“ç³»çš„ã«å­¦ã³ãŸã„æ–¹ã¯ã€Hugging Face ãŒæä¾›ã™ã‚‹ç„¡æ–™ã®å­¦ç¿’ã‚³ãƒ¼ã‚¹ [Audio Course - Chapter 5: Automatic Speech Recognition](https://huggingface.co/learn/audio-course/chapter5/introduction) ã‚’ãŠã™ã™ã‚ã—ã¾ã™ã€‚Whisper ã‚’å«ã‚€æœ€æ–°ã®éŸ³å£°èªè­˜æŠ€è¡“ã‚’å®Ÿè·µçš„ã«å­¦ã¹ã¾ã™ã€‚ç§ã‚‚éŸ³å£°ã¯ç„¡çŸ¥ã‚’æ¥µã‚ã¦ã„ã‚‹ãŸã‚å­¦ã³å§‹ã‚ã¾ã—ãŸã€‚

## ã¯ã˜ã‚ã«

### NxD Inference ã¨ã¯

**NxD Inference** (NeuronX Distributed Inference) ã¯ã€AWS ãŒé–‹ç™ºã—ãŸå…¬å¼ã®åˆ†æ•£æ¨è«–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚AWS Trainium ãŒæ­è¼‰ã•ã‚ŒãŸã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãªã©ã§ã€å¤§è¦æ¨¡ãª Transformer ãƒ¢ãƒ‡ãƒ«ã‚’åŠ¹ç‡çš„ã«å®Ÿè¡Œã™ã‚‹ãŸã‚ã«è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚

### ãªãœ Whisper ã§ NxD Inference ã‚’ä½¿ã†ã®ã‹

OpenAI ã® Whisper ã¯ã€éŸ³å£°èªè­˜ (STT: Speech-to-Text) ã«ãŠã„ã¦é«˜ã„ç²¾åº¦ã‚’èª‡ã‚‹ Encoder-Decoder Transformer ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚NxD Inference ã‚’ä½¿ã†ã“ã¨ã§æ‰‹å‹•ã§ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã€Tensor Parallelismã€KV-Cacheã€ç­‰ã®å¯¾å¿œã‚’å®Ÿè£…ã™ã‚‹ã“ã¨ãªãå¤§è¦æ¨¡ãªåˆ†æ•£æ¨è«–ã®æ çµ„ã¿ã«ä¹—ã£ã‹ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ãªã®ã§ãƒ¢ãƒ‡ãƒ«ãŒå¯¾å¿œã•ã‚Œã¦ã„ã‚‹ãªã‚‰ç©æ¥µçš„ã«ä¹—ã£ã‹ã£ã¦ãŠããŸã„ã§ã™ã€‚ã©ã†ã‚„ã‚‰ NxD Inference 0.7.0 ã‹ã‚‰ Whisper ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½ã®ã‚ˆã†ãªã®ã§å®Ÿéš›ã«å‹•ã‹ã—ã¦ã¿ã¾ã™ã€‚ï¼ˆå¯¾å¿œã•ã‚Œã¦ãªã„ãƒ¢ãƒ‡ãƒ«ã¯è‡ªåˆ†ã§æ—¢å­˜å®Ÿè£…ã‚’å‚è€ƒã«ã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã—ã¦ã¿ãŸã„ã§ã™ï¼‰

## ç’°å¢ƒè¦ä»¶

| ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ | ãƒãƒ¼ã‚¸ãƒ§ãƒ³ | å‚™è€ƒ |
|---------------|---------------|------|
| **Neuron SDK** | 2.27+ | 2.28+ æ¨å¥¨ |
| **neuronxcc** | 2.22+ | |
| **NxD Inference** | 0.7.0 | å…¬å¼ãƒªãƒªãƒ¼ã‚¹æœ€æ–°: v0.6.10598 |
| **PyTorch** | 2.5+ | 2.8.0+ æ¨å¥¨ |
| **transformers** | 4.40+ | Whisper ãƒ¢ãƒ‡ãƒ«ã‚µãƒãƒ¼ãƒˆ |
| **openai-whisper** | 20250625 | éŸ³å£°å‡¦ç†ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ |

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦

### NeuronApplicationWhisper ã®æ§‹é€ 

NxD Inference ã® Whisper å®Ÿè£…ã¯ã€OpenAI ã®å…¬å¼ Whisper ãƒ¢ãƒ‡ãƒ«ã‚’ç¶™æ‰¿ã—ã¤ã¤ã€Neuron ç‰¹æœ‰ã®æœ€é©åŒ–ã‚’åŠ ãˆã¦ã„ã¾ã™ã€‚

**å‚è€ƒå®Ÿè£…**: [`whisper_nxd_model.py`](https://github.com/littlemex/data-science/blob/3fedc61/investigations/neuron-adapter/phase2-nxd-whisper/experiments/whisper_nxd_model.py#L49-L108)

`NeuronApplicationWhisper` ã“ã‚ŒãŒ NxD Inference å‘ã‘ã«å®Ÿè£…ã•ã‚ŒãŸ Whisper ãƒ¢ãƒ‡ãƒ«ç”¨ã®ã‚¯ãƒ©ã‚¹ã§ã™ã€‚ã“ã‚Œã«å„ç¨®ã® `compile` ã‚„ã‚‰ `transcribe` ãªã©ã®ãƒ¡ã‚½ãƒƒãƒ‰ãŒç”Ÿãˆã¦ã„ã¾ã™ã€‚

```python
class NeuronApplicationWhisper(Whisper):
    """
    AWS Neuron å‘ã‘ã«æœ€é©åŒ–ã•ã‚ŒãŸ Whisper ãƒ¢ãƒ‡ãƒ«ã€‚
    """
    def __init__(self, model_path, config, *args, **kwargs):
        super().__init__(config.dims)

        # Encoder ã¨ Decoder ã‚’åˆ†é›¢
        self.encoder = NeuronApplicationWhisperEncoder(...)
        self.decoder = NeuronApplicationWhisperDecoder(...)
```

### è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ä½œæˆ

**NxD Inference ç‰¹æœ‰ã®å‡¦ç†: NeuronConfig ã®ä½œæˆ**

```python
from neuronx_distributed_inference.models.config import NeuronConfig

# AWS Neuron å‘ã‘ã®è¨­å®š
neuron_config = NeuronConfig(
    batch_size=1,
    torch_dtype=torch.float16,
    tp_degree=8,
)
```

**NxD ç‰¹æœ‰ã®å‡¦ç†: WhisperInferenceConfig ã®ä½œæˆ**

```python
from neuronx_distributed_inference.models.whisper.modeling_whisper import WhisperInferenceConfig
from neuronx_distributed_inference.utils.hf_adapter import load_pretrained_config

# NeuronConfig ã¨ Whisper ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’çµ±åˆ
inference_config = WhisperInferenceConfig(
    neuron_config,
    load_config=load_pretrained_config("openai/whisper-large-v3"),
)
```

**WhisperInferenceConfig ã®å½¹å‰²**ã¯ã€ä¸Šè¿°ã—ãŸ `NeuronConfig` ã¨ Whisper ã® `model_config` ã‚’çµ±åˆã™ã‚‹ã“ã¨ã§ã™ã€‚

### ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã¨ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«

**NxD ç‰¹æœ‰ã®å‡¦ç†: NeuronApplicationWhisper ã®åˆæœŸåŒ–**

```python
from neuronx_distributed_inference.models.whisper.modeling_whisper import NeuronApplicationWhisper

MODEL_PATH = "openai/whisper-large-v3"
COMPILED_PATH = "./whisper_large_v3_compiled"

# ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
neuron_model = NeuronApplicationWhisper(
    MODEL_PATH,
    config=inference_config
)
```

ã‚¯ãƒ©ã‚¹å´ã®å®Ÿè£…

https://github.com/aws-neuron/neuronx-distributed-inference/blob/aa7987ffc66ac2bd9894427621ca9b6f3fc40ed9/src/neuronx_distributed_inference/models/whisper/modeling_whisper.py#L678-L690

**åˆæœŸåŒ–å‡¦ç†**
1. `Whisper.__init__(config.dims)` ã‚’å‘¼ã³å‡ºã—ã€æ¨™æº–çš„ãª Whisper ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ§‹ç¯‰
2. æ¨™æº–ã® `Encoder`/`Decoder` ã‚’ Neuron Whisper ç”¨ã«ç½®ãæ›ãˆ

#### NxD ç‰¹æœ‰ã®å‡¦ç†: ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«

https://github.com/aws-neuron/neuronx-distributed-inference/blob/aa7987ffc66ac2bd9894427621ca9b6f3fc40ed9/src/neuronx_distributed_inference/models/whisper/modeling_whisper.py#L692-L694

AWS Neuron ã§ã¯ç¾æ™‚ç‚¹ã§ã¯äº‹å‰ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ãŒå¿…è¦ãªãŸã‚ `compile()` ãƒ¡ã‚½ãƒƒãƒ‰ãŒç”¨æ„ã•ã‚Œã¦ãŠã‚Šã€`compiled_model_path` ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«çµæœã‚’ä¿å­˜ã—ã€`load()` æ™‚ã«ã¯ã™ã§ã«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Œã°ãã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’åˆ©ç”¨ã™ã‚‹ã€‚è¨­å®šã«ã‚ˆã£ã¦ã¯å¤‰æ›´ã™ã‚‹ã¨å†åº¦ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ãŒå¿…è¦ãªã‚‚ã®ã‚‚ã‚ã‚Šã¾ã™ã€‚

### ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰

#### NxD ç‰¹æœ‰ã®å‡¦ç†: ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰

https://github.com/aws-neuron/neuronx-distributed-inference/blob/aa7987ffc66ac2bd9894427621ca9b6f3fc40ed9/src/neuronx_distributed_inference/models/whisper/modeling_whisper.py#L696-L698

### éŸ³å£°ã®å‰å‡¦ç† (Neuron ã«ä¾å­˜ã—ãªã„ä¸€èˆ¬çš„ãªå‡¦ç†)

```python
import librosa
import numpy as np
import torch

# éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ 16kHz ãƒ¢ãƒãƒ©ãƒ«ã§èª­ã¿è¾¼ã¿
audio_path = "audio-sample.mp3"
audio_data, sr = librosa.load(audio_path, sr=16000, mono=True)

# ãƒ¡ãƒ«ã‚¹ãƒšã‚¯ãƒˆãƒ­ã‚°ãƒ©ãƒ ã¸å¤‰æ›
from whisper.audio import log_mel_spectrogram, pad_or_trim

mel = log_mel_spectrogram(audio_data)     # (80, n_frames)
mel = pad_or_trim(mel, 3000)              # (80, 3000) ã«å›ºå®š
mel = torch.from_numpy(mel).unsqueeze(0)  # (1, 80, 3000)
```

**ã“ã‚Œã¯ NxD ã«ä¾å­˜ã—ãªã„** æ¨™æº–çš„ãªéŸ³å£°å‡¦ç†ã§ã€OpenAI ã® `whisper` ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã«å«ã¾ã‚Œã‚‹ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚ã¡ãªã¿ã«ãã‚“ãªã«éŸ³å£°æŠ€è¡“ã‚ã‹ã£ã¦ãªã„ã®ã§å­¦ã³ä¸­ã§ã™ã€‚

**ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿**:
- `sr=16000`: Whisper ã®å…¥åŠ›ã¯ 16kHz ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆãŒå¿…é ˆ
- `mono=True`: ã‚¹ãƒ†ãƒ¬ã‚ªã‚’ãƒ¢ãƒãƒ©ãƒ«ã«å¤‰æ›
- `pad_or_trim(mel, 3000)`: 30 ç§’ (16000 Hz Ã— 30 = 480000 samples â†’ 3000 frames) ã«å›ºå®š
  - çŸ­ã„éŸ³å£°ã¯ 0 ã§ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
  - é•·ã„éŸ³å£°ã¯åˆ‡ã‚Šæ¨ã¦ (ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ãŒå¿…è¦)

### æ¨è«–ã®å®Ÿè¡Œ

#### NxD ç‰¹æœ‰ã®å‡¦ç†: transcribe ãƒ¡ã‚½ãƒƒãƒ‰

https://github.com/aws-neuron/neuronx-distributed-inference/blob/aa7987ffc66ac2bd9894427621ca9b6f3fc40ed9/examples/generation_whisper.py#L44-L50

ä¸Šè¨˜ã§è§£èª¬ã—ãŸ `NeuronApplicationWhisper` ã‚’åˆ©ç”¨ã—ãŸã‚µãƒ³ãƒ—ãƒ«ãŒå­˜åœ¨ã—ã¦ãŠã‚Šã€ãã‚Œã‚’å…ƒã«å‹•ä½œç¢ºèªã‚’å®Ÿæ–½ã—ã¾ã—ãŸã€‚

```python
# éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç›´æ¥æ–‡å­—èµ·ã“ã—
result = neuron_model.transcribe(
    audio_path,           # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    language="ja",
    task="transcribe",
    verbose=True
)

print(result["text"])     # æ–‡å­—èµ·ã“ã—çµæœ
```



**ã“ã‚Œã¯ä¸€èˆ¬çš„ãª Encoder-Decoder Transformer ã®å‡¦ç†** ã§ã‚ã‚Šã€NxD ã«ç‰¹æœ‰ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚

---

## Tensor Parallelism ã®è©³ç´°è§£èª¬

### Tensor Parallelism (TP) ã¨ã¯

Tensor Parallelism ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿è¡Œåˆ—ã‚’è¤‡æ•°ã®ãƒ‡ãƒã‚¤ã‚¹ã«åˆ†å‰²ã—ã€ä¸¦åˆ—è¨ˆç®—ã‚’è¡Œã†æ‰‹æ³•ã§ã™ã€‚

**ä¾‹**: `Linear(1280, 1280)` ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’ TP=8 ã§åˆ†å‰²

```
å…ƒã®é‡ã¿è¡Œåˆ—:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   (1280, 1280) â”‚  â† å…¨é‡ã¿
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

TP=8 åˆ†å‰²å¾Œ:
â”Œâ”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”¬â”€â”€â”
â”‚ 1â”‚ 2â”‚ 3â”‚ 4â”‚ 5â”‚ 6â”‚ 7â”‚ 8â”‚  â† å„ãƒ‡ãƒã‚¤ã‚¹ãŒ (1280, 160) ã‚’ä¿æŒ
â””â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”´â”€â”€â”˜
```

### NxD ã§ã® TP å®Ÿè£…

#### NxD ç‰¹æœ‰ã®å‡¦ç†: ColumnParallelLinear

```python
from neuronx_distributed.parallel_layers import ColumnParallelLinear

# Query, Key, Value ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã«ä½¿ç”¨
self.query = ColumnParallelLinear(
    d_model,           # 1280
    d_model,           # 1280
    gather_output=False,  # å‡ºåŠ›ã‚’é›†ç´„ã—ãªã„
    dtype=torch_dtype,
)
```

**ColumnParallelLinear ã®å‹•ä½œ**:
1. å…¥åŠ› `(batch, seq_len, d_model)` ã‚’å…¨ãƒ‡ãƒã‚¤ã‚¹ã§å—ã‘å–ã‚‹
2. å„ãƒ‡ãƒã‚¤ã‚¹ãŒé‡ã¿ã® **åˆ—æ–¹å‘ã®ä¸€éƒ¨** ã§è¨ˆç®—
3. å‡ºåŠ› `(batch, seq_len, d_model // tp_degree)` ã‚’å„ãƒ‡ãƒã‚¤ã‚¹ãŒä¿æŒ
4. `gather_output=False` ã®ãŸã‚ã€å‡ºåŠ›ã¯åˆ†æ•£ã—ãŸã¾ã¾

**å›³è§£**:
```
Input: (1, 1500, 1280)
       â†“
   [ColumnParallel]
       â†“
Device 0: (1, 1500, 160)
Device 1: (1, 1500, 160)
...
Device 7: (1, 1500, 160)
```

#### NxD ç‰¹æœ‰ã®å‡¦ç†: RowParallelLinear

```python
from neuronx_distributed.parallel_layers import RowParallelLinear

# å‡ºåŠ›ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³ã«ä½¿ç”¨
self.out = RowParallelLinear(
    d_model,           # 1280
    d_model,           # 1280
    input_is_parallel=True,  # å…¥åŠ›ãŒåˆ†æ•£ã—ã¦ã„ã‚‹ã“ã¨ã‚’æ˜ç¤º
    dtype=torch_dtype,
)
```

**RowParallelLinear ã®å‹•ä½œ**:
1. å…¥åŠ› `(batch, seq_len, d_model // tp_degree)` ã‚’å„ãƒ‡ãƒã‚¤ã‚¹ã§å—ã‘å–ã‚‹
2. å„ãƒ‡ãƒã‚¤ã‚¹ãŒé‡ã¿ã® **è¡Œæ–¹å‘ã®ä¸€éƒ¨** ã§è¨ˆç®—
3. è¨ˆç®—çµæœã‚’ **AllReduce** ã§é›†ç´„
4. å‡ºåŠ› `(batch, seq_len, d_model)` ã‚’å…¨ãƒ‡ãƒã‚¤ã‚¹ã§å…±æœ‰

**å›³è§£**:
```
Device 0: (1, 1500, 160)
Device 1: (1, 1500, 160)
...
Device 7: (1, 1500, 160)
       â†“
   [RowParallel + AllReduce]
       â†“
Output: (1, 1500, 1280)
```

### ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒ˜ãƒƒãƒ‰ã®åˆ†å‰²

```python
# å…ƒã®ãƒ˜ãƒƒãƒ‰æ•°
n_head = 20  # Whisper Large V3

# TP=8 åˆ†å‰²å¾Œã€å„ãƒ‡ãƒã‚¤ã‚¹ã®ãƒ˜ãƒƒãƒ‰æ•°
n_head_per_device = ceil_div(n_head, tp_degree)  # 20 / 8 = 3 (åˆ‡ã‚Šä¸Šã’)
```

**é‡è¦**: ãƒ˜ãƒƒãƒ‰æ•°ãŒ tp_degree ã§å‰²ã‚Šåˆ‡ã‚Œãªã„å ´åˆã€ä¸€éƒ¨ã®ãƒ‡ãƒã‚¤ã‚¹ãŒä½™åˆ†ãªãƒ˜ãƒƒãƒ‰ã‚’æŒã¡ã¾ã™ã€‚ã“ã®ä¸å‡è¡¡ã¯é€šå¸¸ã€æ€§èƒ½ã¸ã®å½±éŸ¿ã¯å°ã•ã„ã§ã™ã€‚

---

## KV-Cache ã®è©³ç´°è§£èª¬

### KV-Cache ã¨ã¯

Decoder ã¯é€æ¬¡çš„ã«ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã€æ¯ã‚¹ãƒ†ãƒƒãƒ— **éå»ã®ã™ã¹ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ã«å¯¾ã™ã‚‹ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³** ã‚’è¨ˆç®—ã—ã¾ã™ã€‚KV-Cache ã‚’ä½¿ã‚ãªã„å ´åˆã€ã“ã®è¨ˆç®—é‡ã¯ O(nÂ²) ã«ãªã‚Šã¾ã™ã€‚

**KV-Cache ã®ä»•çµ„ã¿**:
- å„ã‚¹ãƒ†ãƒƒãƒ—ã§è¨ˆç®—ã—ãŸ Key ã¨ Value ã‚’ **ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜**
- æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰éå»ã® Key/Value ã‚’å–å¾—ã—ã€æ–°ã—ã„ãƒˆãƒ¼ã‚¯ãƒ³ã® Key/Value ã®ã¿ã‚’è¨ˆç®—
- ã“ã‚Œã«ã‚ˆã‚Šã€è¨ˆç®—é‡ã‚’ O(nÂ²) ã‹ã‚‰ O(n) ã«å‰Šæ¸›

### NxD ã§ã® KV-Cache å®Ÿè£…

#### NxD ç‰¹æœ‰ã®å‡¦ç†: Self-Attention Cache

```python
# NeuronApplicationWhisperDecoder ã®åˆæœŸåŒ–æ™‚
self.cache_k = nn.Parameter(
    torch.zeros(
        batch_size,          # 1
        n_kv_heads,          # 20 / tp_degree = 3
        max_seq_len,         # 448
        head_dim             # 1280 / 20 = 64
    ),
    requires_grad=False
)
self.cache_v = nn.Parameter(
    torch.zeros(...),  # cache_k ã¨åŒã˜å½¢çŠ¶
    requires_grad=False
)
```

**ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å½¢çŠ¶**:
- `batch_size`: ãƒãƒƒãƒã‚µã‚¤ã‚º
- `n_kv_heads`: TP åˆ†å‰²å¾Œã®å„ãƒ‡ãƒã‚¤ã‚¹ãŒæŒã¤ãƒ˜ãƒƒãƒ‰æ•°
- `max_seq_len`: Decoder ã®æœ€å¤§ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•· (448)
- `head_dim`: å„ãƒ˜ãƒƒãƒ‰ã®æ¬¡å…ƒ (d_model / n_head)

**ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®æ›´æ–°**:

```python
# Prefill ãƒ•ã‚§ãƒ¼ã‚º (å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä¸€åº¦ã«å‡¦ç†)
if prefill:
    cache_k[: , : , : seq_len, : ] = k  # å…¨ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä¿å­˜
    cache_v[: , : , : seq_len, : ] = v

# Decode ãƒ•ã‚§ãƒ¼ã‚º (1 ãƒˆãƒ¼ã‚¯ãƒ³ãšã¤å‡¦ç†)
else:
    cache_k[: , : , last_pos, : ] = k  # æœ€æ–°ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«è¿½åŠ 
    cache_v[: , : , last_pos, : ] = v
```

#### NxD ç‰¹æœ‰ã®å‡¦ç†: Cross-Attention Cache

Decoder ã¯ Encoder ã®å‡ºåŠ›ã«å¯¾ã—ã¦ã‚‚ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã‚’è¨ˆç®—ã—ã¾ã™ (Cross-Attention)ã€‚Encoder å‡ºåŠ›ã¯å›ºå®šãªã®ã§ã€ä¸€åº¦ã ã‘ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã™ã‚Œã°å†è¨ˆç®—ä¸è¦ã§ã™ã€‚

```python
# Encoder å‡ºåŠ›ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
self.cross_cache_k = encoder_output @ W_k  # (1, 1500, 1280) @ (1280, 1280)
self.cross_cache_v = encoder_output @ W_v

# Decoder ã®å„ã‚¹ãƒ†ãƒƒãƒ—ã§å†åˆ©ç”¨
cross_attention_output = attention(query, self.cross_cache_k, self.cross_cache_v)
```

**ã“ã‚Œã«ã‚ˆã‚Š**ã€Decoder ã¯æ¯ã‚¹ãƒ†ãƒƒãƒ—ã§ Cross-Attention ã® Key/Value ã‚’å†è¨ˆç®—ã™ã‚‹å¿…è¦ãŒãªããªã‚Šã¾ã™ã€‚

---

## ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©æœ€é©åŒ–ã®è©³ç´°

### NxD ç‰¹æœ‰ã®å‡¦ç†: ã‚³ãƒ³ãƒ‘ã‚¤ãƒ©å¼•æ•°

```python
# NeuronApplicationWhisper ã® compile ãƒ¡ã‚½ãƒƒãƒ‰å†…éƒ¨
compiler_args = [
    "--model-type=transformer",
    "--tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2'",
    "--auto-cast=none",  # float32 ãƒ¢ãƒ‡ãƒ«ã®å ´åˆ
]
```

**å„å¼•æ•°ã®æ„å‘³**:

1. `--model-type=transformer`
   - Transformer ç‰¹æœ‰ã®æœ€é©åŒ–ã‚’æœ‰åŠ¹åŒ–
   - ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³æ¼”ç®—ã®èåˆã€LayerNorm ã®æœ€é©åŒ–ãªã©

2. `--enable-ccop-compute-overlap`
   - è¨ˆç®—ã¨é€šä¿¡ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—
   - Tensor Parallelism ã§ã® AllReduce é€šä¿¡ã‚’è¨ˆç®—ã¨ä¸¦è¡Œå®Ÿè¡Œ

3. `--cc-pipeline-tiling-factor=2`
   - ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¸¦åˆ—åº¦ã®èª¿æ•´
   - ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¨è¨ˆç®—åŠ¹ç‡ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•

4. `--auto-cast=none` / `--auto-cast=all`
   - `none`: ãƒ‡ãƒ¼ã‚¿å‹ã‚’å¤‰æ›ã—ãªã„ (float32 ã®ã¾ã¾)
   - `all`: å¯èƒ½ãªé™ã‚Šä½ç²¾åº¦ (fp16/bf16) ã«è‡ªå‹•å¤‰æ›

**æ¨å¥¨è¨­å®š**:
- float32 ãƒ¢ãƒ‡ãƒ«: `--auto-cast=all --auto-cast-type=bf16` ã§ 2 å€é«˜é€ŸåŒ–
- float16 ãƒ¢ãƒ‡ãƒ«: `--auto-cast=none` (ã™ã§ã«æœ€é©åŒ–æ¸ˆã¿)

---

## ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰ã®å®Œå…¨ãªå®Ÿè£…ä¾‹

ä»¥ä¸‹ã¯ã€Kotoba Whisper v2.2 ã‚’ NxD Inference ã§ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã€æ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹å®Œå…¨ãªã‚µãƒ³ãƒ—ãƒ«ã§ã™ã€‚

```python
#!/usr/bin/env python3
"""
NxD Inference Whisper å®Ÿè£…ä¾‹
Kotoba Whisper v2.2 (æ—¥æœ¬èªç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«) ã‚’ AWS Neuron ã§å®Ÿè¡Œ
"""

import os
import torch
from pathlib import Path

# NxD Inference ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ (NxD ç‰¹æœ‰)
from neuronx_distributed_inference.models.config import NeuronConfig
from neuronx_distributed_inference.models.whisper.modeling_whisper import (
    WhisperInferenceConfig,
    NeuronApplicationWhisper,
)
from neuronx_distributed_inference.utils.hf_adapter import load_pretrained_config

# è¨­å®š
MODEL_PATH = "kotoba-tech/kotoba-whisper-v2.2"  # Hugging Face ãƒ¢ãƒ‡ãƒ«å
COMPILED_PATH = "./kotoba_whisper_v2.2_compiled_tp8"
BATCH_SIZE = 1
TP_DEGREE = 8  # inf2.xlarge ã¯ 2, trn1.32xlarge ã¯ 8 æ¨å¥¨
DTYPE = torch.float16

print("="*80)
print("NxD Inference Whisper - Kotoba v2.2 å®Ÿè£…ä¾‹")
print("="*80)

# ã‚¹ãƒ†ãƒƒãƒ— 1: NeuronConfig ã®ä½œæˆ (NxD ç‰¹æœ‰)
print("\n[1/5] Creating NeuronConfig...")
neuron_config = NeuronConfig(
    batch_size=BATCH_SIZE,
    torch_dtype=DTYPE,
    tp_degree=TP_DEGREE,
)
print(f"  âœ… TP degree: {TP_DEGREE}, dtype: {DTYPE}")

# ã‚¹ãƒ†ãƒƒãƒ— 2: WhisperInferenceConfig ã®ä½œæˆ (NxD ç‰¹æœ‰)
print("\n[2/5] Creating WhisperInferenceConfig...")
inference_config = WhisperInferenceConfig(
    neuron_config,
    load_config=load_pretrained_config(MODEL_PATH),
)
print(f"  âœ… Loaded model config from {MODEL_PATH}")

# ã‚¹ãƒ†ãƒƒãƒ— 3: ãƒ¢ãƒ‡ãƒ«ã®ã‚³ãƒ³ãƒ‘ã‚¤ãƒ« (NxD ç‰¹æœ‰)
print("\n[3/5] Compiling model...")
if not os.path.exists(COMPILED_PATH):
    print("  ğŸ”§ Compilation starting (this takes 1-2 hours)...")
    print(f"     Model: {MODEL_PATH}")
    print(f"     Output: {COMPILED_PATH}")

    neuron_model = NeuronApplicationWhisper(
        MODEL_PATH,
        config=inference_config
    )
    neuron_model.compile(COMPILED_PATH)
    print(f"  âœ… Compilation complete!")
else:
    print(f"  âœ… Using cached compiled model: {COMPILED_PATH}")

# ã‚¹ãƒ†ãƒƒãƒ— 4: ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ (NxD ç‰¹æœ‰)
print("\n[4/5] Loading compiled model...")
neuron_model = NeuronApplicationWhisper(
    COMPILED_PATH,
    config=inference_config
)
neuron_model.load(COMPILED_PATH)
print(f"  âœ… Model loaded to Neuron")

# ã‚¹ãƒ†ãƒƒãƒ— 5: æ¨è«–ã®å®Ÿè¡Œ (NxD ç‰¹æœ‰ã® transcribe ãƒ¡ã‚½ãƒƒãƒ‰)
print("\n[5/5] Running inference...")
audio_file = "audio-sample.mp3"  # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹

result = neuron_model.transcribe(
    audio_file,
    language="ja",       # æ—¥æœ¬èª
    task="transcribe",   # æ–‡å­—èµ·ã“ã—
    verbose=True         # é€²æ—è¡¨ç¤º
)

print(f"\n{'='*80}")
print("Transcription Result: ")
print(f"{'='*80}")
print(f"Text: {result['text']}")
print(f"Language: {result['language']}")
print(f"{'='*80}")
```

### å®Ÿè¡Œæ–¹æ³•

```bash
# 1. ä»®æƒ³ç’°å¢ƒã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ãƒˆ
source /opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/bin/activate

# 2. ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè¡Œ
python3 nxd_whisper_example.py
```

### æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›

```
================================================================================
NxD Inference Whisper - Kotoba v2.2 å®Ÿè£…ä¾‹
================================================================================

[1/5] Creating NeuronConfig...
  âœ… TP degree: 8, dtype: torch.float16

[2/5] Creating WhisperInferenceConfig...
  âœ… Loaded model config from kotoba-tech/kotoba-whisper-v2.2

[3/5] Compiling model...
  ğŸ”§ Compilation starting (this takes 1-2 hours)...
     Model: kotoba-tech/kotoba-whisper-v2.2
     Output: ./kotoba_whisper_v2.2_compiled_tp8
  ... (ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ãƒ­ã‚°) ...
  âœ… Compilation complete!

[4/5] Loading compiled model...
  âœ… Model loaded to Neuron

[5/5] Running inference...
Detecting language using up to the first 30 seconds. Use `--language` to specify the language
Detected language: Japanese
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [00:05<00:00, 28.34it/s]

================================================================================
Transcription Result:
================================================================================
Text: ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆéŸ³å£°ã§ã™ã€‚NxD Inference ã‚’ä½¿ç”¨ã—ã¦ Whisper ãƒ¢ãƒ‡ãƒ«ã‚’ AWS Neuron ã§å®Ÿè¡Œã—ã¦ã„ã¾ã™ã€‚
Language: ja
================================================================================
```

## å†ç¾æ‰‹é †

ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€inf2.xlarge ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä¸Šã§ Kotoba Whisper v2.2 (æ—¥æœ¬èªç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«) ã‚’ NxD Inference ã§å‹•ä½œã•ã›ã‚‹æ‰‹é †ã‚’ç¤ºã—ã¾ã™ã€‚å…¨ã¦ã®æ‰‹é †ã¯ heredoc å½¢å¼ã§è¨˜è¿°ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ã‚³ãƒ”ãƒ¼&ãƒšãƒ¼ã‚¹ãƒˆã§å®Ÿè¡Œã§ãã¾ã™ã€‚

### æ¨å¥¨ç’°å¢ƒ

- **ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—**: inf2.xlargeã€trn1.2xlargeã€trn2.3xlarge
- **Neuron SDK**: 2.28+
- **neuronxcc**: 2.22+
- **NxD Inference**: 0.7.0+ (é–‹ç™ºç‰ˆ)
- **ãƒ¢ãƒ‡ãƒ«**: kotoba-tech/kotoba-whisper-v2.2 (1550M ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€æ—¥æœ¬èªç‰¹åŒ–)

### Step 0: äº‹å‰æº–å‚™

```bash
source /opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/bin/activate

/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/bin/pip install gTTS scipy soundfile

mkdir -p ~/whisper-kotoba-test && cd ~/whisper-kotoba-test
```

---

### Step 1: ç’°å¢ƒç¢ºèª

Neuron SDK ã¨ NxD Inference ãŒæ­£ã—ãã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¾ã™ã€‚

```bash
python3 << 'EOF'
import sys
print("=" * 80)
print("ç’°å¢ƒç¢ºèª")
print("=" * 80)

# PyTorch & Neuron
import torch
import torch_neuronx
print(f"âœ“ PyTorch: {torch.__version__}")
print(f"âœ“ torch_neuronx: {torch_neuronx.__version__}")

# NxD Inference
sys.path.insert(0, '/tmp/neuronx-distributed-inference/src')
import neuronx_distributed_inference as nxd
print(f"âœ“ NxD Inference: {nxd.__version__}")

# Whisper ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from neuronx_distributed_inference.models.whisper.modeling_whisper import (
    WhisperInferenceConfig,
    NeuronApplicationWhisper,
)
print(f"âœ“ Whisper module: Available")

print("=" * 80)
print("ç’°å¢ƒç¢ºèªå®Œäº†")
print("=" * 80)
EOF
```

**å‡ºåŠ›çµæœ: details ã‚’å±•é–‹ã—ã¦ãã ã•ã„**

::::details å‡ºåŠ›çµæœ
```
================================================================================
ç’°å¢ƒç¢ºèª
================================================================================
âœ“ PyTorch: 2.9.0+cu128
âœ“ torch_neuronx: 2.9.0.2.11.19912+e48cd891
âœ“ NxD Inference: 0.7.0
/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:16: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
  from .mappings import (
/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:16: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
  from .mappings import (
/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:16: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
  from .mappings import (
/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/lib/python3.12/site-packages/neuronx_distributed/modules/moe/blockwise.py:74: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
  component, error = import_nki(config)
/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/lib/python3.12/site-packages/neuronx_distributed/modules/moe/blockwise.py:74: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
  component, error = import_nki(config)
/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/lib/python3.12/site-packages/neuronx_distributed/modules/moe/blockwise.py:74: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
  component, error = import_nki(config)
/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/lib/python3.12/site-packages/neuronx_distributed/modules/moe/blockwise.py:74: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
  component, error = import_nki(config)
/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/lib/python3.12/site-packages/neuronxcc/nki/_pre_prod_kernels/bwmm_mxfp4.py:564: SyntaxWarning: assertion is always true, perhaps remove parentheses?
  assert(token_indices_2D.shape==(128, 1), f"Expect token_indices_2D to have shape (128, 1), got {token_indices_2D.shape}")
/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/lib/python3.12/site-packages/neuronx_distributed/modules/moe/blockwise.py:74: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
  component, error = import_nki(config)
/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/lib/python3.12/site-packages/neuronx_distributed/modules/moe/blockwise.py:74: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
  component, error = import_nki(config)
/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/lib/python3.12/site-packages/neuronx_distributed/modules/moe/blockwise.py:76: UserWarning: Warning: Failed to import blockwise_mm_baseline_shard_n_k1_while_2loops: No module named 'neuronxcc.nki._private.blockwise_matmul_while'
  warnings.warn(f"Warning: {error}")
/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/lib/python3.12/site-packages/neuronx_distributed/modules/moe/moe_fused_tkg.py:49: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
  component, error = import_nki(config)
/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/lib/python3.12/site-packages/neuronx_distributed/modules/moe/moe_fused_tkg.py:49: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
  component, error = import_nki(config)
/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/lib/python3.12/site-packages/neuronx_distributed/modules/moe/moe_fused_tkg.py:49: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
  component, error = import_nki(config)
/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/lib/python3.12/site-packages/neuronx_distributed_inference/modules/attention/utils.py:13: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
  from neuronx_distributed_inference.modules.custom_calls import neuron_cumsum
/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/lib/python3.12/site-packages/neuronx_distributed_inference/modules/lora_serving/lora_checkpoint.py:9: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
  from neuronx_distributed_inference.modules.attention.gqa import replicate_kv
/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/lib/python3.12/site-packages/neuronx_distributed_inference/modules/lora_serving/lora_checkpoint.py:9: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
  from neuronx_distributed_inference.modules.attention.gqa import replicate_kv
/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/lib/python3.12/site-packages/neuronx_distributed_inference/modules/lora_serving/lora_checkpoint.py:9: DeprecationWarning: torch_neuronx.nki_jit is deprecated, use nki.jit instead.
  from neuronx_distributed_inference.modules.attention.gqa import replicate_kv
âœ“ Whisper module: Available
================================================================================
ç’°å¢ƒç¢ºèªå®Œäº†
================================================================================
```
::::

### Step 2: ãƒ¢ãƒ‡ãƒ«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

Kotoba Whisper v2.2 ãƒ¢ãƒ‡ãƒ«ï¼ˆ1550M ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€æ—¥æœ¬èªç‰¹åŒ–ï¼‰ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚

```bash
mkdir -p models
python3 << 'EOF'
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor

print("=" * 80)
print("kotoba-tech/kotoba-whisper-v2.2 ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
print("=" * 80)

model_id = "kotoba-tech/kotoba-whisper-v2.2"
save_dir = "models/kotoba-whisper-v2.2"

print(f"ãƒ¢ãƒ‡ãƒ«: {model_id}")
print(f"ä¿å­˜å…ˆ: {save_dir}\n")

print("ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id, low_cpu_mem_usage=True)
model.save_pretrained(save_dir)
print("âœ“ ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†")

print("ãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...")
processor = AutoProcessor.from_pretrained(model_id)
processor.save_pretrained(save_dir)
print("âœ“ ãƒ—ãƒ­ã‚»ãƒƒã‚µä¿å­˜å®Œäº†")

print(f"\nãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†")
print(f"   ãƒ¢ãƒ‡ãƒ«: {model_id}")
print(f"   æ©Ÿèƒ½: æ—¥æœ¬èªç‰¹åŒ–éŸ³å£°èªè­˜")
print("=" * 80)
EOF
```

**å‡ºåŠ›çµæœ: details ã‚’å±•é–‹ã—ã¦ãã ã•ã„**

::::details å‡ºåŠ›çµæœ
```
================================================================================
kotoba-tech/kotoba-whisper-v2.2 ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
================================================================================
ãƒ¢ãƒ‡ãƒ«: kotoba-tech/kotoba-whisper-v2.2
ä¿å­˜å…ˆ: models/kotoba-whisper-v2.2

ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...
âœ“ ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†
ãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­...
âœ“ ãƒ—ãƒ­ã‚»ãƒƒã‚µä¿å­˜å®Œäº†

ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†
   ãƒ¢ãƒ‡ãƒ«: kotoba-tech/kotoba-whisper-v2.2
   æ©Ÿèƒ½: æ—¥æœ¬èªç‰¹åŒ–éŸ³å£°èªè­˜
================================================================================
```
::::

### Step 3: ã‚µãƒ³ãƒ—ãƒ«å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆä½œæˆ

```bash
cat > whisper_nxd.py << 'PYTHON_EOF'
"""Whisper with NxD Inference"""
import torch
import soundfile as sf
import numpy as np
import scipy.signal
from pathlib import Path
from transformers import AutoProcessor

from neuronx_distributed_inference.models.config import NeuronConfig
from neuronx_distributed_inference.models.whisper.modeling_whisper import (
    WhisperInferenceConfig,
    NeuronApplicationWhisper,
)
from neuronx_distributed_inference.utils.hf_adapter import load_pretrained_config

class WhisperNxD:
    def __init__(self, model_path, compiled_path, tp_degree=2, language="ja"):
        self.model_path = Path(model_path)
        self.compiled_path = Path(compiled_path)
        self.tp_degree = tp_degree
        self.language = language
        self.model = None
        self.processor = None

    def compile(self):
        if self.compiled_path.exists():
            print("Model already compiled")
            return

        print(f"Compiling Kotoba Whisper v2.2 (TP={self.tp_degree})...")

        config = WhisperInferenceConfig(
            NeuronConfig(batch_size=1, torch_dtype=torch.float16, tp_degree=self.tp_degree),
            load_config=load_pretrained_config(str(self.model_path)),
        )

        self.model = NeuronApplicationWhisper(str(self.model_path), config=config)
        self.compiled_path.mkdir(parents=True, exist_ok=True)
        self.model.compile(str(self.compiled_path))
        print(f"Compilation complete")

    def load(self):
        print("Loading model...")
        self.processor = AutoProcessor.from_pretrained(str(self.model_path))

        config = WhisperInferenceConfig(
            NeuronConfig(batch_size=1, torch_dtype=torch.float16, tp_degree=self.tp_degree),
            load_config=load_pretrained_config(str(self.model_path)),
        )

        self.model = NeuronApplicationWhisper(str(self.compiled_path), config=config)
        self.model.load(str(self.compiled_path))
        print("Model loaded")

    def transcribe(self, audio_path):
        # Load audio file with soundfile (avoids FP16 warning)
        audio_data, sr = sf.read(audio_path)

        # Resample to 16kHz if needed (using scipy.signal)
        if sr != 16000:
            audio_data = scipy.signal.resample_poly(
                audio_data, 16000, sr
            ).astype(np.float32)

        # Convert to mono if stereo
        if len(audio_data.shape) > 1:
            audio_data = audio_data.mean(axis=1)

        # Ensure float32 dtype (required by NxD transcribe)
        audio_data = audio_data.astype(np.float32)
        audio_duration = len(audio_data) / 16000

        # Pass numpy array to transcribe (avoids openai-whisper audio processing)
        result = self.model.transcribe(
            audio_data,
            language=self.language,
            verbose=False
        )

        return {'text': result['text'], 'duration': audio_duration}
PYTHON_EOF

echo "whisper_nxd.py ä½œæˆå®Œäº†"
```

**å‡ºåŠ›çµæœ: echo ãŒå‡ºã‚‹ã ã‘**

### Step 4: ãƒ¢ãƒ‡ãƒ«ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«

```bash
python3 << 'EOF'
import sys
import time
from pathlib import Path
sys.path.insert(0, str(Path.cwd()))

from whisper_nxd import WhisperNxD

print("="*80)
print("Step 4: Kotoba Whisper v2.2 ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«")
print("="*80)

model = WhisperNxD(
    model_path="models/kotoba-whisper-v2.2",
    compiled_path="models/kotoba-whisper-v2.2-compiled-tp2",
    tp_degree=2,
    language="ja"  # æ—¥æœ¬èª
)

start = time.time()
model.compile()
print(f"ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚é–“: {time.time()-start: .1f}ç§’")
EOF
```

**å‡ºåŠ›çµæœï¼ˆåˆå›ï¼‰: details ã‚’å±•é–‹ã—ã¦ãã ã•ã„**

::::details å‡ºåŠ›çµæœ
```
================================================================================
Step 4: Kotoba Whisper v2.2 ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
================================================================================
Compiling Kotoba Whisper v2.2 (TP=2)...
INFO:Neuron:Saving the neuron_config to models/kotoba-whisper-v2.2-compiled-tp2/encoder/
INFO:Neuron:Generating HLOs for the following models: ['Encoder']
[2026-02-10 14:54:15.635: I neuronx_distributed/parallel_layers/parallel_state.py:630] > initializing tensor model parallel with size 2
[2026-02-10 14:54:15.635: I neuronx_distributed/parallel_layers/parallel_state.py:631] > initializing pipeline model parallel with size 1
[2026-02-10 14:54:15.635: I neuronx_distributed/parallel_layers/parallel_state.py:632] > initializing context model parallel with size 1
[2026-02-10 14:54:15.635: I neuronx_distributed/parallel_layers/parallel_state.py:633] > initializing data parallel with size 1
[2026-02-10 14:54:15.635: I neuronx_distributed/parallel_layers/parallel_state.py:634] > initializing world size to 2
[2026-02-10 14:54:15.635: I neuronx_distributed/parallel_layers/parallel_state.py:379] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x7f64cde03240>, 'Ascending Ring PG Group')>
[2026-02-10 14:54:15.636: I neuronx_distributed/parallel_layers/parallel_state.py:658] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0, 1]]
[2026-02-10 14:54:15.636: I neuronx_distributed/parallel_layers/parallel_state.py:659] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0], [1]]
[2026-02-10 14:54:15.636: I neuronx_distributed/parallel_layers/parallel_state.py:660] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0], [1]]
[2026-02-10 14:54:15.636: I neuronx_distributed/parallel_layers/parallel_state.py:661] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0], [1]]
[2026-02-10 14:54:15.636: I neuronx_distributed/parallel_layers/parallel_state.py:662] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0], [1]]
[2026-02-10 14:54:15.636: I neuronx_distributed/parallel_layers/parallel_state.py:663] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0], [1]]
INFO:Neuron:Generating 1 hlos for key: Encoder
INFO:Neuron:Minimal metadata will be added to HLO
INFO:Neuron:Started loading module Encoder
INFO:Neuron:Finished loading module Encoder in 0.08839869499206543 seconds
INFO:Neuron:generating HLO: Encoder, input example shape = torch.Size([1, 128, 3000])
/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=False):
INFO:Neuron:Finished generating HLO for Encoder in 0.9561893939971924 seconds, input example shape = torch.Size([1, 128, 3000])
INFO:Neuron:Generated all HLOs in 1.0960869789123535 seconds
INFO:Neuron:Can't find a priority model, skip marking weights
INFO:Neuron:Can't find a priority model, skip optimizing weight layout for other HLOs
INFO:Neuron:Starting compilation for all HLOs
INFO:Neuron:Neuron compiler flags: --model-type=transformer --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2' --internal-hlo2tensorizer-options='--verify-hlo=true'  --auto-cast=none  -O1  --verbose=35 --logfile=/tmp/nxd_model/Encoder/_tp0_bk0/log-neuron-cc.txt
/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/lib/python3.12/site-packages/libneuronxla/neuron_cc_wrapper.py:246: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.
  warnings.warn(SyntaxWarning(
.Roundtrip constructed a transpose sequence [rounds: 1; efficiency: 77]:
  dve_j_optimized   : Fix prefix (0, 1) and permute (2,) with (3, 4) / latency=13,460; shape=(10, 128, 128, 1, 3); dtype_size=2

Roundtrip constructed a transpose sequence [rounds: 1; efficiency: 60]:
  dve_j_optimized   : Fix prefix (0, 1, 2) and permute (3,) with (4, 5) / latency=134,603; shape=(10, 128, 10, 128, 1, 3); dtype_size=2

Completed run_backend_driver.

Compiler status PASS
2026-02-10 14:54:27.000992:  149811  [INFO]: Compilation Successfully Completed for model.MODULE_0ba55c089b6a22a674e2+c9c9fa2d.hlo_module.pb
INFO:Neuron:Finished Compilation for all HLOs in 11.266722202301025 seconds
INFO:Neuron:Can't find a priority model, falling back to the existing weight layout
INFO:Neuron:Finished building model in 12.598812341690063 seconds
INFO:Neuron:SKIPPING pre-sharding the checkpoints. The checkpoints will be sharded during load time.
INFO:Neuron:Saving the neuron_config to models/kotoba-whisper-v2.2-compiled-tp2/decoder/
INFO:Neuron:Generating HLOs for the following models: ['DecoderPrefill', 'DecoderDecode']
[2026-02-10 14:54:28.324: I neuronx_distributed/parallel_layers/parallel_state.py:630] > initializing tensor model parallel with size 2
[2026-02-10 14:54:28.324: I neuronx_distributed/parallel_layers/parallel_state.py:631] > initializing pipeline model parallel with size 1
[2026-02-10 14:54:28.324: I neuronx_distributed/parallel_layers/parallel_state.py:632] > initializing context model parallel with size 1
[2026-02-10 14:54:28.324: I neuronx_distributed/parallel_layers/parallel_state.py:633] > initializing data parallel with size 1
[2026-02-10 14:54:28.324: I neuronx_distributed/parallel_layers/parallel_state.py:634] > initializing world size to 2
[2026-02-10 14:54:28.324: I neuronx_distributed/parallel_layers/parallel_state.py:379] [rank_0_pp-1_tp-1_dp-1_cp-1] Chosen Logic for replica groups ret_logic=<PG_Group_Logic.LOGIC1: (<function ascending_ring_PG_group at 0x7f64cde03240>, 'Ascending Ring PG Group')>
[2026-02-10 14:54:28.324: I neuronx_distributed/parallel_layers/parallel_state.py:658] [rank_0_pp-1_tp-1_dp-1_cp-1] tp_groups: replica_groups.tp_groups=[[0, 1]]
[2026-02-10 14:54:28.325: I neuronx_distributed/parallel_layers/parallel_state.py:659] [rank_0_pp-1_tp-1_dp-1_cp-1] dp_groups: replica_groups.dp_groups=[[0], [1]]
[2026-02-10 14:54:28.325: I neuronx_distributed/parallel_layers/parallel_state.py:660] [rank_0_pp-1_tp-1_dp-1_cp-1] pp_groups: replica_groups.pp_groups=[[0], [1]]
[2026-02-10 14:54:28.325: I neuronx_distributed/parallel_layers/parallel_state.py:661] [rank_0_pp-1_tp-1_dp-1_cp-1] cp_groups: replica_groups.cp_groups=[[0], [1]]
[2026-02-10 14:54:28.325: I neuronx_distributed/parallel_layers/parallel_state.py:662] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_model_groups: replica_groups.ep_model_groups=[[0], [1]]
[2026-02-10 14:54:28.325: I neuronx_distributed/parallel_layers/parallel_state.py:663] [rank_0_pp-1_tp-1_dp-1_cp-1] ep_data_groups: replica_groups.ep_data_groups=[[0], [1]]
INFO:Neuron:Generating 1 hlos for key: DecoderPrefill
INFO:Neuron:Minimal metadata will be added to HLO
INFO:Neuron:Started loading module DecoderPrefill
INFO:Neuron:Finished loading module DecoderPrefill in 0.3752787113189697 seconds
INFO:Neuron:generating HLO: DecoderPrefill, input example shape = torch.Size([1, 448])
/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/lib/python3.12/site-packages/neuronx_distributed/parallel_layers/layers.py:532: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=False):
/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=2, shape=torch.Size([1]), dtype=torch.int32). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
  warnings.warn(
INFO:Neuron:Finished generating HLO for DecoderPrefill in 0.12747669219970703 seconds, input example shape = torch.Size([1, 448])
INFO:Neuron:Generating 1 hlos for key: DecoderDecode
INFO:Neuron:Minimal metadata will be added to HLO
INFO:Neuron:Started loading module DecoderDecode
INFO:Neuron:Finished loading module DecoderDecode in 0.3645610809326172 seconds
INFO:Neuron:generating HLO: DecoderDecode, input example shape = torch.Size([1, 1])
/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/lib/python3.12/site-packages/torch_neuronx/xla_impl/hlo_conversion.py:470: UserWarning: Received an input tensor that was unused or used in a non-static way when traced so the tensor will be ignored. (index=1, shape=torch.Size([1, 1500, 1280]), dtype=torch.float16). The non-static usage could happen when the traced function expects the input tensor's shape to change (i.e., using the shape to do index slicing), which is not allowed by inference trace expecting static input shapes.
  warnings.warn(
INFO:Neuron:Finished generating HLO for DecoderDecode in 0.10927104949951172 seconds, input example shape = torch.Size([1, 1])
INFO:Neuron:Generated all HLOs in 1.0258264541625977 seconds
INFO:Neuron:Can't find a priority model, skip marking weights
INFO:Neuron:Can't find a priority model, skip optimizing weight layout for other HLOs
INFO:Neuron:Starting compilation for all HLOs
INFO:Neuron:Neuron compiler flags: --model-type=transformer --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2' --internal-hlo2tensorizer-options='--verify-hlo=true'  --auto-cast=none  -O1  --verbose=35 --logfile=/tmp/nxd_model/DecoderPrefill/_tp0_bk0/log-neuron-cc.txt
/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/lib/python3.12/site-packages/libneuronxla/neuron_cc_wrapper.py:246: SyntaxWarning: str format compiler_flags is discouraged as its handling involves repeated joining and splitting, which can easily make mistakes if something is quoted or escaped. Use list[str] instead. Refer to documentation of the Python subprocess module for details.
  warnings.warn(SyntaxWarning(
INFO:Neuron:Neuron compiler flags: --model-type=transformer --tensorizer-options='--enable-ccop-compute-overlap --cc-pipeline-tiling-factor=2' --internal-hlo2tensorizer-options='--verify-hlo=true'  --auto-cast=none  -O1  --verbose=35 --logfile=/tmp/nxd_model/DecoderDecode/_tp0_bk0/log-neuron-cc.txt
..Completed run_backend_driver.

Compiler status PASS
2026-02-10 14:54:41.000741:  149811  [INFO]: Compilation Successfully Completed for model.MODULE_769453b82c87e72264dc+7ef8efc5.hlo_module.pb
Completed run_backend_driver.

Compiler status PASS
2026-02-10 14:54:42.000215:  149811  [INFO]: Compilation Successfully Completed for model.MODULE_22b66b0fb7daa9d253e2+f69ccc06.hlo_module.pb
INFO:Neuron:Finished Compilation for all HLOs in 12.869001388549805 seconds
INFO:Neuron:Can't find a priority model, falling back to the existing weight layout
INFO:Neuron:Finished building model in 15.251092433929443 seconds
INFO:Neuron:SKIPPING pre-sharding the checkpoints. The checkpoints will be sharded during load time.
Compilation complete
ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«æ™‚é–“:  32.1ç§’
```
::::

### Step 5: ãƒ†ã‚¹ãƒˆéŸ³å£°ç”Ÿæˆ

æ—¥æœ¬èªã®ãƒ†ã‚¹ãƒˆéŸ³å£°ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

```bash
mkdir -p test_audio
python3 << 'EOF'
from gtts import gTTS

japanese_text = "ã“ã‚“ã«ã¡ã¯ã€‚ã“ã‚Œã¯éŸ³å£°èªè­˜ã®ãƒ†ã‚¹ãƒˆã§ã™ã€‚ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã™ã­ã€‚"
tts = gTTS(text=japanese_text, lang='ja')
tts.save('test_audio/japanese.mp3')

print(f"æ—¥æœ¬èªéŸ³å£°ç”Ÿæˆ: {japanese_text}")
EOF
```

**å‡ºåŠ›çµæœ: **

```bash
æ—¥æœ¬èªéŸ³å£°ç”Ÿæˆ: ã“ã‚“ã«ã¡ã¯ã€‚ã“ã‚Œã¯éŸ³å£°èªè­˜ã®ãƒ†ã‚¹ãƒˆã§ã™ã€‚ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã™ã­ã€‚
```

### Step 6: æ¨è«–å®Ÿè¡Œ

```bash
python3 << 'EOF'
import sys
import time
from pathlib import Path
sys.path.insert(0, str(Path.cwd()))

from whisper_nxd import WhisperNxD

print("="*80)
print("Step 6: æ—¥æœ¬èªéŸ³å£°èªè­˜")
print("="*80)

model = WhisperNxD(
    model_path="models/kotoba-whisper-v2.2",
    compiled_path="models/kotoba-whisper-v2.2-compiled-tp2",
    tp_degree=2,
    language="ja"
)

model.load()

print("éŸ³å£°èªè­˜å®Ÿè¡Œä¸­ï¼ˆæ—¥æœ¬èªï¼‰...")
start = time.time()
result = model.transcribe("test_audio/japanese.mp3")
elapsed = time.time() - start

print(f"\næ¨è«–å®Œäº†")
print(f"   å‡¦ç†æ™‚é–“: {elapsed: .3f}ç§’")
print(f"   éŸ³å£°é•·: {result['duration']: .2f}ç§’")
print(f"   RTF: {elapsed/result['duration']: .3f}x")
print(f"\nèªè­˜çµæœ: {result['text']}")
print("="*80)
EOF
```

**å‡ºåŠ›çµæœ**

```
================================================================================
Step 6: æ—¥æœ¬èªéŸ³å£°èªè­˜
================================================================================
...
Model loaded
éŸ³å£°èªè­˜å®Ÿè¡Œä¸­ï¼ˆæ—¥æœ¬èªï¼‰...
/opt/aws_neuronx_venv_pytorch_2_9_nxd_inference/lib/python3.12/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead
  warnings.warn("FP16 is not supported on CPU; using FP32 instead")
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 662/662 [00:00<00:00, 1134.37frames/s]

æ¨è«–å®Œäº†
   å‡¦ç†æ™‚é–“:  0.669ç§’
   éŸ³å£°é•·:  6.62ç§’
   RTF:  0.101x

èªè­˜çµæœ: ã“ã‚“ã«ã¡ã¯ã“ã‚Œã¯éŸ³å£°èªè­˜ã®ãƒ†ã‚¹ãƒˆã§ã™ä»Šæ—¥ã¯è‰¯ã„å¤©æ°—ã§ã™ã­
================================================================================
```

ã„ã„æ„Ÿã˜ã§ãªã‚“ã¨ 0.6 ç§’ã§éŸ³å£°ç”ŸæˆãŒå®Œäº†ã—ã¦ã„ã¾ã™ã­ï¼çˆ†é€Ÿã§ã™ã€‚

## ã¾ã¨ã‚

OpenAI ã® Whisper ã‚’ AWS ã®ã‚«ã‚¹ã‚¿ãƒ ãƒãƒƒãƒ—æ­è¼‰ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä¸Šã§ NxD Inference ã‚’ç”¨ã„ã¦æ¨è«–å‡¦ç†ã‚’ã‚„ã£ã¦ã¿ã¾ã—ãŸã€‚ä¾¡æ ¼ã‚‚ãŠå®‰ã‚ãªã®ã§æŠ€è¡“å¥½ããªæ–¹ã¯ã‚´ãƒªã‚´ãƒªã«ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ GPU ã‚ˆã‚Šã‚³ã‚¹ãƒˆã‚‚æ€§èƒ½ã‚‚è‰¯ãæ¨è«–ã‚µãƒ¼ãƒ“ãƒ³ã‚°ã™ã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ã¯ã©ã†ã§ã—ã‚‡ã†ã‹ã€‚ã«ã‚ƒãƒ¼ã‚“ã€‚

## ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã«ã¤ã„ã¦

#### Kotoba Whisper v2.2


## å‚è€ƒè³‡æ–™

### å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
- [NxD Inference GitHub Repository](https://github.com/aws-neuron/neuronx-distributed-inference)
- [AWS Neuron Documentation](https://awsdocs-neuron.readthedocs-hosted.com/)
- [Kotoba Whisper v2.2](https://huggingface.co/kotoba-tech/kotoba-whisper-v2.2)

### é–¢é€£è¨˜äº‹
- [AWS Trainium Technical Deep Dive](../../aws-trainium-technical-deep-dive.md)
- [Phase 2 Requirements Investigation](../REQUIREMENTS_INVESTIGATION.md)
- [Phase 2 Migration Guide](../MIGRATION_GUIDE.md)

### ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£
- [AWS Neuron GitHub Issues](https://github.com/aws-neuron/aws-neuron-sdk/issues)
- [Hugging Face Transformers Forum](https://discuss.huggingface.co/)