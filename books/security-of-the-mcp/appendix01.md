---
title: "§A01 AI リスクマネジメント"
free: false
---

___Appendix:___ _AI リスクマネジメントの翻訳_

---

本 Chapter は [NIST AI RMF](https://airc.nist.gov/airmf-resources/airmf/0-ai-rmf-1-0/) の翻訳です。

# 0. NIST AI リスク管理フレームワーク（AI RMF）エグゼクティブサマリー 日本語翻訳

## 翻訳概要

本文書は、米国国立標準技術研究所（NIST）が発行した AI リスク管理フレームワーク（AI RMF）のエグゼクティブサマリーの日本語翻訳版です。AI 技術の社会への変革的な影響と、それに伴うリスクの管理について包括的に説明しています。フレームワークは組織が AI システムの信頼性を高め、責任ある AI の設計、開発、展開、使用を促進するためのアプローチを提供することを目的としています。

## エグゼクティブサマリー

人工知能（AI）技術は、商業、健康、交通、サイバーセキュリティから環境や地球に至るまで、社会と人々の生活を変革する大きな可能性を持っています。AI 技術は包括的な経済成長を推進し、世界の状況を改善する科学的進歩を支援することができます。しかし、AI 技術は個人、グループ、組織、コミュニティ、社会、環境、そして地球に悪影響を与える可能性のあるリスクも提起します。他の種類の技術のリスクと同様に、AI リスクは様々な方法で現れる可能性があり、長期的または短期的、高確率または低確率、システム的または局所的、高影響または低影響として特徴付けることができます。

AI RMF は、AI システムを、与えられた一連の目標に対して、実際の環境または仮想環境に影響を与える予測、推奨、または決定などの出力を生成できる、エンジニアリングされた機械ベースのシステムとして定義しています。AI システムは、様々なレベルの自律性で動作するように設計されています（OECD AI 勧告：2019；ISO/IEC 22989：2022 から適応）。

従来のソフトウェアや情報ベースのシステムのリスクを軽減するために組織を支援する無数の標準とベストプラクティスが存在する一方で、AI システムが提起するリスクは多くの点で独特です（付録 B を参照）。例えば、AI システムは時間の経過とともに変化する可能性のあるデータで訓練される場合があり、時には大幅かつ予期せずに変化し、理解が困難な方法でシステムの機能性と信頼性に影響を与えます。AI システムとそれらが展開される文脈は頻繁に複雑であり、障害が発生した際にそれを検出し対応することを困難にします。AI システムは本質的に社会技術的な性質を持っており、社会的動態と人間の行動に影響されることを意味します。AI のリスクと利益は、システムの使用方法、他の AI システムとの相互作用、それを操作する人、そしてそれが展開される社会的文脈に関連する社会的要因と組み合わされた技術的側面の相互作用から生じる可能性があります。

これらのリスクは、AI を組織内および社会内で展開し活用するために独特に困難な技術にしています。適切な制御なしには、AI システムは個人とコミュニティにとって不公平または望ましくない結果を増幅、永続化、または悪化させる可能性があります。適切な制御があれば、AI システムは不公平な結果を軽減し管理することができます。

AI リスク管理は、AI システムの責任ある開発と使用の重要な構成要素です。責任ある AI の実践は、AI システムの設計、開発、使用に関する決定を意図された目的と価値観に合わせるのに役立ちます。責任ある AI の中核概念は、人間中心性、社会的責任、持続可能性を強調しています。AI リスク管理は、AI を設計、開発、展開する組織とその内部チームに、文脈と潜在的または予期しない負の影響と正の影響についてより批判的に考えるよう促すことで、責任ある使用と実践を推進することができます。AI システムのリスクを理解し管理することは、信頼性を高めるのに役立ち、ひいては公衆の信頼を育むことになります。

社会的責任は、「透明で倫理的な行動を通じて、社会と環境に対するその決定と活動の影響に対する組織の責任」（ISO 26000：2010）を指すことができます。持続可能性は、「現在のニーズを満たしながら、将来の世代が自らのニーズを満たす能力を損なうことのない、環境、社会、経済の側面を含むグローバルシステムの状態」（ISO/IEC TR 24368：2022）を指します。責任ある AI は、公平で説明責任のある技術をもたらすことを意図しています。組織の実践は「専門的責任」に従って実行されることが期待されており、これは ISO によって「AI システムとアプリケーション、または AI ベースの製品やシステムを設計、開発、または展開する専門家が、人々、社会、AI の未来に影響を与える独特の立場にあることを認識することを確実にすることを目的とする」アプローチとして定義されています（ISO/IEC TR 24368：2022）。

2020 年国家人工知能イニシアティブ法（P.L. 116-283）の指示に従い、AI RMF の目標は、AI システムを設計、開発、展開、または使用する組織に対して、AI の多くのリスクを管理し、AI システムの信頼できる責任ある開発と使用を促進するのに役立つリソースを提供することです。フレームワークは自発的、権利保護的、非セクター特定的、使用事例に依存しないものとして意図されており、すべての規模の組織とすべてのセクター、そして社会全体がフレームワークのアプローチを実装する柔軟性を提供します。

フレームワークは、ここで AI アクターと呼ばれる組織と個人に、AI システムの信頼性を高めるアプローチを装備し、時間の経過とともに AI システムの責任ある設計、開発、展開、使用を促進するのに役立つように設計されています。AI アクターは、経済協力開発機構（OECD）によって「AI システムのライフサイクルにおいて積極的な役割を果たす者、AI を展開または運用する組織と個人を含む」と定義されています（付録 A を参照）。

AI RMF は実用的であることを意図し、AI 技術が発展し続ける中で AI の状況に適応し、社会が AI から利益を得ながらその潜在的な害から保護されるよう、組織によって様々な程度と能力で運用されることを意図しています。

フレームワークと支援リソースは、進化する技術、世界中の標準の状況、AI コミュニティの経験とフィードバックに基づいて更新、拡張、改善されます。NIST は、AI RMF と関連ガイダンスを適用可能な国際標準、ガイドライン、実践と継続的に整合させます。AI RMF が使用されるにつれて、将来の更新と追加リソースを情報提供するための追加の教訓が学ばれるでしょう。

フレームワークは 2 つの部分に分かれています。パート 1 では、組織が AI に関連するリスクをどのように枠組み化できるかを議論し、意図された対象者を説明します。次に、AI のリスクと信頼性が分析され、信頼できる AI システムの特性が概説されます。これには、有効で信頼性があり、安全で、セキュアで回復力があり、説明責任があり透明で、説明可能で解釈可能で、プライバシーが強化され、有害なバイアスが管理された公平なものが含まれます。

パート 2 はフレームワークの「コア」を構成します。これは、組織が実際に AI システムのリスクに対処するのに役立つ 4 つの特定の機能を説明します。これらの機能である統治（govern）、マッピング（map）、測定（measure）、管理（manage）は、さらにカテゴリとサブカテゴリに分解されます。統治は組織の AI リスク管理プロセスと手順のすべての段階に適用される一方で、マッピング、測定、管理機能は AI システム固有の文脈と AI ライフサイクルの特定の段階で適用することができます。

フレームワークに関連する追加リソースは、NIST AI RMF ウェブサイト（https://www.nist.gov/itl/ai-risk-management-framework）を通じて利用可能な AI RMF プレイブックに含まれています。

民間および公共セクターとの協力による NIST の AI RMF の開発は、2020 年国家 AI イニシアティブ法、国家人工知能安全保障委員会の勧告、および技術標準と関連ツールの開発における連邦政府の関与計画によって求められた、より広範な AI の取り組みと一致し、それに従って指示されています。このフレームワークの開発中の AI コミュニティとの関与（正式な情報要求への回答、3 つの広く参加されたワークショップ、概念ペーパーとフレームワークの 2 つの草案に対するパブリックコメント、複数の公開フォーラムでの議論、多くの小グループ会議を通じて）は、AI RMF 1.0 の開発、および NIST と他者によって実施される AI 研究開発と評価に情報を提供しました。このフレームワークを強化する優先研究と追加ガイダンスは、NIST とより広範なコミュニティが貢献できる関連する AI リスク管理フレームワークロードマップに記録されます。

## 参照 URL

https://airc.nist.gov/airmf-resources/airmf/0-ai-rmf-1-0/

---

# 1. NIST AI RMF フレーミングリスク - 日本語翻訳

## 翻訳概要

本文書は、米国国立標準技術研究所（NIST）が発行した AI Risk Management Framework（AI RMF）の「フレーミングリスク」セクションの日本語翻訳です。AI システムのリスク管理に関する基本的な考え方、リスクの理解と対処方法、AI リスク管理における課題について詳細に解説しています。特に、リスクの測定、リスク許容度、リスクの優先順位付け、組織統合とリスク管理の観点から、AI システムの信頼性向上に向けた包括的なアプローチを提示しています。

## 1 フレーミングリスク

AI リスク管理は、市民の自由と権利への脅威などの AI システムの潜在的な負の影響を最小化する道筋を提供すると同時に、正の影響を最大化する機会も提供します。AI リスクと潜在的な負の影響を効果的に対処し、文書化し、管理することで、より信頼性の高い AI システムにつながる可能性があります。

### 1.1 リスク、影響、害の理解と対処

AI RMF の文脈において、「リスク」とは、事象が発生する確率と対応する事象の結果の大きさまたは程度の複合的な尺度を指します。AI システムの影響または結果は、正、負、またはその両方であり得、機会または脅威をもたらす可能性があります（ISO 31000:2018 より適用）。潜在的な事象の負の影響を考慮する場合、リスクは 1）状況または事象が発生した場合に生じる負の影響または害の大きさ、および 2）発生の可能性の関数です（OMB Circular A-130:2016 より適用）。負の影響または害は、個人、グループ、コミュニティ、組織、社会、環境、地球によって経験される可能性があります。

「リスク管理とは、リスクに関して組織を指導し制御するための調整された活動を指します」（出典：ISO 31000:2018）。

リスク管理プロセスは一般的に負の影響に対処しますが、このフレームワークは AI システムの予想される負の影響を最小化し、正の影響を最大化する機会を特定するアプローチを提供します。潜在的な害のリスクを効果的に管理することで、より信頼性の高い AI システムにつながり、人々（個人、コミュニティ、社会）、組織、システム・エコシステムへの潜在的な利益を解き放つ可能性があります。リスク管理により、AI 開発者とユーザーは影響を理解し、モデルとシステムの固有の制限と不確実性を考慮することができ、これにより全体的なシステム性能と信頼性、および AI 技術が有益な方法で使用される可能性を向上させることができます。

AI RMF は、新しいリスクが出現した際にそれらに対処するよう設計されています。この柔軟性は、影響が容易に予見できず、アプリケーションが進化している場合に特に重要です。一部の AI リスクと利益はよく知られていますが、負の影響と害の程度を評価することは困難な場合があります。図 1 は、AI システムに関連する可能性のある害の例を示しています。

AI リスク管理の取り組みでは、人間が AI システムが機能し、すべての設定でうまく機能すると仮定する可能性があることを考慮すべきです。例えば、正しいかどうかに関わらず、AI システムはしばしば人間よりも客観的であると認識されたり、一般的なソフトウェアよりも優れた能力を提供すると認識されたりします。

![AI システムに関連する潜在的な害の例](https://airc.nist.gov/img/rmf1/cropped_PotentialHarms.png)

図 1：AI システムに関連する潜在的な害の例。信頼性の高い AI システムとその責任ある使用は、負のリスクを軽減し、人々、組織、エコシステムへの利益に貢献することができます。

### 1.2 AI リスク管理の課題

以下にいくつかの課題を説明します。これらは AI の信頼性を追求する際にリスクを管理する際に考慮されるべきです。

#### 1.1.1 リスク測定

十分に定義されていない、または適切に理解されていない AI リスクまたは失敗は、定量的または定性的に測定することが困難です。AI リスクを適切に測定できないことは、AI システムが必ずしも高リスクまたは低リスクのいずれかを示すことを意味するものではありません。リスク測定の課題には以下が含まれます：

**第三者のソフトウェア、ハードウェア、データに関連するリスク：** 第三者のデータやシステムは、研究開発を加速し、技術移転を促進することができます。また、リスク測定を複雑にする可能性もあります。リスクは、第三者のデータ、ソフトウェア、ハードウェア自体とその使用方法の両方から生じる可能性があります。AI システムを開発する組織が使用するリスク指標や方法論は、システムを展開または運用する組織が使用するリスク指標や方法論と一致しない場合があります。また、AI システムを開発する組織は、使用したリスク指標や方法論について透明性を欠く場合があります。リスクの測定と管理は、顧客が第三者のデータやシステムを AI 製品やサービスに使用または統合する方法、特に十分な内部ガバナンス構造と技術的保護措置なしに行う場合によって複雑になる可能性があります。それにもかかわらず、すべての当事者と AI アクターは、独立したコンポーネントまたは統合されたコンポーネントとして開発、展開、または使用する AI システムのリスクを管理すべきです。

**新興リスクの追跡：** 組織のリスク管理の取り組みは、新興リスクを特定し追跡し、それらを測定するための技術を検討することによって強化されます。AI システム影響評価アプローチは、AI アクターが特定の文脈内での潜在的な影響や害を理解するのに役立ちます。

**信頼できる指標の利用可能性：** リスクと信頼性に関する堅牢で検証可能な測定方法についての現在のコンセンサスの欠如、および異なる AI 使用事例への適用可能性は、AI リスク測定の課題です。負のリスクや害を測定しようとする際の潜在的な落とし穴には、指標の開発がしばしば制度的な取り組みであり、基礎となる影響とは無関係な要因を意図せずに反映する可能性があるという現実が含まれます。さらに、測定アプローチは過度に単純化されたり、ゲーム化されたり、重要なニュアンスを欠いたり、予期しない方法で依存されたり、影響を受けるグループと文脈の違いを考慮しなかったりする可能性があります。

人口への影響を測定するアプローチは、文脈が重要であること、害が様々なグループやサブグループに異なって影響を与える可能性があること、害を受ける可能性のあるコミュニティやその他のサブグループが常にシステムの直接ユーザーではないことを認識する場合に最も効果的に機能します。

**AI ライフサイクルの異なる段階でのリスク：** AI ライフサイクルの初期段階でリスクを測定することは、後期段階でリスクを測定することとは異なる結果をもたらす可能性があります。一部のリスクは特定の時点で潜在的である可能性があり、AI システムが適応し進化するにつれて増加する可能性があります。さらに、AI ライフサイクル全体の異なる AI アクターは、異なるリスクの視点を持つ可能性があります。例えば、事前訓練されたモデルなどの AI ソフトウェアを利用可能にする AI 開発者は、その事前訓練されたモデルを特定の使用事例で展開する責任を負う AI アクターとは異なるリスクの視点を持つ可能性があります。このような展開者は、彼らの特定の使用が初期の開発者によって認識されたものとは異なるリスクを伴う可能性があることを認識しない場合があります。関与するすべての AI アクターは、目的に適した信頼性の高い AI システムを設計、開発、展開する責任を共有しています。

**実世界の設定でのリスク：** 実験室や制御された環境で AI リスクを測定することは、展開前に重要な洞察をもたらす可能性がありますが、これらの測定は運用上の実世界の設定で出現するリスクとは異なる場合があります。

**不可解性：** 不可解な AI システムは、リスク測定を複雑にする可能性があります。不可解性は、AI システムの不透明な性質（限定的な説明可能性または解釈可能性）、AI システムの開発または展開における透明性や文書化の欠如、または AI システムの固有の不確実性の結果である可能性があります。

**人間のベースライン：** 意思決定などの人間の活動を補強または置き換えることを意図した AI システムのリスク管理には、比較のための何らかの形のベースライン指標が必要です。AI システムは人間とは異なるタスクを実行し、タスクを異なって実行するため、これを体系化することは困難です。

#### 1.1.2 リスク許容度

AI RMF はリスクの優先順位付けに使用できますが、リスク許容度を規定するものではありません。「リスク許容度」とは、組織または AI アクター（付録 A 参照）がその目標を達成するためにリスクを負う準備を指します。リスク許容度は、法的または規制要件によって影響を受ける可能性があります（ISO ガイド 73 より適用）。リスク許容度と組織や社会にとって受け入れ可能なリスクのレベルは、高度に文脈的であり、アプリケーションと使用事例に特有です。リスク許容度は、AI システム所有者、組織、業界、コミュニティ、または政策立案者によって確立された政策と規範によって影響を受ける可能性があります。リスク許容度は、AI システム、政策、規範が進化するにつれて時間とともに変化する可能性があります。異なる組織は、特定の組織の優先事項とリソースの考慮事項により、様々なリスク許容度を持つ可能性があります。

害・費用対効果のトレードオフをより良く情報提供するための新興知識と方法は、企業、政府、学術界、市民社会によって継続的に開発され議論されるでしょう。AI リスク許容度を特定するための課題が未解決のままである限り、負の AI リスクを軽減するためにリスク管理フレームワークがまだ容易に適用できない文脈が存在する可能性があります。

フレームワークは柔軟であることを意図しており、適用される法律、規制、規範と一致すべき既存のリスク慣行を補強することを意図しています。組織は、組織、ドメイン、分野、セクター、または専門的要件によって確立されたリスク基準、許容度、対応に関する既存の規制とガイドラインに従うべきです。一部のセクターまたは業界では、害の確立された定義または確立された文書化、報告、開示要件を持つ場合があります。セクター内では、リスク管理は特定のアプリケーションと使用事例設定に関する既存のガイドラインに依存する場合があります。確立されたガイドラインが存在しない場合、組織は合理的なリスク許容度を定義すべきです。許容度が定義されると、この AI RMF はリスクを管理し、リスク管理プロセスを文書化するために使用できます。

#### 1.1.3 リスクの優先順位付け

負のリスクを完全に排除しようとすることは、すべての事故と失敗を排除することはできないため、実際には逆効果になる可能性があります。リスクに関する非現実的な期待は、組織がリスクトリアージを非効率的または非実用的にする方法でリソースを配分したり、希少なリソースを浪費したりする可能性があります。リスク管理文化は、組織がすべての AI リスクが同じではないことを認識し、リソースを目的を持って配分するのに役立ちます。実行可能なリスク管理の取り組みは、組織が開発または展開する各 AI システムの信頼性を評価するための明確なガイドラインを示します。政策とリソースは、評価されたリスクレベルと AI システムの潜在的影響に基づいて優先順位を付けるべきです。AI システムが AI 展開者によって使用の特定の文脈に合わせてカスタマイズまたは調整される可能性の程度は、寄与要因となり得ます。

AI RMF を適用する際、組織が特定の使用文脈内で AI システムにとって最も高いと判断するリスクは、最も緊急の優先順位付けと最も徹底的なリスク管理プロセスを要求します。AI システムが受け入れられない負のリスクレベルを示す場合、例えば重大な負の影響が差し迫っている、深刻な害が実際に発生している、または破滅的なリスクが存在する場合、リスクが十分に管理されるまで、開発と展開は安全な方法で停止すべきです。AI システムの開発、展開、使用事例が特定の文脈で低リスクであることが判明した場合、それは潜在的により低い優先順位付けを示唆する可能性があります。

リスクの優先順位付けは、人間と直接相互作用するよう設計または展開された AI システムと、そうでない AI システムとの間で異なる場合があります。AI システムが個人識別情報などの機密または保護されたデータで構成される大規模なデータセットで訓練されている設定、または AI システムの出力が人間に直接的または間接的な影響を与える設定では、より高い初期優先順位付けが求められる場合があります。計算システムとのみ相互作用するよう設計され、非機密データセット（例えば、物理環境から収集されたデータ）で訓練された AI システムは、より低い初期優先順位付けを求める場合があります。それにもかかわらず、人間に面していない AI システムは下流の安全性や社会的影響を持つ可能性があるため、文脈に基づいてリスクを定期的に評価し優先順位を付けることは重要です。

「残存リスク」は、リスク処理後に残るリスクと定義され（出典：ISO ガイド 73）、エンドユーザーまたは影響を受ける個人とコミュニティに直接影響を与えます。残存リスクを文書化することで、システムプロバイダーは AI 製品を展開するリスクを十分に考慮し、システムとの相互作用の潜在的な負の影響についてエンドユーザーに情報を提供することが求められます。

#### 1.1.4 組織統合とリスク管理

AI リスクは孤立して考慮されるべきではありません。異なる AI アクターは、ライフサイクルでの役割に応じて異なる責任と認識を持ちます。例えば、AI システムを開発する組織は、システムがどのように使用される可能性があるかについての情報を持たないことがよくあります。AI リスク管理は、より広範な企業リスク管理戦略とプロセスに統合され組み込まれるべきです。サイバーセキュリティやプライバシーなどの他の重要なリスクと併せて AI リスクを扱うことで、より統合された結果と組織の効率性が得られます。

AI RMF は、AI システムリスクまたはより広範な企業リスクを管理するための関連ガイダンスとフレームワークと併せて利用される場合があります。AI システムに関連する一部のリスクは、他のタイプのソフトウェア開発と展開に共通しています。重複するリスクの例には、AI システムを訓練するための基礎データの使用に関連するプライバシーの懸念、リソース集約的なコンピューティング需要に関連するエネルギーと環境への影響、システムとその訓練および出力データの機密性、完全性、可用性に関連するセキュリティの懸念、AI システムの基礎となるソフトウェアとハードウェアの一般的なセキュリティが含まれます。

組織は、リスク管理が効果的であるために、適切な説明責任メカニズム、役割と責任、文化、インセンティブ構造を確立し維持する必要があります。AI RMF の使用だけでは、これらの変化をもたらしたり、適切なインセンティブを提供したりすることはありません。効果的なリスク管理は、上級レベルでの組織のコミットメントを通じて実現され、組織または業界内での文化的変化を必要とする場合があります。さらに、AI リスクを管理したり AI RMF を実装したりする中小規模の組織は、その能力とリソースに応じて、大規模組織とは異なる課題に直面する可能性があります。

---

**参照 URL：** https://airc.nist.gov/airmf-resources/airmf/1-sec-risk/

---

# 2. NIST AI RMF 対象者（オーディエンス）

## 翻訳概要

本文書は、NIST AI Risk Management Framework（AI RMF）の第 2 章「対象者」の日本語翻訳版です。AI リスクの特定と管理には AI ライフサイクル全体にわたる幅広い視点と関係者が必要であることを説明し、AI RMF の主要な対象者である AI アクターの役割と責任について詳述しています。また、OECD の AI システム分類フレームワークを基に修正された AI システムのライフサイクルと主要次元について解説し、多様なチームによる集合的責任の重要性を強調しています。

## 2 対象者

AI リスクと潜在的影響（正負両方）の特定と管理には、AI ライフサイクル全体にわたる幅広い視点と関係者が必要です。理想的には、AI アクターは多様な経験、専門知識、背景を代表し、人口統計学的および学際的に多様なチームを構成します。AI RMF は、AI ライフサイクルと次元全体にわたる AI アクターによる使用を意図しています。

OECD は、リスク管理を含む AI 政策とガバナンスに関連する特性を持つ 5 つの主要な社会技術的次元に従って AI ライフサイクル活動を分類するフレームワークを開発しました。図 2 は、このフレームワークの目的のために NIST によってわずかに修正されたこれらの次元を示しています。NIST の修正は、AI ライフサイクル全体を通じたテスト、評価、検証、妥当性確認（TEVV）プロセスの重要性を強調し、AI システムの運用コンテキストを一般化しています。

図 2 に示される AI 次元は、アプリケーションコンテキスト、データと入力、AI モデル、タスクと出力です。これらの次元に関与し、AI システムの設計、開発、展開、評価、使用を実行または管理し、AI リスク管理の取り組みを推進する AI アクターが、AI RMF の主要な対象者です。

ライフサイクル次元全体にわたる代表的な AI アクターは図 3 にリストされ、付録 A で詳細に説明されています。AI RMF 内では、すべての AI アクターが協力してリスクを管理し、信頼できる責任ある AI の目標を達成します。TEVV 固有の専門知識を持つ AI アクターは AI ライフサイクル全体に統合されており、特にフレームワークから恩恵を受ける可能性があります。定期的に実行される TEVV タスクは、技術的、社会的、法的、倫理的基準や規範に関する洞察を提供し、影響の予測と新興リスクの評価と追跡を支援できます。AI ライフサイクル内の定期的なプロセスとして、TEVV は中間的な修正と事後的なリスク管理の両方を可能にします。

図 2 の中心にある人々と地球の次元は、人権と社会と地球のより広い福祉を表しています。この次元の AI アクターは、主要な対象者に情報を提供する別の AI RMF 対象者を構成します。これらの AI アクターには、業界団体、標準開発組織、研究者、アドボカシーグループ、環境グループ、市民社会組織、エンドユーザー、および潜在的に影響を受ける個人とコミュニティが含まれる場合があります。これらのアクターは以下のことができます：

- コンテキストの提供と潜在的および実際の影響の理解の支援
- AI リスク管理のための正式または準正式な規範とガイダンスの情報源となること
- AI 運用の境界（技術的、社会的、法的、倫理的）の指定
- 市民の自由と権利、公平性、環境と地球、経済に関連する社会的価値と優先事項のバランスを取るために必要なトレードオフの議論の促進

成功するリスク管理は、図 3 に示される AI アクター間の集合的責任感に依存します。セクション 5 で説明される AI RMF 機能には、多様な視点、学問分野、職業、経験が必要です。多様なチームは、技術の目的と機能に関するアイデアと仮定のより開かれた共有に貢献し、これらの暗黙的な側面をより明示的にします。この広範な集合的視点は、問題を表面化し、既存および新興のリスクを特定する機会を創出します。

## 参照 URL

https://airc.nist.gov/airmf-resources/airmf/2-sec-audience/

---

# 3. NIST AI RMF：AI リスクと信頼性に関する特性

## 翻訳概要

本文書は、NIST（米国国立標準技術研究所）が発行した AI Risk Management Framework（AI RMF）の第 3 章「AI Risks and Trustworthiness」の日本語翻訳版です。信頼できる AI システムが持つべき 7 つの特性について詳細に解説しており、これらの特性は相互に関連し合いながら AI システムの信頼性を構築する基盤となります。各特性には妥当性と信頼性、安全性、セキュリティと回復力、説明責任と透明性、説明可能性と解釈可能性、プライバシー強化、公平性（有害なバイアスの管理）が含まれます。

## 3 AI リスクと信頼性

AI システムが信頼できるものとなるためには、利害関係者にとって価値のある複数の基準に対応する必要があります。AI の信頼性を向上させるアプローチは、AI の負のリスクを軽減することができます。本フレームワークでは、信頼できる AI の以下の特性を明確にし、それらに対処するためのガイダンスを提供します。信頼できる AI システムの特性には、妥当性と信頼性、安全性、セキュリティと回復力、説明責任と透明性、説明可能性と解釈可能性、プライバシー強化、公平性（有害なバイアスの管理）が含まれます。信頼できる AI を構築するには、AI システムの使用文脈に基づいて、これらの特性のそれぞれのバランスを取る必要があります。すべての特性は社会技術システムの属性ですが、説明責任と透明性は AI システムとその外部環境の内部プロセスと活動にも関連します。これらの特性を軽視すると、負の結果の確率と規模が増大する可能性があります。

![信頼できる AI システムの特性](https://airc.nist.gov/img/rmf1/RMF-redesign-rounded-final-02.png)

図 4：信頼できる AI システムの特性。妥当性と信頼性は信頼性の必要条件であり、他の信頼性特性の基盤として示されています。説明責任と透明性は、他のすべての特性に関連するため、縦のボックスとして表示されています。

信頼性の特性（上記図 4 に示す）は、社会的および組織的行動、AI システムが使用するデータセット、AI モデルとアルゴリズムの選択とそれらを構築する人々による決定、そしてそのようなシステムからの洞察と監視を提供する人間との相互作用と密接に結びついています。AI の信頼性特性に関連する具体的な指標と、それらの指標の正確な閾値を決定する際には、人間の判断を用いるべきです。

AI の信頼性特性を個別に対処しても、AI システムの信頼性は保証されません。通常はトレードオフが伴い、すべての特性がすべての設定で適用されることは稀であり、特定の状況においては一部の特性がより重要または重要でない場合があります。最終的に、信頼性は社会的概念であり、スペクトラム全体にわたって存在し、最も弱い特性と同程度の強さしか持ちません。

AI リスクを管理する際、組織はこれらの特性のバランスを取る上で困難な決定に直面する可能性があります。例えば、特定のシナリオでは、解釈可能性の最適化とプライバシーの実現の間でトレードオフが生じる場合があります。他のケースでは、組織は予測精度と解釈可能性の間でトレードオフに直面する可能性があります。または、データの希少性などの特定の条件下では、プライバシー強化技術が精度の低下をもたらし、特定の領域における公平性やその他の価値に関する決定に影響を与える可能性があります。トレードオフに対処するには、意思決定の文脈を考慮する必要があります。これらの分析は、異なる測定値間のトレードオフの存在と程度を明らかにすることができますが、トレードオフをどのように乗り越えるかという問題には答えません。それらは関連する文脈で働く価値観に依存し、透明で適切に正当化できる方法で解決されるべきです。

AI ライフサイクルにおいて文脈的認識を向上させるための複数のアプローチがあります。例えば、専門家は TEVV（Test, Evaluation, Verification, and Validation）の結果の評価を支援し、製品および展開チームと協力して TEVV パラメータを要件と展開条件に合わせることができます。適切にリソースが提供された場合、AI ライフサイクル全体を通じて利害関係者と関連する AI アクターからの入力の幅と多様性を増やすことで、文脈に敏感な評価を情報提供し、AI システムの利益と正の影響を特定する機会を向上させることができます。これらの実践は、社会的文脈で生じるリスクが適切に管理される可能性を高めることができます。

信頼性特性の理解と取り扱いは、AI ライフサイクル内での AI アクターの特定の役割に依存します。任意の AI システムについて、AI 設計者または開発者は、展開者とは異なる特性の認識を持つ可能性があります。

本文書で説明される信頼性特性は相互に影響し合います。高度にセキュアだが不公平なシステム、正確だが不透明で解釈不可能なシステム、不正確だがセキュアでプライバシーが強化され透明なシステムは、すべて望ましくありません。リスク管理への包括的なアプローチは、信頼性特性間のトレードオフのバランスを取ることを求めます。AI 技術が特定の文脈や目的に適切または必要なツールであるかどうか、そしてそれを責任を持って使用する方法を決定することは、すべての AI アクターの共同責任です。AI システムを委託または展開する決定は、信頼性特性の文脈的評価と相対的なリスク、影響、コスト、利益に基づき、幅広い利害関係者によって情報提供されるべきです。

## 3.1 妥当性と信頼性

妥当性確認は「特定の意図された使用または適用の要件が満たされていることを、客観的証拠の提供を通じて確認すること」です（出典：ISO 9000:2015）。訓練を超えたデータと設定に対して不正確、信頼性が低い、または一般化が不十分な AI システムの展開は、負の AI リスクを生み出し増大させ、信頼性を低下させます。

信頼性は同じ標準で「アイテムが与えられた条件下で、与えられた時間間隔において、故障することなく要求通りに実行する能力」と定義されています（出典：ISO/IEC TS 5723:2022）。信頼性は、システムの全生涯を含む、期待される使用条件下および与えられた期間にわたる AI システム動作の全体的な正確性の目標です。

精度と堅牢性は AI システムの妥当性と信頼性に貢献し、AI システムにおいて相互に緊張関係にある場合があります。

精度は ISO/IEC TS 5723:2022 によって「観測、計算、または推定の結果の真の値または真であると受け入れられる値への近さ」と定義されています。精度の測定は、計算中心の測定（例：偽陽性率と偽陰性率）、人間と AI のチーミング、および外部妥当性（訓練条件を超えて一般化可能）を実証することを考慮すべきです。精度測定は常に、期待される使用条件を代表する明確に定義された現実的なテストセットと、テスト方法論の詳細と組み合わせるべきです。これらは関連文書に含まれるべきです。精度測定には、異なるデータセグメントの結果の分解が含まれる場合があります。

堅牢性または一般化可能性は「様々な状況下でその性能レベルを維持するシステムの能力」と定義されています（出典：ISO/IEC TS 5723:2022）。堅牢性は、最初に予想されなかった AI システムの使用を含む、幅広い条件と状況における適切なシステム機能の目標です。堅牢性は、システムが期待される使用下で行うのと全く同じように実行することだけでなく、予期しない設定で動作している場合に人々への潜在的な害を最小化する方法で実行することも要求します。

展開された AI システムの妥当性と信頼性は、システムが意図通りに実行されていることを確認する継続的なテストまたは監視によってしばしば評価されます。妥当性、精度、堅牢性、信頼性の測定は信頼性に貢献し、特定のタイプの故障がより大きな害を引き起こす可能性があることを考慮すべきです。AI リスク管理の取り組みは潜在的な負の影響の最小化を優先すべきであり、AI システムがエラーを検出または修正できない場合には人間の介入を含む必要がある場合があります。

## 3.2 安全性

AI システムは「定義された条件下で、人間の生命、健康、財産、または環境が危険にさらされる状態に至らない」べきです（出典：ISO/IEC TS 5723:2022）。AI システムの安全な動作は以下を通じて改善されます：

- 責任ある設計、開発、展開の実践
- システムの責任ある使用に関する展開者への明確な情報
- 展開者とエンドユーザーによる責任ある意思決定
- インシデントの経験的証拠に基づくリスクの説明と文書化

異なるタイプの安全リスクは、文脈と提示される潜在的リスクの深刻度に基づいて、調整された AI リスク管理アプローチを必要とする場合があります。重傷または死亡の潜在的リスクをもたらす安全リスクは、最も緊急の優先順位付けと最も徹底的なリスク管理プロセスを求めます。

ライフサイクル中の安全性の考慮を採用し、計画と設計から可能な限り早期に開始することで、システムを危険にする可能性のある故障や条件を防ぐことができます。AI 安全性のための他の実用的なアプローチは、しばしば厳密なシミュレーションとドメイン内テスト、リアルタイム監視、意図されたまたは期待される機能から逸脱するシステムをシャットダウン、修正、または人間が介入する能力に関連します。

AI 安全リスク管理アプローチは、交通や医療などの分野での安全性の取り組みとガイドラインから手がかりを得て、既存のセクターまたはアプリケーション固有のガイドラインや標準と整合すべきです。

## 3.3 セキュリティと回復力

AI システム、およびそれらが展開されるエコシステムは、予期しない有害事象や環境または使用の予期しない変化に耐えることができる場合、または内部および外部の変化に直面して機能と構造を維持し、必要な場合に安全かつ優雅に劣化することができる場合、回復力があると言えます（ISO/IEC TS 5723:2022 から適応）。一般的なセキュリティの懸念は、敵対的例、データ汚染、AI システムエンドポイントを通じたモデル、訓練データ、またはその他の知的財産の流出に関連します。不正なアクセスと使用を防ぐ保護メカニズムを通じて機密性、完全性、可用性を維持できる AI システムは、セキュアであると言えます。NIST サイバーセキュリティフレームワークとリスク管理フレームワークのガイドラインは、ここで適用可能なものの中にあります。

セキュリティと回復力は関連しているが異なる特性です。回復力が予期しない有害事象後に正常機能に戻る能力である一方、セキュリティは回復力を含みますが、攻撃を回避、保護、対応、または回復するためのプロトコルも包含します。回復力は堅牢性に関連し、データの出所を超えて、モデルまたはデータの予期しないまたは敵対的な使用（または乱用または誤用）を包含します。

## 3.4 説明責任と透明性

信頼できる AI は説明責任に依存します。説明責任は透明性を前提とします。透明性は、AI システムとその出力に関する情報が、そのようなシステムと相互作用する個人にとって利用可能である程度を反映します。彼らがそうしていることを認識しているかどうかに関係なく。意味のある透明性は、AI ライフサイクルの段階に基づき、AI アクターまたは AI システムと相互作用または使用する個人の役割や知識に合わせて調整された適切なレベルの情報へのアクセスを提供します。理解のより高いレベルを促進することにより、透明性は AI システムへの信頼を高めます。

この特性の範囲は、設計決定と訓練データから、モデル訓練、モデルの構造、その意図された使用事例、展開、展開後、またはエンドユーザーの決定がどのように、いつ、誰によって行われたかまでにわたります。透明性は、不正確またはその他の負の影響をもたらす AI システム出力に関連する実行可能な救済のためにしばしば必要です。透明性は人間と AI の相互作用を考慮すべきです。例えば、AI システムによって引き起こされる潜在的または実際の有害な結果が検出された場合に、人間のオペレーターまたはユーザーがどのように通知されるか。透明なシステムは必ずしも正確で、プライバシーが強化され、セキュアで、公平なシステムではありません。しかし、不透明なシステムがそのような特性を持っているかどうかを判断することは困難であり、複雑なシステムが進化するにつれて時間をかけてそれを行うことも困難です。

AI システムの結果に対する説明責任を求める際には、AI アクターの役割を考慮すべきです。AI およびより広範な技術システムに関連するリスクと説明責任の関係は、文化的、法的、セクター的、社会的文脈によって異なります。生命と自由が危険にさらされる場合など、結果が深刻な場合、AI 開発者と展開者は、透明性と説明責任の実践を比例的かつ積極的に調整することを検討すべきです。リスク管理のような害の軽減のための組織的実践と統治構造を維持することは、より説明責任のあるシステムにつながることを助けることができます。

透明性と説明責任を向上させる措置は、必要なリソースのレベルと専有情報を保護する必要性を含む、実装エンティティへのこれらの取り組みの影響も考慮すべきです。

訓練データの出所を維持し、AI システムの決定の訓練データのサブセットへの帰属を支援することは、透明性と説明責任の両方を助けることができます。訓練データは著作権の対象となる場合もあり、適用される知的財産権法に従うべきです。

AI システムと関連文書の透明性ツールが進化し続ける中、AI システムの開発者は、AI システムが意図通りに使用されることを確実にするために、AI 展開者と協力して異なるタイプの透明性ツールをテストすることが奨励されます。

## 3.5 説明可能性と解釈可能性

説明可能性は AI システムの動作の基礎となるメカニズムの表現を指し、解釈可能性は設計された機能目的の文脈における AI システムの出力の意味を指します。説明可能性と解釈可能性は一緒になって、AI システムを操作または監督する人々、および AI システムのユーザーが、その出力を含むシステムの機能性と信頼性についてより深い洞察を得ることを支援します。根本的な仮定は、負のリスクの認識がシステム出力を適切に理解または文脈化する能力の欠如から生じるということです。説明可能で解釈可能な AI システムは、エンドユーザーが AI システムの目的と潜在的影響を理解するのに役立つ情報を提供します。

説明可能性の欠如からのリスクは、ユーザーの役割、知識、スキルレベルなどの個人差に合わせて調整された説明で、AI システムがどのように機能するかを記述することによって管理される場合があります。説明可能なシステムはより簡単にデバッグと監視ができ、より徹底的な文書化、監査、ガバナンスに適しています。

解釈可能性へのリスクは、AI システムが特定の予測または推奨を行った理由の説明を伝えることによってしばしば対処できます。（「説明可能な人工知能の 4 つの原則」と「人工知能における説明可能性と解釈可能性の心理学的基盤」を参照してください。こちらで見つけることができます。）

透明性、説明可能性、解釈可能性は、相互に支援し合う異なる特性です。透明性はシステムで「何が起こったか」という質問に答えることができます。説明可能性はシステムで決定が「どのように」行われたかという質問に答えることができます。解釈可能性はシステムによって決定が「なぜ」行われたか、そしてユーザーにとってのその意味や文脈という質問に答えることができます。

## 3.6 プライバシー強化

プライバシーは一般的に、人間の自律性、アイデンティティ、尊厳を保護するのに役立つ規範と実践を指します。これらの規範と実践は通常、侵入からの自由、観察の制限、または個人のアイデンティティの側面（例：身体、データ、評判）の開示または制御に同意する個人の主体性に対処します。NIST プライバシーフレームワーク：企業リスク管理を通じてプライバシーを改善するためのツール

匿名性、機密性、制御などのプライバシー価値は、一般的に AI システムの設計、開発、展開の選択を導くべきです。プライバシー関連のリスクは、セキュリティ、バイアス、透明性に影響を与える可能性があり、これらの他の特性とのトレードオフを伴います。安全性とセキュリティと同様に、AI システムの特定の技術的特徴がプライバシーを促進または削減する場合があります。AI システムは、個人または個人に関する以前にプライベートだった情報を特定するための推論を可能にすることによって、プライバシーに新しいリスクをもたらすこともできます。

AI のためのプライバシー強化技術（「PETs」）、および特定のモデル出力の非識別化と集約などのデータ最小化方法は、プライバシー強化 AI システムの設計を支援できます。データの希少性などの特定の条件下では、プライバシー強化技術が精度の低下をもたらし、特定の領域における公平性やその他の価値に関する決定に影響を与える可能性があります。

## 3.7 公平性 - 有害なバイアスの管理

AI における公平性には、有害なバイアスと差別などの問題に対処することによる平等と公正への懸念が含まれます。公平性の基準は複雑で定義が困難な場合があります。なぜなら、公平性の認識は文化間で異なり、アプリケーションによって変化する可能性があるからです。組織のリスク管理の取り組みは、これらの違いを認識し考慮することによって向上します。有害なバイアスが軽減されたシステムは必ずしも公平ではありません。例えば、人口統計グループ間で予測がある程度バランスが取れているシステムでも、障害を持つ個人にとってアクセスできない場合や、デジタル格差の影響を受ける場合、または既存の格差や体系的バイアスを悪化させる場合があります。

バイアスは人口統計的バランスとデータ代表性よりも広範囲です。NIST は考慮し管理すべき AI バイアスの 3 つの主要カテゴリを特定しました：体系的、計算的・統計的、人間認知的。これらのそれぞれは、偏見、偏向、または差別的意図がない場合でも発生する可能性があります。体系的バイアスは、AI データセット、AI ライフサイクル全体の組織的規範、実践、プロセス、AI システムを使用するより広い社会に存在する可能性があります。計算的・統計的バイアスは、AI データセットとアルゴリズムプロセスに存在する可能性があり、しばしば非代表的サンプルによる体系的エラーから生じます。人間認知バイアスは、個人またはグループが決定を下すまたは欠落情報を埋めるために AI システム情報をどのように認識するか、または人間が AI システムの目的と機能についてどのように考えるかに関連します。人間認知バイアスは、AI システムの設計、実装、運用、保守を含む AI ライフサイクルとシステム使用全体の意思決定プロセスに遍在しています。

バイアスは多くの形で存在し、私たちの生活に関する決定を助ける自動化システムに根付く可能性があります。バイアスは常に負の現象ではありませんが、AI システムはバイアスの速度と規模を潜在的に増加させ、個人、グループ、コミュニティ、組織、社会への害を永続化し拡大する可能性があります。バイアスは透明性の概念および社会における公平性と密接に関連しています。（3 つのカテゴリを含むバイアスに関する詳細情報については、NIST 特別出版物 1270「人工知能におけるバイアスの特定と管理のための標準に向けて」を参照してください。）

---

**参照 URL：** https://airc.nist.gov/airmf-resources/airmf/3-sec-characteristics/

---

# 4. NIST AI RMF の有効性

## 翻訳概要

本文書は、NIST AI Risk Management Framework (AI RMF) の有効性に関する公式文書の日本語翻訳です。AI RMF の効果測定方法、組織が期待できる利益、および将来の NIST 活動における評価計画について詳述しています。特に、AI システムの信頼性向上における具体的な成果指標と、フレームワーク利用者が得られる包括的なメリットについて説明されています。

## AI RMF の有効性

AI RMF の有効性評価は、AI システムの信頼性における根本的な改善を測定する方法を含めて、AI コミュニティと連携した今後の NIST 活動の一部となる予定です。

組織およびフレームワークの他の利用者は、AI RMF が AI リスク管理能力を向上させたかどうかを定期的に評価することが推奨されています。これには、政策、プロセス、実践、実装計画、指標、測定、および期待される成果が含まれますが、これらに限定されるものではありません。NIST は、AI RMF の有効性を評価するためのメトリクス、方法論、および目標を開発し、結果と支援情報を広く共有するために、他の関係者と協力して取り組む予定です。

フレームワーク利用者は以下の利益を得ることが期待されています。

AI リスクのガバナンス、マッピング、測定、および管理のためのプロセスが強化され、成果が明確に文書化されます。信頼性特性、社会技術的アプローチ、および AI リスク間の関係とトレードオフに対する認識が向上します。システムの運用開始と展開に関する実行可否決定のための明示的なプロセスが確立されます。

AI システムリスクに関連する組織の説明責任向上のための政策、プロセス、実践、および手順が確立されます。AI システムリスクと個人、コミュニティ、組織、および社会への潜在的影響の特定と管理を優先する組織文化が強化されます。

リスク、意思決定プロセス、責任、共通の落とし穴、TEVV 実践、および継続的改善のアプローチについて、組織内および組織間での情報共有が改善されます。下流リスクに対する認識を高めるための文脈的知識が向上します。

関係する当事者および関連する AI アクターとの関与が強化されます。AI システムおよび関連リスクの TEVV に対する能力が増強されます。

これらの利益は、AI RMF の実装により組織が達成できる包括的な改善を示しており、技術的な側面だけでなく、組織文化や社会的責任の観点からも AI リスク管理の向上を図ることができます。NIST は今後も AI コミュニティと協力して、これらの効果を測定し評価するための具体的な手法を開発していく予定です。

## 参照 URL

https://airc.nist.gov/airmf-resources/airmf/4-effectiveness/

---

# 5. NIST AI RMF コア機能 - 日本語翻訳

## 翻訳概要

本文書は、NIST（米国国立標準技術研究所）が公開している AI Risk Management Framework（AI RMF）のコア機能に関する公式文書を日本語に翻訳したものです。AI RMF コアは、AI システムの信頼性を確保し、AI リスクを管理するための 4 つの主要機能（ガバナンス、マッピング、測定、管理）を定義しています。

この文書では、各機能の詳細な説明、カテゴリ、サブカテゴリが体系的に整理されており、組織が AI システムのライフサイクル全体を通じてリスク管理を実施するための包括的なガイダンスを提供しています。特に、多様性、公平性、包摂性、アクセシビリティの観点から AI リスク管理を行うことの重要性が強調されています。

参照 URL: https://airc.nist.gov/airmf-resources/airmf/5-sec-core/

---

## 5 AI RMF コア

AI RMF コアは、AI リスクを管理し、信頼できる AI システムを責任を持って開発するための対話、理解、活動を可能にする成果とアクションを提供します。図 5 に示すように、コアは 4 つの機能で構成されています：**ガバナンス（govern）**、**マッピング（map）**、**測定（measure）**、**管理（manage）** です。これらの高レベル機能はそれぞれカテゴリとサブカテゴリに分解されます。カテゴリとサブカテゴリは、具体的なアクションと成果に細分化されます。アクションはチェックリストを構成するものではなく、必ずしも順序立てられた一連のステップでもありません。

![機能は AI リスク管理活動を整理する](https://airc.nist.gov/img/pblanding_image_vsusanne_cropped.png)

図 5：機能は最高レベルで AI リスク管理活動を整理し、AI リスクをガバナンス、マッピング、測定、管理します。ガバナンスは、他の 3 つの機能に情報を提供し、それらに浸透する横断的機能として設計されています。

リスク管理は継続的で、タイムリーであり、AI システムライフサイクルの次元全体を通じて実行されるべきです。AI RMF コア機能は、組織外の AI アクターの見解を含む可能性のある、多様で学際的な視点を反映する方法で実行されるべきです。多様なチームを持つことは、設計、開発、展開、または評価される技術の目的と機能に関するアイデアと仮定のより開かれた共有に貢献し、問題を表面化し、既存および新興のリスクを特定する機会を創出できます。

AI RMF のオンライン付随リソースである NIST AI RMF プレイブックは、組織が AI RMF をナビゲートし、自身のコンテキスト内で適用できる提案された戦術的アクションを通じてその成果を達成するのを支援するために利用可能です。AI RMF と同様に、プレイブックは任意であり、組織は自身のニーズと関心に応じて提案を活用できます。プレイブックユーザーは、自身の使用のために提案された材料から選択されたカスタマイズされたガイダンスを作成し、より広いコミュニティとの共有のために提案を貢献できます。AI RMF と共に、プレイブックは NIST 信頼できる責任ある AI リソースセンターの一部です。

フレームワークユーザーは、リソースと能力に基づいて AI リスクを管理するニーズに最も適するように、これらの機能を適用できます。一部の組織は、カテゴリとサブカテゴリの中から選択することを選ぶかもしれません。他の組織は、すべてのカテゴリとサブカテゴリを適用することを選択し、その能力を持つかもしれません。ガバナンス構造が整備されていることを前提として、機能は、フレームワークのユーザーが価値を追加すると判断した通り、AI ライフサイクル全体で任意の順序で実行できます。**ガバナンス** の成果を制定した後、AI RMF のほとんどのユーザーは **マッピング** 機能から開始し、**測定** または **管理** に続くでしょう。しかし、ユーザーが機能を統合する方法に関わらず、プロセスは反復的であるべきで、必要に応じて機能間の相互参照を行うべきです。同様に、複数の機能に適用される要素を持つカテゴリとサブカテゴリ、または論理的に特定のサブカテゴリの決定前に行われるべきものがあります。
##
 5.1 ガバナンス

**ガバナンス** 機能は：

* AI システムを設計、開発、展開、評価、または取得する組織内でリスク管理の文化を育成し実装する
* システムがユーザーや社会全体の他の人々に対してもたらす可能性のあるリスクを予測、特定、管理するプロセス、文書、組織スキーム、およびそれらの成果を達成するための手順を概説する
* 潜在的な影響を評価するプロセスを組み込む
* AI リスク管理機能が組織の原則、政策、戦略的優先事項と整合できる構造を提供する
* AI システム設計と開発の技術的側面を組織の価値と原則に接続し、そのようなシステムの取得、訓練、展開、監視に関わる個人のための組織的実践と能力を可能にする
* 第三者ソフトウェアやハードウェアシステムおよびデータの使用に関する法的およびその他の問題を含む、完全な製品ライフサイクルと関連プロセスに対処する

**ガバナンス** は、AI リスク管理全体に浸透し、プロセスの他の機能を可能にする横断的機能です。**ガバナンス** の側面、特にコンプライアンスや評価に関連するものは、他の各機能に統合されるべきです。ガバナンスへの注意は、AI システムの寿命と組織の階層にわたる効果的な AI リスク管理のための継続的で内在的な要件です。

強力なガバナンスは、組織のリスク文化を促進するために内部実践と規範を推進し強化できます。統治当局は、組織の使命、目標、価値、文化、リスク許容度を指示する包括的な政策を決定できます。上級リーダーシップは組織内でのリスク管理のトーンを設定し、それと共に組織文化を設定します。管理は AI リスク管理の技術的側面を政策と運営に整合させます。文書化は透明性を向上させ、人間のレビュープロセスを改善し、AI システムチームでの説明責任を強化できます。

**ガバナンス** 機能で説明される構造、システム、プロセス、チームを整備した後、組織はリスクの理解と管理に焦点を当てた目的主導の文化から恩恵を受けるべきです。フレームワークユーザーは、知識、文化、AI アクターからのニーズや期待が時間とともに進化するにつれて、**ガバナンス** 機能を継続して実行することが求められます。

AI リスクのガバナンスに関連する実践は、NIST AI RMF プレイブックで説明されています。表 1 は **ガバナンス** 機能のカテゴリとサブカテゴリを一覧表示しています。

**表 1：GOVERN 機能のカテゴリとサブカテゴリ**

| カテゴリ | サブカテゴリ |
|---------|-------------|
| Govern 1：AI リスクのマッピング、測定、管理に関連する組織全体の政策、プロセス、手順、実践が整備され、透明で、効果的に実装されている。 | Govern 1.1：AI に関わる法的および規制要件が理解され、管理され、文書化されている。 |
| | Govern 1.2：信頼できる AI の特性が組織の政策、プロセス、手順、実践に統合されている。 |
| | Govern 1.3：組織のリスク許容度に基づいて必要なリスク管理活動のレベルを決定するためのプロセス、手順、実践が整備されている。 |
| | Govern 1.4：リスク管理プロセスとその成果が、組織のリスク優先事項に基づく透明な政策、手順、その他の統制を通じて確立されている。 |
| | Govern 1.5：リスク管理プロセスとその成果の継続的監視と定期的レビューが計画され、定期的レビューの頻度の決定を含む組織の役割と責任が明確に定義されている。 |
| | Govern 1.6：AI システムを棚卸しするメカニズムが整備され、組織のリスク優先事項に応じてリソースが配分されている。 |
| | Govern 1.7：AI システムを安全に廃止・段階的廃止し、リスクを増加させたり組織の信頼性を低下させたりしない方法でのプロセスと手順が整備されている。 |
| Govern 2：適切なチームと個人が AI リスクのマッピング、測定、管理のために権限を与えられ、責任を負い、訓練を受けるよう説明責任構造が整備されている。 | Govern 2.1：AI リスクのマッピング、測定、管理に関連する役割と責任および連絡系統が文書化され、組織全体の個人とチームに明確である。 |
| | Govern 2.2：組織の人員とパートナーは、関連する政策、手順、合意と一致して職務と責任を遂行できるよう AI リスク管理訓練を受ける。 |
| | Govern 2.3：組織の執行リーダーシップが AI システム開発と展開に関連するリスクについての決定に責任を負う。 |
| Govern 3：ライフサイクル全体を通じた AI リスクのマッピング、測定、管理において、労働力の多様性、公平性、包摂性、アクセシビリティプロセスが優先されている。 | Govern 3.1：ライフサイクル全体を通じた AI リスクのマッピング、測定、管理に関連する意思決定が、多様なチーム（例：人口統計、分野、経験、専門知識、背景の多様性）によって情報提供されている。 |
| | Govern 3.2：人間-AI 構成と AI システムの監督のための役割と責任を定義し区別するための政策と手順が整備されている。 |
| Govern 4：組織チームが AI リスクを考慮し伝達する文化にコミットしている。 | Govern 4.1：AI システムの設計、開発、展開、使用において潜在的な負の影響を最小化するための批判的思考と安全第一の考え方を育成する組織政策と実践が整備されている。 |
| | Govern 4.2：組織チームは、設計、開発、展開、評価、使用する AI 技術のリスクと潜在的影響を文書化し、より広く影響について伝達する。 |
| | Govern 4.3：AI テスト、インシデントの特定、情報共有を可能にする組織実践が整備されている。 |
| Govern 5：関連する AI アクターとの堅牢な関与のためのプロセスが整備されている。 | Govern 5.1：AI リスクに関連する潜在的な個人的および社会的影響について、AI システムを開発または展開したチーム外部からのフィードバックを収集、考慮、優先順位付け、統合するための組織政策と実践が整備されている。 |
| | Govern 5.2：AI システムを開発または展開したチームが、関連する AI アクターからの裁定されたフィードバックをシステム設計と実装に定期的に組み込むメカニズムが確立されている。 |
| Govern 6：第三者ソフトウェアとデータおよびその他のサプライチェーン問題から生じる AI リスクと利益に対処するための政策と手順が整備されている。 | Govern 6.1：第三者の知的財産権やその他の権利の侵害リスクを含む、第三者エンティティに関連する AI リスクに対処する政策と手順が整備されている。 |
| | Govern 6.2：高リスクと判断される第三者データまたは AI システムの障害やインシデントを処理するための緊急時プロセスが整備されている。 |## 5.2 
マッピング

**マッピング** 機能は、AI システムに関連するリスクをフレーム化するためのコンテキストを確立します。AI ライフサイクルは、多様なアクターセットを含む多くの相互依存する活動で構成されています（図 3 参照）。実際には、プロセスの一部を担当する AI アクターは、多くの場合、他の部分とその関連コンテキストに対する完全な可視性や制御を持ちません。これらの活動間、および関連する AI アクター間の相互依存性は、AI システムの影響を確実に予測することを困難にする可能性があります。例えば、AI システムの目的と目標を特定する初期の決定は、その行動と能力を変更する可能性があり、展開設定の動態（エンドユーザーや影響を受ける個人など）は AI システム決定の影響を形作る可能性があります。その結果、AI ライフサイクルの一つの次元内での最良の意図は、他の後の活動での決定と条件との相互作用を通じて損なわれる可能性があります。この複雑性と可視性のレベルの変動は、リスク管理実践に不確実性をもたらす可能性があります。負のリスクの潜在的源泉を予測、評価、その他の方法で対処することは、この不確実性を軽減し、決定プロセスの完全性を向上させることができます。

**マッピング** 機能を実行する際に収集される情報は、負のリスク防止を可能にし、モデル管理などのプロセスの決定、および AI ソリューションの適切性や必要性についての初期決定を情報提供します。**マッピング** 機能の成果は、**測定** と **管理** 機能の基礎となります。コンテキストの知識と、特定されたコンテキスト内でのリスクの認識なしに、リスク管理は実行が困難です。**マッピング** 機能は、組織のリスクとより広い貢献要因を特定する能力を向上させることを意図しています。

この機能の実装は、多様な内部チームからの視点の組み込みと、AI システムを開発または展開したチーム外部の人々との関与によって向上されます。外部協力者、エンドユーザー、潜在的に影響を受けるコミュニティ、その他との関与は、特定の AI システムのリスクレベル、内部チームの構成、組織政策に基づいて変わる可能性があります。そのような広い視点を収集することは、組織が以下によって負のリスクを積極的に防止し、より信頼できる AI システムを開発するのを支援できます：

* コンテキストを理解する能力の向上
* 使用コンテキストについての仮定の確認
* システムが意図されたコンテキスト内またはその外で機能しない時の認識の可能化
* 既存の AI システムの積極的で有益な使用の特定
* AI と ML プロセスの制限の理解の向上
* 負の影響につながる可能性のある実世界のアプリケーションでの制約の特定
* AI システムの意図された使用に関連する既知で予見可能な負の影響の特定
* 意図された使用を超えた AI システムの使用のリスクの予測

**マッピング** 機能を完了した後、フレームワークユーザーは、AI システムを設計、開発、または展開するかどうかについての初期の進行/中止決定を情報提供するのに十分な AI システム影響についてのコンテキスト知識を持つべきです。進行する決定が下された場合、組織は **ガバナンス** 機能で整備された政策と手順と共に **測定** と **管理** 機能を活用して、AI リスク管理努力を支援すべきです。フレームワークユーザーは、コンテキスト、能力、リスク、利益、潜在的影響が時間とともに進化するにつれて、AI システムに **マッピング** 機能を継続して適用することが求められます。

AI リスクのマッピングに関連する実践は、NIST AI RMF プレイブックで説明されています。表 2 は **マッピング** 機能のカテゴリとサブカテゴリを一覧表示しています。

**表 2：MAP 機能のカテゴリとサブカテゴリ**

| カテゴリ | サブカテゴリ |
|---------|-------------|
| Map 1：コンテキストが確立され理解されている。 | Map 1.1：意図された目的、潜在的に有益な使用、コンテキスト固有の法律、規範と期待、AI システムが展開される予想される設定が理解され文書化されている。考慮事項には以下が含まれる：期待と共に特定のセットまたはタイプのユーザー、個人、コミュニティ、組織、社会、地球へのシステム使用の潜在的な積極的および消極的影響、開発または製品 AI ライフサイクル全体での AI システムの目的、使用、リスクについての仮定と関連制限、関連する TEVV とシステムメトリクス。 |
| | Map 1.2：コンテキスト確立のための学際的 AI アクター、能力、スキル、容量が人口統計の多様性と広いドメインおよびユーザー経験の専門知識を反映し、その参加が文書化されている。学際的協力の機会が優先されている。 |
| | Map 1.3：組織の使命と AI 技術の関連目標が理解され文書化されている。 |
| | Map 1.4：ビジネス価値またはビジネス使用のコンテキストが明確に定義されている、または既存の AI システムを評価する場合は再評価されている。 |
| | Map 1.5：組織のリスク許容度が決定され文書化されている。 |
| | Map 1.6：システム要件（例：「システムはユーザーのプライバシーを尊重すべきである」）が関連する AI アクターから引き出され理解されている。設計決定は AI リスクに対処するために社会技術的含意を考慮に入れている。 |
| Map 2：AI システムの分類が実行されている。 | Map 2.1：AI システムがサポートする特定のタスクとタスクを実装するために使用される方法が定義されている（例：分類器、生成モデル、推薦システム）。 |
| | Map 2.2：AI システムの知識限界とシステム出力が人間によってどのように活用され監督される可能性があるかについての情報が文書化されている。文書化は、関連する AI アクターが決定を下し後続のアクションを取る際に支援するのに十分な情報を提供する。 |
| | Map 2.3：実験設計、データ収集と選択（例：可用性、代表性、適合性）、システム信頼性、構成妥当性に関連するものを含む、科学的完全性と TEVV 考慮事項が特定され文書化されている。 |
| Map 3：AI 能力、対象使用、目標、期待される利益とコストが適切なベンチマークと比較して理解されている。 | Map 3.1：意図された AI システム機能と性能の潜在的利益が検討され文書化されている。 |
| | Map 3.2：組織のリスク許容度に関連する期待されるまたは実現された AI エラーまたはシステム機能と信頼性から生じる非金銭的コストを含む潜在的コストが検討され文書化されている。 |
| | Map 3.3：対象アプリケーション範囲が、システムの能力、確立されたコンテキスト、AI システム分類に基づいて指定され文書化されている。 |
| | Map 3.4：AI システム性能と信頼性に関するオペレーターと実践者の習熟度のプロセス、および関連する技術標準と認証が定義、評価、文書化されている。 |
| | Map 3.5：人間の監督のプロセスが、ガバナンス機能からの組織政策に従って定義、評価、文書化されている。 |
| Map 4：第三者ソフトウェアとデータを含む AI システムのすべてのコンポーネントについてリスクと利益がマッピングされている。 | Map 4.1：第三者データまたはソフトウェアの使用を含む AI 技術とそのコンポーネントの法的リスクをマッピングするアプローチが整備され、従われ、文書化されており、第三者の知的財産権またはその他の権利の侵害リスクも同様である。 |
| | Map 4.2：第三者 AI 技術を含む AI システムのコンポーネントの内部リスク統制が特定され文書化されている。 |
| Map 5：個人、グループ、コミュニティ、組織、社会への影響が特徴付けられている。 | Map 5.1：期待される使用、類似のコンテキストでの AI システムの過去の使用、公開インシデント報告、AI システムを開発または展開したチーム外部からのフィードバック、またはその他のデータに基づく各特定された影響（潜在的に有益および有害の両方）の可能性と規模が特定され文書化されている。 |
| | Map 5.2：関連する AI アクターとの定期的関与をサポートし、積極的、消極的、予期しない影響についてのフィードバックを統合するための実践と人員が整備され文書化されている。 |## 
5.3 測定

**測定** 機能は、AI リスクと関連影響を分析、評価、ベンチマーク、監視するために定量的、定性的、または混合方法のツール、技術、方法論を採用します。これは **マッピング** 機能で特定された AI リスクに関連する知識を使用し、**管理** 機能に情報を提供します。AI システムは展開前にテストされ、運用中に定期的にテストされるべきです。AI リスク測定には、システムの機能性と信頼性の側面の文書化が含まれます。

AI リスクの測定には、信頼できる特性、社会的影響、人間-AI 構成のメトリクスの追跡が含まれます。**測定** 機能で開発または採用されるプロセスには、関連する不確実性の測定、性能ベンチマークとの比較、結果の正式な報告と文書化を伴う厳密なソフトウェアテストと性能評価方法論が含まれるべきです。独立レビューのプロセスは、テストの効果を向上させ、内部バイアスと潜在的な利益相反を軽減できます。

信頼できる特性間でトレードオフが生じる場合、測定は管理決定を情報提供するための追跡可能な基礎を提供します。選択肢には、再較正、影響軽減、設計、開発、生産、または使用からのシステムの除去、および一連の補償、検出、抑止、指示、回復統制が含まれる可能性があります。

**測定** 機能を完了した後、メトリクス、方法、方法論を含む客観的で反復可能またはスケーラブルなテスト、評価、検証、妥当性確認（TEVV）プロセスが整備され、従われ、文書化されます。メトリクスと測定方法論は科学的、法的、倫理的規範に従い、開かれた透明なプロセスで実行されるべきです。新しいタイプの測定、定性的および定量的が開発される必要があるかもしれません。各測定タイプが AI リスクの評価に独特で意味のある情報を提供する程度が考慮されるべきです。フレームワークユーザーは、システム信頼性を包括的に評価し、既存および新興のリスクを特定・追跡し、メトリクスの効果を検証する能力を向上させます。測定成果は、リスク監視と対応努力を支援するために **管理** 機能で活用されます。フレームワークユーザーは、知識、方法論、リスク、影響が時間とともに進化するにつれて、AI システムに **測定** 機能を継続して適用することが求められます。

AI リスクの測定に関連する実践は、NIST AI RMF プレイブックで説明されています。表 3 は **測定** 機能のカテゴリとサブカテゴリを一覧表示しています。

**表 3：MEASURE 機能のカテゴリとサブカテゴリ**

| カテゴリ | サブカテゴリ |
|---------|-------------|
| Measure 1：適切な方法とメトリクスが特定され適用されている。 | Measure 1.1：マップ機能中に列挙された AI リスクの測定のためのアプローチとメトリクスが、最も重要な AI リスクから開始して実装のために選択されている。測定されない、または測定できないリスクまたは信頼性特性が適切に文書化されている。 |
| | Measure 1.2：AI メトリクスの適切性と既存統制の効果が、エラーの報告と影響を受けるコミュニティへの潜在的影響を含めて定期的に評価され更新されている。 |
| | Measure 1.3：システムの最前線開発者として従事しなかった内部専門家および/または独立評価者が定期的な評価と更新に関与している。ドメイン専門家、ユーザー、AI システムを開発または展開したチーム外部の AI アクター、影響を受けるコミュニティが、組織のリスク許容度に応じて必要に応じて評価のサポートで相談されている。 |
| Measure 2：AI システムが信頼できる特性について評価されている。 | Measure 2.1：TEVV 中に使用されるテストセット、メトリクス、ツールについての詳細が文書化されている。 |
| | Measure 2.2：人間の被験者を含む評価が適用要件（人間の被験者保護を含む）を満たし、関連人口を代表している。 |
| | Measure 2.3：AI システム性能または保証基準が展開設定に類似した条件について定性的または定量的に測定され実証されている。測定が文書化されている。 |
| | Measure 2.4：マップ機能で特定された AI システムとそのコンポーネントの機能と行動が生産時に監視されている。 |
| | Measure 2.5：展開される AI システムが有効で信頼できることが実証されている。技術が開発された条件を超えた一般化可能性の制限が文書化されている。 |
| | Measure 2.6：AI システムがマップ機能で特定された安全リスクについて定期的に評価されている。展開される AI システムが安全であることが実証され、その残存負リスクがリスク許容度を超えず、特に知識限界を超えて動作させられた場合に安全に失敗できる。安全メトリクスはシステム信頼性と堅牢性、リアルタイム監視、AI システム障害の応答時間を反映している。 |
| | Measure 2.7：マップ機能で特定された AI システムのセキュリティと回復力が評価され文書化されている。 |
| | Measure 2.8：マップ機能で特定された透明性と説明責任に関連するリスクが検討され文書化されている。 |
| | Measure 2.9：AI モデルが説明、検証、文書化され、AI システム出力がマップ機能で特定されたコンテキスト内で解釈されて、責任ある使用とガバナンスを情報提供している。 |
| | Measure 2.10：マップ機能で特定された AI システムのプライバシーリスクが検討され文書化されている。 |
| | Measure 2.11：マップ機能で特定された公平性とバイアスが評価され結果が文書化されている。 |
| | Measure 2.12：マップ機能で特定された AI モデル訓練と管理活動の環境影響と持続可能性が評価され文書化されている。 |
| | Measure 2.13：測定機能で採用された TEVV メトリクスとプロセスの効果が評価され文書化されている。 |
| Measure 3：特定された AI リスクを時間をかけて追跡するメカニズムが整備されている。 | Measure 3.1：展開されたコンテキストでの意図された実際の性能などの要因に基づいて、既存、予期しない、新興の AI リスクを定期的に特定し追跡するためのアプローチ、人員、文書化が整備されている。 |
| | Measure 3.2：現在利用可能な測定技術を使用して AI リスクを評価することが困難な設定、またはメトリクスがまだ利用できない設定について、リスク追跡アプローチが考慮されている。 |
| | Measure 3.3：エンドユーザーと影響を受けるコミュニティが問題を報告しシステム成果に異議を申し立てるためのフィードバックプロセスが確立され、AI システム評価メトリクスに統合されている。 |
| Measure 4：測定の効果についてのフィードバックが収集され評価されている。 | Measure 4.1：AI リスクを特定するための測定アプローチが展開コンテキストに接続され、ドメイン専門家とその他のエンドユーザーとの相談を通じて情報提供されている。アプローチが文書化されている。 |
| | Measure 4.2：展開コンテキストと AI ライフサイクル全体での AI システム信頼性に関する測定結果が、システムが意図された通りに一貫して実行されているかどうかを検証するためにドメイン専門家と関連 AI アクターからの入力によって情報提供されている。結果が文書化されている。 |
| | Measure 4.3：影響を受けるコミュニティを含む関連 AI アクターとの相談、およびコンテキスト関連リスクと信頼性特性についてのフィールドデータに基づく測定可能な性能改善または低下が特定され文書化されている。 |##
 5.4 管理

**管理** 機能は、**ガバナンス** 機能によって定義された通り、定期的にマッピングされ測定されたリスクにリスクリソースを配分することを伴います。リスク処理は、インシデントまたはイベントに対応し、回復し、伝達するための計画を含みます。

**ガバナンス** で確立され **マッピング** で実行された専門家相談と関連 AI アクターからの入力から得られたコンテキスト情報は、この機能でシステム障害と負の影響の可能性を減少させるために活用されます。**ガバナンス** で確立され **マッピング** と **測定** で活用された体系的文書化実践は、AI リスク管理努力を強化し、透明性と説明責任を向上させます。新興リスクを評価するプロセスが整備され、継続的改善のメカニズムと共に存在します。

**管理** 機能を完了した後、リスクを優先順位付けし定期的監視と改善のための計画が整備されます。フレームワークユーザーは、展開された AI システムのリスクを管理し、評価され優先順位付けされたリスクに基づいてリスク管理リソースを配分する能力を向上させます。フレームワークユーザーは、方法、コンテキスト、リスク、関連 AI アクターからのニーズや期待が時間とともに進化するにつれて、展開された AI システムに **管理** 機能を継続して適用することが求められます。

AI リスクの管理に関連する実践は、NIST AI RMF プレイブックで説明されています。表 4 は **管理** 機能のカテゴリとサブカテゴリを一覧表示しています。

**表 4：MANAGE 機能のカテゴリとサブカテゴリ**

| カテゴリ | サブカテゴリ |
|---------|-------------|
| Manage 1：MAP と MEASURE 機能からの評価とその他の分析出力に基づく AI リスクが優先順位付けされ、対応され、管理されている。 | Manage 1.1：AI システムが意図された目的と述べられた目標を達成するかどうか、およびその開発または展開を進めるべきかどうかの決定が行われている。 |
| | Manage 1.2：文書化された AI リスクの処理が影響、可能性、利用可能なリソースまたは方法に基づいて優先順位付けされている。 |
| | Manage 1.3：マップ機能によって特定された高優先度と判断された AI リスクへの対応が開発、計画、文書化されている。リスク対応選択肢には、軽減、移転、回避、受容が含まれる可能性がある。 |
| | Manage 1.4：AI システムの下流取得者とエンドユーザーの両方への負の残存リスク（すべての軽減されていないリスクの合計として定義）が文書化されている。 |
| Manage 2：AI の利益を最大化し負の影響を最小化する戦略が、関連 AI アクターからの入力によって情報提供され、計画、準備、実装、文書化されている。 | Manage 2.1：AI リスクを管理するために必要なリソースが、潜在的影響の規模または可能性を減少させるための実行可能な非 AI 代替システム、アプローチ、または方法と共に考慮されている。 |
| | Manage 2.2：展開された AI システムの価値を維持するメカニズムが整備され適用されている。 |
| | Manage 2.3：以前に未知のリスクが特定された時にそれに対応し回復するための手順が従われている。 |
| | Manage 2.4：意図された使用と一致しない性能または成果を示す AI システムを優先、離脱、または非活性化するメカニズムが整備され適用され、責任が割り当てられ理解されている。 |
| Manage 3：第三者エンティティからの AI リスクと利益が管理されている。 | Manage 3.1：第三者リソースからの AI リスクと利益が定期的に監視され、リスク統制が適用され文書化されている。 |
| | Manage 3.2：開発に使用される事前訓練モデルが AI システムの定期的監視と保守の一部として監視されている。 |
| Manage 4：特定され測定された AI リスクに対する対応と回復、コミュニケーション計画を含むリスク処理が文書化され定期的に監視されている。 | Manage 4.1：ユーザーとその他の関連 AI アクターからの入力を捕捉し評価するメカニズム、異議申立てと優先、廃止、インシデント対応、回復、変更管理を含む展開後 AI システム監視計画が実装されている。 |
| | Manage 4.2：継続的改善のための測定可能な活動が AI システム更新に統合され、関連 AI アクターを含む利害関係者との定期的関与を含んでいる。 |
| | Manage 4.3：インシデントとエラーが影響を受けるコミュニティを含む関連 AI アクターに伝達されている。インシデントとエラーの追跡、対応、回復のプロセスが従われ文書化されている。 |

## 参照 URL

https://airc.nist.gov/airmf-resources/airmf/5-sec-core/

---

# 6. NIST AI RMF プロファイル - 日本語翻訳

## 翻訳概要

本文書は、NIST AI Risk Management Framework (AI RMF) のプロファイルに関するセクションの日本語翻訳です。AI RMF プロファイルは、特定の用途や設定に応じて AI RMF の機能、カテゴリ、サブカテゴリを実装するためのガイドラインを提供します。本セクションでは、ユースケースプロファイル、時間的プロファイル、分野横断プロファイルの 3 つの主要なプロファイルタイプについて説明しています。

## 6 AI RMF プロファイル

AI RMF ユースケースプロファイルは、フレームワーク利用者の要件、リスク許容度、リソースに基づいて、特定の設定やアプリケーションに対する AI RMF の機能、カテゴリ、サブカテゴリの実装です。例えば、AI RMF 採用プロファイルや AI RMF 公正住宅プロファイルなどがあります。プロファイルは、AI ライフサイクルの様々な段階や、特定の分野、技術、最終用途アプリケーションにおいて、リスクがどのように管理できるかについての洞察を示し、提供することができます。AI RMF プロファイルは、組織が自らの目標とよく整合し、法的・規制要件とベストプラクティスを考慮し、リスク管理の優先順位を反映した AI リスクの最適な管理方法を決定する際に支援します。

AI RMF 時間的プロファイルは、特定の分野、業界、組織、またはアプリケーションコンテキスト内における特定の AI リスク管理活動の現在の状態または望ましい目標状態のいずれかの記述です。AI RMF 現在プロファイルは、現在の成果の観点から、AI が現在どのように管理されているか、および関連するリスクを示します。目標プロファイルは、望ましいまたは目標とする AI リスク管理目標を達成するために必要な成果を示します。

現在プロファイルと目標プロファイルを比較することで、AI リスク管理目標を満たすために対処すべきギャップが明らかになる可能性があります。これらのギャップに対処し、特定のカテゴリやサブカテゴリにおける成果を達成するためのアクションプランを策定することができます。ギャップ緩和の優先順位付けは、利用者のニーズとリスク管理プロセスによって推進されます。このリスクベースのアプローチにより、フレームワーク利用者は自らのアプローチを他のアプローチと比較し、費用対効果が高く優先順位付けされた方法で AI リスク管理目標を達成するために必要なリソース（例：人員配置、資金調達）を測定することも可能になります。

AI RMF 分野横断プロファイルは、ユースケースや分野を超えて使用できるモデルやアプリケーションのリスクをカバーします。分野横断プロファイルは、大規模言語モデル、クラウドベースサービス、調達の使用など、分野共通の活動やビジネスプロセスに対するリスクのガバナンス、マッピング、測定、管理方法もカバーできます。

本フレームワークは、実装における柔軟性を可能にするため、プロファイルテンプレートを規定していません。

---

**参照 URL:** https://airc.nist.gov/airmf-resources/airmf/6-sec-profile/