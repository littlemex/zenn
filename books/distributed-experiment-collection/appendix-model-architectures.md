---
title: "Appendix: Model Architectures"
emoji: "ğŸ”§"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["aws", "sagemaker", "hyperpod", "distributed", "infrastructure"]
free: false
---

::::details MoE ã§ All-to-All é€šä¿¡ãŒå¿…è¦ã«ãªã‚‹ç†ç”±

## æ¦‚è¦

Mixture-of-Expertsï¼ˆMoEï¼‰ãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦ All-to-All é€šä¿¡ãŒå¿…è¦ã«ãªã‚‹ç†ç”±ã‚’ã€å¾“æ¥ã® Dense ãƒ¢ãƒ‡ãƒ«ã¨ã®æ¯”è¼ƒã‚’é€šã˜ã¦ç†è§£ã—ã¾ã—ã‚‡ã†ã€‚

## Dense ãƒ¢ãƒ‡ãƒ« vs MoE ãƒ¢ãƒ‡ãƒ«ã®åŸºæœ¬çš„ãªé•ã„

### Dense ãƒ¢ãƒ‡ãƒ«ã®å‹•ä½œ

```mermaid
flowchart LR
    Input[å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³] --> GPU1[GPU_1<br/>FFN]
    Input --> GPU2[GPU_2<br/>FFN]
    Input --> GPU3[GPU_3<br/>FFN]
    
    GPU1 --> Out1[å‡ºåŠ›]
    GPU2 --> Out2[å‡ºåŠ›]
    GPU3 --> Out3[å‡ºåŠ›]
    
    style Input fill:#e1f5ff
    style GPU1 fill:#fff4e1
    style GPU2 fill:#fff4e1
    style GPU3 fill:#fff4e1
```

**ç‰¹å¾´**
- å„ GPU ã§åŒã˜ FFN ã‚’å®Ÿè¡Œï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯è¤‡è£½ï¼‰
- All-reduce ã§ã®å‹¾é…åŒæœŸã®ã¿
- é€šä¿¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯ã‚·ãƒ³ãƒ—ãƒ«

### MoE ãƒ¢ãƒ‡ãƒ«ã®å‹•ä½œ

```mermaid
flowchart TB
    Input[å…¥åŠ›ãƒˆãƒ¼ã‚¯ãƒ³] --> Router[Router]
    
    Router -.->|çŒ«| E1[GPU_1<br/>Expert_1]
    Router -.->|ãŒ| E2[GPU_2<br/>Expert_2]
    Router -.->|å¥½ã| E3[GPU_3<br/>Expert_3]
    
    E1 --> Combine[çµ±åˆ]
    E2 --> Combine
    E3 --> Combine
    Combine --> Output[å‡ºåŠ›]
    
    style Router fill:#ffe1e1
    style E1 fill:#d4ffd4
    style E2 fill:#ffd4ff
    style E3 fill:#ffffd4
```

**ç‰¹å¾´**
- å„ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãŒç•°ãªã‚‹åˆ†é‡ã«ç‰¹åŒ–
- ãƒˆãƒ¼ã‚¯ãƒ³ã‚’é©åˆ‡ãªã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã«å‹•çš„ã«ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
- **All-to-All é€šä¿¡ãŒå¿…è¦**ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ã®åˆ†æ•£ã¨åé›†ï¼‰

## All-to-All Dispatch: ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã«é€ã‚‹

```mermaid
sequenceDiagram
    participant GPU1 as GPU 1<br/>[Expert A, B]
    participant GPU2 as GPU 2<br/>[Expert C, D]
    participant GPU3 as GPU 3<br/>[Expert E, F]
    
    Note over GPU1,GPU3: å„GPUãŒç‹¬è‡ªã«ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŒã£ã¦ã„ã‚‹
    
    GPU1->>GPU1: Token1 to Expert A (local)
    GPU1->>GPU2: Token2 to Expert C
    GPU1->>GPU3: Token3 to Expert E
    
    GPU2->>GPU1: Token4 to Expert B
    GPU2->>GPU2: Token5 to Expert D (local)
    GPU2->>GPU3: Token6 to Expert F
    
    GPU3->>GPU1: Token7 to Expert A
    GPU3->>GPU2: Token8 to Expert C  
    GPU3->>GPU3: Token9 to Expert E (local)
    
    Note over GPU1,GPU3: All-to-All Dispatch å®Œäº†<br/>å„GPUãŒæ‹…å½“ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®<br/>ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å—ä¿¡
```

## Expert å‡¦ç† + All-to-All Combine: çµæœã‚’å…ƒã«æˆ»ã™

```mermaid
sequenceDiagram
    participant GPU1 as GPU 1<br/>[Expert A, B]
    participant GPU2 as GPU 2<br/>[Expert C, D]  
    participant GPU3 as GPU 3<br/>[Expert E, F]
    
    Note over GPU1,GPU3: å„ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãŒæ‹…å½“ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å‡¦ç†
    
    GPU1->>GPU1: Expert A processes Token1, Token7
    GPU1->>GPU1: Expert B processes Token4
    
    GPU2->>GPU2: Expert C processes Token2, Token8
    GPU2->>GPU2: Expert D processes Token5
    
    GPU3->>GPU3: Expert E processes Token3, Token9
    GPU3->>GPU3: Expert F processes Token6
    
    Note over GPU1,GPU3: å‡¦ç†çµæœã‚’å…ƒã®ãƒ‡ãƒã‚¤ã‚¹ã«æˆ»ã™<br/>All-to-All Combine
    
    GPU1->>GPU1: Token1 result (local)
    GPU2->>GPU1: Token2 result
    GPU3->>GPU1: Token3 result
    
    GPU1->>GPU2: Token4 result
    GPU2->>GPU2: Token5 result (local)
    GPU3->>GPU2: Token6 result
    
    GPU1->>GPU3: Token7 result
    GPU2->>GPU3: Token8 result
    GPU3->>GPU3: Token9 result (local)
    
    Note over GPU1,GPU3: All-to-All Combine å®Œäº†<br/>å„GPUãŒå…ƒã®ãƒˆãƒ¼ã‚¯ãƒ³çµæœã‚’å–å¾—
```

## é€šä¿¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è©³ç´°åˆ†æ

### Forward Pass ã§ã® All-to-All ãƒ‘ã‚¿ãƒ¼ãƒ³

```mermaid
flowchart TD
    subgraph Phase1["Phase 1: Token Dispatch All-to-All"]
        subgraph Before["é€ä¿¡å‰ã®çŠ¶æ…‹"]
            GPU1Before[GPU 1<br/>Token A to Expert 3<br/>Token B to Expert 1<br/>Token C to Expert 2]
            GPU2Before[GPU 2<br/>Token D to Expert 1<br/>Token E to Expert 3<br/>Token F to Expert 2]  
            GPU3Before[GPU 3<br/>Token G to Expert 2<br/>Token H to Expert 1<br/>Token I to Expert 3]
        end
        
        Before --> DispatchComm[All-to-All<br/>Communication]
        
        subgraph After["å—ä¿¡å¾Œã®çŠ¶æ…‹"]
            GPU1After[GPU 1 Expert 1<br/>Token B local<br/>Token D from GPU2<br/>Token H from GPU3]
            GPU2After[GPU 2 Expert 2<br/>Token C from GPU1<br/>Token F local<br/>Token G from GPU3]
            GPU3After[GPU 3 Expert 3<br/>Token A from GPU1<br/>Token E from GPU2<br/>Token I local]
        end
        
        DispatchComm --> After
    end
    
    subgraph Phase2["Phase 2: Expert Processing"]
        GPU1After --> Process1[Expert 1<br/>å‡¦ç†]
        GPU2After --> Process2[Expert 2<br/>å‡¦ç†]
        GPU3After --> Process3[Expert 3<br/>å‡¦ç†]
    end
    
    subgraph Phase3["Phase 3: Result Combine All-to-All"]
        Process1 --> CombineComm[All-to-All<br/>Communication]
        Process2 --> CombineComm
        Process3 --> CombineComm
        
        subgraph Final["æœ€çµ‚çŠ¶æ…‹"]
            GPU1Final[GPU 1<br/>Token A result<br/>Token B result<br/>Token C result]
            GPU2Final[GPU 2<br/>Token D result<br/>Token E result<br/>Token F result]
            GPU3Final[GPU 3<br/>Token G result<br/>Token H result<br/>Token I result]
        end
        
        CombineComm --> Final
    end
    
    classDef gpu1Style fill:#e1f5ff,stroke:#333,stroke-width:2px
    classDef gpu2Style fill:#fff4e1,stroke:#333,stroke-width:2px
    classDef gpu3Style fill:#f0fff4,stroke:#333,stroke-width:2px
    classDef commStyle fill:#ffe1e1,stroke:#333,stroke-width:2px
    classDef processStyle fill:#d4ffd4,stroke:#333,stroke-width:2px
    
    class GPU1Before,GPU1After,GPU1Final gpu1Style
    class GPU2Before,GPU2After,GPU2Final gpu2Style
    class GPU3Before,GPU3After,GPU3Final gpu3Style
    class DispatchComm,CombineComm commStyle
    class Process1,Process2,Process3 processStyle
```

## ãªãœ All-to-All ãŒå¿…è¦ãªã®ã‹

### Dense ãƒ¢ãƒ‡ãƒ«ã§ã®é€šä¿¡ãƒ‘ã‚¿ãƒ¼ãƒ³

**Data Parallelism ã®ã¿**
- å„ GPU ãŒåŒã˜ãƒ¢ãƒ‡ãƒ«ã®è¤‡è£½ã‚’æŒã¤
- å‹¾é…è¨ˆç®—å¾Œã® All-reduce ã®ã¿
- é€šä¿¡é »åº¦: ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµ‚äº†æ™‚ã« 1 å›

### MoE ãƒ¢ãƒ‡ãƒ«ã§ã®é€šä¿¡ãƒ‘ã‚¿ãƒ¼ãƒ³

**Expert Parallelism + Data Parallelism**
- å„ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãŒç‰¹å®šã® GPU ã«ã®ã¿å­˜åœ¨
- ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç‰©ç†çš„ã«é©åˆ‡ãª GPU ã«ç§»å‹•
- å‡¦ç†çµæœã‚’å…ƒã® GPU ã«æˆ»ã™
- **é€šä¿¡é »åº¦: å„ MoE å±¤ã§ 2 å›ï¼ˆDispatch + Combineï¼‰**

## å®Ÿéš›ã®æ€§èƒ½ãƒ‡ãƒ¼ã‚¿

# DeepSeek-V3 ã§ã®å®Ÿæ¸¬å€¤ï¼ˆDeepEP ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼‰

### é€šå¸¸ã®è¨“ç·´ãƒ»æ¨è«–ãƒ—ãƒªãƒ•ã‚£ãƒ«ç”¨ã‚«ãƒ¼ãƒãƒ«

**ãƒ†ã‚¹ãƒˆç’°å¢ƒ**: H800ï¼ˆ~160 GB/s NVLinkï¼‰+ CX7 InfiniBand 400 Gb/sï¼ˆ~50 GB/s RDMAï¼‰
**è¨­å®š**: 4096 tokens/batch, 7168 hidden, top-4 groups, top-8 experts, FP8 dispatching + BF16 combining

| ã‚¿ã‚¤ãƒ— | Expert ä¸¦åˆ—åº¦ | Dispatch å¸¯åŸŸå¹… | Combine å¸¯åŸŸå¹… | ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ |
|------|-------------|---------------|---------------|------------|
| **Intranode** | 8 | 153 GB/s | 158 GB/s | NVLink |
| **Internode** | 16 | 43 GB/s | 43 GB/s | RDMA |
| **Internode** | 32 | 58 GB/s | 57 GB/s | RDMA |  
| **Internode** | 64 | 51 GB/s | 50 GB/s | RDMA |

### ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·æ¨è«–ãƒ‡ã‚³ãƒ¼ãƒ‰ç”¨ã‚«ãƒ¼ãƒãƒ«

**è¨­å®š**: 128 tokens/batch, 7168 hidden, top-8 experts, FP8 dispatching + BF16 combining

| Expert ä¸¦åˆ—åº¦ | Dispatch | Combine | RDMA å¸¯åŸŸå¹… |
|-------------|----------|---------|-------------|
| 8 | 77 Î¼s | 114 Î¼s | 98-127 GB/s |
| 16 | 118 Î¼s | 195 Î¼s | 63-74 GB/s |
| 32 | 155 Î¼s | 273 Î¼s | 48-53 GB/s |
| 64 | 173 Î¼s | 314 Î¼s | 43-46 GB/s |
| 128 | 192 Î¼s | 369 Î¼s | 39 GB/s |
| 256 | 194 Î¼s | 360 Î¼s | 39-40 GB/s |

**é‡è¦ãªè¦³å¯Ÿ**
- Intranodeï¼ˆNVLinkï¼‰ã¯ Internodeï¼ˆRDMAï¼‰ã‚ˆã‚Šç´„ 3-4 å€é«˜é€Ÿ
- Expert ä¸¦åˆ—åº¦ãŒå¢—åŠ ã™ã‚‹ã¨ã€ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã‚‚å¢—åŠ 
- å¤§è¦æ¨¡ä¸¦åˆ—åŒ–ã§ã¯ RDMA å¸¯åŸŸå¹…ãŒãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã«ãªã‚‹

## é€šä¿¡é‡ã®åˆ†æ

### MoE vs Dense ãƒ¢ãƒ‡ãƒ«ã®é€šä¿¡é‡æ¯”è¼ƒ

```mermaid
flowchart LR
    subgraph Dense["Dense ãƒ¢ãƒ‡ãƒ«"]
        DenseComp[è¨ˆç®—<br/>å…¨GPUã§åŒã˜FFN]
        DenseComm[é€šä¿¡<br/>All-reduce<br/>ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµ‚äº†æ™‚ã®ã¿]
        
        DenseComp --> DenseComm
    end
    
    subgraph MoE["MoE ãƒ¢ãƒ‡ãƒ«"]
        MoEDispatch[é€šä¿¡1<br/>All-to-All Dispatch<br/>å„MoEå±¤ã§å®Ÿè¡Œ]
        MoEComp[è¨ˆç®—<br/>å„GPUã§ç•°ãªã‚‹Expert]
        MoECombine[é€šä¿¡2<br/>All-to-All Combine<br/>å„MoEå±¤ã§å®Ÿè¡Œ]
        MoEAllReduce[é€šä¿¡3<br/>All-reduce<br/>ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµ‚äº†æ™‚]
        
        MoEDispatch --> MoEComp
        MoEComp --> MoECombine
        MoECombine --> MoEAllReduce
    end
    
    classDef compStyle fill:#e1f5ff,stroke:#333,stroke-width:2px
    classDef commStyle fill:#ffe1e1,stroke:#333,stroke-width:2px
    classDef allReduceStyle fill:#ffd4ff,stroke:#333,stroke-width:2px
    
    class DenseComp,MoEComp compStyle
    class DenseComm,MoEDispatch,MoECombine commStyle
    class MoEAllReduce allReduceStyle
```

**é€šä¿¡é »åº¦ã®é•ã„**
- **Dense ãƒ¢ãƒ‡ãƒ«**: ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµ‚äº†æ™‚ã« All-reduce Ã— 1 å›
- **MoE ãƒ¢ãƒ‡ãƒ«**: å„ MoE å±¤ã§ All-to-All Ã— 2 å› + ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµ‚äº†æ™‚ã« All-reduce Ã— 1 å›

### é€šä¿¡é‡ã®è¨ˆç®—ä¾‹

**è¨­å®šä¾‹**: 8 å±¤ã® MoE ãƒ¢ãƒ‡ãƒ«ã€64 GPUã€4096 tokens/batch

```mermaid
flowchart TD
    subgraph Calculation["é€šä¿¡é‡ã®è¨ˆç®—"]
        TokenSize[Token ã‚µã‚¤ã‚º<br/>4096 tokens Ã— 7168 hidden<br/>Ã— 2 bytes FP16<br/>= 58.7 MB]
        
        PerLayer[1 MoE å±¤ã‚ãŸã‚Š<br/>Dispatch: 58.7 MB<br/>Combine: 58.7 MB<br/>åˆè¨ˆ: 117.4 MB]
        
        Total[å…¨ä½“é€šä¿¡é‡<br/>117.4 MB Ã— 8 layers<br/>= 939.2 MB per iteration]
        
        TokenSize --> PerLayer
        PerLayer --> Total
        
        Compare[Dense ãƒ¢ãƒ‡ãƒ«æ¯”è¼ƒ<br/>All-reduce ã®ã¿<br/>ç´„ 140 MB per iteration<br/>MoE ã¯ç´„ 7 å€ã®é€šä¿¡é‡]
        
        Total --> Compare
    end
    
    classDef tokenStyle fill:#e1f5ff,stroke:#333,stroke-width:2px
    classDef layerStyle fill:#fff4e1,stroke:#333,stroke-width:2px
    classDef totalStyle fill:#f0fff4,stroke:#333,stroke-width:2px
    classDef compareStyle fill:#ffe1e1,stroke:#333,stroke-width:2px
    
    class TokenSize tokenStyle
    class PerLayer layerStyle
    class Total totalStyle
    class Compare compareStyle
```

## All-to-All ãŒå¿…è¦ã«ãªã‚‹å…·ä½“çš„ãªç†ç”±

### 1. Expert Parallelism ã«ã‚ˆã‚‹ç‰©ç†çš„åˆ†æ•£

```mermaid
flowchart TB
    subgraph Scenario["ã‚·ãƒŠãƒªã‚ª: 4ã¤ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¨3ã¤ã®GPU"]
        subgraph InitialState["åˆæœŸçŠ¶æ…‹ï¼šå„GPUãŒç•°ãªã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æŒã¤"]
            GPU1Init["GPU 1<br/>Token A: çŒ«<br/>Token B: èµ°ã‚‹"]
            GPU2Init["GPU 2<br/>Token C: ã§ã™<br/>Token D: ç¾ã—ã„"]
            GPU3Init["GPU 3<br/>ï¼ˆå‡¦ç†ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ãªã—ï¼‰"]
        end
        
        subgraph Routing["Router ã«ã‚ˆã‚‹åˆ¤å®š"]
            Router["ã‚²ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯<br/>ã®åˆ¤å®šçµæœ"]
            TokenA["Token Aï¼ˆçŒ«ï¼‰<br/>â†’ Expert 1ï¼ˆå‹•ç‰©ï¼‰"]
            TokenB["Token Bï¼ˆèµ°ã‚‹ï¼‰<br/>â†’ Expert 2ï¼ˆå‹•ä½œï¼‰"]
            TokenC["Token Cï¼ˆã§ã™ï¼‰<br/>â†’ Expert 3ï¼ˆæ–‡æ³•ï¼‰"]
            TokenD["Token Dï¼ˆç¾ã—ã„ï¼‰<br/>â†’ Expert 2ï¼ˆå‹•ä½œï¼‰"]
            
            Router --> TokenA
            Router --> TokenB
            Router --> TokenC
            Router --> TokenD
        end
        
        subgraph ExpertLocation["ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®é…ç½®"]
            GPU1Expert["GPU 1<br/>Expert 1ï¼ˆå‹•ç‰©ï¼‰"]
            GPU2Expert["GPU 2<br/>Expert 2ï¼ˆå‹•ä½œï¼‰"]
            GPU3Expert["GPU 3<br/>Expert 3ï¼ˆæ–‡æ³•ï¼‰"]
        end
        
        subgraph Problem["å•é¡Œï¼šãƒˆãƒ¼ã‚¯ãƒ³ã¨ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãŒ<br/>ç•°ãªã‚‹GPUã«å­˜åœ¨"]
            TokenA -.->|å¿…è¦| GPU1Expert
            TokenB -.->|å¿…è¦| GPU2Expert
            TokenC -.->|å¿…è¦| GPU3Expert
            TokenD -.->|å¿…è¦| GPU2Expert
            
            ProblemText["Token A ã¯ GPU 1 ã«ã‚ã‚‹ãŒ<br/>Token B ã¯ GPU 1 ã«ã‚ã‚‹ãŒ Expert 2 ã¯ GPU 2<br/>Token C ã¯ GPU 2 ã«ã‚ã‚‹ãŒ Expert 3 ã¯ GPU 3<br/>Token D ã¯ GPU 2 ã«ã‚ã‚‹ãŒ Expert 2 ã‚‚ GPU 2"]
        end
        
        InitialState --> Routing
        Routing --> ExpertLocation
        ExpertLocation --> Problem
    end
    
    classDef gpu1Style fill:#e1f5ff,stroke:#333,stroke-width:2px
    classDef gpu2Style fill:#fff4e1,stroke:#333,stroke-width:2px
    classDef gpu3Style fill:#f0fff4,stroke:#333,stroke-width:2px
    classDef routerStyle fill:#ffe1e1,stroke:#333,stroke-width:2px
    classDef problemStyle fill:#ffd4d4,stroke:#333,stroke-width:2px
    
    class GPU1Init,GPU1Expert,TokenA gpu1Style
    class GPU2Init,GPU2Expert,TokenB,TokenD gpu2Style
    class GPU3Init,GPU3Expert,TokenC gpu3Style
    class Router,TokenA,TokenB,TokenC,TokenD routerStyle
    class ProblemText problemStyle
```

### 2. All-to-All Dispatch ã«ã‚ˆã‚‹è§£æ±º

```mermaid
flowchart TB
    subgraph DispatchSolution["All-to-All Dispatch ã«ã‚ˆã‚‹è§£æ±º"]
        subgraph SendPhase["é€ä¿¡ãƒ•ã‚§ãƒ¼ã‚º"]
            GPU1Send["GPU 1 ã‹ã‚‰é€ä¿¡<br/>Token A â†’ GPU 1ï¼ˆlocalï¼‰<br/>Token B â†’ GPU 2"]
            GPU2Send["GPU 2 ã‹ã‚‰é€ä¿¡<br/>Token C â†’ GPU 3<br/>Token D â†’ GPU 2ï¼ˆlocalï¼‰"]
            GPU3Send["GPU 3 ã‹ã‚‰é€ä¿¡<br/>ï¼ˆé€ä¿¡ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒ³ãªã—ï¼‰"]
        end
        
        subgraph AllToAll["All-to-All Communication"]
            Communication[ã™ã¹ã¦ã®GPUé–“ã§<br/>åŒæ™‚ã«ãƒˆãƒ¼ã‚¯ãƒ³ã‚’äº¤æ›]
        end
        
        subgraph ReceivePhase["å—ä¿¡ãƒ•ã‚§ãƒ¼ã‚º"]
            GPU1Receive["GPU 1 ãŒå—ä¿¡<br/>Token Aï¼ˆlocalï¼‰<br/>Expert 1 ã§å‡¦ç†æº–å‚™å®Œäº†"]
            GPU2Receive["GPU 2 ãŒå—ä¿¡<br/>Token Bï¼ˆfrom GPU 1ï¼‰<br/>Token Dï¼ˆlocalï¼‰<br/>Expert 2 ã§å‡¦ç†æº–å‚™å®Œäº†"]
            GPU3Receive["GPU 3 ãŒå—ä¿¡<br/>Token Cï¼ˆfrom GPU 2ï¼‰<br/>Expert 3 ã§å‡¦ç†æº–å‚™å®Œäº†"]
        end
        
        SendPhase --> AllToAll
        AllToAll --> ReceivePhase
        
        subgraph Result["çµæœï¼šå„GPUãŒæ‹…å½“ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®<br/>ã™ã¹ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å–å¾—"]
            Solved["å…¨ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒ<br/>é©åˆ‡ãªã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®å ´æ‰€ã«é…ç½®"]
        end
        
        ReceivePhase --> Result
    end
    
    classDef sendStyle fill:#e1f5ff,stroke:#333,stroke-width:2px
    classDef commStyle fill:#ffe1e1,stroke:#333,stroke-width:2px
    classDef receiveStyle fill:#f0fff4,stroke:#333,stroke-width:2px
    classDef resultStyle fill:#d4ffd4,stroke:#333,stroke-width:2px
    
    class GPU1Send,GPU2Send,GPU3Send sendStyle
    class Communication commStyle
    class GPU1Receive,GPU2Receive,GPU3Receive receiveStyle
    class Solved resultStyle
```

## ãªãœã“ã®è¨­è¨ˆãŒåŠ¹ç‡çš„ãªã®ã‹

### è¨ˆç®—åŠ¹ç‡ã®å‘ä¸Š

MoE ã§ã¯ä»¥ä¸‹ã®ç†ç”±ã«ã‚ˆã‚Šã€é€šä¿¡ã‚³ã‚¹ãƒˆã‚’ä¸Šå›ã‚‹è¨ˆç®—åŠ¹ç‡ãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚

**å¤§ããªè¡Œåˆ—æ¼”ç®—ã¸ã®é›†ç´„**
- å„ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãŒè¤‡æ•° GPU ã‹ã‚‰ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å‡¦ç†
- å°ã•ãªè¡Œåˆ—æ¼”ç®—ã‚’å¤§ããªè¡Œåˆ—æ¼”ç®—ã«é›†ç´„
- GPU ã®ä¸¦åˆ—è¨ˆç®—èƒ½åŠ›ã‚’æœ€å¤§é™æ´»ç”¨

**ã‚¹ãƒ‘ãƒ¼ã‚¹ ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³**
- å„ãƒˆãƒ¼ã‚¯ãƒ³ã¯å…¨ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã§ã¯ãªã top-k ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®ã¿ã‚’ä½¿ç”¨
- è¨ˆç®—é‡ã¯ç·šå½¢å¢—åŠ ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã¯æŒ‡æ•°çš„å¢—åŠ å¯èƒ½

## å®Ÿè£…ä¸Šã®èª²é¡Œã¨è§£æ±ºç­–

### Load Balancing ã®å•é¡Œ

```mermaid
flowchart TB
    subgraph Problem["å•é¡Œ: è² è·ã®ä¸å‡è¡¡"]
        Unbalanced[ä¸å‡è¡¡ãªé…ç½®<br/>Expert 1: éè² è·<br/>Expert 2: è»½è² è·<br/>Expert 3: æœªä½¿ç”¨]
        
        Impact[å½±éŸ¿<br/>ä¸€éƒ¨GPUã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯<br/>å…¨ä½“æ€§èƒ½ã®ä½ä¸‹<br/>ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆç‰¹åŒ–ã®é˜»å®³]
        
        Unbalanced --> Impact
    end
    
    subgraph Solution["è§£æ±ºç­–"]
        LoadBalanceLoss[Load Balance Loss<br/>å‡ç­‰é…ç½®ã‚’ä¿ƒé€²]
        
        GroupLimited[Group-Limited Gating<br/>DeepSeek-V3 æ‰‹æ³•<br/>ã‚°ãƒ«ãƒ¼ãƒ—å†…ã§ã®ç«¶åˆ]
        
        DroplessToken[Dropless Token<br/>MegaBlocks æ‰‹æ³•<br/>ãƒˆãƒ¼ã‚¯ãƒ³å»ƒæ£„ãªã—]
        
        LoadBalanceLoss --> Balanced[ãƒãƒ©ãƒ³ã‚¹ã•ã‚ŒãŸé…ç½®<br/>å…¨ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆãŒ<br/>é©åˆ‡ã«æ´»ç”¨ã•ã‚Œã‚‹]
        GroupLimited --> Balanced
        DroplessToken --> Balanced
    end
    
    classDef problemStyle fill:#ffe1e1,stroke:#333,stroke-width:2px
    classDef impactStyle fill:#ffd4d4,stroke:#333,stroke-width:2px
    classDef solutionStyle fill:#d4ffd4,stroke:#333,stroke-width:2px
    classDef balancedStyle fill:#e1f5ff,stroke:#333,stroke-width:2px
    
    class Unbalanced problemStyle
    class Impact impactStyle
    class LoadBalanceLoss,GroupLimited,DroplessToken solutionStyle
    class Balanced balancedStyle
```

## å‚è€ƒæ–‡çŒ®ã¨ãƒªã‚½ãƒ¼ã‚¹

### ä¸»è¦è«–æ–‡
- [Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer](https://arxiv.org/abs/1701.06538) - Google ã«ã‚ˆã‚‹ MoE ã®åŸºç¤è«–æ–‡
- [RailS: Load Balancing for All-to-All Communication in Distributed MoE Training](https://arxiv.org/abs/2510.19262) - Rail ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã® MoE æœ€é©åŒ–
- [Chain-of-Experts: Unlocking the Communication Power of MoE Models](https://arxiv.org/abs/2506.18945) - æ–°ã—ã„ MoE ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ææ¡ˆ

### å®Ÿè£…ã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
- [Training MoEs at Scale with PyTorch](https://pytorch.org/blog/training-moes/) - PyTorch + MegaBlocks ã«ã‚ˆã‚‹å¤§è¦æ¨¡ MoE è¨“ç·´
- [DeepEP: DeepSeek é«˜æ€§èƒ½ All-to-All ãƒ©ã‚¤ãƒ–ãƒ©ãƒª](https://github.com/deepseek-ai/DeepEP) - FP8 å¯¾å¿œã®æœ€é©åŒ–ã•ã‚ŒãŸå®Ÿè£…
- [All-to-All Communication in MoE Training](https://apxml.com/courses/mixture-of-experts/chapter-4-scaling-moe-distributed-training/all-to-all-communication-moe) - è©³ç´°ãªæŠ€è¡“è§£èª¬

### å•†ç”¨å®Ÿè£…
- [What Is Mixture of Experts (MoE) and How It Works?](https://www.nvidia.com/en-us/glossary/mixture-of-experts/) - NVIDIA ã«ã‚ˆã‚‹ MoE æ¦‚è¦ã¨ GB200 NVL72 ã§ã®æœ€é©åŒ–

## çµè«–

MoE ãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹ All-to-All é€šä¿¡ã¯ã€Expert Parallelism ã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã«ä¸å¯æ¬ ãªä»•çµ„ã¿ã§ã™ã€‚Dense ãƒ¢ãƒ‡ãƒ«ã¨æ¯”è¼ƒã—ã¦é€šä¿¡é‡ã¯å¢—åŠ ã—ã¾ã™ãŒã€ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆã®å°‚é–€åŒ–ã¨å¤§ããªè¡Œåˆ—æ¼”ç®—ã¸ã®é›†ç´„ã«ã‚ˆã‚Šã€å…¨ä½“çš„ãªè¨ˆç®—åŠ¹ç‡ã®å‘ä¸Šã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚

ç‰¹ã«å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§ã¯ã€é©åˆ‡ãª Load Balancing ã¨é«˜é€Ÿãªãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆNVLinkã€EFAï¼‰ã«ã‚ˆã‚Šã€All-to-All é€šä¿¡ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’æœ€å°åŒ–ã—ãªãŒã‚‰ã€ãƒ¢ãƒ‡ãƒ«ã®è¡¨ç¾èƒ½åŠ›ã‚’å¤§å¹…ã«æ‹¡å¼µã™ã‚‹ã“ã¨ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚
::::
