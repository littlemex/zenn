---
title: "Appendix: Optimizer"
emoji: "🚀"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["aws", "sagemaker", "hyperpod", "ai", "ml"]
free: false
---

::::details Adam と AdamW の違い
### Refs

https://zenn.dev/bilzard/articles/adamw-demistified

### 結論：メモリ要求は同じ

まず重要なポイントとして、**Adam も AdamW もメモリ要求は全く同じ**です。どちらも以下を保持します：
- Momentum（1次モーメント）: 280 GB
- Variance（2次モーメント）: 280 GB
- **合計**: 560 GB

### 何が違うのか？

違いは「重み減衰（Weight Decay）」という過学習を防ぐ仕組みの適用方法です。

#### 重み減衰とは？

**料理に例えると**：
調味料（モデルのパラメータ）を入れすぎると味が濃すぎて失敗します。重み減衰は「調味料を少しずつ減らして、入れすぎを防ぐ仕組み」です。

**技術的には**：
モデルのパラメータが大きくなりすぎないように制限することで、学習データに過度に適応しすぎる（過学習）のを防ぎます。

### Adam の問題点

Adam では、重み減衰を「勾配に加える」形で実装されていました。

```
更新式（簡略版）:
パラメータ更新 = - 学習率 × (勾配 + 重み減衰) / √(分散)
```

**問題**：重み減衰が適応的学習率（√(分散) の部分）によってスケーリングされるため、本来の重み減衰の効果が弱まったり強まったりしてしまいます。

**料理の例え**：
「塩を減らす」という指示が、「今日の気温に応じて塩の減らし方を変える」になってしまい、意図した通りに塩が減らせない状態です。

### AdamW の改善

AdamW では、重み減衰を「パラメータに直接適用」する形に変更しました。

```
更新式（簡略版）:
パラメータ更新 = (1 - 学習率 × 重み減衰率) × パラメータ 
                 - 学習率 × 勾配 / √(分散)
```

**改善点**：重み減衰が適応的学習率の影響を受けず、確実にパラメータを減衰させられます。

**料理の例え**：
「塩を毎回 1% 減らす」というシンプルで確実なルールになり、意図通りに調味料の量を管理できるようになりました。

### 実装上の違い

**PyTorch での例**：
```python
# Adam (weight_decay は実質機能しない)
optimizer = torch.optim.Adam(model.parameters(), 
                            lr=0.001, 
                            weight_decay=0.01)  # 効果が弱い

# AdamW (weight_decay が正しく機能する)
optimizer = torch.optim.AdamW(model.parameters(), 
                             lr=0.001, 
                             weight_decay=0.01)  # 効果が確実
```

### なぜ AdamW が標準になったのか

1. **過学習の防止**：重み減衰が正しく機能することで、より汎化性能の高いモデルが学習できる
2. **性能向上**：多くのタスクで Adam より良い結果を達成
3. **直感的**：学習率と重み減衰が独立して調整可能

### まとめ表

| 項目 | Adam | AdamW |
|------|------|-------|
| **メモリ要求** | 560 GB | 560 GB（同じ） |
| **重み減衰の適用** | 勾配に加算 | パラメータに直接適用 |
| **重み減衰の効果** | 適応的学習率の影響を受ける | 確実に機能する |
| **過学習の防止** | 効果が弱い | 効果的 |
| **大規模モデルでの採用** | 少ない | 標準（Llama 3, GPT-2 等） |
::::