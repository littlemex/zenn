---
title: "Appendix: ãƒãƒ«ãƒ GPU å‡¦ç†æ‰‹æ³•ã®æ•´ç†"
emoji: "ğŸ”§"
type: "tech"
topics: ["aws", "gpu", "distributed", "ai", "ml"]
free: false
---

::::details å‰æ
:::message
**å¯¾è±¡èª­è€…**: å¤§è¦æ¨¡åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®åˆ†æ•£å­¦ç¿’ã«èˆˆå‘³ãŒã‚ã‚Šã€ä¸¦åˆ—åŒ–æ‰‹æ³•ã«ã¤ã„ã¦ç†è§£ã‚’æ·±ã‚ãŸã„æ–¹ã€‚æ©Ÿæ¢°å­¦ç¿’ã®åŸºç¤çŸ¥è­˜ãŒã‚ã‚‹ã¨ç†è§£ã—ã‚„ã™ã„å†…å®¹ã§ã™ã€‚GPU ã‚’ç”¨ã„ãŸå­¦ç¿’ã®çµŒé¨“ãŒã‚ã‚‹ã¨ã‚ˆã‚Šç†è§£ãŒæ·±ã¾ã‚Šã¾ã™ã€‚
:::
:::message
**åˆå­¦è€…ã®æ–¹ã¸ - èª­ã¿æ–¹ã‚¬ã‚¤ãƒ‰**
ã“ã®ç« ã«ã¯è©³ç´°ãªæŠ€è¡“æƒ…å ±ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ãŒã€æœ€åˆã¯ã™ã¹ã¦ã‚’ç†è§£ã™ã‚‹å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
1. ã¾ãšå„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã®æ¦‚è¦éƒ¨åˆ†ï¼ˆå†’é ­ã®èª¬æ˜ï¼‰ã‚’èª­ã‚“ã§å…¨ä½“åƒã‚’æŠŠæ¡ã—ã¦ãã ã•ã„
2. `::::details` ã‚¿ã‚°ã§æŠ˜ã‚ŠãŸãŸã¾ã‚ŒãŸè©³ç´°æƒ…å ±ã¯ã€èˆˆå‘³ã®ã‚ã‚‹éƒ¨åˆ†ã®ã¿èª­ã‚ã°ååˆ†ã§ã™
3. mermaid å›³ã‚’ä¸­å¿ƒã«è¦–è¦šçš„ã«ç†è§£ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™
4. ã‚ã‹ã‚‰ãªã„ç”¨èªã¯å„æ‰€ã®è©³ç´°ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‚ç…§ã—ã¦ãã ã•ã„
:::
:::message
**ãƒ©ã‚¤ã‚»ãƒ³ã‚¹**: Â© 2025 littlemex.
æœ¬æ–‡ãŠã‚ˆã³è‡ªä½œå›³è¡¨: CC BY 4.0
â€»å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ã®å¼•ç”¨ã‚„ç¿»è¨³éƒ¨åˆ†ã¯åŸå…¸ã®è‘—ä½œæ¨©ã«å¾“ã„ã¾ã™ã€‚
å¼•ç”¨ç”»åƒ: å„ç”»åƒã®å‡ºå…¸ã«è¨˜è¼‰ã•ã‚ŒãŸãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã«å¾“ã„ã¾ã™ã€‚
:::
:::message
ä¸€éƒ¨ AI ã‚’ç”¨ã„ã¦æ–‡ç« ã‚’ä½œæˆã—ã¾ã™ã€‚ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¯å®Ÿæ–½ã—ã¾ã™ãŒã€è¦‹é€ƒã›ãªã„é‡å¤§ãªé–“é•ã„ãªã©ãŒã‚ã‚Œã°[ã“ã¡ã‚‰ã® Issue](https://github.com/littlemex/samples/issues) ã‹ã‚‰é€£çµ¡ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚
:::
::::

**æœ¬ç« ã§ã¯å¤§è¦æ¨¡åŸºç›¤ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚’åŠ¹ç‡çš„ã«å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ãƒãƒ«ãƒ GPU å‡¦ç†æ‰‹æ³•ã«ã¤ã„ã¦æ•´ç†ã—ã¾ã™ã€‚**

## å‰æçŸ¥è­˜

:::message
**Point !** ***å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã¯å˜ä¸€ GPU ã§ã¯å­¦ç¿’ã§ããªã„***
:::

background ç« ã§ç¢ºèªã—ãŸé€šã‚Šã€Llama 3 70B ã®ã‚ˆã†ãªå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‚’ BF16 ç²¾åº¦ã§å­¦ç¿’ã™ã‚‹ã«ã¯ç´„ 0.6TB ã® GPU ãƒ¡ãƒ¢ãƒªãŒå¿…è¦ã§ã™ã€‚ã—ã‹ã—ã€[NVIDIA H100 GPU](https://www.nvidia.com/ja-jp/data-center/h100/) ã§ã‚‚ 1 å°ã‚ãŸã‚Š 80GB ã®ãƒ¡ãƒ¢ãƒªã—ã‹ã‚ã‚Šã¾ã›ã‚“ã€‚ã¤ã¾ã‚Šã€ç†è«–ä¸Šæœ€å°ã§ã‚‚ 8 å°ã® GPU ãŒå¿…è¦ã§ã‚ã‚Šã€å®Ÿç”¨çš„ã«ã¯ 10ï½14 å°ãŒå¿…è¦ã§ã™ã€‚

ã“ã®ã‚ˆã†ãªå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‚’è¤‡æ•°ã® GPU ã«åˆ†æ•£ã—ã¦å­¦ç¿’ã™ã‚‹ãŸã‚ã®æ‰‹æ³•ãŒ**åˆ†æ•£å­¦ç¿’**ã§ã‚ã‚Šã€ãã®ä¸­æ ¸ã¨ãªã‚‹ã®ãŒ**ä¸¦åˆ—åŒ–æ‰‹æ³•**ã§ã™ã€‚

### åŸºæœ¬ç”¨èªã®èª¬æ˜

::::details èª­ã¿é€²ã‚ã‚‹ä¸Šã§å¿…é ˆã®ç”¨èª
- **ãƒãƒ¼ãƒ‰**: GPU ã‚’æ­è¼‰ã—ãŸè¨ˆç®—æ©Ÿï¼ˆã‚µãƒ¼ãƒãƒ¼ï¼‰ã®å˜ä½ã€‚ä¾‹ãˆã° [NVIDIA DGX H100](https://www.nvidia.com/en-us/data-center/dgx-h100/) ã¯ 8 ã¤ã® H100 GPU ã‚’æ­è¼‰ã—ãŸ 1 ãƒãƒ¼ãƒ‰ã§ã™ã€‚
- **ãƒ©ãƒ³ã‚¯ (Rank)**: åˆ†æ•£å­¦ç¿’ã«ãŠã‘ã‚‹å„ãƒ—ãƒ­ã‚»ã‚¹ã®è­˜åˆ¥ç•ªå·ã€‚é€šå¸¸ã¯ GPU 1 å°ã«ã¤ã 1 ã¤ã®ãƒ©ãƒ³ã‚¯ãŒå‰²ã‚Šå½“ã¦ã‚‰ã‚Œã¾ã™ã€‚
- **ãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚µã‚¤ã‚º (World Size)**: åˆ†æ•£å­¦ç¿’ã«å‚åŠ ã™ã‚‹å…¨ãƒ—ãƒ­ã‚»ã‚¹ï¼ˆãƒ©ãƒ³ã‚¯ï¼‰ã®ç·æ•°ã€‚8 å°ã® GPU ã§å­¦ç¿’ã™ã‚‹å ´åˆã€ãƒ¯ãƒ¼ãƒ«ãƒ‰ã‚µã‚¤ã‚ºã¯ 8 ã§ã™ã€‚
- **Forward Pass (é †ä¼æ’­)**: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¢ãƒ‡ãƒ«ã«é€šã—ã¦äºˆæ¸¬çµæœã‚’å‡ºåŠ›ã™ã‚‹å‡¦ç†ã€‚
- **Backward Pass (é€†ä¼æ’­)**: äºˆæ¸¬ã¨æ­£è§£ã®èª¤å·®ã‚’è¨ˆç®—ã—ã€å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®å‹¾é…ã‚’æ±‚ã‚ã‚‹å‡¦ç†ã€‚
- **All-Reduce**: å…¨ GPU ãŒæŒã¤å€¤ï¼ˆä¾‹ãˆã°å‹¾é…ï¼‰ã‚’é›†ç´„ã—ã€å…¨ GPU ã«çµæœã‚’é…å¸ƒã™ã‚‹é€šä¿¡æ“ä½œã€‚
- **Activation (æ´»æ€§åŒ–å€¤)**: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å„å±¤ã«ãŠã‘ã‚‹ä¸­é–“è¨ˆç®—çµæœã€‚é€†ä¼æ’­æ™‚ã«å†åˆ©ç”¨ã•ã‚Œã¾ã™ã€‚
::::

::::details åˆ†æ•£å­¦ç¿’ã«é–¢ã™ã‚‹å‚è€ƒæƒ…å ±
- [PyTorch Distributed Overview](https://docs.pytorch.org/tutorials/beginner/dist_overview.html)
- [Colossal-AI: Distributed Training](https://colossalai.org/docs/concepts/distributed_training)
- [Colossal-AI: Paradigms of Parallelism](https://colossalai.org/docs/concepts/paradigms_of_parallelism)
::::

## åˆ†æ•£å­¦ç¿’ã«ãŠã‘ã‚‹ä¸¦åˆ—åŒ–æ‰‹æ³•

å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’è¤‡æ•° GPU ã«åˆ†æ•£ã™ã‚‹æ‰‹æ³•ã«ã¯ã€ä¸»ã«ä»¥ä¸‹ã® 4 ã¤ã®ãƒ‘ãƒ©ãƒ€ã‚¤ãƒ ãŒã‚ã‚Šã¾ã™ã€‚

1. **Data Parallelism (ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—)**ï¼šãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²
2. **Pipeline Parallelism (ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¸¦åˆ—)**ï¼šãƒ¢ãƒ‡ãƒ«ã‚’å±¤ã”ã¨ã«åˆ†å‰²
3. **Tensor Parallelism (ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—)**ï¼šãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ãƒ³ã‚½ãƒ«å˜ä½ã§åˆ†å‰²
4. **Hybrid Parallelism (ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ä¸¦åˆ—)**ï¼šè¤‡æ•°æ‰‹æ³•ã®çµ„ã¿åˆã‚ã›

```mermaid
graph TB
    subgraph "ä¸¦åˆ—åŒ–æ‰‹æ³•ã®å…¨ä½“åƒ"
        A[åˆ†æ•£å­¦ç¿’ã®ä¸¦åˆ—åŒ–] --> B[Data Parallelism<br/>ãƒ‡ãƒ¼ã‚¿åˆ†å‰²]
        A --> C[Model Parallelism<br/>ãƒ¢ãƒ‡ãƒ«åˆ†å‰²]
        
        C --> D[Pipeline Parallelism<br/>å±¤ã”ã¨ã«åˆ†å‰²]
        C --> E[Tensor Parallelism<br/>ãƒ†ãƒ³ã‚½ãƒ«å˜ä½ã§åˆ†å‰²]
        
        A --> F[Hybrid Parallelism<br/>çµ„ã¿åˆã‚ã›]
        
        B -.-> F
        D -.-> F
        E -.-> F
    end
    
    style A fill:#e8f4f8
    style B fill:#fff4e1
    style C fill:#f0e6ff
    style D fill:#e6f3ff
    style E fill:#ffe6f0
    style F fill:#e6ffe6
```

### Data Parallelismï¼ˆãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—ï¼‰

::::details æ¦‚è¦
Data Parallelism ã¯ã€æœ€ã‚‚ã‚·ãƒ³ãƒ—ãƒ«ã§åºƒãä½¿ã‚ã‚Œã¦ã„ã‚‹ä¸¦åˆ—åŒ–æ‰‹æ³•ã§ã™ã€‚å„ GPU ãŒ**ãƒ¢ãƒ‡ãƒ«å…¨ä½“ã®ã‚³ãƒ”ãƒ¼**ã‚’ä¿æŒã—ã€ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒã‚’å‡¦ç†ã—ã¾ã™ã€‚
::::

**åŸºæœ¬çš„ãªå‹•ä½œãƒ•ãƒ­ãƒ¼**

1. **åˆæœŸåŒ–**: å…¨ GPU ãŒåŒã˜ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æŒã¤
2. **Forward Pass**: å„ GPU ãŒç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒã§é †ä¼æ’­ã‚’å®Ÿè¡Œ
3. **Backward Pass**: å„ GPU ãŒå‹¾é…ã‚’è¨ˆç®—
4. **All-Reduce**: å…¨ GPU ã®å‹¾é…ã‚’å¹³å‡åŒ–
5. **Parameter Update**: å…¨ GPU ãŒåŒã˜å‹¾é…ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°

```mermaid
graph TB
    subgraph "Data Parallelism ã®å‹•ä½œãƒ•ãƒ­ãƒ¼"
        subgraph GPU0["GPU 0"]
            M0[Model Copy] --> D0[Batch 0<br/>Forward] --> G0[Gradients 0]
        end
        
        subgraph GPU1["GPU 1"]
            M1[Model Copy] --> D1[Batch 1<br/>Forward] --> G1[Gradients 1]
        end
        
        subgraph GPU2["GPU 2"]
            M2[Model Copy] --> D2[Batch 2<br/>Forward] --> G2[Gradients 2]
        end
        
        G0 --> AR[All-Reduce<br/>å‹¾é…ã®å¹³å‡åŒ–]
        G1 --> AR
        G2 --> AR
        
        AR --> U0[Parameter Update]
        AR --> U1[Parameter Update]
        AR --> U2[Parameter Update]
    end
    
    style AR fill:#fff4e1
    style M0 fill:#e6f3ff
    style M1 fill:#e6f3ff
    style M2 fill:#e6f3ff
```

**åˆ©ç‚¹ã¨åˆ¶ç´„**

::::details åˆ©ç‚¹
- å®Ÿè£…ãŒã‚·ãƒ³ãƒ—ãƒ«
- æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ã‚’å®¹æ˜“ã«åˆ†æ•£åŒ–ã§ãã‚‹
- ç·šå½¢ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ãŒæœŸå¾…ã§ãã‚‹ï¼ˆç†æƒ³çš„ã«ã¯ GPU æ•°ã«æ¯”ä¾‹ã—ãŸé«˜é€ŸåŒ–ï¼‰
- PyTorch ã® [DistributedDataParallel (DDP)](https://pytorch.org/docs/stable/notes/ddp.html) ãªã©ã€æˆç†Ÿã—ãŸãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨å¯èƒ½
::::

::::details åˆ¶ç´„
- å„ GPU ãŒãƒ¢ãƒ‡ãƒ«å…¨ä½“ã‚’ä¿æŒã™ã‚‹å¿…è¦ãŒã‚ã‚‹ãŸã‚ã€ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãŒæ‚ªã„
- ãƒ¢ãƒ‡ãƒ«ãŒå¤§ãã™ãã‚‹ã¨å˜ä¸€ GPU ã®ãƒ¡ãƒ¢ãƒªã«åã¾ã‚‰ãªã„
- å‹¾é…ã® All-Reduce é€šä¿¡ãŒãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã«ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹
- 70B ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä»¥ä¸Šã®å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§ã¯å˜ç‹¬ã§ã¯ä¸ååˆ†
::::

### Pipeline Parallelismï¼ˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¸¦åˆ—ï¼‰

::::details æ¦‚è¦
Pipeline Parallelism ã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚’**å±¤ã”ã¨ã«åˆ†å‰²**ã—ã€ç•°ãªã‚‹ GPU ã«é…ç½®ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚å„ GPU ã¯è‡ªåˆ†ã«å‰²ã‚Šå½“ã¦ã‚‰ã‚ŒãŸå±¤ã®ã¿ã‚’ä¿æŒã—ã€è¨ˆç®—çµæœã‚’æ¬¡ã® GPU ã«æ¸¡ã—ã¦ã„ãã¾ã™ã€‚
::::

**åŸºæœ¬çš„ãªå‹•ä½œãƒ•ãƒ­ãƒ¼**

1. ãƒ¢ãƒ‡ãƒ«ã‚’é€£ç¶šã™ã‚‹å±¤ã®ã‚°ãƒ«ãƒ¼ãƒ—ã«åˆ†å‰²
2. å„ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ç•°ãªã‚‹ GPU ã«é…ç½®
3. ãƒ‡ãƒ¼ã‚¿ã‚’å°ã•ãªãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã«åˆ†å‰²
4. ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ–¹å¼ã§å‡¦ç†ï¼ˆå‰æ®µã® GPU ã®å‡ºåŠ›ã‚’æ¬¡æ®µã® GPU ãŒå‡¦ç†ï¼‰

```mermaid
graph LR
    subgraph "Pipeline Parallelism ã®å‹•ä½œ"
        subgraph GPU0["GPU 0"]
            L0[Layer 0-3]
        end
        
        subgraph GPU1["GPU 1"]
            L1[Layer 4-7]
        end
        
        subgraph GPU2["GPU 2"]
            L2[Layer 8-11]
        end
        
        subgraph GPU3["GPU 3"]
            L3[Layer 12-15]
        end
        
        D[Input Data] --> L0
        L0 -->|ä¸­é–“çµæœ| L1
        L1 -->|ä¸­é–“çµæœ| L2
        L2 -->|ä¸­é–“çµæœ| L3
        L3 --> O[Output]
    end
    
    style L0 fill:#e6f3ff
    style L1 fill:#ffe6f0
    style L2 fill:#e6ffe6
    style L3 fill:#fff4e1
```

**ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒãƒ–ãƒ«ã®å•é¡Œ**

ç´ æœ´ãªå®Ÿè£…ã§ã¯ã€å„ GPU ãŒå‰æ®µã® GPU ã‹ã‚‰ã®å‡ºåŠ›ã‚’å¾…ã¤é–“ã‚¢ã‚¤ãƒ‰ãƒ«çŠ¶æ…‹ã«ãªã‚Šã¾ã™ã€‚ã“ã®ç„¡é§„ãªå¾…ã¡æ™‚é–“ã‚’**ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒãƒ–ãƒ«**ã¨å‘¼ã³ã¾ã™ã€‚

```mermaid
gantt
    title Pipeline Parallelism ã®ãƒãƒ–ãƒ«å•é¡Œ
    dateFormat X
    axisFormat %s
    
    section GPU 0
    Batch 1 F0 :a1, 0, 1
    Batch 2 F0 :a2, 1, 2
    Batch 3 F0 :a3, 2, 3
    Batch 1 B0 :a4, 6, 7
    Batch 2 B0 :a5, 7, 8
    Batch 3 B0 :a6, 8, 9
    
    section GPU 1
    Idle :b0, 0, 1
    Batch 1 F1 :b1, 1, 2
    Batch 2 F1 :b2, 2, 3
    Batch 3 F1 :b3, 3, 4
    Batch 1 B1 :b4, 5, 6
    Batch 2 B1 :b5, 6, 7
    Batch 3 B1 :b6, 7, 8
    Idle :b7, 8, 9
    
    section GPU 2
    Idle :c0, 0, 2
    Batch 1 F2 :c1, 2, 3
    Batch 2 F2 :c2, 3, 4
    Batch 3 F2 :c3, 4, 5
    Batch 1 B2 :c4, 5, 6
    Batch 2 B2 :c5, 6, 7
    Idle :c6, 7, 9
    
    section GPU 3
    Idle :d0, 0, 3
    Batch 1 F3 :d1, 3, 4
    Batch 2 F3 :d2, 4, 5
    Batch 3 F3 :d3, 5, 6
    Idle :d4, 6, 9
```

::::details ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒãƒ–ãƒ«å‰Šæ¸›æ‰‹æ³•
[GPipe](https://arxiv.org/abs/1811.06965) ã‚„ [PipeDream](https://arxiv.org/abs/1806.03377) ãªã©ã®æ‰‹æ³•ã§ã¯ã€ãƒãƒƒãƒã‚’ã•ã‚‰ã«å°ã•ãªãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã«åˆ†å‰²ã—ã€è¤‡æ•°ã®ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã‚’ä¸¦è¡Œå‡¦ç†ã™ã‚‹ã“ã¨ã§ãƒãƒ–ãƒ«ã‚’å‰Šæ¸›ã—ã¾ã™ã€‚

**ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã«ã‚ˆã‚‹åŠ¹ç‡åŒ–**
- ãƒãƒƒãƒã‚µã‚¤ã‚º 32 ã‚’ 8 ã¤ã®ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒï¼ˆå„ 4 ã‚µãƒ³ãƒ—ãƒ«ï¼‰ã«åˆ†å‰²
- å„ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã‚’é †æ¬¡ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«æŠ•å…¥
- å‰æ®µã® GPU ãŒæ¬¡ã®ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã‚’å‡¦ç†ã—ã¦ã„ã‚‹é–“ã«ã€å¾Œæ®µã® GPU ãŒå‰ã®ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã‚’å‡¦ç†
::::

**åˆ©ç‚¹ã¨åˆ¶ç´„**

::::details åˆ©ç‚¹
- ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã®åˆ¶ç´„ã‚’çªç ´ã§ãã‚‹ï¼ˆå„ GPU ã¯ä¸€éƒ¨ã®å±¤ã®ã¿ä¿æŒï¼‰
- GPU é–“ã®é€šä¿¡é‡ãŒå°‘ãªã„ï¼ˆå±¤ã®å¢ƒç•Œã§ã®ã¿é€šä¿¡ï¼‰
- æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ¯”è¼ƒçš„å®¹æ˜“ã«åˆ†å‰²å¯èƒ½
::::

::::details åˆ¶ç´„
- ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒãƒ–ãƒ«ã«ã‚ˆã‚‹ GPU åˆ©ç”¨åŠ¹ç‡ã®ä½ä¸‹
- ãƒã‚¤ã‚¯ãƒ­ãƒãƒƒãƒã®ç®¡ç†ãŒè¤‡é›‘
- å±¤ã®åˆ†å‰²ãƒãƒ©ãƒ³ã‚¹ãŒæ€§èƒ½ã«å¤§ããå½±éŸ¿
- Forward ã¨ Backward ã§ç•°ãªã‚‹ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã®åæŸæ€§ã®èª²é¡Œ
::::

### Tensor Parallelismï¼ˆãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—ï¼‰

::::details æ¦‚è¦
Tensor Parallelism ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®**å„å±¤å†…ã®ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—ã‚’åˆ†å‰²**ã—ã€è¤‡æ•° GPU ã§ä¸¦è¡Œå‡¦ç†ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚[Megatron-LM](https://arxiv.org/abs/1909.08053) ã§æå”±ã•ã‚Œã€Transformer ãƒ¢ãƒ‡ãƒ«ã®åŠ¹ç‡çš„ãªä¸¦åˆ—åŒ–ã‚’å®Ÿç¾ã—ã¾ã™ã€‚
::::

**Transformer ã«ãŠã‘ã‚‹ Tensor Parallelism**

Transformer ã® Self-Attention ã¨ MLP (Multi-Layer Perceptron) å±¤ã¯ã€å¤§ããªè¡Œåˆ—ç©æ¼”ç®—ã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚Tensor Parallelism ã§ã¯ã€ã“ã‚Œã‚‰ã®è¡Œåˆ—ã‚’åˆ—æ–¹å‘ã¾ãŸã¯è¡Œæ–¹å‘ã«åˆ†å‰²ã—ã¾ã™ã€‚

**Self-Attention ã®åˆ†å‰²ä¾‹**

```mermaid
graph TB
    subgraph "Tensor Parallelism: Self-Attention"
        Input[Input Tensor<br/>batch Ã— seq Ã— hidden] --> Split[åˆ—æ–¹å‘ã«åˆ†å‰²]
        
        Split --> Q0[Query Matrix 0<br/>GPU 0]
        Split --> Q1[Query Matrix 1<br/>GPU 1]
        
        Split --> K0[Key Matrix 0<br/>GPU 0]
        Split --> K1[Key Matrix 1<br/>GPU 1]
        
        Split --> V0[Value Matrix 0<br/>GPU 0]
        Split --> V1[Value Matrix 1<br/>GPU 1]
        
        Q0 --> A0[Attention 0<br/>GPU 0]
        K0 --> A0
        V0 --> A0
        
        Q1 --> A1[Attention 1<br/>GPU 1]
        K1 --> A1
        V1 --> A1
        
        A0 --> Concat[çµåˆ + All-Reduce]
        A1 --> Concat
        
        Concat --> Output[Output Tensor]
    end
    
    style Q0 fill:#e6f3ff
    style Q1 fill:#e6f3ff
    style K0 fill:#ffe6f0
    style K1 fill:#ffe6f0
    style V0 fill:#e6ffe6
    style V1 fill:#e6ffe6
    style Concat fill:#fff4e1
```

**MLP å±¤ã®åˆ†å‰²ä¾‹**

```mermaid
graph TB
    subgraph "Tensor Parallelism: MLP"
        Input2[Input] --> W1_Split[W1 ã‚’åˆ—æ–¹å‘ã«åˆ†å‰²]
        
        W1_Split --> G0[GPU 0: W1_0 Ã— Input<br/>+ GeLU]
        W1_Split --> G1[GPU 1: W1_1 Ã— Input<br/>+ GeLU]
        
        G0 --> W2_Split[W2 ã‚’è¡Œæ–¹å‘ã«åˆ†å‰²]
        G1 --> W2_Split
        
        W2_Split --> M0[GPU 0: W2_0 Ã— Hidden]
        W2_Split --> M1[GPU 1: W2_1 Ã— Hidden]
        
        M0 --> AR[All-Reduce]
        M1 --> AR
        
        AR --> Output2[Output]
    end
    
    style G0 fill:#e6f3ff
    style G1 fill:#ffe6f0
    style AR fill:#fff4e1
```

::::details é€šä¿¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æœ€é©åŒ–
Megatron-LM ã®é‡è¦ãªå·¥å¤«ã¯ã€é€šä¿¡å›æ•°ã‚’æœ€å°åŒ–ã™ã‚‹ã“ã¨ã§ã™ã€‚

**Forward Pass ã§ã®å·¥å¤«**
1. Self-Attention å‡ºåŠ›ã®çµåˆæ™‚ã« All-Reduce
2. MLP ã®ç¬¬ 1 å±¤ã§ã¯é€šä¿¡ãªã—ï¼ˆåˆ—åˆ†å‰²ãªã®ã§å„ GPU ãŒç‹¬ç«‹ã«è¨ˆç®—å¯èƒ½ï¼‰
3. MLP ã®ç¬¬ 2 å±¤å‡ºåŠ›ã§ All-Reduce

**Backward Pass ã§ã®å·¥å¤«**
- Forward ã®é€†é †ã§é€šä¿¡ãŒç™ºç”Ÿ
- é€šä¿¡ã¨è¨ˆç®—ã‚’ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã•ã›ã‚‹ã“ã¨ã§åŠ¹ç‡åŒ–

çµæœã¨ã—ã¦ã€1 ã¤ã® Transformer ãƒ–ãƒ­ãƒƒã‚¯ã‚ãŸã‚Š **2 å›ã® All-Reduce** ã®ã¿ã§ä¸¦åˆ—åŒ–ãŒå®Ÿç¾ã•ã‚Œã¾ã™ã€‚
::::

**åˆ©ç‚¹ã¨åˆ¶ç´„**

::::details åˆ©ç‚¹
- ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒãƒ–ãƒ«ãŒãªã„ï¼ˆå…¨ GPU ãŒå¸¸ã«è¨ˆç®—ã‚’å®Ÿè¡Œï¼‰
- ãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—ã®ä¸­ã§æœ€ã‚‚åŠ¹ç‡ãŒè‰¯ã„
- é€šä¿¡é‡ã‚’æœ€å°åŒ–ã§ãã‚‹ï¼ˆMegatron-LM ã®å ´åˆï¼‰
- Transformer ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ã®ç›¸æ€§ãŒè‰¯ã„
::::

::::details åˆ¶ç´„
- å®Ÿè£…ãŒè¤‡é›‘ï¼ˆãƒ¢ãƒ‡ãƒ«ã®å†…éƒ¨æ§‹é€ ã‚’æ·±ãç†è§£ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ï¼‰
- GPU é–“ã®é«˜é€Ÿãªé€šä¿¡ãŒå¿…è¦ï¼ˆNVLink ã‚„ InfiniBand ãŒæ¨å¥¨ï¼‰
- é€šå¸¸ã¯ 1 ãƒãƒ¼ãƒ‰å†…ï¼ˆ8 GPU ç¨‹åº¦ï¼‰ã§ã®ä¸¦åˆ—åŒ–ã«é™å®šã•ã‚Œã‚‹
- ãƒãƒ¼ãƒ‰ã‚’ã¾ãŸãå ´åˆã¯é€šä¿¡ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒå¢—å¤§
::::

### Hybrid Parallelismï¼ˆãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ä¸¦åˆ—ï¼‰

::::details æ¦‚è¦
å®Ÿéš›ã®å¤§è¦æ¨¡å­¦ç¿’ã§ã¯ã€Data Parallelismã€Pipeline Parallelismã€Tensor Parallelism ã‚’çµ„ã¿åˆã‚ã›ãŸ **Hybrid Parallelism** ãŒä½¿ç”¨ã•ã‚Œã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€æ•°åƒã‹ã‚‰æ•°ä¸‡ã® GPU ã‚’åŠ¹ç‡çš„ã«æ´»ç”¨ã§ãã¾ã™ã€‚
::::

**å…¸å‹çš„ãªçµ„ã¿åˆã‚ã›**

```mermaid
graph TB
    subgraph "Hybrid Parallelism ã®æ§‹æˆä¾‹"
        subgraph "ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—è»¸"
            subgraph DP0["Data Parallel Group 0"]
                subgraph Node0["Node 0 (8 GPU)"]
                    TP0_0["GPU 0-7<br/>Tensor Parallel"]
                end
                PP0_0[Pipeline Stage 0-3] --> TP0_0
            end
            
            subgraph DP1["Data Parallel Group 1"]
                subgraph Node1["Node 1 (8 GPU)"]
                    TP1_0["GPU 8-15<br/>Tensor Parallel"]
                end
                PP1_0[Pipeline Stage 0-3] --> TP1_0
            end
        end
        
        subgraph "ãƒ‡ãƒ¼ã‚¿ä¸¦åˆ—è»¸2"
            subgraph DP2["Data Parallel Group 2"]
                subgraph Node2["Node 2 (8 GPU)"]
                    TP2_0["GPU 16-23<br/>Tensor Parallel"]
                end
                PP2_0[Pipeline Stage 4-7] --> TP2_0
            end
            
            subgraph DP3["Data Parallel Group 3"]
                subgraph Node3["Node 3 (8 GPU)"]
                    TP3_0["GPU 24-31<br/>Tensor Parallel"]
                end
                PP3_0[Pipeline Stage 4-7] --> TP3_0
            end
        end
        
        PP0_0 -.->|Pipeline| PP2_0
        PP1_0 -.->|Pipeline| PP3_0
        
        DP0 <-.->|Data Parallel<br/>å‹¾é…åŒæœŸ| DP1
        DP2 <-.->|Data Parallel<br/>å‹¾é…åŒæœŸ| DP3
    end
    
    style TP0_0 fill:#e6f3ff
    style TP1_0 fill:#e6f3ff
    style TP2_0 fill:#ffe6f0
    style TP3_0 fill:#ffe6f0
```

**å…·ä½“ä¾‹: Llama 3 405B ã®å­¦ç¿’æ§‹æˆ**

::::details 16,000 GPU ã§ã®æ§‹æˆ
Meta ãŒ [Llama 3 ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°](https://ai.meta.com/blog/meta-training-inference-generative-ai-llama-3/) ã§ä½¿ç”¨ã—ãŸæ§‹æˆï¼ˆæ¨å®šï¼‰

- **Total GPUs**: 16,000 (H100-80GB)
- **Tensor Parallelism**: 8 (1 ãƒãƒ¼ãƒ‰å†…)
- **Pipeline Parallelism**: 16 (ãƒ¢ãƒ‡ãƒ«ã‚’ 16 æ®µéšã«åˆ†å‰²)
- **Data Parallelism**: 125 (16,000 Ã· 8 Ã· 16 = 125)

**æ§‹æˆã®æ„å›³**
1. **Tensor Parallelism = 8**: 1 ãƒãƒ¼ãƒ‰å†…ã® 8 GPU é–“ã®é«˜é€Ÿé€šä¿¡ (NVLink) ã‚’æ´»ç”¨
2. **Pipeline Parallelism = 16**: 405B ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ 16 æ®µéšã«åˆ†å‰²ã—ã¦ãƒ¡ãƒ¢ãƒªã«åã‚ã‚‹
3. **Data Parallelism = 125**: æ®‹ã‚Šã® 125 ã‚°ãƒ«ãƒ¼ãƒ—ã§ç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’ä¸¦è¡Œå‡¦ç†
::::

**åˆ©ç‚¹**

- å„æ‰‹æ³•ã®åˆ©ç‚¹ã‚’çµ„ã¿åˆã‚ã›ã‚‰ã‚Œã‚‹
- æ•°ä¸‡ GPU è¦æ¨¡ã«ã‚¹ã‚±ãƒ¼ãƒ«å¯èƒ½
- é€šä¿¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æœ€é©åŒ–ã§ãã‚‹
- ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãƒˆãƒãƒ­ã‚¸ãƒ¼ã«é©åˆã•ã›ã‚‰ã‚Œã‚‹

## ZeRO: Zero Redundancy Optimizer

::::details æ¦‚è¦
ZeRO (Zero Redundancy Optimizer) ã¯ Microsoft Research ãŒé–‹ç™ºã—ãŸã€Data Parallelism ã«ãŠã‘ã‚‹ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’å¤§å¹…ã«æ”¹å–„ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚[DeepSpeed](https://www.deepspeed.ai/) ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§å®Ÿè£…ã•ã‚Œã¦ãŠã‚Šã€å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚
:::

### Data Parallelism ã®ãƒ¡ãƒ¢ãƒªèª²é¡Œ

é€šå¸¸ã® Data Parallelism ã§ã¯ã€å„ GPU ãŒä»¥ä¸‹ã‚’ä¿æŒã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

1. **Model Parameters** (ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)
2. **Gradients** (å‹¾é…)
3. **Optimizer States** (ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶çŠ¶æ…‹: Momentum + Variance)

ã“ã‚Œã¯**ãƒ¡ãƒ¢ãƒªã®å†—é•·æ€§**ã‚’æ„å‘³ã—ã¾ã™ã€‚ä¾‹ãˆã° 8 GPU ã§å­¦ç¿’ã™ã‚‹å ´åˆã€åŒã˜ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒ 8 ã¤ã® GPU ã«é‡è¤‡ã—ã¦ä¿å­˜ã•ã‚Œã¦ã„ã¾ã™ã€‚

```mermaid
graph TB
    subgraph "é€šå¸¸ã® Data Parallelism ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨"
        subgraph GPU0["GPU 0"]
            P0[Parameters<br/>70B Ã— 2B = 140GB]
            G0[Gradients<br/>140GB]
            O0[Optimizer States<br/>280GB]
        end
        
        subgraph GPU1["GPU 1"]
            P1[Parameters<br/>140GB]
            G1[Gradients<br/>140GB]
            O1[Optimizer States<br/>280GB]
        end
        
        subgraph GPU2["GPU 2"]
            P2[Parameters<br/>140GB]
            G2[Gradients<br/>140GB]
            O2[Optimizer States<br/>280GB]
        end
    end
    
    note[å„ GPU ã§ 560GB ã®ãƒ¡ãƒ¢ãƒª<br/>8 GPU ã§åˆè¨ˆ 4.48TB ã®å†—é•·æ€§]
    
    style P0 fill:#e6f3ff
    style P1 fill:#e6f3ff
    style P2 fill:#e6f3ff
    style note fill:#fff4e1
```

### ZeRO ã®åŸºæœ¬ã‚¢ã‚¤ãƒ‡ã‚¢

ZeRO ã¯ã€ã“ã®å†—é•·æ€§ã‚’æ’é™¤ã™ã‚‹ãŸã‚ã«ã€Parametersã€Gradientsã€Optimizer States ã‚’ GPU é–“ã§**åˆ†å‰² (Partition)** ã—ã¾ã™ã€‚å„ GPU ã¯è‡ªåˆ†ã«å‰²ã‚Šå½“ã¦ã‚‰ã‚ŒãŸéƒ¨åˆ†ã®ã¿ã‚’ä¿æŒã—ã€å¿…è¦ã«å¿œã˜ã¦ä»–ã® GPU ã‹ã‚‰å–å¾—ã—ã¾ã™ã€‚

ZeRO ã«ã¯ 3 ã¤ã®ã‚¹ãƒ†ãƒ¼ã‚¸ãŒã‚ã‚Šã€æ®µéšçš„ã«ãƒ¡ãƒ¢ãƒªã‚’å‰Šæ¸›ã—ã¾ã™ã€‚

### ZeRO Stage 1: Optimizer State Partitioning

**Optimizer States ã®ã¿ã‚’åˆ†å‰²**

Optimizer Statesï¼ˆAdamW ã® Momentum ã¨ Varianceï¼‰ã¯ Parameters ã‚„ Gradients ã¨æ¯”ã¹ã¦æœ€ã‚‚ãƒ¡ãƒ¢ãƒªã‚’æ¶ˆè²»ã—ã¾ã™ï¼ˆ2 å€ï¼‰ã€‚Stage 1 ã§ã¯ã€ã“ã‚Œã‚’ GPU é–“ã§åˆ†å‰²ã—ã¾ã™ã€‚

```mermaid
graph TB
    subgraph "ZeRO Stage 1: Optimizer State Partitioning"
        subgraph GPU0["GPU 0"]
            P0[Parameters<br/>140GB å…¨ä½“]
            G0[Gradients<br/>140GB å…¨ä½“]
            O0[Optimizer States<br/>35GB (1/8)]
        end
        
        subgraph GPU1["GPU 1"]
            P1[Parameters<br/>140GB å…¨ä½“]
            G1[Gradients<br/>140GB å…¨ä½“]
            O1[Optimizer States<br/>35GB (1/8)]
        end
        
        subgraph GPU2["GPU 2"]
            P2[Parameters<br/>140GB å…¨ä½“]
            G2[Gradients<br/>140GB å…¨ä½“]
            O2[Optimizer States<br/>35GB (1/8)]
        end
        
        note1[å„ GPU ã§ 315GB ã®ãƒ¡ãƒ¢ãƒª<br/>æ¨™æº–ã® 560GB ã‹ã‚‰ 44% å‰Šæ¸›]
    end
    
    style O0 fill:#e6f3ff
    style O1 fill:#ffe6f0
    style O2 fill:#e6ffe6
    style note1 fill:#fff4e1
```

**å‹•ä½œã®æµã‚Œ**

1. Forward & Backward Pass: å„ GPU ãŒå…¨ Gradients ã‚’è¨ˆç®—
2. All-Reduce: å…¨ GPU ã§ Gradients ã‚’å¹³å‡åŒ–
3. Parameter Update: å„ GPU ã¯è‡ªåˆ†ãŒæ‹…å½“ã™ã‚‹ Parameters ã®ã¿ã‚’æ›´æ–°
4. Broadcast: æ›´æ–°ã•ã‚ŒãŸ Parameters ã‚’å…¨ GPU ã«é…å¸ƒ

**ãƒ¡ãƒ¢ãƒªå‰Šæ¸›åŠ¹æœï¼ˆ70B ãƒ¢ãƒ‡ãƒ«ã€8 GPU ã®å ´åˆï¼‰**
- æ¨™æº– Data Parallelism: 560GB/GPU
- ZeRO Stage 1: 315GB/GPUï¼ˆ**44% å‰Šæ¸›**ï¼‰

### ZeRO Stage 2: Gradient Partitioning

**Optimizer States ã¨ Gradients ã‚’åˆ†å‰²**

Stage 2 ã§ã¯ã€Gradients ã‚‚åˆ†å‰²ã—ã¾ã™ã€‚å„ GPU ã¯ Backward Pass ã§è¨ˆç®—ã—ãŸ Gradients ã®ã†ã¡ã€è‡ªåˆ†ãŒæ‹…å½“ã™ã‚‹éƒ¨åˆ†ã®ã¿ã‚’ä¿æŒã—ã¾ã™ã€‚

```mermaid
graph TB
    subgraph "ZeRO Stage 2: Gradient Partitioning"
        subgraph GPU0["GPU 0"]
            P0[Parameters<br/>140GB å…¨ä½“]
            G0[Gradients<br/>17.5GB (1/8)]
            O0[Optimizer States<br/>35GB (1/8)]
        end
        
        subgraph GPU1["GPU 1"]
            P1[Parameters<br/>140GB å…¨ä½“]
            G1[Gradients<br/>17.5GB (1/8)]
            O1[Optimizer States<br/>35GB (1/8)]
        end
        
        subgraph GPU2["GPU 2"]
            P2[Parameters<br/>140GB å…¨ä½“]
            G2[Gradients<br/>17.5GB (1/8)]
            O2[Optimizer States<br/>35GB (1/8)]
        end
        
        note2[å„ GPU ã§ 192.5GB ã®ãƒ¡ãƒ¢ãƒª<br/>æ¨™æº–ã® 560GB ã‹ã‚‰ 66% å‰Šæ¸›]
    end
    
    style G0 fill:#e6f3ff
    style G1 fill:#ffe6f0
    style G2 fill:#e6ffe6
    style note2 fill:#fff4e1
```

**å‹•ä½œã®æµã‚Œ**

1. Forward Pass: å„ GPU ãŒè‡ªåˆ†ã®ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒã§é †ä¼æ’­
2. Backward Pass: å„ GPU ãŒ Gradients ã‚’è¨ˆç®—
3. Reduce-Scatter: å„ GPU ãŒè‡ªåˆ†ã®æ‹…å½“éƒ¨åˆ†ã® Gradients ã®ã¿ã‚’é›†ç´„
4. Parameter Update: å„ GPU ãŒè‡ªåˆ†ã®æ‹…å½“éƒ¨åˆ†ã®ã¿ã‚’æ›´æ–°
5. All-Gather: æ›´æ–°ã•ã‚ŒãŸ Parameters ã‚’å…¨ GPU ã§å…±æœ‰

**ãƒ¡ãƒ¢ãƒªå‰Šæ¸›åŠ¹æœï¼ˆ70B ãƒ¢ãƒ‡ãƒ«ã€8 GPU ã®å ´åˆï¼‰**
- æ¨™æº– Data Parallelism: 560GB/GPU
- ZeRO Stage 2: 192.5GB/GPUï¼ˆ**66% å‰Šæ¸›**ï¼‰

### ZeRO Stage 3: Parameter Partitioning

**Optimizer Statesã€Gradientsã€Parameters ã™ã¹ã¦ã‚’åˆ†å‰²**

Stage 3 ã§ã¯ã€Parameters ã‚‚åˆ†å‰²ã—ã¾ã™ã€‚ã“ã‚ŒãŒ ZeRO ã®æœ€ã‚‚ç©æ¥µçš„ãªãƒ¡ãƒ¢ãƒªå‰Šæ¸›æ‰‹æ³•ã§ã™ã€‚å„ GPU ã¯è¨ˆç®—ã«å¿…è¦ãª Parameters ã‚’å‹•çš„ã«ä»–ã® GPU ã‹ã‚‰å–å¾—ã—ã¾ã™ã€‚

```mermaid
graph TB
    subgraph "ZeRO Stage 3: Parameter Partitioning"
        subgraph GPU0["GPU 0"]
            P0[Parameters<br/>17.5GB (1/8)]
            G0[Gradients<br/>17.5GB (1/8)]
            O0[Optimizer States<br/>35GB (1/8)]
        end
        
        subgraph GPU1["GPU 1"]
            P1[Parameters<br/>17.5GB (1/8)]
            G1[Gradients<br/>17.5GB (1/8)]
            O1[Optimizer States<br/>35GB (1/8)]
        end
        
        subgraph GPU2["GPU 2"]
            P2[Parameters<br/>17.5GB (1/8)]
            G2[Gradients<br/>17.5GB (1/8)]
            O2[Optimizer States<br/>35GB (1/8)]
        end
        
        note3[å„ GPU ã§ 70GB ã®ãƒ¡ãƒ¢ãƒª<br/>æ¨™æº–ã® 560GB ã‹ã‚‰ 87.5% å‰Šæ¸›]
    end
    
    style P0 fill:#e6f3ff
    style P1 fill:#ffe6f0
    style P2 fill:#e6ffe6
    style note3 fill:#fff4e1
```

**å‹•ä½œã®æµã‚Œ**

1. Forward Pass é–‹å§‹å‰: å¿…è¦ãªå±¤ã® Parameters ã‚’ All-Gather ã§åé›†
2. Forward Pass: è¨ˆç®—ã‚’å®Ÿè¡Œ
3. Forward Pass å®Œäº†å¾Œ: Parameters ã‚’ç ´æ£„ï¼ˆä»–ã® GPU ãŒä¿æŒï¼‰
4. Backward Pass: åŒæ§˜ã« Parameters ã‚’ All-Gather â†’ è¨ˆç®— â†’ ç ´æ£„
5. Reduce-Scatter: è‡ªåˆ†ã®æ‹…å½“ Gradients ã‚’é›†ç´„
6. Parameter Update: è‡ªåˆ†ã®æ‹…å½“ Parameters ã‚’æ›´æ–°

**ãƒ¡ãƒ¢ãƒªå‰Šæ¸›åŠ¹æœï¼ˆ70B ãƒ¢ãƒ‡ãƒ«ã€8 GPU ã®å ´åˆï¼‰**
- æ¨™æº– Data Parallelism: 560GB/GPU
- ZeRO Stage 3: 70GB/GPUï¼ˆ**87.5% å‰Šæ¸›**ï¼‰

### ZeRO ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•

::::details é€šä¿¡é‡ã®å¢—åŠ 
ZeRO ã¯ãƒ¡ãƒ¢ãƒªã‚’å‰Šæ¸›ã™ã‚‹ä»£ã‚ã‚Šã«ã€GPU é–“é€šä¿¡ãŒå¢—åŠ ã—ã¾ã™ã€‚

| Stage | ãƒ¡ãƒ¢ãƒªå‰Šæ¸› | è¿½åŠ é€šä¿¡é‡ | é©ç”¨å ´é¢ |
|-------|-----------|-----------|----------|
| **Stage 1** | 44% | æœ€å° | é€šä¿¡å¸¯åŸŸå¹…ãŒé™ã‚‰ã‚Œã¦ã„ã‚‹å ´åˆ |
| **Stage 2** | 66% | ä¸­ç¨‹åº¦ | ãƒãƒ©ãƒ³ã‚¹ã®å–ã‚ŒãŸé¸æŠ |
| **Stage 3** | 87.5% | å¤§ãã„ | ãƒ¡ãƒ¢ãƒªãŒæœ€å¤§ã®åˆ¶ç´„ã®å ´åˆ |

é€šä¿¡å¸¯åŸŸå¹…ãŒååˆ†ã«ã‚ã‚‹ç’°å¢ƒï¼ˆNVLinkã€InfiniBandï¼‰ã§ã¯ã€Stage 3 ã§ã‚‚åŠ¹ç‡çš„ã«å‹•ä½œã—ã¾ã™ã€‚
::::

::::details ZeRO-Offload ã¨ ZeRO-Infinity
DeepSpeed ã¯ ZeRO ã‚’ã•ã‚‰ã«æ‹¡å¼µã—ãŸæ©Ÿèƒ½ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚

**ZeRO-Offload**
- Optimizer States ã‚’ CPU ãƒ¡ãƒ¢ãƒªã«ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰
- GPU ãƒ¡ãƒ¢ãƒªã‚’ã•ã‚‰ã«å‰Šæ¸›
- GPU ã¨ CPU é–“ã®é€šä¿¡ãŒå¿…è¦

**ZeRO-Infinity**
- Parametersã€Gradientsã€Optimizer States ã‚’ NVMe SSD ã«ã‚‚ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰å¯èƒ½
- äº‹å®Ÿä¸Šç„¡åˆ¶é™ã®ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«å¯¾å¿œ
- ãŸã ã— I/O ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ãŒç™ºç”Ÿ

ã“ã‚Œã‚‰ã®æ‰‹æ³•ã«ã‚ˆã‚Šã€1 å…†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä»¥ä¸Šã®ãƒ¢ãƒ‡ãƒ«ã‚‚å­¦ç¿’å¯èƒ½ã«ãªã‚Šã¾ã™ã€‚
::::

## åˆ†æ•£å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®æ¯”è¼ƒ

å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã®ä¸»è¦ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¨ãƒ„ãƒ¼ãƒ«ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚

### PyTorch DDP (Distributed Data Parallel)

::::details æ¦‚è¦
PyTorch ã®æ¨™æº–çš„ãªåˆ†æ•£å­¦ç¿’æ©Ÿèƒ½ã§ã™ã€‚Data Parallelism ã®ã¿ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ãŒã€ã‚·ãƒ³ãƒ—ãƒ«ã§ä½¿ã„ã‚„ã™ãã€å¤šãã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§æ¡ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚PyTorch 1.0 ã‹ã‚‰æ¨™æº–æ­è¼‰ã•ã‚Œã€æ¥­ç•Œæ¨™æº–ã®åˆ†æ•£å­¦ç¿’ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆã¨ã—ã¦åºƒãèªçŸ¥ã•ã‚Œã¦ã„ã¾ã™ã€‚
::::

**ç‰¹å¾´**
- PyTorch ã«æ¨™æº–æ­è¼‰ï¼ˆè¿½åŠ ã®ä¾å­˜é–¢ä¿‚ä¸è¦ï¼‰
- Data Parallelism ã®ã¿ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ã§ç†è§£ã—ã‚„ã™ã„ï¼‰
- Ring All-Reduce ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªå‹¾é…åŒæœŸ
- Gradient bucketing ã«ã‚ˆã‚‹é€šä¿¡ã¨è¨ˆç®—ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—
- ãƒ—ãƒ­ã‚»ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ã®æŸ”è»Ÿãªç®¡ç†
- æ—¢å­˜ã‚³ãƒ¼ãƒ‰ã¸ã®çµ±åˆãŒå®¹æ˜“ï¼ˆæ•°è¡Œã®å¤‰æ›´ã§å¯¾å¿œå¯èƒ½ï¼‰

**PyTorch ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã‘ã‚‹ä½ç½®ä»˜ã‘**

DDP ã¯ PyTorch ã®åˆ†æ•£å­¦ç¿’ã«ãŠã‘ã‚‹åŸºç›¤æŠ€è¡“ã§ã‚ã‚Šã€ä»¥ä¸‹ã®ç‰¹å¾´ãŒã‚ã‚Šã¾ã™ã€‚

- **æ¥­ç•Œæ¨™æº–ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ**: ã»ã¼ã™ã¹ã¦ã® PyTorch ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæœ€åˆã«ä½¿ã†åˆ†æ•£å­¦ç¿’æ©Ÿèƒ½
- **Production ã§ã®å®Ÿç¸¾**: OpenAIã€Anthropicã€Cohere ãªã©å¤šãã®ä¼æ¥­ã§ä½¿ç”¨
- **é«˜ã„äº’æ›æ€§**: PyTorch ã®å…¨æ©Ÿèƒ½ã¨å®Œå…¨ã«çµ±åˆ
- **å®‰å®šæ€§**: é•·æœŸé–“ã®é–‹ç™ºã¨ä½¿ç”¨ã«ã‚ˆã‚Šé«˜ã„å®‰å®šæ€§ã‚’å®Ÿç¾
- **ãƒ‡ãƒãƒƒã‚°ã®ã—ã‚„ã™ã•**: ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒæ˜ç¢ºã§å•é¡Œã®ç‰¹å®šãŒå®¹æ˜“

**æŠ€è¡“çš„è©³ç´°**

::::details Ring All-Reduce ã¨æœ€é©åŒ–
DDP ã¯åŠ¹ç‡çš„ãªé€šä¿¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å®Ÿè£…ã—ã¦ã„ã¾ã™ã€‚

**Ring All-Reduce**
- å„ GPU ãŒéš£æ¥ GPU ã¨ã®ã¿é€šä¿¡
- é€šä¿¡é‡ãŒ GPU æ•°ã«ä¾å­˜ã—ãªã„ï¼ˆO(N) ã§ã¯ãªã O(1)ï¼‰
- å¸¯åŸŸå¹…ã‚’æœ€å¤§é™æ´»ç”¨

**Gradient Bucketing**
- å‹¾é…ã‚’è¤‡æ•°ã®ãƒã‚±ãƒƒãƒˆã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
- ãƒã‚±ãƒƒãƒˆå˜ä½ã§é€šä¿¡ã‚’é–‹å§‹ï¼ˆå…¨å‹¾é…ã®è¨ˆç®—å®Œäº†ã‚’å¾…ãŸãªã„ï¼‰
- é€šä¿¡ã¨è¨ˆç®—ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã«ã‚ˆã‚ŠåŠ¹ç‡åŒ–

**é€šä¿¡ã®æœ€é©åŒ–**
- å°ã•ãªå‹¾é…ãƒ†ãƒ³ã‚½ãƒ«ã‚’ã¾ã¨ã‚ã¦é€šä¿¡ï¼ˆé€šä¿¡å›æ•°ã‚’å‰Šæ¸›ï¼‰
- FP16 å‹¾é…ã®åœ§ç¸®é€šä¿¡ï¼ˆå¸¯åŸŸå¹…ã‚’ç¯€ç´„ï¼‰
- NCCL ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã«ã‚ˆã‚‹ GPU é–“é€šä¿¡ã®æœ€é©åŒ–
::::

**é©ç”¨å ´é¢**

- **å°ã€œä¸­è¦æ¨¡ãƒ¢ãƒ‡ãƒ«**ï¼ˆ10B ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä»¥ä¸‹ï¼‰ãŒå˜ä¸€ GPU ãƒ¡ãƒ¢ãƒªã«åã¾ã‚‹å ´åˆ
- **ã‚·ãƒ³ãƒ—ãƒ«ãªåˆ†æ•£å­¦ç¿’**ã‚’å®Ÿç¾ã—ãŸã„å ´åˆ
- **æ—¢å­˜ã® PyTorch ã‚³ãƒ¼ãƒ‰**ã‚’æœ€å°é™ã®å¤‰æ›´ã§åˆ†æ•£åŒ–ã—ãŸã„å ´åˆ
- **ãƒ‡ãƒãƒƒã‚°ã®ã—ã‚„ã™ã•**ã‚’é‡è¦–ã™ã‚‹å ´åˆ
- **è¿½åŠ ã®ä¾å­˜é–¢ä¿‚ã‚’é¿ã‘ãŸã„**å ´åˆ

**ä»–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã¨ã®æ¯”è¼ƒã«ãŠã‘ã‚‹ä½ç½®ä»˜ã‘**

| è¦³ç‚¹ | DDP | FSDP | DeepSpeed |
|------|-----|------|-----------|
| ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ | ä½ | é«˜ | é«˜ |
| å®Ÿè£…ã®è¤‡é›‘ã• | ä½ | ä¸­ | é«˜ |
| ãƒ‡ãƒãƒƒã‚°ã®ã—ã‚„ã™ã• | é«˜ | ä¸­ | ä½ |
| ä¾å­˜é–¢ä¿‚ | ãªã— | ãªã— | ã‚ã‚Š |
| ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºä¸Šé™ | ã€œ10B | ã€œ100B | 100B+ |
| å­¦ç¿’æ›²ç·š | ç·©ã‚„ã‹ | ä¸­ç¨‹åº¦ | æ€¥ |

**æ¡ç”¨äº‹ä¾‹**

- Hugging Face Transformers ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆåˆ†æ•£å­¦ç¿’æ©Ÿèƒ½
- PyTorch Lightning ã§ã®æ¨™æº–ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰
- å¤šãã®ç ”ç©¶æ©Ÿé–¢ã§ã®æ¨™æº–çš„ãªé¸æŠè‚¢
- ä¸­å°è¦æ¨¡ã®ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ã§ã®å®Ÿç”¨ä¾‹å¤šæ•°

::::details å‚è€ƒæƒ…å ±
- [PyTorch DDP Tutorial](https://pytorch.org/tutorials/intermediate/ddp_tutorial.html)
- [Getting Started with Distributed Data Parallel](https://pytorch.org/tutorials/intermediate/ddp_series_intro.html)
- [DDP API Documentation](https://pytorch.org/docs/stable/generated/torch.nn.parallel.DistributedDataParallel.html)
- [PyTorch Distributed Overview](https://pytorch.org/docs/stable/distributed.html)
::::

### DeepSpeed

::::details æ¦‚è¦
Microsoft Research ãŒé–‹ç™ºã—ãŸã€å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã«ç‰¹åŒ–ã—ãŸãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚ZeRO ã‚’å®Ÿè£…ã—ã¦ãŠã‚Šã€ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®é«˜ã„å­¦ç¿’ã‚’å®Ÿç¾ã—ã¾ã™ã€‚
::::

**ç‰¹å¾´**
- ZeRO Stage 1-3 ã«ã‚ˆã‚‹ç©æ¥µçš„ãªãƒ¡ãƒ¢ãƒªæœ€é©åŒ–
- ZeRO-Offloadã€ZeRO-Infinity ã«ã‚ˆã‚‹ã•ã‚‰ãªã‚‹ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
- Pipeline Parallelism ã®ã‚µãƒãƒ¼ãƒˆ
- FP16/BF16 Mixed Precision Training
- Gradient Accumulation
- 3D Parallelismï¼ˆData + Pipeline + Tensorï¼‰ã®ã‚µãƒãƒ¼ãƒˆ

**é©ç”¨å ´é¢**
- å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆæ•°åå„„ï½æ•°å…†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰ã®å­¦ç¿’
- GPU ãƒ¡ãƒ¢ãƒªãŒåˆ¶ç´„ã¨ãªã‚‹å ´åˆ
- è¤‡æ•°ã®ä¸¦åˆ—åŒ–æ‰‹æ³•ã‚’çµ„ã¿åˆã‚ã›ãŸã„å ´åˆ

::::details å‚è€ƒæƒ…å ±
- [DeepSpeed å…¬å¼ã‚µã‚¤ãƒˆ](https://www.deepspeed.ai/)
- [DeepSpeed æ¦‚è¦ï¼ˆæ—¥æœ¬èª PDFï¼‰](https://www.deepspeed.ai/assets/files/DeepSpeed_Overview_Japanese_2023Jun7th.pdf)
- [DeepSpeed GitHub](https://github.com/microsoft/DeepSpeed)
::::

### Megatron-LM

::::details æ¦‚è¦
NVIDIA ãŒé–‹ç™ºã—ãŸã€Transformer ãƒ¢ãƒ‡ãƒ«ã«ç‰¹åŒ–ã—ãŸ Tensor Parallelism ã®å®Ÿè£…ã§ã™ã€‚åŠ¹ç‡çš„ãªãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—åŒ–ã‚’å®Ÿç¾ã—ã¾ã™ã€‚
::::

**ç‰¹å¾´**
- é«˜åŠ¹ç‡ãª Tensor Parallelism ã®å®Ÿè£…
- Transformer ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«æœ€é©åŒ–
- Pipeline Parallelism ã®ã‚µãƒãƒ¼ãƒˆ
- Distributed Optimizerï¼ˆZeRO-1 ç›¸å½“ï¼‰
- FlashAttention ãªã©ã®æœ€é©åŒ–ã‚«ãƒ¼ãƒãƒ«çµ±åˆ

**é©ç”¨å ´é¢**
- Transformer ãƒ™ãƒ¼ã‚¹ã®å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
- ãƒãƒ¼ãƒ‰å†…ï¼ˆ8 GPUï¼‰ã§ã®åŠ¹ç‡çš„ãªãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—åŒ–
- NVIDIA GPU ã¨ InfiniBand ç’°å¢ƒ

::::details å‚è€ƒæƒ…å ±
- [Megatron-LM GitHub](https://github.com/NVIDIA/Megatron-LM)
- [Megatron-LM è«–æ–‡](https://arxiv.org/abs/1909.08053)
- [Efficient Large-Scale Language Model Training](https://arxiv.org/abs/2104.04473)
::::

### Megatron-DeepSpeed

::::details æ¦‚è¦
Megatron-LM ã¨ DeepSpeed ã‚’çµ±åˆã—ãŸãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚Tensor Parallelismã€Pipeline Parallelismã€ZeRO ã‚’çµ„ã¿åˆã‚ã›ãŸ 3D Parallelism ã‚’å®Ÿç¾ã—ã¾ã™ã€‚
::::

**ç‰¹å¾´**
- Megatron ã® Tensor Parallelism + DeepSpeed ã® ZeRO
- 3D Parallelismï¼ˆTensor + Pipeline + Dataï¼‰
- è¶…å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆæ•°ç™¾å„„ï½æ•°å…†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰ã«å¯¾å¿œ
- è¤‡æ•°ã®æœ€é©åŒ–æ‰‹æ³•ã®çµ±åˆ

**é©ç”¨å ´é¢**
- è¶…å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ï¼ˆ100B+ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
- æ•°åƒï½æ•°ä¸‡ GPU ã§ã®å­¦ç¿’
- æœ€é«˜ã®åŠ¹ç‡ãŒæ±‚ã‚ã‚‰ã‚Œã‚‹å ´åˆ

::::details å‚è€ƒæƒ…å ±
- [Megatron-DeepSpeed GitHub](https://github.com/microsoft/Megatron-DeepSpeed)
- [Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B](https://arxiv.org/abs/2201.11990)
::::

### GPT-NeoX

::::details æ¦‚è¦
EleutherAI ãŒé–‹ç™ºã—ãŸã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã«ç‰¹åŒ–ã—ãŸãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚Megatron-LM ã¨ DeepSpeed ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€usability ã¨æœ€é©åŒ–ã‚’å¤§å¹…ã«å‘ä¸Šã•ã›ã¦ã„ã¾ã™ã€‚
::::

**ç‰¹å¾´**
- Megatron ã® Tensor Parallelism ã¨ DeepSpeed ã® ZeRO ã‚’çµ±åˆ
- 3D Parallelismï¼ˆData + Tensor + Pipelineï¼‰ã®ã‚µãƒãƒ¼ãƒˆ
- å¤šæ§˜ãªã‚·ã‚¹ãƒ†ãƒ ã¨ãƒ©ãƒ³ãƒãƒ£ãƒ¼å¯¾å¿œï¼ˆSlurmã€MPIã€IBM Job Step Managerï¼‰
- AWSã€CoreWeaveã€ORNL Summit/Frontierã€LUMI ãªã©ã§å¤§è¦æ¨¡é‹ç”¨å®Ÿç¸¾
- Flash Attentionã€Transformer Engine çµ±åˆ
- Rotary/ALiBi positional embeddingsã€parallel feedforward attention layers
- äº‹å‰è¨­å®šæ¸ˆã¿ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼ˆPythiaã€PaLMã€Falconã€LLaMA 1&2ï¼‰
- Curriculum Learning ã‚µãƒãƒ¼ãƒˆ
- Hugging Face ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã¨ã®é€£æºï¼ˆtokenizersã€transformersã€Evaluation Harnessï¼‰

**2024 å¹´ã®ä¸»è¦ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ**
- Transformer Engine çµ±åˆï¼ˆ2024 å¹´ 10 æœˆï¼‰
- DPOã€KTO ã«ã‚ˆã‚‹ preference learningã€Comet ML çµ±åˆï¼ˆ2024 å¹´ 9 æœˆï¼‰
- RWKV with pipeline parallelismï¼ˆ2024 å¹´ 5 æœˆï¼‰
- Mixture-of-Experts (MoE)ã€AMD MI250X GPU ã‚µãƒãƒ¼ãƒˆï¼ˆ2024 å¹´ 3 æœˆï¼‰
- Mamba with tensor parallelismï¼ˆ2024 å¹´ 3 æœˆï¼‰

**æ¡ç”¨å®Ÿç¸¾**
Oak Ridge National Labã€CarperAIã€Stability AIã€Together.aiã€Korea Universityã€Carnegie Mellon Universityã€University of Tokyo ãªã©ã€å­¦è¡“æ©Ÿé–¢ã€ç”£æ¥­ç•Œã€æ”¿åºœç ”ç©¶æ‰€ã§åºƒãæ¡ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚

**é©ç”¨å ´é¢**
- å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®äº‹å‰å­¦ç¿’ï¼ˆæ•°åå„„ã€œæ•°ç™¾å„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
- å­¦è¡“ç ”ç©¶ã‚„å®Ÿé¨“çš„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ
- Megatron-DeepSpeed ã‚ˆã‚Šã‚‚ä½¿ã„ã‚„ã™ã•ã‚’é‡è¦–ã™ã‚‹å ´åˆ
- å¤šæ§˜ãªãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ç’°å¢ƒã§ã®é‹ç”¨

**æ³¨æ„ç‚¹**
- æ•°åå„„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä»¥ä¸Šã®ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¹ã‚¯ãƒ©ãƒƒãƒã‹ã‚‰å­¦ç¿’ã™ã‚‹ç”¨é€”ã«ç‰¹åŒ–
- æ¨è«–ã‚„ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¯ Hugging Face transformers ã®ä½¿ç”¨ã‚’æ¨å¥¨

::::details å‚è€ƒæƒ…å ±
- [GPT-NeoX GitHub](https://github.com/EleutherAI/gpt-neox)
- [GPT-NeoX-20B: An Open-Source Autoregressive Language Model](https://arxiv.org/abs/2204.06745)
- [GPT-NeoX Documentation](https://www.eleuther.ai/artifacts/gpt-neox)
::::

### Nanotron

::::details æ¦‚è¦
Hugging Face ãŒé–‹ç™ºã—ãŸã€ãƒ¢ãƒ€ãƒ³ã§ä½¿ã„ã‚„ã™ã„åˆ†æ•£å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚Llama ãªã©ã®äººæ°—ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚
::::

**ç‰¹å¾´**
- ã‚¯ãƒªãƒ¼ãƒ³ã§ä¿å®ˆæ€§ã®é«˜ã„ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹
- 3D Parallelism ã®ã‚µãƒãƒ¼ãƒˆ
- Hugging Face ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã¨ã®çµ±åˆ
- ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªé–‹ç™ºã¨ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã‚µãƒãƒ¼ãƒˆ

**é©ç”¨å ´é¢**
- ç ”ç©¶ãƒ»å®Ÿé¨“ã§ã®ä½¿ç”¨
- Hugging Face ãƒ¢ãƒ‡ãƒ«ã®ç¶™ç¶šå­¦ç¿’ã‚„ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
- ã‚³ãƒ¼ãƒ‰ã®å¯èª­æ€§ã¨ä¿å®ˆæ€§ã‚’é‡è¦–ã™ã‚‹å ´åˆ

::::details å‚è€ƒæƒ…å ±
- [Nanotron GitHub](https://github.com/huggingface/nanotron)
::::

### Picotron

::::details æ¦‚è¦
Hugging Face ãŒ 2025 å¹´ã«ãƒªãƒªãƒ¼ã‚¹ã—ãŸæ•™è‚²ãƒ»å®Ÿé¨“ç”¨ã®åˆ†æ•£å­¦ç¿’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚NanoGPT ã®ç²¾ç¥ï¼ˆãƒŸãƒ‹ãƒãƒªã‚¹ãƒˆã€hackableï¼‰ã‚’ç¶™æ‰¿ã—ã€åˆ†æ•£å­¦ç¿’æŠ€è¡“ã®å­¦ç¿’ã«æœ€é©åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚
::::

**ç‰¹å¾´**
- æ¥µã‚ã¦ã‚·ãƒ³ãƒ—ãƒ«ãªã‚³ãƒ¼ãƒ‰æ§‹æˆï¼ˆå„ãƒ•ã‚¡ã‚¤ãƒ«ãŒ 300 è¡Œæœªæº€ï¼‰
- 4D Parallelismï¼ˆData + Tensor + Pipeline + Context parallelï¼‰
- ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«å‹•ç”»ã¨ã‚³ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹
- æ•™è‚²ç›®çš„ã«ç‰¹åŒ–ã—ãŸè¨­è¨ˆ
- Llama-like ãƒ¢ãƒ‡ãƒ«ã®äº‹å‰å­¦ç¿’ã‚’ã‚µãƒãƒ¼ãƒˆ

**ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**
- LLaMA-2-7B: 38% MFUï¼ˆ64 H100 GPUï¼‰
- SmolLM-1.7B: 50% MFUï¼ˆ8 H100 GPUï¼‰
- æ€§èƒ½ã‚ˆã‚Šå­¦ç¿’åŠ¹æœã‚’å„ªå…ˆã—ãŸè¨­è¨ˆ

**Nanotron ã¨ã®é•ã„**
- **Picotron**: æ•™è‚²ãƒ»å­¦ç¿’ç”¨é€”ã«ç‰¹åŒ–ã€ã‚³ãƒ¼ãƒ‰ã®èª­ã¿ã‚„ã™ã•ã¨ç†è§£ã—ã‚„ã™ã•ã‚’æœ€å„ªå…ˆ
- **Nanotron**: ç ”ç©¶ãƒ»å®Ÿé¨“ç”¨é€”ã€production ãƒ¬ãƒ™ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¨æ©Ÿèƒ½ã‚’æä¾›

**é©ç”¨å ´é¢**
- åˆ†æ•£å­¦ç¿’æŠ€è¡“ã®å­¦ç¿’ãƒ»æ•™è‚²
- ä¸¦åˆ—åŒ–æ‰‹æ³•ã®å®Ÿé¨“ã¨ç†è§£
- å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã®å…¥é–€

**æ³¨æ„ç‚¹**
- æ€§èƒ½ã¯ä»–ã®ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚ˆã‚ŠåŠ£ã‚‹ï¼ˆæ•™è‚²ç›®çš„ã®ãŸã‚ï¼‰
- 2025 å¹´ãƒªãƒªãƒ¼ã‚¹ã®éå¸¸ã«æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ

::::details å‚è€ƒæƒ…å ±
- [Picotron GitHub](https://github.com/huggingface/picotron)
- [Picotron Tutorial (Playlist)](https://www.youtube.com/playlist?list=PLo2EIpI_JMQtNtKNFFSMNIZwspj8H7-sQ)
::::

### PyTorch FSDP (Fully Sharded Data Parallel)

::::details æ¦‚è¦
PyTorch 1.11 ä»¥é™ã§æ¨™æº–æ­è¼‰ã•ã‚ŒãŸã€ZeRO Stage 3 ç›¸å½“ã®æ©Ÿèƒ½ã‚’æä¾›ã™ã‚‹åˆ†æ•£å­¦ç¿’æ©Ÿèƒ½ã§ã™ã€‚Meta ãŒ DeepSpeed ZeRO ã®ç ”ç©¶æˆæœã‚’ PyTorch ãƒã‚¤ãƒ†ã‚£ãƒ–ã«å®Ÿè£…ã—ãŸã‚‚ã®ã§ã‚ã‚Šã€Llama 2ã€Llama 3 ãªã©ã®å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã§å®Ÿéš›ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚
::::

**ç‰¹å¾´**
- ZeRO Stage 3 ã¨åŒç­‰ã®ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ï¼ˆParametersã€Gradientsã€Optimizer States ã‚’åˆ†å‰²ï¼‰
- PyTorch ãƒã‚¤ãƒ†ã‚£ãƒ–ã®å®Ÿè£…ï¼ˆè¿½åŠ ã®ä¾å­˜é–¢ä¿‚ä¸è¦ï¼‰
- æŸ”è»Ÿãª Sharding Strategyï¼ˆFULL_SHARDã€SHARD_GRAD_OPã€HYBRID_SHARD ãªã©ï¼‰
- Mixed Precision Training ã®ã‚µãƒãƒ¼ãƒˆï¼ˆBF16ã€FP16ï¼‰
- Activation Checkpointing ã¨ã®çµ±åˆ
- CPU Offloading ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³
- Transformer Auto Wrap Policy ã«ã‚ˆã‚‹è‡ªå‹•ãƒ¢ãƒ‡ãƒ«åˆ†å‰²

**PyTorch ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã‘ã‚‹ä½ç½®ä»˜ã‘**

FSDP ã¯ã€PyTorch ã«ãŠã‘ã‚‹æ¬¡ä¸–ä»£ã®åˆ†æ•£å­¦ç¿’æ©Ÿèƒ½ã¨ã—ã¦ä½ç½®ä»˜ã‘ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚

- **DDP ã®é€²åŒ–ç‰ˆ**: ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã‚’å¤§å¹…ã«æ”¹å–„ã—ãªãŒã‚‰ DDP ã®ä½¿ã„ã‚„ã™ã•ã‚’ç¶­æŒ
- **Meta ã®å…¬å¼é¸æŠ**: Llama ã‚·ãƒªãƒ¼ã‚ºã®å­¦ç¿’ã§å®Ÿéš›ã«ä½¿ç”¨
- **PyTorch ã®æˆ¦ç•¥çš„æ©Ÿèƒ½**: PyTorch ãƒãƒ¼ãƒ ãŒç©æ¥µçš„ã«é–‹ç™ºãƒ»æ”¹å–„
- **ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã¨ã®çµ±åˆ**: Hugging Face Transformersã€PyTorch Lightning ãªã©ã§ã‚µãƒãƒ¼ãƒˆæ‹¡å¤§
- **é•·æœŸçš„ãªã‚µãƒãƒ¼ãƒˆ**: PyTorch ã®æ¨™æº–æ©Ÿèƒ½ã¨ã—ã¦ç¶™ç¶šçš„ãªãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹

**ZeRO Stage 3 ã¨ã®é–¢ä¿‚**

FSDP ã¯ DeepSpeed ZeRO Stage 3 ã¨åŒã˜ã‚³ãƒ³ã‚»ãƒ—ãƒˆã‚’ PyTorch ãƒã‚¤ãƒ†ã‚£ãƒ–ã«å®Ÿè£…ã—ãŸã‚‚ã®ã§ã™ã€‚

| æ©Ÿèƒ½ | FSDP | DeepSpeed ZeRO-3 |
|------|------|------------------|
| Parameter Sharding | âœ… | âœ… |
| Gradient Sharding | âœ… | âœ… |
| Optimizer State Sharding | âœ… | âœ… |
| CPU Offloading | âœ… | âœ…ï¼ˆZeRO-Offloadï¼‰ |
| NVMe Offloading | âŒ | âœ…ï¼ˆZeRO-Infinityï¼‰ |
| Pipeline Parallelism | âŒ | âœ… |
| ä¾å­˜é–¢ä¿‚ | PyTorch ã®ã¿ | DeepSpeed ãƒ©ã‚¤ãƒ–ãƒ©ãƒª |

**DeepSpeed ã¨ã®æ¯”è¼ƒ**

::::details ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã¨æ©Ÿèƒ½ã®æ¯”è¼ƒ
**ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–**
- FSDP: ZeRO Stage 3 ç›¸å½“ï¼ˆ87.5% å‰Šæ¸›ï¼‰
- DeepSpeed: ZeRO Stage 1-3 ã‚’é¸æŠå¯èƒ½ã€ZeRO-Infinity ã§ NVMe ã‚‚æ´»ç”¨

**å®Ÿè£…ã®è¤‡é›‘ã•**
- FSDP: PyTorch ã®æ¨™æº– API ã§ç›´æ„Ÿçš„
- DeepSpeed: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«ã‚ˆã‚‹è©³ç´°ãªåˆ¶å¾¡ï¼ˆå­¦ç¿’æ›²ç·šãŒæ€¥ï¼‰

**ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹**
- ä¸¡è€…ã¨ã‚‚ã»ã¼åŒç­‰ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ
- ç’°å¢ƒã‚„ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦å„ªåŠ£ãŒå¤‰ã‚ã‚‹
- FSDP ã¯ PyTorch ã¨ã®çµ±åˆã«ã‚ˆã‚Šæœ€é©åŒ–ã®ä½™åœ°

**æ©Ÿèƒ½ã®è±Šå¯Œã•**
- FSDP: åŸºæœ¬çš„ãªãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã«ç‰¹åŒ–
- DeepSpeed: Pipeline Parallelismã€ZeRO-Infinityã€DeepSpeed-Chat ãªã©å¤šæ©Ÿèƒ½

**é©ç”¨ç¯„å›²**
- FSDP: 10Bï½100B ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ¢ãƒ‡ãƒ«ã«æœ€é©
- DeepSpeed: 100B+ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¶…å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã«ã‚‚å¯¾å¿œ
::::

**Meta ã§ã®ä½¿ç”¨ä¾‹**

Meta ã¯è‡ªç¤¾ã®å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã§ FSDP ã‚’ç©æ¥µçš„ã«æ¡ç”¨ã—ã¦ã„ã¾ã™ã€‚

::::details Llama ã‚·ãƒªãƒ¼ã‚ºã§ã®å®Ÿç¸¾
**Llama 2 (7B, 13B, 70B)**
- FSDP ã‚’ä½¿ç”¨ã—ã¦åŠ¹ç‡çš„ã«å­¦ç¿’
- æ•°ç™¾ã€œæ•°åƒ GPU ã§ã®å­¦ç¿’å®Ÿç¸¾

**Llama 3 (8B, 70B, 405B)**
- FSDP ã«ã‚ˆã‚‹å¤§è¦æ¨¡åˆ†æ•£å­¦ç¿’
- 16,000 H100 GPU ã§ã® 405B ãƒ¢ãƒ‡ãƒ«å­¦ç¿’
- PyTorch ãƒã‚¤ãƒ†ã‚£ãƒ–ã®ãƒ¡ãƒªãƒƒãƒˆã‚’æ´»ç”¨

**Meta ã®é¸æŠç†ç”±**
1. PyTorch ã®é–‹ç™ºå…ƒã¨ã—ã¦ã®çŸ¥è¦‹ã‚’æ´»ç”¨
2. è‡ªç¤¾ã‚¤ãƒ³ãƒ•ãƒ©ã¨ã®æ·±ã„çµ±åˆ
3. é•·æœŸçš„ãªãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ä¿è¨¼
4. ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¸ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯å¾ªç’°
::::

**æŠ€è¡“çš„è©³ç´°**

::::details Sharding Strategy
FSDP ã¯è¤‡æ•°ã® sharding strategy ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚

**FULL_SHARDï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰**
- Parametersã€Gradientsã€Optimizer States ã™ã¹ã¦ã‚’åˆ†å‰²
- æœ€å¤§ã®ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ï¼ˆZeRO Stage 3 ç›¸å½“ï¼‰
- é€šä¿¡é‡ã¯æœ€å¤§

**SHARD_GRAD_OP**
- Gradients ã¨ Optimizer States ã®ã¿ã‚’åˆ†å‰²
- ZeRO Stage 2 ç›¸å½“
- ãƒ¡ãƒ¢ãƒªã¨é€šä¿¡ã®ãƒãƒ©ãƒ³ã‚¹

**HYBRID_SHARD**
- ãƒãƒ¼ãƒ‰å†…ã§ FULL_SHARDã€ãƒãƒ¼ãƒ‰é–“ã§ SHARD_GRAD_OP
- ãƒãƒ¼ãƒ‰å†…ã®é«˜é€Ÿé€šä¿¡ï¼ˆNVLinkï¼‰ã¨ãƒãƒ¼ãƒ‰é–“é€šä¿¡ã®ãƒãƒ©ãƒ³ã‚¹
- å¤§è¦æ¨¡ã‚¯ãƒ©ã‚¹ã‚¿ã§åŠ¹æœçš„

**NO_SHARD**
- åˆ†å‰²ãªã—ï¼ˆDDP ã¨åŒç­‰ï¼‰
- ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ãªã—ã€é€šä¿¡ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰æœ€å°
::::

::::details CPU Offloading ã¨æœ€é©åŒ–
**CPU Offloading**
- Parameters ã¨ Gradients ã‚’ CPU ãƒ¡ãƒ¢ãƒªã«ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰å¯èƒ½
- GPU ãƒ¡ãƒ¢ãƒªã‚’ã•ã‚‰ã«å‰Šæ¸›
- PCIe å¸¯åŸŸå¹…ãŒãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã«ãªã‚‹å¯èƒ½æ€§

**Transformer Auto Wrap Policy**
- Transformer ãƒ–ãƒ­ãƒƒã‚¯å˜ä½ã§è‡ªå‹•çš„ã«ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ©ãƒƒãƒ—
- æ‰‹å‹•ã§ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æŒ‡å®šãŒä¸è¦
- Hugging Face ãƒ¢ãƒ‡ãƒ«ã¨ã®ç›¸æ€§ãŒè‰¯ã„

**é€šä¿¡ã®æœ€é©åŒ–**
- Backward Prefetch: æ¬¡ã®å±¤ã® Parameters ã‚’å…ˆèª­ã¿
- Forward Prefetch: Forward ä¸­ã«æ¬¡ã®å±¤ã‚’æº–å‚™
- é€šä¿¡ã¨è¨ˆç®—ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’æœ€å¤§åŒ–
::::

**é©ç”¨å ´é¢**

- **ä¸­ã€œå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«**ï¼ˆ10Bï½100B ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰ã§ãƒ¡ãƒ¢ãƒªãŒåˆ¶ç´„ã¨ãªã‚‹å ´åˆ
- **PyTorch ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ **ã§å®Œçµã•ã›ãŸã„å ´åˆ
- **DeepSpeed ã®ä¾å­˜é–¢ä¿‚**ã‚’é¿ã‘ãŸã„å ´åˆ
- **Meta ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹**ã«å¾“ã„ãŸã„å ´åˆ
- **ç¶™ç¶šçš„ãªã‚µãƒãƒ¼ãƒˆ**ãŒå¿…è¦ãª production ç’°å¢ƒ
- **Hugging Face ãƒ¢ãƒ‡ãƒ«**ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆ

**DDP ã‹ã‚‰ã®ç§»è¡Œ**

::::details FSDP ã¸ã®ç§»è¡Œã‚¬ã‚¤ãƒ‰
DDP ã‹ã‚‰ FSDP ã¸ã®ç§»è¡Œã¯æ¯”è¼ƒçš„å®¹æ˜“ã§ã™ã€‚

**å¿…è¦ãªå¤‰æ›´ï¼ˆæœ€å°é™ï¼‰**
```python
# DDP ã®å ´åˆ
from torch.nn.parallel import DistributedDataParallel as DDP
model = DDP(model, device_ids=[rank])

# FSDP ã®å ´åˆ
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
model = FSDP(model)
```

**æ®µéšçš„ãªç§»è¡Œæˆ¦ç•¥**
1. ã¾ãš DDP ã§å‹•ä½œç¢ºèª
2. ãƒ¡ãƒ¢ãƒªãŒä¸è¶³ã™ã‚‹å ´åˆã« FSDP ã¸ç§»è¡Œ
3. Sharding Strategy ã‚’èª¿æ•´ã—ã¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–
4. å¿…è¦ã«å¿œã˜ã¦ CPU Offloading ã‚’æœ‰åŠ¹åŒ–

**ç§»è¡Œã®ãƒ¡ãƒªãƒƒãƒˆ**
- ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã®åˆ¶ç´„ã‚’çªç ´
- åŒã˜ GPU æ•°ã§ã‚ˆã‚Šå¤§ããªãƒãƒƒãƒã‚µã‚¤ã‚º
- PyTorch ã®æœ€æ–°æ©Ÿèƒ½ã‚’æ´»ç”¨
::::

**åˆ¶ç´„äº‹é …**

- Pipeline Parallelism ã¯æœªå¯¾å¿œï¼ˆData Parallelism ã®ã¿ï¼‰
- NVMe ã¸ã®ã‚ªãƒ•ãƒ­ãƒ¼ãƒ‰æ©Ÿèƒ½ãªã—ï¼ˆDeepSpeed ZeRO-Infinity ç›¸å½“ã®æ©Ÿèƒ½ã¯æœªå®Ÿè£…ï¼‰
- ä¸€éƒ¨ã® PyTorch æ©Ÿèƒ½ã¨ã®äº’æ›æ€§ã«æ³¨æ„ãŒå¿…è¦ï¼ˆcompileã€TorchScript ãªã©ï¼‰
- DeepSpeed ã¨æ¯”è¼ƒã™ã‚‹ã¨æ©Ÿèƒ½ã¯é™å®šçš„

::::details å‚è€ƒæƒ…å ±
- [PyTorch FSDP Tutorial](https://pytorch.org/tutorials/intermediate/FSDP_tutorial.html)
- [FSDP API Documentation](https://pytorch.org/docs/stable/fsdp.html)
- [Getting Started with FSDP](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/)
- [Meta's FSDP Best Practices](https://pytorch.org/tutorials/intermediate/FSDP_adavnced_tutorial.html)
- [Hugging Face: Training with FSDP](https://huggingface.co/docs/transformers/main/en/fsdp)
::::

### Monarch

::::details æ¦‚è¦
Metaï¼ˆFacebookï¼‰ãŒé–‹ç™ºã—ãŸ PyTorch å‘ã‘ã®åˆ†æ•£ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã§ã™ã€‚è¤‡æ•°ã® GPU ã‚„ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚’ä½¿ã£ãŸå¤§è¦æ¨¡ãªæ©Ÿæ¢°å­¦ç¿’ã‚’ã€ç°¡å˜ã«ãƒ»å®‰å…¨ã«ãƒ»é«˜é€Ÿã«å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚
::::

**ç‰¹å¾´**
- é«˜ãƒ¬ãƒ™ãƒ«ã®æŠ½è±¡åŒ–ã«ã‚ˆã‚Šåˆ†æ•£ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã‚’ç°¡ç´ åŒ–
- è¤‡æ•°ã®ä¸¦åˆ—åŒ–æˆ¦ç•¥ã‚’çµ±ä¸€çš„ã«æ‰±ãˆã‚‹
- PyTorch ã® torch.distributed API ã¨ã®äº’æ›æ€§
- ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã¨éšœå®³å¾©æ—§æ©Ÿèƒ½ã®å¼·åŒ–

**é©ç”¨å ´é¢**
- Meta ã®å¤§è¦æ¨¡å­¦ç¿’ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£
- è¤‡é›‘ãªåˆ†æ•£å­¦ç¿’ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å®Ÿè£…
- éšœå®³ãŒé »ç¹ã«ç™ºç”Ÿã™ã‚‹å¤§è¦æ¨¡ã‚¯ãƒ©ã‚¹ã‚¿ç’°å¢ƒ

::::details å‚è€ƒæƒ…å ±
- [Monarch ã«é–¢ã™ã‚‹ Zenn ã‚¹ã‚¯ãƒ©ãƒƒãƒ—](https://zenn.dev/tosshi/scraps/d36bb9b3168809)
::::

### ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯æ¯”è¼ƒè¡¨

| ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ | Data Parallel | Tensor Parallel | Pipeline Parallel | ZeRO | é›£æ˜“åº¦ | ä¸»ãªç”¨é€” |
|--------------|---------------|-----------------|-------------------|------|--------|----------|
| **PyTorch DDP** | âœ… | âŒ | âŒ | âŒ | ä½ | æ¨™æº–çš„ãªåˆ†æ•£å­¦ç¿’ |
| **PyTorch FSDP** | âœ… | âŒ | âŒ | âœ… (Stage 3) | ä½ | ãƒ¡ãƒ¢ãƒªåŠ¹ç‡é‡è¦– |
| **DeepSpeed** | âœ… | âŒ | âœ… | âœ… (Stage 1-3) | ä¸­ | å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ |
| **Megatron-LM** | âœ… | âœ… | âœ… | éƒ¨åˆ†çš„ | é«˜ | Transformer æœ€é©åŒ– |
| **Megatron-DeepSpeed** | âœ… | âœ… | âœ… | âœ… | é«˜ | è¶…å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ« |
| **GPT-NeoX** | âœ… | âœ… | âœ… | âœ… | ä¸­ã€œé«˜ | ç ”ç©¶ãƒ»å­¦è¡“ç”¨é€” |
| **Nanotron** | âœ… | âœ… | âœ… | âœ… | ä¸­ | ç ”ç©¶ãƒ»å®Ÿé¨“ |
| **Picotron** | âœ… | âœ… | âœ… | âœ… | ä½ã€œä¸­ | æ•™è‚²ãƒ»å­¦ç¿’ç”¨é€” |
| **Monarch** | âœ… | âœ… | âœ… | - | ä¸­ | Meta å†…éƒ¨ä½¿ç”¨ |

### PyTorch ãƒã‚¤ãƒ†ã‚£ãƒ–ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®é¸æŠã‚¬ã‚¤ãƒ‰

PyTorch ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ å†…ã§åˆ†æ•£å­¦ç¿’ã‚’è¡Œã†å ´åˆã€DDP ã¨ FSDP ã®ã©ã¡ã‚‰ã‚’é¸ã¶ã¹ãã‹ã‚’åˆ¤æ–­ã™ã‚‹ãŸã‚ã®ã‚¬ã‚¤ãƒ‰ã§ã™ã€‚

**ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã«ã‚ˆã‚‹é¸æŠ**

```mermaid
graph TD
    Start[ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º] --> Size{ãƒ¢ãƒ‡ãƒ«ãŒå˜ä¸€ GPU<br/>ãƒ¡ãƒ¢ãƒªã«åã¾ã‚‹ã‹?}
    
    Size -->|Yes<br/>10B ä»¥ä¸‹| DDP[PyTorch DDP ã‚’ä½¿ç”¨]
    Size -->|No<br/>10B ä»¥ä¸Š| Memory{ãƒ¡ãƒ¢ãƒªåˆ¶ç´„ãŒ<br/>æœ€å¤§ã®èª²é¡Œã‹?}
    
    Memory -->|Yes| FSDP_Choice{PyTorch ã®ã¿ã§<br/>å®Œçµã—ãŸã„ã‹?}
    Memory -->|No<br/>Pipeline ã‚‚å¿…è¦| DeepSpeed[DeepSpeed ã‚’æ¤œè¨]
    
    FSDP_Choice -->|Yes| FSDP[PyTorch FSDP ã‚’ä½¿ç”¨]
    FSDP_Choice -->|No<br/>é«˜åº¦ãªæ©Ÿèƒ½ãŒå¿…è¦| DeepSpeed2[DeepSpeed ã‚’ä½¿ç”¨]
    
    DDP --> Check{ãƒ¡ãƒ¢ãƒªä¸è¶³?}
    Check -->|Yes| FSDP
    Check -->|No| Success[å­¦ç¿’é–‹å§‹]
    
    FSDP --> Success
    DeepSpeed --> Success
    DeepSpeed2 --> Success
    
    style DDP fill:#e6f3ff
    style FSDP fill:#ffe6f0
    style DeepSpeed fill:#fff4e1
    style DeepSpeed2 fill:#fff4e1
    style Success fill:#e6ffe6
```

**ä½¿ç”¨ã‚±ãƒ¼ã‚¹åˆ¥ã®æ¨å¥¨**

| ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ | æ¨å¥¨ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ | ç†ç”± |
|------------|-----------------|------|
| ç ”ç©¶ãƒ»ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚° | **DDP** â†’ **FSDP** | ã‚·ãƒ³ãƒ—ãƒ«ã•ã‚’å„ªå…ˆã€å¿…è¦ã«å¿œã˜ã¦ FSDP ã¸ |
| å°è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆ<10Bï¼‰ | **DDP** | ãƒ¡ãƒ¢ãƒªãŒååˆ†ãªã‚‰ DDP ã§ååˆ† |
| ä¸­è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆ10B-70Bï¼‰ | **FSDP** | ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã¨ä½¿ã„ã‚„ã™ã•ã®ãƒãƒ©ãƒ³ã‚¹ |
| å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆ70B-100Bï¼‰ | **FSDP** ã¾ãŸã¯ **DeepSpeed** | FSDP ã§ååˆ†ãªå ´åˆã‚‚å¤šã„ |
| è¶…å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆ100B+ï¼‰ | **DeepSpeed** ã¾ãŸã¯ **Megatron-DeepSpeed** | Pipeline Parallelism ãŒå¿…è¦ |
| Hugging Face ãƒ¢ãƒ‡ãƒ« | **FSDP** | Transformers ã¨ã®çµ±åˆãŒè‰¯å¥½ |
| æ•™è‚²ãƒ»å­¦ç¿’ç›®çš„ | **DDP** â†’ **Picotron** | ç†è§£ã—ã‚„ã™ã•ã‚’é‡è¦– |
| Production ç’°å¢ƒ | **DDP** ã¾ãŸã¯ **FSDP** | å®‰å®šæ€§ã¨ã‚µãƒãƒ¼ãƒˆã‚’é‡è¦– |

**æ®µéšçš„ãªç§»è¡Œãƒ‘ã‚¹**

å¤šãã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ä»¥ä¸‹ã®ãƒ‘ã‚¹ã‚’è¾¿ã‚Šã¾ã™ã€‚

```mermaid
graph LR
    A[1. å˜ä¸€ GPU ã§é–‹ç™º] --> B[2. DDP ã§åˆ†æ•£åŒ–]
    B --> C{ãƒ¡ãƒ¢ãƒªä¸è¶³?}
    C -->|No| D[DDP ã§ç¶™ç¶š]
    C -->|Yes| E[3. FSDP ã¸ç§»è¡Œ]
    E --> F{ã•ã‚‰ã«å¤§è¦æ¨¡åŒ–?}
    F -->|No| G[FSDP ã§ç¶™ç¶š]
    F -->|Yes| H[4. DeepSpeed æ¤œè¨]
    
    style A fill:#e8f4f8
    style B fill:#e6f3ff
    style D fill:#e6f3ff
    style E fill:#ffe6f0
    style G fill:#ffe6f0
    style H fill:#fff4e1
```

**å®Ÿè·µçš„ãªåˆ¤æ–­åŸºæº–**

::::details ã„ã¤ DDP ã‹ã‚‰ FSDP ã«ç§»è¡Œã™ã¹ãã‹

**ç§»è¡Œã‚’æ¤œè¨ã™ã¹ãã‚µã‚¤ãƒ³**
1. OOM (Out of Memory) ã‚¨ãƒ©ãƒ¼ãŒé »ç™º
2. ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ 1 ã«ã—ã¦ã‚‚ãƒ¡ãƒ¢ãƒªä¸è¶³
3. Gradient Accumulation ã‚’ä½¿ã£ã¦ã‚‚ä¸ååˆ†
4. ã‚ˆã‚Šå¤§ããªãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã—ãŸã„

**ç§»è¡Œã—ãªã„æ–¹ãŒè‰¯ã„å ´åˆ**
1. ç¾çŠ¶ã§å•é¡Œãªãå­¦ç¿’ã§ãã¦ã„ã‚‹
2. ãƒ‡ãƒãƒƒã‚°ã‚’é »ç¹ã«è¡Œã†å¿…è¦ãŒã‚ã‚‹ï¼ˆDDP ã®æ–¹ãŒç°¡å˜ï¼‰
3. ãƒãƒ¼ãƒ å…¨å“¡ãŒ DDP ã«æ…£ã‚Œã¦ã„ã‚‹
4. æ—¢å­˜ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’å¤‰æ›´ã—ãŸããªã„

**ç§»è¡Œã®ã‚³ã‚¹ãƒˆ**
- ã‚³ãƒ¼ãƒ‰å¤‰æ›´: å°ï¼ˆæ•°è¡Œã€œæ•°åè¡Œï¼‰
- å­¦ç¿’æ›²ç·š: ä¸­ï¼ˆSharding Strategy ã®ç†è§£ãŒå¿…è¦ï¼‰
- ãƒ‡ãƒãƒƒã‚°é›£æ˜“åº¦: ã‚„ã‚„ä¸Šæ˜‡
- ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹èª¿æ•´: å¿…è¦ï¼ˆSharding Strategy ã®é¸æŠï¼‰
::::

::::details ã„ã¤ DeepSpeed ã‚’é¸ã¶ã¹ãã‹

**DeepSpeed ãŒå¿…é ˆã®å ´åˆ**
1. 100B+ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¶…å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«
2. Pipeline Parallelism ãŒå¿…è¦
3. ZeRO-Infinityï¼ˆNVMe offloadingï¼‰ãŒå¿…è¦
4. DeepSpeed-Chat ã«ã‚ˆã‚‹ RLHF ã‚’å®Ÿè£…ã—ãŸã„
5. 3D Parallelismï¼ˆData + Tensor + Pipelineï¼‰ãŒå¿…è¦

**DeepSpeed ãŒæœ‰åˆ©ãªå ´åˆ**
1. ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ã‚’æ¥µé™ã¾ã§è¿½æ±‚ã—ãŸã„
2. Microsoft ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã¨ã®çµ±åˆ
3. è±Šå¯Œãªè¨­å®šã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«ã‚ˆã‚‹ç´°ã‹ã„ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
4. ZeRO Stage 1-3 ã‚’æŸ”è»Ÿã«åˆ‡ã‚Šæ›¿ãˆãŸã„

**FSDP ã§ååˆ†ãªå ´åˆ**
1. 10Bï½70B ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒ¢ãƒ‡ãƒ«
2. PyTorch ã®ã¿ã§å®Œçµã•ã›ãŸã„
3. ã‚·ãƒ³ãƒ—ãƒ«ã•ã‚’é‡è¦–
4. Meta ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã«å¾“ã„ãŸã„
::::

**ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¯”è¼ƒã®ç›®å®‰**

| ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º | GPU æ•° | DDP | FSDP | DeepSpeed | æ¨å¥¨ |
|------------|--------|-----|------|-----------|------|
| 1B | 8 | âœ…âœ…âœ… | âœ…âœ… | âœ…âœ… | **DDP** |
| 7B | 8 | âœ…âœ… | âœ…âœ…âœ… | âœ…âœ…âœ… | **DDP** ã¾ãŸã¯ **FSDP** |
| 13B | 8 | âš ï¸ | âœ…âœ…âœ… | âœ…âœ…âœ… | **FSDP** |
| 30B | 16 | âŒ | âœ…âœ…âœ… | âœ…âœ…âœ… | **FSDP** ã¾ãŸã¯ **DeepSpeed** |
| 70B | 64 | âŒ | âœ…âœ… | âœ…âœ…âœ… | **FSDP** ã¾ãŸã¯ **DeepSpeed** |
| 175B | 256 | âŒ | âš ï¸ | âœ…âœ…âœ… | **DeepSpeed** |

**å‡¡ä¾‹**
- âœ…âœ…âœ…: æœ€é©
- âœ…âœ…: é©åˆ‡
- âœ…: å¯èƒ½ã ãŒéåŠ¹ç‡
- âš ï¸: å›°é›£
- âŒ: ä¸å¯èƒ½

## å­¦ç¿’æ‰‹æ³•ã¨ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å¯¾å¿œ

å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®é–‹ç™ºã«ã¯ã€è¤‡æ•°ã®å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚ºãŒã‚ã‚Šã¾ã™ã€‚ã“ã“ã§ã¯ä¸»è¦ãªå­¦ç¿’æ‰‹æ³•ã¨ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®å¯¾å¿œçŠ¶æ³ã‚’æ•´ç†ã—ã¾ã™ã€‚

### å­¦ç¿’æ‰‹æ³•ã®æ¦‚è¦

::::details äº‹å‰å­¦ç¿’ (Pre-training)
**äº‹å‰å­¦ç¿’**ã¯ã€å¤§é‡ã®æœªãƒ©ãƒ™ãƒ«ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ãŒè¨€èªã®çµ±è¨ˆçš„ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã™ã‚‹ãƒ•ã‚§ãƒ¼ã‚ºã§ã™ã€‚

**ç‰¹å¾´**
- æ¬¡ãƒˆãƒ¼ã‚¯ãƒ³äºˆæ¸¬ (Next Token Prediction) ã«ã‚ˆã‚‹è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’
- æ•°å…†ãƒˆãƒ¼ã‚¯ãƒ³ã®å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆä¾‹: The Pileã€Common Crawlï¼‰
- æœ€ã‚‚è¨ˆç®—ã‚³ã‚¹ãƒˆãŒé«˜ã„ï¼ˆæ•°åƒï½æ•°ä¸‡ GPU Ã— æ•°é€±é–“ï½æ•°ãƒ¶æœˆï¼‰
- ãƒ¢ãƒ‡ãƒ«ã®åŸºç¤çš„ãªè¨€èªç†è§£èƒ½åŠ›ã‚’ç²å¾—

**ä¾‹**
- GPT-3ã€LLaMAã€Mistral ãªã©ã®åŸºç›¤ãƒ¢ãƒ‡ãƒ«
- Llama 3 405B: 16,000 H100 GPU ã§ç´„ 15 å…†ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å­¦ç¿’
::::

::::details SFT (Supervised Fine-Tuning / æ•™å¸«ã‚ã‚Šå¾®èª¿æ•´)
**SFT** ã¯ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã‚„å½¢å¼ã«é©å¿œã•ã›ã‚‹ãƒ•ã‚§ãƒ¼ã‚ºã§ã™ã€‚

**ç‰¹å¾´**
- é«˜å“è³ªãªæ•™å¸«ãƒ‡ãƒ¼ã‚¿ï¼ˆè³ªå•-å›ç­”ãƒšã‚¢ã€æŒ‡ç¤º-å¿œç­”ãƒšã‚¢ãªã©ï¼‰ã‚’ä½¿ç”¨
- ãƒ‡ãƒ¼ã‚¿é‡ã¯æ¯”è¼ƒçš„å°‘ãªã„ï¼ˆæ•°åƒï½æ•°åä¸‡ã‚µãƒ³ãƒ—ãƒ«ï¼‰
- è¨ˆç®—ã‚³ã‚¹ãƒˆã¯äº‹å‰å­¦ç¿’ã‚ˆã‚Šå¤§å¹…ã«ä½ã„
- ãƒ¢ãƒ‡ãƒ«ã‚’ç‰¹å®šã®ç”¨é€”ï¼ˆãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã€ã‚³ãƒ¼ãƒ‰ç”Ÿæˆãªã©ï¼‰ã«ç‰¹åŒ–

**ä¾‹**
- ChatGPT ã®æŒ‡ç¤ºè¿½å¾“èƒ½åŠ›ã®å­¦ç¿’
- ã‚³ãƒ¼ãƒ‰ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ï¼ˆCodexï¼‰ã®ç‰¹åŒ–å­¦ç¿’
- ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹åŒ–ãƒ¢ãƒ‡ãƒ«ï¼ˆåŒ»ç™‚ã€æ³•å¾‹ãªã©ï¼‰ã®æ§‹ç¯‰
::::

::::details DPO (Direct Preference Optimization / ç›´æ¥é¸å¥½æœ€é©åŒ–)
**DPO** ã¯ã€äººé–“ã®é¸å¥½ã‚’ç›´æ¥ãƒ¢ãƒ‡ãƒ«ã«å­¦ç¿’ã•ã›ã‚‹æ‰‹æ³•ã§ã€RLHF (Reinforcement Learning from Human Feedback) ã®ä»£æ›¿ã¨ã—ã¦æ³¨ç›®ã•ã‚Œã¦ã„ã¾ã™ã€‚

**ç‰¹å¾´**
- äººé–“ãŒé¸ã‚“ã ã€Œå¥½ã¾ã—ã„å¿œç­”ã€ã¨ã€Œå¥½ã¾ã—ããªã„å¿œç­”ã€ã®ãƒšã‚¢ã‚’ä½¿ç”¨
- RLHF ã‚ˆã‚Šå®Ÿè£…ãŒã‚·ãƒ³ãƒ—ãƒ«ï¼ˆå ±é…¬ãƒ¢ãƒ‡ãƒ«ã‚„ RL ãŒä¸è¦ï¼‰
- è¨ˆç®—ã‚³ã‚¹ãƒˆã¯ SFT ã¨åŒç¨‹åº¦
- ãƒ¢ãƒ‡ãƒ«ã®å¿œç­”å“è³ªã€å®‰å…¨æ€§ã€æœ‰ç”¨æ€§ã‚’å‘ä¸Š

**RLHF ã¨ã®é•ã„**
- RLHF: å ±é…¬ãƒ¢ãƒ‡ãƒ«ã‚’åˆ¥é€”å­¦ç¿’ â†’ RL ã§æœ€é©åŒ–ï¼ˆè¤‡é›‘ã€ä¸å®‰å®šï¼‰
- DPO: é¸å¥½ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç›´æ¥æœ€é©åŒ–ï¼ˆã‚·ãƒ³ãƒ—ãƒ«ã€å®‰å®šï¼‰

**é–¢é€£æ‰‹æ³•**
- KTO (Kahneman-Tversky Optimization): DPO ã®æ”¹è‰¯ç‰ˆ
- PPO (Proximal Policy Optimization): RLHF ã§ä½¿ã‚ã‚Œã‚‹ RL ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ 

**ä¾‹**
- GPT-4 ã®äººé–“ã®ä¾¡å€¤è¦³ã¨ã®æ•´åˆæ€§å‘ä¸Š
- Claude ã®æœ‰å®³å‡ºåŠ›ã®æŠ‘åˆ¶
::::

### ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®å­¦ç¿’æ‰‹æ³•å¯¾å¿œè¡¨

| ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ | äº‹å‰å­¦ç¿’ | SFT | DPO/RLHF | å‚™è€ƒ |
|--------------|---------|-----|----------|------|
| **PyTorch DDP** | âœ… | âœ… | âœ… | æ¨™æº–çš„ãª PyTorch ã‚³ãƒ¼ãƒ‰ã§å…¨æ‰‹æ³•ã«å¯¾å¿œå¯èƒ½ |
| **PyTorch FSDP** | âœ… | âœ… | âœ… | PyTorch ãƒã‚¤ãƒ†ã‚£ãƒ–ã§å…¨æ‰‹æ³•ã«å¯¾å¿œ |
| **DeepSpeed** | âœ… | âœ… | âœ… | DeepSpeed-Chat ã§ RLHF/DPO ã‚’ã‚µãƒãƒ¼ãƒˆ |
| **Megatron-LM** | âœ… | âœ… | âŒ | äº‹å‰å­¦ç¿’ã¨ SFT ã«ç‰¹åŒ–ã€DPO ã¯æœªå¯¾å¿œ |
| **Megatron-DeepSpeed** | âœ… | âœ… | âœ… | DeepSpeed ã®æ©Ÿèƒ½ã«ã‚ˆã‚Š DPO/RLHF å¯èƒ½ |
| **GPT-NeoX** | âœ… | âš ï¸ | âœ… | äº‹å‰å­¦ç¿’ãŒä¸»ç›®çš„ã€SFT ã¯ Transformers æ¨å¥¨ã€DPO/KTO å¯¾å¿œï¼ˆ2024/9ï¼‰ |
| **Nanotron** | âœ… | âœ… | âš ï¸ | äº‹å‰å­¦ç¿’ã¨ SFT ã«å¯¾å¿œã€DPO ã¯é™å®šçš„ |
| **Picotron** | âœ… | âŒ | âŒ | æ•™è‚²ç”¨ã®ãŸã‚äº‹å‰å­¦ç¿’ã®ã¿å¯¾å¿œ |
| **Monarch** | âœ… | âœ… | âœ… | Meta å†…éƒ¨ã§å…¨ãƒ•ã‚§ãƒ¼ã‚ºã«ä½¿ç”¨ |

**å‡¡ä¾‹**
- âœ…: å…¬å¼ã«ã‚µãƒãƒ¼ãƒˆ
- âš ï¸: éƒ¨åˆ†çš„ã‚µãƒãƒ¼ãƒˆã¾ãŸã¯ä»–ãƒ„ãƒ¼ãƒ«ã¨ã®çµ„ã¿åˆã‚ã›ãŒå¿…è¦
- âŒ: æœªå¯¾å¿œã¾ãŸã¯éæ¨å¥¨

::::details å„ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®è©³ç´°

**DeepSpeed ã®å¼·ã¿**
- [DeepSpeed-Chat](https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat) ã§ RLHF ã®å…¨ã‚¹ãƒ†ãƒƒãƒ—ã‚’çµ±åˆ
- DPOã€PPOã€KTO ãªã©ã® preference learning ã‚’ã‚µãƒãƒ¼ãƒˆ
- ZeRO ã«ã‚ˆã‚Šå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®åŠ¹ç‡çš„ãªå­¦ç¿’ãŒå¯èƒ½

**GPT-NeoX ã®é¸å¥½å­¦ç¿’å¯¾å¿œ**
- 2024 å¹´ 9 æœˆã« DPOã€KTOã€reward modeling ã‚’è¿½åŠ 
- äº‹å‰å­¦ç¿’å¾Œã® alignment ãƒ•ã‚§ãƒ¼ã‚ºã«ã‚‚å¯¾å¿œå¯èƒ½ã«

**Megatron-LM ã®åˆ¶ç´„**
- äº‹å‰å­¦ç¿’ã¨ SFT ã«ç‰¹åŒ–ã—ãŸè¨­è¨ˆ
- Preference learning ã«ã¯ Megatron-DeepSpeed ã®ä½¿ç”¨ã‚’æ¨å¥¨

**æ•™è‚²ç”¨ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**
- Picotron ã¯äº‹å‰å­¦ç¿’ã®å­¦ç¿’ã«ç‰¹åŒ–ã—ã¦ãŠã‚Šã€SFT/DPO ã¯å¯¾è±¡å¤–
::::

### å­¦ç¿’ãƒ•ã‚§ãƒ¼ã‚ºåˆ¥ã®è¨ˆç®—ã‚³ã‚¹ãƒˆæ¯”è¼ƒ

å…¸å‹çš„ãª 70B ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ã®ä¾‹ï¼ˆæ¦‚ç®—ï¼‰

| ãƒ•ã‚§ãƒ¼ã‚º | GPU æ•° | å­¦ç¿’æ™‚é–“ | ãƒ‡ãƒ¼ã‚¿é‡ | ç›¸å¯¾ã‚³ã‚¹ãƒˆ |
|---------|--------|----------|----------|-----------|
| **äº‹å‰å­¦ç¿’** | 1,000-10,000 | æ•°é€±é–“ï½æ•°ãƒ¶æœˆ | æ•°å…†ãƒˆãƒ¼ã‚¯ãƒ³ | 100Ã— |
| **SFT** | 8-64 | æ•°æ™‚é–“ï½æ•°æ—¥ | æ•°åƒï½æ•°åä¸‡ã‚µãƒ³ãƒ—ãƒ« | 1Ã— |
| **DPO/RLHF** | 8-64 | æ•°æ™‚é–“ï½æ•°æ—¥ | æ•°åƒï½æ•°åä¸‡ãƒšã‚¢ | 1-2Ã— |

**ãƒã‚¤ãƒ³ãƒˆ**
- äº‹å‰å­¦ç¿’ã¯åœ§å€’çš„ã«é«˜ã‚³ã‚¹ãƒˆï¼ˆå…¨ä½“ã® 95% ä»¥ä¸Šï¼‰
- SFT ã¨ DPO ã¯æ¯”è¼ƒçš„ä½ã‚³ã‚¹ãƒˆã§å®Ÿæ–½å¯èƒ½
- å¤šãã®å ´åˆã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰é–‹å§‹ã—ã€SFT/DPO ã®ã¿å®Ÿæ–½

## å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã®æŠ€è¡“çš„èª²é¡Œ

### Backpropagation ã«ãŠã‘ã‚‹åŒæ–¹å‘ä¾å­˜æ€§

::::details æ¦‚è¦
å¤§è¦æ¨¡åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã§æœ€ã‚‚è¤‡é›‘ãªå´é¢ã® 1 ã¤ãŒã€Forward Pass ã¨ Backward Pass ã®ç›¸äº’ä¾å­˜é–¢ä¿‚ã§ã™ã€‚ã“ã®åŒæ–¹å‘ä¾å­˜æ€§ï¼ˆbi-directional, interdependentï¼‰ãŒã€åŠ¹ç‡çš„ãªåˆ†æ•£å­¦ç¿’ã‚’å›°é›£ã«ã—ã¦ã„ã¾ã™ã€‚
::::

**Forward Pass ã¨ Backward Pass ã®é–¢ä¿‚**

```mermaid
graph TB
    subgraph "Forward Pass ã¨ Backward Pass ã®ä¾å­˜é–¢ä¿‚"
        subgraph Forward["Forward Pass (é †ä¼æ’­)"]
            I[Input] --> L1[Layer 1<br/>è¨ˆç®— + Activation ä¿å­˜]
            L1 --> L2[Layer 2<br/>è¨ˆç®— + Activation ä¿å­˜]
            L2 --> L3[Layer 3<br/>è¨ˆç®— + Activation ä¿å­˜]
            L3 --> Loss[Loss è¨ˆç®—]
        end
        
        subgraph Backward["Backward Pass (é€†ä¼æ’­)"]
            Loss --> B3[Layer 3 å‹¾é…<br/>Activation 3 ã‚’ä½¿ç”¨]
            B3 --> B2[Layer 2 å‹¾é…<br/>Activation 2 ã‚’ä½¿ç”¨]
            B2 --> B1[Layer 1 å‹¾é…<br/>Activation 1 ã‚’ä½¿ç”¨]
        end
        
        L1 -.->|Activation 1| B1
        L2 -.->|Activation 2| B2
        L3 -.->|Activation 3| B3
    end
    
    style Forward fill:#e6f3ff
    style Backward fill:#ffe6f0
```

**ãƒ¡ãƒ¢ãƒªã®èª²é¡Œ**

Backward Pass ã§å‹¾é…ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã«ã¯ã€Forward Pass ã§è¨ˆç®—ã—ãŸ Activationï¼ˆä¸­é–“çµæœï¼‰ãŒå¿…è¦ã§ã™ã€‚ã—ã‹ã—ã€ã™ã¹ã¦ã® Activation ã‚’ä¿æŒã™ã‚‹ã¨è†¨å¤§ãªãƒ¡ãƒ¢ãƒªãŒå¿…è¦ã«ãªã‚Šã¾ã™ã€‚

**Llama 3 70B ã§ã® Activation ãƒ¡ãƒ¢ãƒªã®ä¾‹**
- ãƒãƒƒãƒã‚µã‚¤ã‚º 1ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•· 2048 ã®å ´åˆ
- 1 å±¤ã‚ãŸã‚Šç´„ 1GB ã® Activation
- 80 å±¤ã§ç´„ 80GB
- ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å¢—ã‚„ã™ã¨ç·šå½¢ã«å¢—åŠ 

### Activation Checkpointingï¼ˆå‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆï¼‰

::::details è§£æ±ºç­–
Activation Checkpointing ã¯ã€ãƒ¡ãƒ¢ãƒªã¨è¨ˆç®—ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’èª¿æ•´ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚ã™ã¹ã¦ã®å±¤ã® Activation ã‚’ä¿å­˜ã™ã‚‹ä»£ã‚ã‚Šã«ã€ä¸€éƒ¨ã®å±¤ã®ã¿ã‚’ä¿å­˜ã—ã€å¿…è¦ã«å¿œã˜ã¦å†è¨ˆç®—ã—ã¾ã™ã€‚
::::

```mermaid
graph TB
    subgraph "Activation Checkpointing"
        subgraph "ãƒ¡ãƒ¢ãƒªã«ä¿å­˜"
            C1[Checkpoint 1<br/>Layer 0]
            C2[Checkpoint 2<br/>Layer 20]
            C3[Checkpoint 3<br/>Layer 40]
            C4[Checkpoint 4<br/>Layer 60]
        end
        
        subgraph "Backward æ™‚ã«å†è¨ˆç®—"
            R1[Layer 1-19<br/>å†è¨ˆç®—]
            R2[Layer 21-39<br/>å†è¨ˆç®—]
            R3[Layer 41-59<br/>å†è¨ˆç®—]
            R4[Layer 61-79<br/>å†è¨ˆç®—]
        end
        
        C1 --> R1
        C2 --> R2
        C3 --> R3
        C4 --> R4
    end
    
    style C1 fill:#e6f3ff
    style C2 fill:#e6f3ff
    style C3 fill:#e6f3ff
    style C4 fill:#e6f3ff
    style R1 fill:#ffe6f0
    style R2 fill:#ffe6f0
    style R3 fill:#ffe6f0
    style R4 fill:#ffe6f0
```

**ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**
- **ãƒ¡ãƒ¢ãƒªå‰Šæ¸›**: ç´„ 1/âˆšNï¼ˆN ã¯ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ•°ï¼‰
- **è¨ˆç®—å¢—åŠ **: ç´„ 33%ï¼ˆæœ€é©ãªåˆ†å‰²ã®å ´åˆï¼‰
- å®Ÿç”¨çš„ã«ã¯è¨±å®¹å¯èƒ½ãªã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰

### é€šä¿¡ã¨ãƒ¡ãƒ¢ãƒªã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯

åˆ†æ•£å­¦ç¿’ã§ã¯ã€ä»¥ä¸‹ã® 2 ã¤ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ãŒåŒæ™‚ã«ç™ºç”Ÿã—ã¾ã™ã€‚

**ãƒ¡ãƒ¢ãƒªãƒœãƒˆãƒ«ãƒãƒƒã‚¯**
1. Parametersï¼ˆãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰
2. Gradientsï¼ˆå‹¾é…ï¼‰
3. Optimizer Statesï¼ˆã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶çŠ¶æ…‹ï¼‰
4. Activationsï¼ˆä¸­é–“çµæœï¼‰

**é€šä¿¡ãƒœãƒˆãƒ«ãƒãƒƒã‚¯**
1. All-Reduceï¼ˆå‹¾é…é›†ç´„ï¼‰
2. All-Gatherï¼ˆParameter åé›†ã€ZeRO Stage 3ï¼‰
3. Reduce-Scatterï¼ˆå‹¾é…åˆ†æ•£ã€ZeRO Stage 2/3ï¼‰
4. Point-to-Pointï¼ˆPipeline Parallelismï¼‰

```mermaid
graph TB
    subgraph "ãƒ¡ãƒ¢ãƒªã¨é€šä¿¡ã®æœ€é©åŒ–"
        A[ãƒ¡ãƒ¢ãƒªå‰Šæ¸›<br/>ZeRO, Checkpointing] -->|ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•| B[é€šä¿¡å¢—åŠ ]
        B -->|æœ€é©åŒ–| C[é€šä¿¡ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—<br/>è¨ˆç®—ã¨é€šä¿¡ã®ä¸¦è¡Œå®Ÿè¡Œ]
        C -->|å®Ÿç¾| D[é«˜é€Ÿãªå­¦ç¿’]
        
        E[é«˜é€Ÿãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯<br/>NVLink, InfiniBand] --> C
    end
    
    style A fill:#e6f3ff
    style B fill:#fff4e1
    style C fill:#e6ffe6
    style D fill:#ffe6f0
```

## ç™ºå±•: ä½ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§ã®æœ€é©åŒ–

å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã®åŠ¹ç‡åŒ–ã¯ã€ä¸¦åˆ—åŒ–æ‰‹æ³•ã ã‘ã§ãªãã€CUDA Kernel ãƒ¬ãƒ™ãƒ«ã§ã®æœ€é©åŒ–ã‚‚é‡è¦ã§ã™ã€‚

### Kernel Fusion

::::details æ¦‚è¦
è¤‡æ•°ã®æ¼”ç®—ã‚’ 1 ã¤ã® CUDA Kernel ã«ã¾ã¨ã‚ã‚‹ã“ã¨ã§ã€GPU ãƒ¡ãƒ¢ãƒªã¸ã®ã‚¢ã‚¯ã‚»ã‚¹å›æ•°ã‚’å‰Šæ¸›ã—ã€æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹æ‰‹æ³•ã§ã™ã€‚
::::

**æ¨™æº–çš„ãªå®Ÿè£…**
```
x = input
y = LayerNorm(x)      # GPU ãƒ¡ãƒ¢ãƒª â† æ›¸ãè¾¼ã¿
z = GELU(y)           # GPU ãƒ¡ãƒ¢ãƒª â† èª­ã¿è¾¼ã¿/æ›¸ãè¾¼ã¿
output = Linear(z)    # GPU ãƒ¡ãƒ¢ãƒª â† èª­ã¿è¾¼ã¿
```

**Kernel Fusion å¾Œ**
```
output = FusedLayerNormGELULinear(input)  # 1å›ã® Kernel èµ·å‹•
```

ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ãŒå‰Šæ¸›ã•ã‚Œã€2-3å€ã®é«˜é€ŸåŒ–ãŒå¯èƒ½ã§ã™ã€‚

### FlashAttention

::::details æ¦‚è¦
[FlashAttention](https://arxiv.org/abs/2205.14135) ã¯ã€Self-Attention ã®è¨ˆç®—ã‚’å¤§å¹…ã«åŠ¹ç‡åŒ–ã™ã‚‹æ‰‹æ³•ã§ã™ã€‚ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æœ€é©åŒ–ã™ã‚‹ã“ã¨ã§ã€é«˜é€ŸåŒ–ã¨ãƒ¡ãƒ¢ãƒªå‰Šæ¸›ã‚’åŒæ™‚ã«å®Ÿç¾ã—ã¾ã™ã€‚
::::

**æ¨™æº–çš„ãª Attention ã®å•é¡Œ**
- O(NÂ²) ã®ãƒ¡ãƒ¢ãƒªãŒå¿…è¦ï¼ˆN ã¯ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ï¼‰
- GPU ã® HBMï¼ˆHigh Bandwidth Memoryï¼‰ã¸ã®é »ç¹ãªã‚¢ã‚¯ã‚»ã‚¹

**FlashAttention ã®å·¥å¤«**
1. Attention è¨ˆç®—ã‚’ã‚¿ã‚¤ãƒ«ï¼ˆãƒ–ãƒ­ãƒƒã‚¯ï¼‰ã«åˆ†å‰²
2. GPU ã® SRAMï¼ˆé«˜é€Ÿãƒ¡ãƒ¢ãƒªï¼‰ã‚’æ´»ç”¨
3. ã‚¿ã‚¤ãƒ«ã”ã¨ã«è¨ˆç®—ã‚’å®Œäº†ã—ã¦ã‹ã‚‰æ¬¡ã®ã‚¿ã‚¤ãƒ«ã¸
4. ä¸­é–“çµæœã‚’ HBM ã«æ›¸ãæˆ»ã•ãªã„

```mermaid
graph LR
    subgraph "æ¨™æº– Attention"
        A1[Query] --> HBM1[HBM çµŒç”±]
        A2[Key] --> HBM1
        HBM1 --> A3[Attention Score]
        A3 --> HBM2[HBM çµŒç”±]
        A4[Value] --> HBM2
        HBM2 --> A5[Output]
    end
    
    subgraph "FlashAttention"
        B1[Query Tile] --> SRAM[SRAM ä¸Šã§è¨ˆç®—å®Œäº†]
        B2[Key Tile] --> SRAM
        B3[Value Tile] --> SRAM
        SRAM --> B4[Output Tile]
    end
    
    style HBM1 fill:#ffcccc
    style HBM2 fill:#ffcccc
    style SRAM fill:#ccffcc
```

**åŠ¹æœ**
- 2-4å€ã®é«˜é€ŸåŒ–
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®å‰Šæ¸›
- ã‚ˆã‚Šé•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®å‡¦ç†ãŒå¯èƒ½

::::details å‚è€ƒæƒ…å ±
- [FlashAttention è«–æ–‡](https://arxiv.org/abs/2205.14135)
- [FlashAttention-2](https://arxiv.org/abs/2307.08691)
- [FlashAttention GitHub](https://github.com/Dao-AILab/flash-attention)
::::

### ãã®ä»–ã®æœ€é©åŒ–æ‰‹æ³•

::::details Mixed Precision Training
FP32ï¼ˆå˜ç²¾åº¦ï¼‰ã¨ FP16/BF16ï¼ˆåŠç²¾åº¦ï¼‰ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›ã—ãªãŒã‚‰å­¦ç¿’ã®å®‰å®šæ€§ã‚’ä¿ã¤æ‰‹æ³•ã§ã™ã€‚

**ä»•çµ„ã¿**
1. Forward/Backward ã¯ FP16/BF16 ã§å®Ÿè¡Œ
2. Parameters ã® Master Copy ã¯ FP32 ã§ä¿æŒ
3. Gradient Scaling ã§æ•°å€¤ã®å®‰å®šæ€§ã‚’ç¢ºä¿

**åŠ¹æœ**
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒç´„åŠåˆ†
- è¨ˆç®—é€Ÿåº¦ãŒ 2-3å€ï¼ˆTensor Core æ´»ç”¨ï¼‰
::::

::::details Gradient Accumulation
ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’ç–‘ä¼¼çš„ã«å¤§ããã™ã‚‹æ‰‹æ³•ã§ã™ã€‚è¤‡æ•°ã®å°ã•ãªãƒãƒƒãƒã§å‹¾é…ã‚’è¨ˆç®—ãƒ»è“„ç©ã—ã€ã¾ã¨ã‚ã¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ›´æ–°ã—ã¾ã™ã€‚

**ãƒ¡ãƒªãƒƒãƒˆ**
- GPU ãƒ¡ãƒ¢ãƒªãŒé™ã‚‰ã‚Œã¦ã„ã¦ã‚‚å¤§ããªãƒãƒƒãƒã‚µã‚¤ã‚ºç›¸å½“ã®å­¦ç¿’ãŒå¯èƒ½
- å­¦ç¿’ã®å®‰å®šæ€§å‘ä¸Š

**ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•**
- ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°ã®é »åº¦ãŒä¸‹ãŒã‚‹
- åæŸã¾ã§ã®ã‚¹ãƒ†ãƒƒãƒ—æ•°ã¯å¤‰ã‚ã‚‰ãªã„
::::

## ã¾ã¨ã‚

æœ¬ç« ã§ã¯ã€å¤§è¦æ¨¡åŸºç›¤ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã®ãƒãƒ«ãƒ GPU å‡¦ç†æ‰‹æ³•ã«ã¤ã„ã¦æ•´ç†ã—ã¾ã—ãŸã€‚

**é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ**

1. **ä¸¦åˆ—åŒ–æ‰‹æ³•ã®ç†è§£**
   - Data Parallelismã€Pipeline Parallelismã€Tensor Parallelismã€Hybrid Parallelism
   - ãã‚Œãã‚Œã«ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ãŒã‚ã‚Šã€çŠ¶æ³ã«å¿œã˜ãŸé¸æŠãŒé‡è¦

2. **ZeRO ã«ã‚ˆã‚‹ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–**
   - Stage 1-3 ã§æ®µéšçš„ã«ãƒ¡ãƒ¢ãƒªã‚’å‰Šæ¸›
   - é€šä¿¡é‡ã¨ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’ç†è§£ã™ã‚‹

3. **ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã®é¸æŠ**
   - ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã€GPU æ•°ã€è¦æ±‚ã•ã‚Œã‚‹åŠ¹ç‡ã«å¿œã˜ã¦é©åˆ‡ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’é¸æŠ
   - PyTorch DDPã€DeepSpeedã€Megatron-LM ãªã©

4. **æŠ€è¡“çš„èª²é¡Œã¸ã®å¯¾å‡¦**
   - Backpropagation ã®åŒæ–¹å‘ä¾å­˜æ€§
   - Activation Checkpointing ã«ã‚ˆã‚‹ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
   - é€šä¿¡ã¨è¨ˆç®—ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—

5. **ä½ãƒ¬ã‚¤ãƒ¤ãƒ¼ã§ã®æœ€é©åŒ–**
   - Kernel Fusionã€FlashAttention ãªã©ã® CUDA ãƒ¬ãƒ™ãƒ«ã®æœ€é©åŒ–
   - Mixed Precision Trainingã€Gradient Accumulation

ã“ã‚Œã‚‰ã®æŠ€è¡“ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€æ•°åƒã‹ã‚‰æ•°ä¸‡ã® GPU ã‚’ä½¿ç”¨ã—ãŸè¶…å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãŒå®Ÿç¾å¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

::::details å‚è€ƒè³‡æ–™

**åˆ†æ•£å­¦ç¿’ã®åŸºç¤**
- [PyTorch Distributed Overview](https://docs.pytorch.org/tutorials/beginner/dist_overview.html)
- [Colossal-AI: Distributed Training](https://colossalai.org/docs/concepts/distributed_training)
- [Colossal-AI: Paradigms of Parallelism](https://colossalai.org/docs/concepts/paradigms_of_parallelism)

**ä¸¦åˆ—åŒ–æ‰‹æ³•**
- [A Beginner-Friendly Guide to Multi-GPU Model Training](https://www.dailydoseofds.com/a-beginner-friendly-guide-to-multi-gpu-model-training/)
- [Turing Motors: åˆ†æ•£å­¦ç¿’ã®åŸºç¤](https://zenn.dev/turing_motors/articles/d00c46a79dc976)
- [Turing Motors: Pipeline Parallelism](https://zenn.dev/turing_motors/articles/0e6e2baf72ebbc)
- [Turing Motors: Tensor Parallelism](https://zenn.dev/turing_motors/articles/04c1328bf6095a)
- [Turing Motors: ZeRO ã®è©³ç´°](https://zenn.dev/turing_motors/articles/da7fa101ecb9a1)

**DeepSpeed ã¨ ZeRO**
- [DeepSpeed å…¬å¼ã‚µã‚¤ãƒˆ](https://www.deepspeed.ai/)
- [DeepSpeed ãƒ–ãƒ­ã‚°](https://www.deepspeed.ai/posts/)
- [DeepSpeed æ¦‚è¦ï¼ˆæ—¥æœ¬èª PDFï¼‰](https://www.deepspeed.ai/assets/files/DeepSpeed_Overview_Japanese_2023Jun7th.pdf)
- [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/abs/1910.02054)

**Megatron-LM**
- [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/abs/1909.08053)
- [Efficient Large-Scale Language Model Training on GPU Clusters](https://arxiv.org/abs/2104.04473)

**Pipeline Parallelism**
- [GPipe: Easy Scaling with Micro-Batch Pipeline Parallelism](https://arxiv.org/abs/1811.06965)
- [PipeDream: Generalized Pipeline Parallelism for DNN Training](https://arxiv.org/abs/1806.03377)

**æœ€é©åŒ–æ‰‹æ³•**
- [FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness](https://arxiv.org/abs/2205.14135)
- [FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning](https://arxiv.org/abs/2307.08691)
- [Mixed Precision Training (NVIDIA)](https://docs.nvidia.com/deeplearning/performance/mixed-precision-training/index.html)

**Monarch**
- [Monarch ã«é–¢ã™ã‚‹ Zenn ã‚¹ã‚¯ãƒ©ãƒƒãƒ—](https://zenn.dev/tosshi/scraps/d36bb9b3168809)

**ãã®ä»–ã®æœ‰ç”¨ãªè³‡æ–™**
- [ARISE Analytics: ç”Ÿæˆ AI ã®åˆ†æ•£å­¦ç¿’æŠ€è¡“å…¥é–€](https://www.ariseanalytics.com/tech-info/20231210)

::::



---


# NVIDIA NeMo Megatron Bridge å®Œå…¨ã‚¬ã‚¤ãƒ‰

## ğŸ“‹ ç›®æ¬¡

1. [æ¦‚è¦](#æ¦‚è¦)
2. [PyTorchã€Hugging Faceã€Megatronã®é–¢ä¿‚](#pytorch-hugging-face-megatronã®é–¢ä¿‚)
3. [NeMo Megatron Bridgeã®è©³ç´°](#nemo-megatron-bridgeã®è©³ç´°)
4. [ä¸»è¦æ©Ÿèƒ½](#ä¸»è¦æ©Ÿèƒ½)
5. [ã‚µãƒãƒ¼ãƒˆãƒ¢ãƒ‡ãƒ«](#ã‚µãƒãƒ¼ãƒˆãƒ¢ãƒ‡ãƒ«)
6. [ä½¿ç”¨ä¾‹](#ä½¿ç”¨ä¾‹)
7. [æŠ€è¡“çš„è©³ç´°](#æŠ€è¡“çš„è©³ç´°)
8. [ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹](#ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹)
9. [æ¯”è¼ƒè¡¨](#æ¯”è¼ƒè¡¨)
10. [ã¾ã¨ã‚](#ã¾ã¨ã‚)
11. [å‚è€ƒãƒªãƒ³ã‚¯](#å‚è€ƒãƒªãƒ³ã‚¯)

---

## æ¦‚è¦

**NeMo Megatron Bridge**ã¯ã€NeMo Frameworkå†…ã®PyTorchãƒã‚¤ãƒ†ã‚£ãƒ–ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã€å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã¨ãƒ“ã‚¸ãƒ§ãƒ³è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆVLMï¼‰ã®ãƒ—ãƒªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã€æ•™å¸«ã‚ã‚Šå¾®èª¿æ•´ï¼ˆSFTï¼‰ã€LoRAã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚

### ä¸»ãªç›®çš„

1. **ğŸ¤— Hugging Faceã¨Megatron Coreé–“ã®æ©‹æ¸¡ã—**
   - åŒæ–¹å‘ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå¤‰æ›ã‚’æä¾›
   - ä»–ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãŒMegatron Coreã®ä¸¦åˆ—åŒ–æ©Ÿèƒ½ã‚’æ´»ç”¨å¯èƒ½ã«
   - æ¤œè¨¼ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã«ã‚ˆã‚Šå¤‰æ›ç²¾åº¦ã‚’ä¿è¨¼

2. **é«˜æ€§èƒ½ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°åŸºç›¤**
   - Megatron Coreã‚’æ´»ç”¨ã—ãŸæœ€å…ˆç«¯ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ
   - ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—åŒ–ã¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¸¦åˆ—åŒ–ã‚’ã‚µãƒãƒ¼ãƒˆ
   - æ··åˆç²¾åº¦ï¼ˆFP8, BF16, FP4ãªã©ï¼‰ã«å¯¾å¿œ

---

## PyTorchã€Hugging Faceã€Megatronã®é–¢ä¿‚

### éšå±¤æ§‹é€ 

```mermaid
graph TB
    subgraph Application["ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å±¤"]
        HF["Hugging Face Transformers<br/>ï¼ˆä½¿ã„ã‚„ã™ã•é‡è¦–ï¼‰"]
        Bridge["NeMo Megatron Bridge<br/>ï¼ˆHFã¨Megatronã®æ©‹æ¸¡ã—ï¼‰"]
    end
    
    subgraph Framework["ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å±¤"]
        Standard["æ¨™æº–çš„ãª PyTorch<br/>åˆ†æ•£ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°"]
        Megatron["Megatron-Core / Megatron-LM<br/>ï¼ˆè¶…å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ç‰¹åŒ–ï¼‰"]
    end
    
    subgraph Base["åŸºç›¤ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°å±¤"]
        PyTorch["PyTorch<br/>ï¼ˆåŸºç›¤ã¨ãªã‚‹DLãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ï¼‰"]
    end
    
    HF --> Standard
    Bridge --> HF
    Bridge --> Megatron
    Standard --> PyTorch
    Megatron --> PyTorch
    
    style Bridge fill:#90EE90
    style HF fill:#87CEEB
    style Megatron fill:#FFB6C1
    style PyTorch fill:#FFE4B5
```

### å„å±¤ã®å½¹å‰²

#### 1ï¸âƒ£ PyTorchï¼ˆåŸºç›¤å±¤ï¼‰

**å½¹å‰²**: ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã®åŸºç›¤ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

- **æä¾›ã™ã‚‹ã‚‚ã®**:
  - ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—ï¼ˆGPUå¯¾å¿œï¼‰
  - è‡ªå‹•å¾®åˆ†ï¼ˆAutogradï¼‰
  - ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®åŸºæœ¬æ§‹æˆè¦ç´ ï¼ˆnn.Moduleï¼‰
  - åŸºæœ¬çš„ãªåˆ†æ•£ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ©Ÿèƒ½ï¼ˆDDP, FSDPï¼‰

- **ç‰¹å¾´**:
  - Pythonãƒã‚¤ãƒ†ã‚£ãƒ–ã§ä½¿ã„ã‚„ã™ã„
  - ç ”ç©¶è€…ã«äººæ°—ï¼ˆæŸ”è»Ÿæ€§ãŒé«˜ã„ï¼‰
  - åŸºæœ¬çš„ãªåˆ†æ•£ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¯å¯èƒ½ã ãŒã€è¶…å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã«ã¯é™ç•ŒãŒã‚ã‚‹

**ä¾‹**:
```python
import torch
import torch.nn as nn

# åŸºæœ¬çš„ãªãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
class SimpleModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.linear = nn.Linear(768, 768)
    
    def forward(self, x):
        return self.linear(x)
```

#### 2ï¸âƒ£ Hugging Face Transformersï¼ˆä½¿ã„ã‚„ã™ã•é‡è¦–ï¼‰

**å½¹å‰²**: Transformerãƒ¢ãƒ‡ãƒ«ã®ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ 

- **æä¾›ã™ã‚‹ã‚‚ã®**:
  - äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®å·¨å¤§ãªHubï¼ˆãƒ¢ãƒ‡ãƒ«ã®"GitHub"ï¼‰
  - ç°¡å˜ã«ä½¿ãˆã‚‹APIï¼ˆ`from_pretrained`ã§æ•°è¡Œã§ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿ï¼‰
  - ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
  - æ¨è«–ã«æœ€é©åŒ–ã•ã‚ŒãŸè¨­è¨ˆ

- **ç‰¹å¾´**:
  - **ä½¿ã„ã‚„ã™ã•æœ€å„ªå…ˆ**: ç ”ç©¶è€…ã‚„ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãŒç´ æ—©ããƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã‚’ä½œæˆã§ãã‚‹
  - **æ¨™æº–åŒ–**: ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ï¼ˆBERT, GPT, T5ãªã©ï¼‰ã‚’çµ±ä¸€ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã§æ‰±ãˆã‚‹
  - **ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£**: 10ä¸‡ä»¥ä¸Šã®ãƒ¢ãƒ‡ãƒ«ãŒå…±æœ‰ã•ã‚Œã¦ã„ã‚‹
  - **åˆ¶ç´„**: è¶…å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ï¼ˆ100B+ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¯ä¸å‘ã

**ä¾‹**:
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# ãŸã£ãŸ2è¡Œã§ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚ã‚‹
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.2-1B")
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.2-1B")

# æ¨è«–ã‚‚ç°¡å˜
outputs = model.generate(**tokenizer("Hello", return_tensors="pt"))
```

#### 3ï¸âƒ£ Megatron-LM / Megatron-Coreï¼ˆè¶…å¤§è¦æ¨¡ç‰¹åŒ–ï¼‰

**å½¹å‰²**: è¶…å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®åŠ¹ç‡çš„ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°

- **æä¾›ã™ã‚‹ã‚‚ã®**:
  - **ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—åŒ–ï¼ˆTensor Parallelism, TP)**: ãƒ¢ãƒ‡ãƒ«ã®å±¤ã‚’è¤‡æ•°GPUã«åˆ†å‰²
  - **ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¸¦åˆ—åŒ–ï¼ˆPipeline Parallelism, PP)**: å±¤ã‚’ç¸¦ã«åˆ†å‰²
  - **ã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆä¸¦åˆ—åŒ–ï¼ˆExpert Parallelism, EP)**: MoEãƒ¢ãƒ‡ãƒ«ç”¨
  - **ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ä¸¦åˆ—åŒ–ã€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¸¦åˆ—åŒ–**ãªã©é«˜åº¦ãªä¸¦åˆ—åŒ–æŠ€è¡“
  - ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–æŠ€è¡“ï¼ˆå‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã€FP8ãªã©ï¼‰

- **ç‰¹å¾´**:
  - **ã‚¹ã‚±ãƒ¼ãƒ«é‡è¦–**: æ•°åƒGPUã§ã®è¶…å¤§è¦æ¨¡ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«æœ€é©åŒ–
  - **é«˜ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ**: æº–ç·šå½¢ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ï¼ˆGPUã‚’å¢—ã‚„ã›ã°æ€§èƒ½ã‚‚ã»ã¼æ¯”ä¾‹ï¼‰
  - **è¤‡é›‘**: è¨­å®šãŒé›£ã—ãã€å°‚é–€çŸ¥è­˜ãŒå¿…è¦
  - **NVIDIAãŒé–‹ç™º**: GPT-3ã€ChatGPTã®åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ä½¿ç”¨

**ä¸¦åˆ—åŒ–ã®ä¾‹**:
```
ãƒ¢ãƒ‡ãƒ«ãŒå¤§ãã™ãã¦1ã¤ã®GPUã«è¼‰ã‚‰ãªã„å ´åˆ:

[ãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—åŒ– (TP)]
   å±¤ã®é‡ã¿è¡Œåˆ—ã‚’æ¨ªã«åˆ†å‰²
   GPU1: [W ã®å‰åŠ]
   GPU2: [W ã®å¾ŒåŠ]
   â†’ è¨ˆç®—çµæœã‚’çµ±åˆ

[ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¸¦åˆ—åŒ– (PP)]
   å±¤ã‚’ç¸¦ã«åˆ†å‰²
   GPU1: [Layer 0-11]
   GPU2: [Layer 12-23]
   GPU3: [Layer 24-35]
   â†’ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çš„ã«å‡¦ç†
```

#### 4ï¸âƒ£ NeMo Megatron Bridgeï¼ˆæ©‹æ¸¡ã—å½¹ï¼‰

**å½¹å‰²**: Hugging Faceã¨Megatronã®"ç¿»è¨³è€…"

### è§£æ±ºã™ã‚‹å•é¡Œ

```mermaid
graph LR
    subgraph Problem["å•é¡Œ"]
        HF_Issue["Hugging Face<br/>- safetensorså½¢å¼<br/>- æ¨è«–ãƒ»å¾®èª¿æ•´ã«å¼·ã„<br/>- è¶…å¤§è¦æ¨¡ã¯è‹¦æ‰‹"]
        Megatron_Issue["Megatron<br/>- ç‹¬è‡ªã®åˆ†æ•£å½¢å¼<br/>- è¶…å¤§è¦æ¨¡ã«å¼·ã„<br/>- ä½¿ã„ã«ãã„"]
    end
    
    subgraph Solution["è§£æ±ºç­–"]
        Bridge_Sol["NeMo Megatron Bridge<br/>âœ… åŒæ–¹å‘å¤‰æ›<br/>âœ… ä¸¦åˆ—åŒ–ã‚’ä¿æŒ<br/>âœ… ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„"]
    end
    
    HF_Issue -.->|äº’æ›æ€§ãªã—| Megatron_Issue
    HF_Issue --> Bridge_Sol
    Megatron_Issue --> Bridge_Sol
    Bridge_Sol -->|çµ±åˆ| Result["ä¸¡æ–¹ã®é•·æ‰€ã‚’<br/>çµ„ã¿åˆã‚ã›ã‚‰ã‚Œã‚‹"]
    
    style HF_Issue fill:#87CEEB
    style Megatron_Issue fill:#FFB6C1
    style Bridge_Sol fill:#90EE90
    style Result fill:#FFD700
```

---

## NeMo Megatron Bridgeã®è©³ç´°

### ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```mermaid
graph TB
    HF["Hugging Face<br/>PreTrained Model/Config"]
    AutoBridge["AutoBridge<br/>ï¼ˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è‡ªå‹•æ¤œå‡ºï¼‰"]
    Bridge["MegatronModelBridge<br/>ï¼ˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ç‰¹åŒ–ï¼‰"]
    Provider["Model Provider<br/>ï¼ˆTransformerConfigæ§‹ç¯‰ï¼‰"]
    Registry["MegatronMappingRegistry<br/>ï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒƒãƒ”ãƒ³ã‚°ï¼‰"]
    Mappings["Param Mapping<br/>ï¼ˆAuto/Row/Col/QKVç­‰ï¼‰"]
    Megatron["Distributed Megatron Model<br/>ï¼ˆTP/PP/EPå¯¾å¿œï¼‰"]
    
    HF -->|1. æ¤œå‡º| AutoBridge
    AutoBridge -->|2. é¸æŠ| Bridge
    Bridge -->|3. ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æ§‹ç¯‰| Provider
    Provider -->|4. ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–| Megatron
    Bridge -->|ãƒãƒƒãƒ”ãƒ³ã‚°ç™»éŒ²| Registry
    Registry -->|è§£æ±º| Mappings
    HF <-->|ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å˜ä½å¤‰æ›| Bridge
    Megatron <-->|TP/PP/EPå¯¾å¿œ| Mappings
    
    style AutoBridge fill:#FFE4B5
    style Bridge fill:#90EE90
    style Provider fill:#87CEEB
    style Registry fill:#DDA0DD
    style Mappings fill:#F0E68C
    style Megatron fill:#FFB6C1
```

### å¤‰æ›ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

```mermaid
sequenceDiagram
    participant User
    participant AutoBridge
    participant Bridge as MegatronModelBridge
    participant Registry as MappingRegistry
    participant Model as Megatron Model(s)
    
    User->>AutoBridge: from_hf_pretrained(path)
    AutoBridge->>Bridge: ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¤œå‡ºãƒ»é¸æŠ
    
    Note over User,Model: HF â†’ Megatron å¤‰æ›
    
    User->>AutoBridge: load_hf_weights(model)
    AutoBridge->>Bridge: load_hf_weights(model)
    Bridge->>Model: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆ—æŒ™ï¼ˆå…¨PP rankï¼‰
    Bridge->>Bridge: ã‚°ãƒ­ãƒ¼ãƒãƒ«åã‚’åé›†ãƒ»ã‚½ãƒ¼ãƒˆ
    
    loop å„Megatronãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        Bridge->>Registry: ãƒãƒƒãƒ”ãƒ³ã‚°è§£æ±º
        Registry-->>Bridge: ãƒãƒƒãƒ”ãƒ³ã‚°è¿”å´
        Bridge->>Bridge: HFãƒ†ãƒ³ã‚½ãƒ«å–å¾—ï¼ˆsafetensorsï¼‰
        Bridge->>Bridge: å¤‰æ›ï¼ˆQKV/MLP/Row/Colç­‰ï¼‰
        Bridge->>Model: TP/EP scatterã€PP broadcast
        Bridge->>Model: å®›å…ˆãƒ†ãƒ³ã‚½ãƒ«ã«ã‚³ãƒ”ãƒ¼
    end
    
    Bridge-->>User: ã‚¦ã‚§ã‚¤ãƒˆèª­ã¿è¾¼ã¿å®Œäº†
    
    Note over User,Model: Megatron â†’ HF å¤‰æ›
    
    User->>AutoBridge: save_hf_pretrained(model, path)
    AutoBridge->>Bridge: export_hf_weights(model)
    
    loop å„Megatronãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        Bridge->>Model: PPæ‰€æœ‰rankã‹ã‚‰èª­ã¿å–ã‚Š
        Bridge->>Model: TP/EP gather
        Bridge->>Bridge: é€†å¤‰æ›ï¼ˆsplit QKV/MLPç­‰ï¼‰
        Bridge-->>User: (hf_name, tensor) yield
    end
    
    Bridge-->>User: ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†
```

---

## ä¸»è¦æ©Ÿèƒ½

### ğŸš€ 1. Hugging Faceã¨ã®çµ±åˆ

- âœ… ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ãªåŒæ–¹å‘å¤‰æ›ï¼ˆHF â‡” Megatronï¼‰
- âœ… ä¸­é–“ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¸è¦ã®ã‚ªãƒ³ãƒ©ã‚¤ãƒ³å¤‰æ›
- âœ… ä¸¦åˆ—åŒ–ã‚’è€ƒæ…®ã—ãŸå¤‰æ›ï¼ˆTP/PP/VPP/CP/EP/ETPï¼‰
- âœ… ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å˜ä½ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
- âœ… AutoBridge APIã«ã‚ˆã‚‹è‡ªå‹•ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¤œå‡º
- âœ… Transformer Engineä½¿ç”¨æ™‚ã®æœ€é©åŒ–ãƒ‘ã‚¹

### ğŸ› ï¸ 2. æŸ”è»Ÿãªã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º

- è»½é‡ãªã‚«ã‚¹ã‚¿ãƒ ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—
- ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã€åˆ†æ•£ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã€è©•ä¾¡ã€ãƒ­ã‚®ãƒ³ã‚°ã®ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºãŒå®¹æ˜“
- PyTorchãƒã‚¤ãƒ†ã‚£ãƒ–ãªè¨­è¨ˆ

### ğŸ“ 3. ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆSFT & PEFTï¼‰

- Megatronãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«å°‚ç”¨ã®SFTå®Ÿè£…
- LoRAã€DoRAãªã©ã®PEFTæ‰‹æ³•ã‚’ã‚µãƒãƒ¼ãƒˆ
- ãƒ¦ãƒ¼ã‚¶ãƒ¼å®šç¾©ã®PEFTãƒ¡ã‚½ãƒƒãƒ‰ã‚‚å¯èƒ½

### ğŸ“š 4. æœ€å…ˆç«¯ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¬ã‚·ãƒ”

- Llama 3ãªã©ã®äººæ°—ãƒ¢ãƒ‡ãƒ«å‘ã‘ã«æœ€é©åŒ–ã•ã‚ŒãŸãƒ¬ã‚·ãƒ”
- æœ¬ç•ªç’°å¢ƒå¯¾å¿œã®è¨­å®šã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- YAMLé§†å‹•ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚µãƒãƒ¼ãƒˆ

### âš¡ 5. ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–

- FP8ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãƒ“ãƒ«ãƒˆã‚¤ãƒ³ã‚µãƒãƒ¼ãƒˆ
- ãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—åŒ–ã¨ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–æŠ€è¡“
- é€šä¿¡ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—æœ€é©åŒ–
- æ•°åƒãƒãƒ¼ãƒ‰ã¸ã®æº–ç·šå½¢ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£

---

## ã‚µãƒãƒ¼ãƒˆãƒ¢ãƒ‡ãƒ«

### å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰

| ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ãƒŸãƒªãƒ¼ | ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå¤‰æ› | ãƒ—ãƒªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¬ã‚·ãƒ” | SFT & LoRAãƒ¬ã‚·ãƒ” |
|-----------------|---------------------|----------------------|------------------|
| **Llama 2** | âœ… | âœ… (7B) | Coming soon |
| **Llama 3** | âœ… | âœ… (8B/70B) | âœ… (8B/70B) |
| **Llama 3.1** | âœ… | âœ… (8B/70B/405B) | âœ… (8B/70B/405B) |
| **Llama 3.2** | âœ… | âœ… (1B/3B) | âœ… (1B/3B) |
| **Llama 3.3** | âœ… | Coming soon | Coming soon |
| **Qwen2** | âœ… | âœ… (500M-72B) | âœ… (500M-72B) |
| **Qwen2.5** | âœ… | âœ… (500M-72B) | âœ… (500M-72B) |
| **Qwen3** | âœ… | âœ… (600M-32B) | âœ… (600M-32B) |
| **Qwen3-MoE** | âœ… | âœ… (A3B/A22B) | âœ… (A3B/A22B) |
| **DeepSeek V2** | âœ… | âœ… (v2) | Coming soon |
| **DeepSeek V3** | âœ… | âœ… (v3) | Coming soon |
| **Gemma 3** | âœ… | âœ… (1B) | âœ… (1B) |
| **GLM-4.5** | âœ… | âœ… (106B/355B) | âœ… (106B/355B) |
| **Mistral** | âœ… | Coming soon | Coming soon |
| **Ministral** | âœ… | âœ… (3B/8B/14B) | âœ… (3B/8B/14B) |
| **Nemotron-3** | âœ… | âœ… (A3B) | âœ… (A3B) |
| **OlMoE** | âœ… | âœ… (7B) | âœ… (7B) |

### ãƒ“ã‚¸ãƒ§ãƒ³è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆVLMï¼‰

- **Gemma 3-VL**: âœ… (4B/12B/27B) - SFT & LoRA
- **Qwen2.5-VL**: âœ… (3B/7B/32B/72B) - SFT & LoRA
- **Qwen3-VL**: âœ… (8B/A3B-A30B-MoE) - SFT & LoRA
- **Nemotron Nano v2 VL**: âœ… (9B/12B) - SFT & LoRA

---

## ä½¿ç”¨ä¾‹

### 1. å¤‰æ›ã®ã¿ã®ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

```python
from megatron.bridge import AutoBridge

# 1) HFãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ãƒ–ãƒªãƒƒã‚¸ã‚’ä½œæˆ
bridge = AutoBridge.from_hf_pretrained(
    "meta-llama/Llama-3.2-1B", 
    trust_remote_code=True
)

# 2) Megatronãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’å–å¾—ã—ä¸¦åˆ—åŒ–ã‚’è¨­å®š
provider = bridge.to_megatron_provider()
provider.tensor_model_parallel_size = 1
provider.pipeline_model_parallel_size = 1
provider.finalize()

# 3) Megatron Coreãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–
model = provider.provide_distributed_model(wrap_with_ddp=False)

# 4a) Megatron â†’ Hugging Face ã¸ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆå®Œå…¨ãªHFãƒ•ã‚©ãƒ«ãƒ€ï¼‰
bridge.save_hf_pretrained(model, "./hf_exports/llama32_1b")

# 4b) ã¾ãŸã¯ã‚¦ã‚§ã‚¤ãƒˆã®ã¿ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
for name, weight in bridge.export_hf_weights(model, cpu=True):
    print(name, tuple(weight.shape))
```

### 2. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ

```python
from megatron.bridge.recipes.llama import llama32_1b_pretrain_config
from megatron.bridge.training.gpt_step import forward_step
from megatron.bridge.training.pretrain import pretrain

if __name__ == "__main__":
    # Llama 3.2 1Bãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’ä½¿ç”¨
    cfg = llama32_1b_pretrain_config(seq_length=1024)
    
    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰
    cfg.train.train_iters = 10
    cfg.scheduler.lr_decay_iters = 10000
    cfg.model.vocab_size = 8192
    cfg.tokenizer.vocab_size = cfg.model.vocab_size
    
    pretrain(cfg, forward_step)
```

**èµ·å‹•ã‚³ãƒãƒ³ãƒ‰**:
```bash
torchrun --nproc-per-node=8 /path/to/script.py
```

### 3. å®Ÿéš›ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ä¾‹

```mermaid
graph TD
    A["1. Hugging Face<br/>ãƒ¢ãƒ‡ãƒ«é¸æŠ:<br/>Llama-3.1-70B"] --> B["2. NeMo Bridge<br/>HF â†’ Megatronå¤‰æ›<br/>TP=8, PP=4<br/>ï¼ˆ32GPUä½¿ç”¨ï¼‰"]
    B --> C["3. Megatron<br/>åŠ¹ç‡çš„ãªå¤§è¦æ¨¡ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°<br/>- FP8æ··åˆç²¾åº¦<br/>- æœ€é©åŒ–ã•ã‚ŒãŸé€šä¿¡<br/>- é«˜ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆ"]
    C --> D["4. NeMo Bridge<br/>Megatron â†’ HFå¤‰æ›"]
    D --> E["5. Hugging Face<br/>æ¨è«–ã‚¨ãƒ³ã‚¸ãƒ³ã§å±•é–‹<br/>- vLLM<br/>- TensorRT-LLM<br/>- ONNX Runtime"]
    
    style A fill:#87CEEB
    style B fill:#90EE90
    style C fill:#FFB6C1
    style D fill:#90EE90
    style E fill:#87CEEB
```

---

## æŠ€è¡“çš„è©³ç´°

### å¤‰æ›ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®ç‰¹å¾´

#### 1. ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å˜ä½ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°
- ç¾åœ¨å‡¦ç†ä¸­ã®ã‚¦ã‚§ã‚¤ãƒˆã®ã¿ã‚’ãƒ¡ãƒ¢ãƒªã«ä¿æŒ
- å®Œå…¨ãªãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã¯ä¸è¦
- ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªå¤‰æ›ã‚’å®Ÿç¾

#### 2. ä¸¦åˆ—åŒ–å¯¾å¿œ
- **TPï¼ˆãƒ†ãƒ³ã‚½ãƒ«ä¸¦åˆ—åŒ–ï¼‰**: é‡ã¿è¡Œåˆ—ã‚’åˆ†å‰²
- **PPï¼ˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¸¦åˆ—åŒ–ï¼‰**: å±¤ã‚’ç¸¦ã«åˆ†å‰²
- **EPï¼ˆã‚¨ã‚­ã‚¹ãƒ‘ãƒ¼ãƒˆä¸¦åˆ—åŒ–ï¼‰**: MoEãƒ¢ãƒ‡ãƒ«ã®å°‚é–€å®¶ã‚’åˆ†æ•£
- **VPPï¼ˆä»®æƒ³ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ä¸¦åˆ—åŒ–ï¼‰**: ã‚ˆã‚Šç´°ã‹ã„ç²’åº¦ã§ã®åˆ†å‰²
- **CPï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆä¸¦åˆ—åŒ–ï¼‰**: ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã®åˆ†å‰²

#### 3. æ±ºå®šè«–çš„ãƒãƒƒãƒ”ãƒ³ã‚°
- ãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰ã‚µãƒãƒ¼ãƒˆã‚’å«ã‚€åå‰è§£æ±º
- MegatronMappingRegistryã«ã‚ˆã‚‹ä¸€å…ƒç®¡ç†
- ã‚°ãƒ­ãƒ¼ãƒãƒ«ãªé †åºã‚’ä¿è¨¼

#### 4. å‹å¤‰æ›ã¨FP8ã‚µãƒãƒ¼ãƒˆ
- HFã¨Megatronã®dtypeé–“ã§è‡ªå‹•å¤‰æ›
- ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ™‚ã®FP8é€†é‡å­åŒ–
- è­¦å‘Šã‚’ä¼´ã†å®‰å…¨ãªå‹å¤‰æ›

### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒƒãƒ”ãƒ³ã‚°ã®ç¨®é¡

```mermaid
graph TB
    subgraph Mappings["ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒƒãƒ”ãƒ³ã‚°ã®ç¨®é¡"]
        Auto["AutoMapping<br/>ï¼ˆè‡ªå‹•æ¤œå‡ºãƒ»æ±ç”¨1:1ï¼‰"]
        Column["ColumnParallelMapping<br/>ï¼ˆå‡ºåŠ›æ¬¡å…ƒã§åˆ†å‰²ï¼‰"]
        Row["RowParallelMapping<br/>ï¼ˆå…¥åŠ›æ¬¡å…ƒã§åˆ†å‰²ï¼‰"]
        QKV["QKVMapping<br/>ï¼ˆQ, K, Vçµ±åˆ/åˆ†å‰²ï¼‰"]
        Gated["GatedMLPMapping<br/>ï¼ˆgate, upçµ±åˆ/åˆ†å‰²ï¼‰"]
        Replicated["ReplicatedMapping<br/>ï¼ˆå®Œå…¨è¤‡è£½ï¼‰"]
    end
    
    Auto --> Column
    Auto --> Row
    Auto --> Replicated
    
    style Auto fill:#FFE4B5
    style Column fill:#90EE90
    style Row fill:#87CEEB
    style QKV fill:#DDA0DD
    style Gated fill:#F0E68C
    style Replicated fill:#FFB6C1
```

#### å„ãƒãƒƒãƒ”ãƒ³ã‚°ã®è©³ç´°

1. **AutoMapping**: 
   - ãƒ¬ã‚¤ãƒ¤ãƒ¼/ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¿ã‚¤ãƒ—ã«åŸºã¥ã„ã¦è‡ªå‹•çš„ã«Column/Row/Replicatedã«æŒ¯ã‚Šåˆ†ã‘
   - ãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰ã‚µãƒãƒ¼ãƒˆ

2. **ColumnParallelMapping**: 
   - å‡ºåŠ›æ¬¡å…ƒï¼ˆdim 0ï¼‰ã§åˆ†å‰²
   - ä¾‹: ç·šå½¢å±¤ã®å‡ºåŠ›æŠ•å½±

3. **RowParallelMapping**: 
   - å…¥åŠ›æ¬¡å…ƒï¼ˆdim 1ï¼‰ã§åˆ†å‰²
   - ä¾‹: ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ã®å‡ºåŠ›æŠ•å½±

4. **QKVMapping**: 
   - HFã®ç‹¬ç«‹ã—ãŸQ, K, VæŠ•å½±ã‚’Megatronã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒªãƒ¼ãƒ–QKVå½¢å¼ã«çµ±åˆ
   - é€†å¤‰æ›ã‚‚å¯¾å¿œ

5. **GatedMLPMapping**: 
   - gateã¨upæŠ•å½±ã‚’é€£çµ/åˆ†å‰²
   - Llamaã€Mistralãªã©ã§ä½¿ç”¨

6. **ReplicatedMapping**: 
   - TPãƒ©ãƒ³ã‚¯å…¨ä½“ã§å®Œå…¨è¤‡è£½
   - ä¾‹: LayerNormã€åŸ‹ã‚è¾¼ã¿å±¤

### å¤‰æ›ãƒ—ãƒ­ã‚»ã‚¹ï¼ˆColumnParallelMappingã®ä¾‹ï¼‰

```mermaid
sequenceDiagram
    participant HF as Hugging Face
    participant R0 as TP Rank 0
    participant R1 as TP Rank 1
    participant R2 as TP Rank 2
    participant R3 as TP Rank 3
    
    Note over HF,R3: HF â†’ Megatron (import)
    
    HF->>R0: å®Œå…¨ãªãƒ†ãƒ³ã‚½ãƒ«èª­ã¿è¾¼ã¿
    R0->>R0: dim 0ã§åˆ†å‰²ï¼ˆ4ã¤ã®ãƒãƒ£ãƒ³ã‚¯ï¼‰
    R0->>R0: ãƒãƒ£ãƒ³ã‚¯ 0 ä¿æŒ
    R0->>R1: ãƒãƒ£ãƒ³ã‚¯ 1 é€ä¿¡
    R0->>R2: ãƒãƒ£ãƒ³ã‚¯ 2 é€ä¿¡
    R0->>R3: ãƒãƒ£ãƒ³ã‚¯ 3 é€ä¿¡
    
    Note over HF,R3: Megatron â†’ HF (export)
    
    Note over R0,R3: æ‰€æœ‰PP stageãŒå…¨PPãƒ©ãƒ³ã‚¯ã«ãƒ–ãƒ­ãƒ¼ãƒ‰ã‚­ãƒ£ã‚¹ãƒˆ
    
    R0->>R0: ãƒ­ãƒ¼ã‚«ãƒ«ã‚·ãƒ£ãƒ¼ãƒ‰æº–å‚™
    R1->>R1: ãƒ­ãƒ¼ã‚«ãƒ«ã‚·ãƒ£ãƒ¼ãƒ‰æº–å‚™
    R2->>R2: ãƒ­ãƒ¼ã‚«ãƒ«ã‚·ãƒ£ãƒ¼ãƒ‰æº–å‚™
    R3->>R3: ãƒ­ãƒ¼ã‚«ãƒ«ã‚·ãƒ£ãƒ¼ãƒ‰æº–å‚™
    
    R0->>R0: all_gatheré–‹å§‹
    R1->>R0: ã‚·ãƒ£ãƒ¼ãƒ‰é€ä¿¡
    R2->>R0: ã‚·ãƒ£ãƒ¼ãƒ‰é€ä¿¡
    R3->>R0: ã‚·ãƒ£ãƒ¼ãƒ‰é€ä¿¡
    
    R0->>R0: dim 0ã§é€£çµ
    R0->>HF: å®Œå…¨ãªãƒ†ãƒ³ã‚½ãƒ«æ›¸ãå‡ºã—
```

---

## ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹

### ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ç’°å¢ƒ

NVIDIAã®å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã§ã¯ã€ä»¥ä¸‹ã®ã‚·ã‚¹ãƒ†ãƒ ã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãŒæä¾›ã•ã‚Œã¦ã„ã¾ã™ï¼š

- **DGX-GB200**: æœ€æ–°ã®Blackwellã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- **DGX-B200**: Blackwell GPUæ­è¼‰
- **DGX-H100**: Hopper GPUæ­è¼‰

### ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£

- **æº–ç·šå½¢ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£**: GPUã‚’å¢—ã‚„ã™ã¨ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚‚ã»ã¼æ¯”ä¾‹ã—ã¦å‘ä¸Š
- **æ•°åƒãƒãƒ¼ãƒ‰å¯¾å¿œ**: è¶…å¤§è¦æ¨¡ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«æœ€é©åŒ–
- **é«˜GPUåˆ©ç”¨ç‡**: æœ€é©åŒ–ã•ã‚ŒãŸé€šä¿¡ã¨ãƒ¡ãƒ¢ãƒªç®¡ç†

### æœ€é©åŒ–æŠ€è¡“

1. **æ··åˆç²¾åº¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°**
   - FP8: æœ€é«˜ã®åŠ¹ç‡
   - BF16: ãƒãƒ©ãƒ³ã‚¹å‹
   - FP4: æ¥µé™ã®åœ§ç¸®

2. **é€šä¿¡ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—**
   - è¨ˆç®—ã¨é€šä¿¡ã‚’ä¸¦è¡Œå®Ÿè¡Œ
   - ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã®éš è”½

3. **ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–**
   - å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
   - ã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³å†è¨ˆç®—
   - ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³

4. **ã‚«ãƒ¼ãƒãƒ«æœ€é©åŒ–**
   - Transformer Engineçµ±åˆ
   - FlashAttentionã‚µãƒãƒ¼ãƒˆ
   - ã‚«ã‚¹ã‚¿ãƒ CUDAã‚«ãƒ¼ãƒãƒ«

---

## ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 

```
Megatron-Bridge/
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ models/              # Bridgeã®ä½¿ç”¨ä¾‹
â”‚   â””â”€â”€ recipes/             # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚µãƒ³ãƒ—ãƒ«
â”œâ”€â”€ src/megatron/bridge/
â”‚   â”œâ”€â”€ data/                # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã¨ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚¿ãƒ¼
â”‚   â”œâ”€â”€ models/              # HF BridgeåŸºç›¤ã¨ãƒ¢ãƒ‡ãƒ«å›ºæœ‰å®Ÿè£…
â”‚   â”‚   â”œâ”€â”€ llama/           # Llamaãƒ¢ãƒ‡ãƒ«ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼
â”‚   â”‚   â”œâ”€â”€ qwen/            # Qwenãƒ¢ãƒ‡ãƒ«ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼
â”‚   â”‚   â””â”€â”€ .../             # ãã®ä»–ã®ãƒ¢ãƒ‡ãƒ«
â”‚   â”œâ”€â”€ peft/                # PEFTå¤‰æ›ã¨ãƒ©ãƒƒãƒ‘ãƒ¼
â”‚   â”œâ”€â”€ recipes/             # å®Œå…¨ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¬ã‚·ãƒ”
â”‚   â”œâ”€â”€ training/            # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ«ãƒ¼ãƒ—ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
â”‚   â”‚   â”œâ”€â”€ tokenizers/      # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
â”‚   â”‚   â””â”€â”€ utils/           # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å›ºæœ‰ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
â”‚   â””â”€â”€ utils/               # ãƒªãƒã‚¸ãƒˆãƒªå…¨ä½“ç”¨ã®æ±ç”¨ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
â””â”€â”€ tests/                   # åŒ…æ‹¬çš„ãªãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆ
```

---

## ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¨æ¡ç”¨äº‹ä¾‹

### ğŸŒŸ æœ€è¿‘ã®æˆæœ

- **Mind Lab**: 64å°ã®H800ã§å…†ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«ã®GRPO LoRAãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«æˆåŠŸ
- **Nemotron-3-Nano-30B-A3B-FP8**: Day 0ã‚µãƒãƒ¼ãƒˆï¼ˆ2025/12/15ï¼‰

### ğŸ¤ ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æ¡ç”¨

1. **veRL**: Megatron-Coreã¸ã®ã‚³ãƒã‚¯ã‚¿ã¨ã—ã¦æ¡ç”¨
2. **slime**: Megatron-Coreãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚³ãƒ³ãƒãƒ¼ã‚¿ãƒ¼ã¨ã—ã¦æ¡ç”¨
3. **SkyRL**: Megatron-Coreã‚³ãƒã‚¯ã‚¿ã¨ã—ã¦æ¡ç”¨ã€Megatron-Bridgeã¸ç§»è¡Œä¸­
4. **Nemo-RL**: Megatron-Coreã‚³ãƒã‚¯ã‚¿ã¨ã—ã¦æ¡ç”¨

### ğŸ™ ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã‚³ãƒ³ãƒˆãƒªãƒ“ãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³

ç‰¹åˆ¥ãªæ„Ÿè¬ï¼š
- **Guanyou He** ãŠã‚ˆã³ **Junyu Wu** (Weixin Group Infrastructure Center)

---

## æ¯”è¼ƒè¡¨

### PyTorchã€Hugging Faceã€Megatronã€NeMo Bridgeã®æ¯”è¼ƒ

| ç‰¹å¾´ | PyTorch | Hugging Face | Megatron | NeMo Bridge |
|-----|---------|-------------|----------|-------------|
| **ä¸»ãªç”¨é€”** | DLåŸºç›¤ | ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—<br/>æ¨è«– | è¶…å¤§è¦æ¨¡<br/>ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° | HFâ‡”Megatron<br/>å¤‰æ› |
| **ä½¿ã„ã‚„ã™ã•** | â­â­â­ | â­â­â­â­â­ | â­â­ | â­â­â­ |
| **ã‚¹ã‚±ãƒ¼ãƒ«æ€§** | â­â­â­ | â­â­ | â­â­â­â­â­ | - |
| **ãƒ¢ãƒ‡ãƒ«Hub** | - | â­â­â­â­â­<br/>(10ä¸‡ä»¥ä¸Š) | - | ä¸¡æ–¹ä½¿ãˆã‚‹ |
| **å­¦ç¿’æ›²ç·š** | ç·©ã‚„ã‹ | éå¸¸ã«ç·©ã‚„ã‹ | æ€¥ | ä¸­ç¨‹åº¦ |
| **ä¸¦åˆ—åŒ–** | DDP/FSDP | åŸºæœ¬çš„ | TP/PP/EP<br/>VPP/CP | TP/PP/EPå¯¾å¿œ |
| **æœ€å¤§ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º** | ã€œ10B | ã€œ70B | æ•°åƒB+ | å¤‰æ›ã®ã¿ |
| **GPUåŠ¹ç‡** | â­â­â­ | â­â­ | â­â­â­â­â­ | - |
| **ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£** | å·¨å¤§ | å·¨å¤§ | å°‚é–€å®¶å‘ã‘ | æˆé•·ä¸­ |
| **ä¼æ¥­ã‚µãƒãƒ¼ãƒˆ** | Meta | Hugging Face | NVIDIA | NVIDIA |
| **é©ç”¨å ´é¢** | ç ”ç©¶ãƒ»é–‹ç™º | ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—<br/>æ¨è«–å±•é–‹ | å¤§è¦æ¨¡<br/>ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° | ä¸¡æ–¹ã®çµ±åˆ |

### ä½¿ç”¨ã‚·ãƒ¼ãƒ³ã”ã¨ã®æ¨å¥¨

```mermaid
graph TD
    Start["ãƒ¢ãƒ‡ãƒ«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®è¦ä»¶"]
    
    Start --> Q1{"ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã¯?"}
    
    Q1 -->|< 10B| Small["å°ã€œä¸­è¦æ¨¡ãƒ¢ãƒ‡ãƒ«"]
    Q1 -->|10B-70B| Medium["ä¸­ã€œå¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«"]
    Q1 -->|> 70B| Large["è¶…å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«"]
    
    Small --> HF_Simple["Hugging Face Transformers<br/>âœ… æœ€ã‚‚ç°¡å˜<br/>âœ… è¿…é€Ÿãªé–‹ç™º"]
    
    Medium --> Q2{"åˆ©ç”¨å¯èƒ½ãªGPUæ•°ã¯?"}
    Q2 -->|1-8| HF_Med["Hugging Face + FSDP<br/>âš ï¸ ãƒ¡ãƒ¢ãƒªã«æ³¨æ„"]
    Q2 -->|8+| Bridge_Med["NeMo Bridge + Megatron<br/>âœ… ã‚ˆã‚ŠåŠ¹ç‡çš„"]
    
    Large --> Bridge_Large["NeMo Bridge + Megatron<br/>âœ… å¿…é ˆã®é¸æŠè‚¢<br/>âœ… æœ€é«˜ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£"]
    
    style HF_Simple fill:#87CEEB
    style HF_Med fill:#87CEEB
    style Bridge_Med fill:#90EE90
    style Bridge_Large fill:#90EE90
```

---

## ã¾ã¨ã‚

### ğŸ¯ é‡è¦ãªãƒã‚¤ãƒ³ãƒˆ

1. **PyTorch**: ã™ã¹ã¦ã®åŸºç›¤ã¨ãªã‚‹ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
   - ãƒ†ãƒ³ã‚½ãƒ«æ¼”ç®—ã€è‡ªå‹•å¾®åˆ†ã€åŸºæœ¬çš„ãªåˆ†æ•£ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ©Ÿèƒ½ã‚’æä¾›
   - æŸ”è»Ÿã§ä½¿ã„ã‚„ã™ã„ãŒã€è¶…å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã«ã¯é™ç•ŒãŒã‚ã‚‹

2. **Hugging Face**: ä½¿ã„ã‚„ã™ã•ã¨ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã‚’é‡è¦–
   - 10ä¸‡ä»¥ä¸Šã®äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«
   - çµ±ä¸€ã•ã‚ŒãŸAPIã€ç°¡å˜ãªãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°
   - æ¨è«–ãƒ»å¾®èª¿æ•´ã«æœ€é©ã ãŒã€100B+ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¯ä¸å‘ã

3. **Megatron**: è¶…å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ç‰¹åŒ–
   - é«˜åº¦ãªä¸¦åˆ—åŒ–æŠ€è¡“ï¼ˆTP/PP/EP/VPP/CPï¼‰
   - æº–ç·šå½¢ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã€æ•°åƒGPUå¯¾å¿œ
   - æœ€é«˜ã®ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã ãŒã€è¨­å®šãŒè¤‡é›‘

4. **NeMo Megatron Bridge**: HFã¨Megatronã®ã€Œé€šè¨³ã€
   - åŒæ–¹å‘å¤‰æ›ã€ä¸¦åˆ—åŒ–ã‚’ä¿æŒ
   - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å˜ä½ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„
   - ä¸¡è€…ã®å¼·ã¿ã‚’çµ„ã¿åˆã‚ã›ã‚‰ã‚Œã‚‹

### ğŸ’¡ ä½¿ã„åˆ†ã‘ã®ã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³

**Hugging Faceã‚’é¸ã¶ã¹ãå ´åˆ:**
- ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—ã‚’ç´ æ—©ãä½œã‚ŠãŸã„
- ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºãŒ10Bæœªæº€
- ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã—ãŸã„
- æ¨è«–ã«é‡ç‚¹ã‚’ç½®ã„ã¦ã„ã‚‹

**Megatron + Bridgeã‚’é¸ã¶ã¹ãå ´åˆ:**
- ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºãŒ70Bä»¥ä¸Š
- è¤‡æ•°GPUãƒãƒ¼ãƒ‰ã§ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå¿…è¦
- æœ€é«˜ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãŒå¿…è¦
- å¤§è¦æ¨¡ãªæœ¬ç•ªç’°å¢ƒã§ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°

**ç†æƒ³çš„ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼:**
1. Hugging Faceã§å®Ÿé¨“ãƒ»ãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°
2. NeMo Bridgeã§å¤‰æ›
3. Megatronã§å¤§è¦æ¨¡ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
4. NeMo Bridgeã§å†å¤‰æ›
5. Hugging Faceã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã§æ¨è«–å±•é–‹

### ğŸ”„ ã“ã‚Œã‚‰ã¯ç«¶åˆã§ã¯ãªãè£œå®Œé–¢ä¿‚

```mermaid
mindmap
  root((AI/ML<br/>ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ))
    PyTorch
      åŸºç›¤ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯
      æŸ”è»Ÿæ€§
      ç ”ç©¶è€…ã«äººæ°—
    Hugging Face
      ä½¿ã„ã‚„ã™ã•
      ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£
      ãƒ¢ãƒ‡ãƒ«Hub
      æ¨è«–æœ€é©åŒ–
    Megatron
      è¶…å¤§è¦æ¨¡
      ä¸¦åˆ—åŒ–
      ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£
      æœ¬ç•ªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
    NeMo Bridge
      ç›¸äº’é‹ç”¨æ€§
      å¤‰æ›
      çµ±åˆ
      ä¸¡æ–¹ã®é•·æ‰€
```

### ğŸš€ ä»Šå¾Œã®å±•æœ›

NeMo Megatron Bridgeã¯ã€ä»¥ä¸‹ã®ç‚¹ã§ AI/ML ã‚¨ã‚³ã‚·ã‚¹ãƒ†ãƒ ã«ãŠã„ã¦é‡è¦ãªå½¹å‰²ã‚’æœãŸã—ã¦ã„ã¾ã™ï¼š

1. **ã‚ªãƒ¼ãƒ—ãƒ³ã‚µã‚¤ã‚¨ãƒ³ã‚¹ã®ä¿ƒé€²**: HFã®è±Šå¯Œãªãƒ¢ãƒ‡ãƒ«ã‚’Megatronã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ã«
2. **æŠ€è¡“ã®æ°‘ä¸»åŒ–**: è¶…å¤§è¦æ¨¡ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’å®¹æ˜“ã«
3. **ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ—ãƒ©ã‚¤ã‚ºå¯¾å¿œ**: æœ¬ç•ªç’°å¢ƒã§ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ãªãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°
4. **ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã®åŠ é€Ÿ**: ç ”ç©¶ã‹ã‚‰æœ¬ç•ªã¸ã®ãƒ‘ã‚¹ã‚’çŸ­ç¸®

---

## å‚è€ƒãƒªãƒ³ã‚¯

### ğŸ“š å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

- **NeMo Megatron Bridge**
  - [GitHub Repository](https://github.com/NVIDIA-NeMo/Megatron-Bridge)
  - [å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://docs.nvidia.com/nemo/megatron-bridge/latest/)
  - [å¤‰æ›æŠ€è¡“è©³ç´°](https://docs.nvidia.com/nemo/megatron-bridge/latest/bridge-tech-details.html)

- **Hugging Face**
  - [Transformers Documentation](https://huggingface.co/docs/transformers)
  - [Model Hub](https://huggingface.co/models)

- **Megatron-LM**
  - [GitHub Repository](https://github.com/NVIDIA/Megatron-LM)
  - [Megatron-Core](https://github.com/NVIDIA/Megatron-Core)

- **PyTorch**
  - [PyTorch Documentation](https://pytorch.org/docs/)
  - [Distributed Training](https://pytorch.org/tutorials/beginner/dist_overview.html)

### ğŸ“ ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã¨ã‚¬ã‚¤ãƒ‰

- [NeMo Bridge ã‚¯ã‚¤ãƒƒã‚¯ã‚¹ã‚¿ãƒ¼ãƒˆ](https://docs.nvidia.com/nemo/megatron-bridge/latest/#quickstart)
- [Using Recipes Guide](https://docs.nvidia.com/nemo/megatron-bridge/latest/recipes.html)
- [Performance Summary](https://docs.nvidia.com/nemo/megatron-bridge/latest/performance.html)

### ğŸŒŸ ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒªã‚½ãƒ¼ã‚¹

- [Mind Lab Tech Blog - GRPO LoRA Training](https://mindcorner.com/grpo-lora-trillion-parameter-model)
- [NeMo Framework User Guide](https://docs.nvidia.com/nemo-framework/user-guide/)

### ğŸ³ ã‚³ãƒ³ãƒ†ãƒŠã‚¤ãƒ¡ãƒ¼ã‚¸

```bash
# NeMo Framework ã‚³ãƒ³ãƒ†ãƒŠ
docker pull nvcr.io/nvidia/nemo:${TAG}

# Nemotron-3-Nano ç”¨ã‚«ã‚¹ã‚¿ãƒ ã‚³ãƒ³ãƒ†ãƒŠ
docker pull nvcr.io/nvidia/nemo:25.11.nemotron_3_nano
```

### ğŸ’» ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# Hugging Face CLI
pip install huggingface_hub
huggingface-cli login

# NeMo Megatron Bridge (é–‹ç™ºç‰ˆ)
git clone https://github.com/NVIDIA-NeMo/Megatron-Bridge
cd Megatron-Bridge
pip install -e .
```

---

**ä½œæˆæ—¥**: 2025å¹´12æœˆ19æ—¥  
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0  
**èª¿æŸ»å¯¾è±¡**: NVIDIA NeMo Megatron Bridge (GitHub: NVIDIA-NeMo/Megatron-Bridge)

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯ã€NeMo Megatron Bridgeã®å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¨GitHubãƒªãƒã‚¸ãƒˆãƒªã®æƒ…å ±ã«åŸºã¥ã„ã¦ä½œæˆã•ã‚Œã¾ã—ãŸã€‚æœ€æ–°æƒ…å ±ã«ã¤ã„ã¦ã¯ã€ä¸Šè¨˜ã®å…¬å¼ãƒªãƒ³ã‚¯ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
