---
title: "Blueprints by Slurm: ãƒ¬ã‚¸ãƒªã‚¨ãƒ³ã‚·ãƒ¼ã¨å¯è¦³æ¸¬æ€§-å‰ç·¨"
emoji: "ğŸ”§"
type: "tech"
topics: ["aws", "sagemaker", "hyperpod", "slurm", "resiliency", "observability"]
free: true
---

::::details å‰æ
:::message
**å¯¾è±¡èª­è€…**: Amazon SageMaker HyperPod Slurm ç’°å¢ƒã‚’æ§‹ç¯‰æ¸ˆã¿ã§ã€å®Ÿéš›ã® resiliency æ©Ÿèƒ½ã¨ observability ã®å‹•ä½œã‚’ç¢ºèªã—ãŸã„æ–¹ã€‚åˆ†æ•£å­¦ç¿’ã®é‹ç”¨é¢ã«èˆˆå‘³ãŒã‚ã‚‹æ–¹ã€‚
:::
:::message
**ãƒ©ã‚¤ã‚»ãƒ³ã‚¹**: Â© 2025 littlemex.
æœ¬æ–‡ãŠã‚ˆã³è‡ªä½œå›³è¡¨: CC BY 4.0
â€»å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ã®å¼•ç”¨ã‚„ç¿»è¨³éƒ¨åˆ†ã¯åŸå…¸ã®è‘—ä½œæ¨©ã«å¾“ã„ã¾ã™ã€‚
å¼•ç”¨ç”»åƒ: å„ç”»åƒã®å‡ºå…¸ã«è¨˜è¼‰ã•ã‚ŒãŸãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã«å¾“ã„ã¾ã™ã€‚
:::
:::message
ä¸€éƒ¨ AI ã‚’ç”¨ã„ã¦æ–‡ç« ã‚’ä½œæˆã—ã¾ã™ã€‚ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¯å®Ÿæ–½ã—ã¾ã™ãŒã€è¦‹é€ƒã›ãªã„é‡å¤§ãªé–“é•ã„ãªã©ãŒã‚ã‚Œã°[ã“ã¡ã‚‰ã® Issue](https://github.com/littlemex/samples/issues) ã‹ã‚‰é€£çµ¡ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚
:::
::::

:::message
å®Ÿè£…ãŒå¤‰æ›´ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚å¿…è¦ã«å¿œã˜ã¦[å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://awslabs.github.io/ai-on-sagemaker-hyperpod/docs/validation-and-testing/resiliency/overview)ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚
:::

**æœ¬ç« ã§ã¯ Amazon SageMaker HyperPod Slurm ç’°å¢ƒã«ãŠã‘ã‚‹éšœå®³å¯¾å¿œåŠ›ã®æ¤œè¨¼ã¨å¯è¦–åŒ–ã«ã¤ã„ã¦å®Ÿè·µã—ã¾ã™ã€‚**

---

[HyperPod Resiliency ãƒ†ã‚¹ãƒˆã‚¬ã‚¤ãƒ‰](https://awslabs.github.io/ai-on-sagemaker-hyperpod/docs/validation-and-testing/resiliency/slurm-resiliency)ã¨ [Observability è¨­å®šæ‰‹é †](https://awslabs.github.io/ai-on-sagemaker-hyperpod/docs/add-ons/Observability/observability-slurm)ã€ãŠã‚ˆã³[ç’°å¢ƒæ¤œè¨¼ã‚¬ã‚¤ãƒ‰](https://awslabs.github.io/ai-on-sagemaker-hyperpod/docs/validation-and-testing/environment-validation/pytorch-environment-validation)ã‚’å‚ç…§ã—ãªãŒã‚‰ã€æ„å›³çš„ãªéšœå®³æ³¨å…¥ã«ã‚ˆã‚‹ãƒ¬ã‚¸ãƒªã‚¨ãƒ³ã‚·ãƒ¼ã®å®Ÿé¨“ã‚’å®Ÿæ–½ã—ã¾ã™ã€‚

# éšœå®³å¯¾å¿œåŠ›æ¤œè¨¼

Amazon SageMaker HyperPod ã§ã¯ã€å¤§è¦æ¨¡ãªåˆ†æ•£å­¦ç¿’ã«ãŠã‘ã‚‹éšœå®³ã‹ã‚‰ã®è‡ªå‹•å¾©æ—§ãŒé‡è¦ãªæ©Ÿèƒ½ã¨ã—ã¦å®Ÿè£…ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã¯ã™ã§ã«ã“ã‚Œã¾ã§ã®ç« ã§è§£èª¬ã—ã¾ã—ãŸã€‚

æœ¬ç« ã§ã¯ Amazon SageMaker HyperPod ã® Slurm ç’°å¢ƒã«ãŠã„ã¦ã€æ„å›³çš„ãªéšœå®³æ³¨å…¥ã«ã‚ˆã‚Šéšœå®³å¯¾å¿œåŠ›ã‚’å®Ÿéš›ã«æ¤œè¨¼ã—ã€observability ã‚·ã‚¹ãƒ†ãƒ ã‚’é€šã˜ã¦å¾©æ—§ãƒ—ãƒ­ã‚»ã‚¹ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚åˆ¶å¾¡ã•ã‚ŒãŸç’°å¢ƒã§ã®éšœå®³ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€Auto-Resume æ©Ÿèƒ½ã«ã‚ˆã‚‹è‡ªå‹•å¾©æ—§ã€ãã—ã¦ Grafana ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ã«ã‚ˆã‚Šã€å¤§è¦æ¨¡å­¦ç¿’ç’°å¢ƒã«ãŠã‘ã‚‹éšœå®³å¯¾å¿œãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®å®ŸåŠ¹æ€§ã‚’ç¢ºèªã—ã¾ã™ã€‚


## Node Recovery ã®å‹•ä½œãƒ•ãƒ­ãƒ¼

[HyperPod ã® Automatic Node Recovery](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-resiliency-slurm-auto-resume.html) ã¯ã€Health Monitoring Agentï¼ˆHMAï¼‰ã«ã‚ˆã‚‹éšœå®³æ¤œå‡ºã‹ã‚‰å§‹ã¾ã‚Šã¾ã™ã€‚HMA ãŒ GPU ã®æ¸©åº¦ç•°å¸¸ã€ãƒ¡ãƒ¢ãƒªã‚¨ãƒ©ãƒ¼ã€NVLink éšœå®³ãªã©ã®ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢å•é¡Œã‚’æ¤œå‡ºã™ã‚‹ã¨ã€è©²å½“ãƒãƒ¼ãƒ‰ã¯è‡ªå‹•çš„ã«ãƒ‰ãƒ¬ã‚¤ãƒ³çŠ¶æ…‹ã«ãƒãƒ¼ã‚¯ã•ã‚Œã¾ã™ã€‚å®Ÿè¡Œä¸­ã®ã‚¸ãƒ§ãƒ–ãŒã™ã¹ã¦çµ‚äº†ã—ãŸå¾Œã€å•é¡Œã®ã‚ã‚‹ãƒãƒ¼ãƒ‰ã¯æ–°ã—ã„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«è‡ªå‹•çš„ã«äº¤æ›ã•ã‚Œã¾ã™ã€‚

é‡è¦ãªç‚¹ã¨ã—ã¦ã€Slurm ç’°å¢ƒã§ã® auto-resume æ©Ÿèƒ½ä½¿ç”¨æ™‚ã¯ã€å•é¡Œã®ã‚ã‚‹ãƒãƒ¼ãƒ‰ã‚’å¸¸ã«äº¤æ›ã—ã€ãƒªãƒ–ãƒ¼ãƒˆã¯å®Ÿè¡Œã•ã‚Œã¾ã›ã‚“ã€‚

```mermaid
graph TB
    subgraph "HyperPod Slurm Resiliency ãƒ•ãƒ­ãƒ¼"
        subgraph "ç›£è¦–ãƒ»æ¤œå‡ºãƒ•ã‚§ãƒ¼ã‚º"
            HMA[Health Monitoring Agent<br/>ç¶™ç¶šçš„ãªç›£è¦–]
            DETECT[éšœå®³æ¤œå‡º<br/>GPU/æ¸©åº¦/ãƒ¡ãƒ¢ãƒªã‚¨ãƒ©ãƒ¼]
            DRAIN[ãƒãƒ¼ãƒ‰ãƒ‰ãƒ¬ã‚¤ãƒ³<br/>æ–°è¦ã‚¸ãƒ§ãƒ–åœæ­¢]
        end
        
        subgraph "å¾©æ—§ãƒ•ã‚§ãƒ¼ã‚º"
            WAIT[å®Ÿè¡Œä¸­ã‚¸ãƒ§ãƒ–ã®<br/>æ­£å¸¸çµ‚äº†å¾…æ©Ÿ]
            REPLACE[ãƒãƒ¼ãƒ‰äº¤æ›<br/>æ–°ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èµ·å‹•]
            VALIDATE[æ–°ãƒãƒ¼ãƒ‰æ¤œè¨¼<br/>Health Check å®Ÿè¡Œ]
        end
        
        subgraph "å†é–‹ãƒ•ã‚§ãƒ¼ã‚º"
            RESUME[Auto-Resume<br/>ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹]
            NORMAL[é€šå¸¸é‹ç”¨å¾©å¸°]
        end
        
        HMA --> DETECT
        DETECT --> DRAIN
        DRAIN --> WAIT
        WAIT --> REPLACE
        REPLACE --> VALIDATE
        VALIDATE --> RESUME
        RESUME --> NORMAL
        
        NORMAL --> HMA
    end
    
    style HMA fill:#2d5986,color:#fff
    style DETECT fill:#8b4513,color:#fff
    style DRAIN fill:#6b4513,color:#fff
    style WAIT fill:#4a6b35,color:#fff
    style REPLACE fill:#2d6b35,color:#fff
    style VALIDATE fill:#5a7a96,color:#fff
    style RESUME fill:#4a5986,color:#fff
    style NORMAL fill:#2d5986,color:#fff
```

## Auto-Resume ã¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®é–¢ä¿‚

Auto-resume æ©Ÿèƒ½ã¯ã€[`--auto-resume=1` ãƒ•ãƒ©ã‚°ã‚’ä»˜ã‘ã¦æŠ•å…¥ã•ã‚ŒãŸã‚¸ãƒ§ãƒ–](https://awslabs.github.io/ai-on-sagemaker-hyperpod/docs/validation-and-testing/resiliency/slurm-resiliency)ã«å¯¾ã—ã¦è‡ªå‹•çš„ã«å‹•ä½œã—ã¾ã™ã€‚ã“ã®æ©Ÿèƒ½ãŒæœ‰åŠ¹ãªã‚¸ãƒ§ãƒ–ã§ã¯ã€ãƒãƒ¼ãƒ‰éšœå®³ãŒç™ºç”Ÿã—ãŸéš›ã«æœ€å¾Œã«ä¿å­˜ã•ã‚ŒãŸãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰è‡ªå‹•çš„ã«å­¦ç¿’ãŒå†é–‹ã•ã‚Œã¾ã™ã€‚

ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ä¿å­˜ã¯ã€å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆå†…ã§å®šæœŸçš„ã«å®Ÿè¡Œã•ã‚Œã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚PyTorch ã® `torch.save()` é–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã® state_dictã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã®çŠ¶æ…‹ã€ç¾åœ¨ã®ã‚¨ãƒãƒƒã‚¯æ•°ã‚’ä¿å­˜ã™ã‚‹ã“ã¨ã§ã€éšœå®³ç™ºç”Ÿæ™‚ã®å­¦ç¿’é€²æ—ã®æå¤±ã‚’æœ€å°é™ã«æŠ‘ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

::::details ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å´ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå®Ÿè£…ã®è©³ç´°

:::message
**é‡è¦**: HyperPod ã® Auto-Resume æ©Ÿèƒ½ã¯ã€ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ãƒ¬ãƒ™ãƒ«ã§ã®ãƒãƒ¼ãƒ‰äº¤æ›ã¨ã‚¸ãƒ§ãƒ–å†é–‹ã‚’è‡ªå‹•åŒ–ã—ã¾ã™ãŒã€å­¦ç¿’çŠ¶æ…‹ã®ä¿å­˜ã¨å¾©å…ƒã¯ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å´ã§å®Ÿè£…ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ©Ÿèƒ½ãŒé©åˆ‡ã«å®Ÿè£…ã•ã‚Œã¦ã„ãªã„å ´åˆã€ãƒãƒ¼ãƒ‰ãŒå¾©æ—§ã—ã¦ã‚‚ã‚¼ãƒ­ã‹ã‚‰å­¦ç¿’ã‚’é–‹å§‹ã™ã‚‹ã“ã¨ã«ãªã‚Šã¾ã™ã€‚
:::

[PyTorch DistributedDataParallel (DDP)](https://docs.pytorch.org/tutorials/beginner/ddp_series_multigpu.html) ã§ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ã¨å¾©æ—§ã®å®Ÿè£…ä¾‹ã‚’ç´¹ä»‹ã—ã¾ã™ãŒã€FSDP ã®å ´åˆã¯[ã“ã¡ã‚‰](https://docs.pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html)ã‚’ç¢ºèªã—ã¾ã—ã‚‡ã†ã€‚

```mermaid
graph TB
    subgraph "DDP ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ"
        subgraph S["ä¿å­˜æ®µéš"]
            SAVE_TRIGGER[ä¿å­˜ãƒˆãƒªã‚¬ãƒ¼<br/>å®šæœŸå®Ÿè¡Œãƒ»ã‚·ã‚°ãƒŠãƒ«å—ä¿¡]
            RANK_CHECK[ãƒ©ãƒ³ã‚¯ç¢ºèª<br/>rank 0 ã®ã¿ä¿å­˜]
            STATE_COLLECT[çŠ¶æ…‹åé›†<br/>model.module.state_dict]
            ATOMIC_WRITE[åŸå­çš„æ›¸ãè¾¼ã¿<br/>.tmp â†’ rename]
        end
        
        subgraph R["å¾©æ—§æ®µéš"]
            DETECT[ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ¤œå‡º<br/>latest_checkpoint.pth]
            MAP_LOAD[ãƒ‡ãƒã‚¤ã‚¹å¯¾å¿œèª­ã¿è¾¼ã¿<br/>map_location æŒ‡å®š]
            STATE_RESTORE[çŠ¶æ…‹å¾©å…ƒ<br/>load_state_dict]
            RESUME[å­¦ç¿’å†é–‹<br/>ä¿å­˜ã‚¨ãƒãƒƒã‚¯ã‹ã‚‰ç¶™ç¶š]
        end
        
        SAVE_TRIGGER --> RANK_CHECK
        RANK_CHECK --> STATE_COLLECT
        STATE_COLLECT --> ATOMIC_WRITE
        
        DETECT --> MAP_LOAD
        MAP_LOAD --> STATE_RESTORE
        STATE_RESTORE --> RESUME
        S --> R
    end
    
    style SAVE_TRIGGER fill:#2d5986,color:#fff
    style RANK_CHECK fill:#4a6b35,color:#fff
    style STATE_COLLECT fill:#5a7a96,color:#fff
    style ATOMIC_WRITE fill:#6b4513,color:#fff
    style DETECT fill:#8b4513,color:#fff
    style MAP_LOAD fill:#4a5986,color:#fff
    style STATE_RESTORE fill:#2d6b35,color:#fff
    style RESUME fill:#2d5986,color:#fff
```

**1. DDP ã§ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜å®Ÿè£…**

[PyTorch å…¬å¼ DDP ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](https://docs.pytorch.org/tutorials/beginner/ddp_series_multigpu.html)ã«å¾“ã£ãŸæ¨™æº–çš„ãªãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã™ã€‚

```python
import torch
import torch.distributed as dist
import os

def save_checkpoint(model, optimizer, epoch, loss, checkpoint_dir):
    """DDP å¯¾å¿œãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜"""
    # rank 0 ã®ã¿ãŒãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜
    if dist.get_rank() == 0:
        checkpoint_state = {
            'epoch': epoch,
            'model': model.module.state_dict(),  # DDP wrapperå¯¾å¿œ
            'optimizer': optimizer.state_dict(),
            'loss': loss,
            'world_size': dist.get_world_size(),
        }
        
        # åŸå­çš„ä¿å­˜ã§ç ´æãƒ•ã‚¡ã‚¤ãƒ«ã‚’é˜²ãï¼ˆé‡è¦ãªæ¦‚å¿µï¼‰
        checkpoint_path = os.path.join(checkpoint_dir, f"checkpoint_epoch_{epoch}.pth")
        tmp_path = f"{checkpoint_path}.tmp"
        
        # 1. ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«å®Œå…¨ã«æ›¸ãè¾¼ã¿
        torch.save(checkpoint_state, tmp_path)
        
        # 2. åŸå­çš„æ“ä½œã§ç¬æ™‚ã«ç½®æ›
        # os.rename() ã¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ãƒ¬ãƒ™ãƒ«ã§ã€ŒAll or Nothingã€ã‚’ä¿è¨¼
        # â†’ å®Œå…¨ãªãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ãŒå­˜åœ¨ã—ã€éƒ¨åˆ†çš„ã«æ›¸ãè¾¼ã¾ã‚ŒãŸç ´æãƒ•ã‚¡ã‚¤ãƒ«ã¯ä½œã‚‰ã‚Œãªã„
        os.rename(tmp_path, checkpoint_path)
        
        print(f"Checkpoint saved: {checkpoint_path}")
```

**2. DDP ã§ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿å®Ÿè£…**

[PyTorch DDP ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹](https://docs.pytorch.org/tutorials/beginner/ddp_series_multigpu.html#save-and-load-checkpoints)ã«å¾“ã£ãŸå¾©æ—§å‡¦ç†ã§ã™ã€‚

```python
def load_checkpoint(model, optimizer, rank, checkpoint_path):
    """DDP å¯¾å¿œãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿"""
    # å„rankã«é©åˆ‡ãªãƒ‡ãƒã‚¤ã‚¹ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’æŒ‡å®š
    map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}
    checkpoint_state = torch.load(checkpoint_path, map_location=map_location)
    
    # ãƒ¢ãƒ‡ãƒ«ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã®çŠ¶æ…‹å¾©å…ƒ
    model.module.load_state_dict(checkpoint_state['model'])
    optimizer.load_state_dict(checkpoint_state['optimizer'])
    
    start_epoch = checkpoint_state['epoch'] + 1
    loss = checkpoint_state['loss']
    
    print(f"Resumed from epoch {start_epoch} on rank {rank}")
    return start_epoch, loss
```

**3. Auto-Resume ã¨ã®çµ±åˆãƒã‚¤ãƒ³ãƒˆ**

HyperPod ã® [`--auto-resume=1` ãƒ•ãƒ©ã‚°](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-resiliency-slurm-auto-resume.html)ã¨ DDP ã‚’çµ„ã¿åˆã‚ã›ã‚‹ãŸã‚ã®å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã™ã€‚

```python
import signal

def setup_checkpoint_handler(model, optimizer, checkpoint_dir):
    """HyperPod Auto-Resume å¯¾å¿œã®ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    def emergency_checkpoint_save(signum, frame):
        if dist.get_rank() == 0:
            emergency_path = os.path.join(checkpoint_dir, "emergency_checkpoint.pth")
            emergency_state = {
                'model': model.module.state_dict(),
                'optimizer': optimizer.state_dict(),
                'emergency': True
            }
            torch.save(emergency_state, emergency_path)
            print(f"Emergency checkpoint saved: {emergency_path}")
        exit(0)
    
    signal.signal(signal.SIGTERM, emergency_checkpoint_save)
    signal.signal(signal.SIGINT, emergency_checkpoint_save)

# å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã§ã®ä½¿ç”¨ä¾‹ï¼ˆå®Ÿéš›ã®ãƒã‚¦ãƒ³ãƒˆãƒã‚¤ãƒ³ãƒˆã‚’ç¢ºèªï¼‰
# FSx_MOUNT=$(df -h | grep fsx_lustre | awk '{print $NF}')
setup_checkpoint_handler(ddp_model, optimizer, f"{FSX_MOUNT}/checkpoints")
```

**4. å®Ÿé‹ç”¨ã§ã®æ¨å¥¨è¨­å®š**

- **ä¿å­˜é »åº¦**: 5-15 åˆ†é–“éš”ã¾ãŸã¯ 100-500 ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨
- **ä¿å­˜å ´æ‰€**: FSx for Lustre ã®å…±æœ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (`/fsx/checkpoints/`)
- **ãƒ‡ãƒã‚¤ã‚¹ãƒãƒƒãƒ”ãƒ³ã‚°**: å„ rank ã«é©åˆ‡ãª map_location æŒ‡å®š
- **DDP å¯¾å¿œ**: `model.module.state_dict()` ã«ã‚ˆã‚‹æ­£ã—ã„çŠ¶æ…‹å–å¾—

DDP ã®è©³ç´°ãªå®Ÿè£…æ–¹æ³•ã«ã¤ã„ã¦ã¯ [PyTorch DDP ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](https://docs.pytorch.org/tutorials/beginner/ddp_series_multigpu.html)ã€HyperPod ç’°å¢ƒã§ã®å®Ÿè·µã«ã¤ã„ã¦ã¯æœ¬æ›¸ã® [PyTorch DDP ç« ](./pytorch-ddp.md) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
::::

## Observability ã®éšå±¤æ§‹é€ 

[HyperPod Slurm ç’°å¢ƒã§ã® observability](https://awslabs.github.io/ai-on-sagemaker-hyperpod/docs/add-ons/Observability/observability-slurm) ã¯ã€Amazon Managed Service for Prometheus ã¨ Amazon Managed Grafana ã‚’æ‰‹å‹•ã§çµ±åˆã™ã‚‹ã“ã¨ã§å®Ÿç¾ã•ã‚Œã¾ã™ã€‚EKS ç’°å¢ƒã®ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯çµ±åˆã¨ã¯ç•°ãªã‚Šã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¿ãƒ¼ã®æ‰‹å‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨è¨­å®šãŒå¿…è¦ã¨ãªã‚Šã¾ã™ã€‚

Observability ã®éšå±¤ã¯ã€å‰ã®ç« ã§èª¬æ˜ã—ãŸ**çµ±åˆãƒ†ãƒ¬ãƒ¡ãƒˆãƒª**ã®æ¦‚å¿µã‚’å…·ç¾åŒ–ã—ãŸã‚‚ã®ã§ã™ã€‚ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ¬ãƒ™ãƒ«ã§ã¯ Slurm ã®ã‚¸ãƒ§ãƒ–ã‚­ãƒ¥ãƒ¼ã®çŠ¶æ…‹ã€ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æƒ…å ±ã€ãƒãƒ¼ãƒ‰ã®åˆ©ç”¨ç‡ã‚’ç›£è¦–ã—ã¾ã™ã€‚ãƒãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã§ã¯ GPU ã®æ¸©åº¦ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã€é›»åŠ›æ¶ˆè²»é‡ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚’è¿½è·¡ã—ã¾ã™ã€‚ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¬ãƒ™ãƒ«ã§ã¯å­¦ç¿’ã®é€²æ—ã€æå¤±é–¢æ•°ã®å€¤ã€ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’è¨˜éŒ²ã—ã¾ã™ã€‚

ã“ã‚Œã‚‰ã®å¤šå±¤çš„ãªç›£è¦–ã«ã‚ˆã‚Šã€éšœå®³ã®æ ¹æœ¬åŸå› ã‚’è¿…é€Ÿã«ç‰¹å®šã—ã€äºˆé˜²çš„ãªå¯¾ç­–ã‚’è¬›ã˜ã‚‹ã“ã¨ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚ä¾‹ãˆã°ã€ç‰¹å®šã® GPU ã§æ¸©åº¦ä¸Šæ˜‡ãŒç¶™ç¶šçš„ã«è¦³æ¸¬ã•ã‚Œã‚‹å ´åˆã€ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢éšœå®³ã®äºˆå…†ã¨ã—ã¦äº‹å‰ã«ãƒãƒ¼ãƒ‰ã‚’äº¤æ›ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

---

# Amazon SageMaker HyperPod Slurm ã§ã®å®Ÿè£…

ã“ã“ã‹ã‚‰ã¯ã€å®Ÿéš›ã« HyperPod Slurm ç’°å¢ƒã§ resiliency ã¨ observability ã‚’ç¢ºèªã—ã¾ã™ã€‚å‰ç« ã§æ§‹ç¯‰ã—ãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’åŸºç›¤ã¨ã—ã¦ã€å®Ÿéš›ã®éšœå®³æ³¨å…¥ã‹ã‚‰å¾©æ—§ã¾ã§ã®ä¸€é€£ã®å‹•ä½œã‚’æ¤œè¨¼ã—ã¾ã—ã‚‡ã†ã€‚

## å‰ææ¡ä»¶

::::details ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£è¦ä»¶

:::message
**Slurm ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®æº–å‚™**

æœ¬ç« ã®å®Ÿè·µã«ã¯ã€å‰ç« ã§æ§‹ç¯‰ã—ãŸ Amazon SageMaker HyperPod Slurm ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãŒç¨¼åƒã—ã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãŒå‰Šé™¤ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€[Amazon SageMaker HyperPod Getting Started by SLURM](./amazon-sagemaker-hyperpod-slurm-tutorial) ã‚’å‚ç…§ã—ã¦ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’å†ä½œæˆã—ã¦ãã ã•ã„ã€‚
:::

:::message
AWS CLI v2 ã¨SSM Session Manager ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ãŒé©åˆ‡ã«è¨­å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ã¾ãŸã€Amazon Managed Service for Prometheus ã¨ Amazon Managed Grafana ã®ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã‚’ä½œæˆã™ã‚‹æ¨©é™ãŒå¿…è¦ã§ã™ã€‚
:::


## æ¨å¥¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ§‹æˆï¼ˆresiliency ãƒ†ã‚¹ãƒˆç”¨ï¼‰

å®Ÿéš›ã® resiliency ãƒ†ã‚¹ãƒˆã«ã¯ GPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®è¿½åŠ ã‚’æ¨å¥¨ã—ã¾ã™ã€‚GPU å›ºæœ‰ã®éšœå®³ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ãã®å¾©æ—§å‹•ä½œã‚’ç¢ºèªã™ã‚‹ãŸã‚ã§ã™ã€‚å‰ç« ã® CPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹æ§‹æˆã«åŠ ãˆã¦ã€Worker ã‚°ãƒ«ãƒ¼ãƒ—ã« `ml.g5.xlarge` ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ 2 å°è¿½åŠ ã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šå®Ÿè·µçš„ãªãƒ†ã‚¹ãƒˆç’°å¢ƒã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚

### provisioning_parameters.json è‡ªå‹•æ›´æ–°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã“ã® `provisioning_parameters.json` ãƒ•ã‚¡ã‚¤ãƒ«ã¯ `slurm.conf` ã¨ã„ã† slurm ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ Hyperpod ãŒè‡ªå‹•ç”Ÿæˆã™ã‚‹éš›ã«åˆ©ç”¨ã•ã‚Œã¾ã™ã€‚Quick Setup æ™‚ã«ã¯å‹æ‰‹ã«ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒä½œæˆã•ã‚Œã‚‹ãŸã‚æ„è­˜ã—ã¾ã›ã‚“ã§ã—ãŸãŒã€GPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’è¿½åŠ ã™ã‚‹å ´åˆã«ã¯**ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã™ã‚‹**å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ä»¥ä¸‹ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ GPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ã‚’è¨­å®šã«è¿½åŠ ã—ã€S3 ä¸Šã® json ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•æ›´æ–°ã§ãã¾ã™ã€‚

```bash
curl -sSL https://raw.githubusercontent.com/littlemex/samples/main/ml_distributed_experiment_collection/amazon-sagemaker-hyperpod-slurm-observability/update_provisioning_params.sh -o update_provisioning_params.sh
chmod +x update_provisioning_params.sh
```

ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã™ã‚‹å‰ã«ã€`BUCKET_NAME` ã‚’å®Ÿéš›ã® S3 ãƒã‚±ãƒƒãƒˆåã«å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚æœ¬ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã¨ç•°ãªã‚‹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—ã‚’æŒ‡å®šã™ã‚‹å ´åˆã‚‚æ‰‹å‹•ã§ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚

```bash
# ã‚¹ã‚¯ãƒªãƒ—ãƒˆå†…ã®ãƒã‚±ãƒƒãƒˆåã‚’æ›´æ–°ã—ã¦ã‹ã‚‰å®Ÿè¡Œ
sed -i 's/your-hyperpod-bucket-name/actual-bucket-name-here/' update_provisioning_params.sh
./update_provisioning_params.sh
```

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã«ã‚ˆã‚Šã€GPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—è¨­å®šãŒé©ç”¨ã•ã‚Œã¾ã™ã€‚
::::

::::details GPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®è¿½åŠ æ–¹æ³•

:::message alert
`ml.g5.xlarge for cluster usage` ãªã©ã® Service Quotas ã‚’ç¢ºèªã—ã¦å¿…è¦ã«å¿œã˜ã¦è¿½åŠ ãƒªã‚¯ã‚¨ã‚¹ãƒˆã—ã¾ã—ã‚‡ã†ã€‚
:::

å‰ç« ã§ä½œæˆã—ãŸ CPU ãƒ™ãƒ¼ã‚¹ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã« GPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’è¿½åŠ ã™ã‚‹å ´åˆã¯ã€ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ›´æ–°æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚æ—¢å­˜ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’å‰Šé™¤ã™ã‚‹ã“ã¨ãªãã€æ–°ã—ã„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ã‚’è¿½åŠ ã§ãã¾ã™ã€‚

![](/images/books/ml-distributed-experiment-collection/hyperpod-slurm-add-instance-group.png)

![](/images/books/ml-distributed-experiment-collection/hyperpod-slurm-add-gpu.png)

![](/images/books/ml-distributed-experiment-collection/hyperpod-slurm-updating-gpu.png)

SageMaker HyperPod ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ç®¡ç†ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã‹ã‚‰å¯¾è±¡ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’é¸æŠã—ã€ã€ŒEditã€ã‚’é¸æŠã—ã¾ã™ã€‚ã€ŒCreate instance groupã€ã‚’é¸æŠã—ã€ã€Œgpu-workerã€ã¨ã„ã†åå‰ã§ `ml.g5.xlarge` ã‚’ 2 å°è¿½åŠ ã—ã¾ã™ã€‚æ›´æ–°ã«ã¯ç´„ 10 æ•°åˆ†ã‹ã‹ã‚Šã€æ—¢å­˜ã® CPU ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹ã“ã¨ãªã GPU ãƒãƒ¼ãƒ‰ãŒè¿½åŠ ã•ã‚Œã¾ã™ã€‚GPU ãƒãƒ¼ãƒ‰ã§ã¯ CUDA ãƒ‰ãƒ©ã‚¤ãƒã¨ NCCL ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè‡ªå‹•çš„ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã€åˆ†æ•£å­¦ç¿’ã«å¿…è¦ãªç’°å¢ƒãŒæ•´å‚™ã•ã‚Œã¾ã™ã€‚
::::

::::details èª¿æŸ»ä¸­: ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³è‡ªå‹•ä½œæˆã®ä¸å…·åˆ

ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã¯ Slurm ã«ãŠã„ã¦**è¨ˆç®—ãƒãƒ¼ãƒ‰ã®è«–ç†çš„ãªã‚°ãƒ«ãƒ¼ãƒ—**ã§ã™ã€‚ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å†…ã®è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã‚’ç›®çš„ã‚„ç‰¹æ€§ã«å¿œã˜ã¦åˆ†é¡ãƒ»ç®¡ç†ã™ã‚‹ãŸã‚ã®ä»•çµ„ã¿ã§ã™ã€‚ç‰¹å®šã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®ã¿ã«ã‚¸ãƒ§ãƒ–ã‚’å‰²ã‚ŠæŒ¯ã‚ŠãŸã„ã‚ˆã†ãªã‚±ãƒ¼ã‚¹ã§åˆ©ç”¨ã§ãã¾ã™ã€‚

```
# ç‰¹å®šã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã§ã‚¸ãƒ§ãƒ–å®Ÿè¡Œ
sbatch --partition=gpu myjob.sh

# ã¾ãŸã¯ slurm ã‚¹ã‚¯ãƒªãƒ—ãƒˆå†…ã§æŒ‡å®š
#SBATCH --partition=gpu
```

:::message alert
`provisioning_parameters.json` ã§ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã¦ã‚‚ã€HyperPod Agent ãŒ GPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”¨ã®å°‚ç”¨ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ï¼ˆä¾‹ï¼š`ml.g5.xlarge`ï¼‰ã‚’ slurm.conf ã«åæ˜ ã—ãªã„å ´åˆãŒã‚ã‚Šã¾ã™ã€‚GPU ãƒãƒ¼ãƒ‰ã¯èªè­˜ã•ã‚Œã€Slurm ã§åˆ©ç”¨å¯èƒ½ã§ã™ãŒã€ã™ã¹ã¦ `dev` ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«é…ç½®ã•ã‚Œã‚‹çŠ¶æ³ãŒç™ºç”Ÿã—ã¾ã™ã€‚
:::

ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§æ‰‹å‹•ã§ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³è¨­å®šã‚’è¿½åŠ ã§ãã¾ã™ã€‚Nodes ã® IP ã‚¢ãƒ‰ãƒ¬ã‚¹ã¯ã”è‡ªèº«ã®ç’°å¢ƒã«åˆã‚ã›ã¦å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚ãŸã ã—æœ¬æ¥ã§ã‚ã‚Œã° HyperPod Agent ãŒè‡ªå‹•ç®¡ç†ã™ã‚‹ slurm.conf ã‚’ç›´æ¥ç·¨é›†ã™ã‚‹ã“ã¨ã®æ¨å¥¨åº¦ã¯ä¸æ˜ã§ã™ã€‚HyperPod Agent ã«ã‚ˆã‚‹è¨­å®šæ›´æ–°ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‚„é »åº¦ã¯ç¾åœ¨èª¿æŸ»ä¸­ã§ã™ã€‚è¨­å®šãŒã‚¯ãƒªã‚¢ã•ã‚Œã‚‹å¯èƒ½æ€§ã‚‚ã‚ã‚‹ãŸã‚è‡ªå·±åˆ¤æ–­ã§è¨­å®šã—ã€é‡è¦ãªãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã§ã¯ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‹ã‚‰ä½œã‚Šç›´ã™ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚

```bash
# GPU ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’æ‰‹å‹•è¿½åŠ 
echo "PartitionName=ml.g5.xlarge Nodes=ip-10-3-135-7,ip-10-3-96-49 Default=NO MaxTime=INFINITE State=UP" | sudo tee -a /opt/slurm/etc/slurm.conf
sudo scontrol reconfigure

# è¨­å®šç¢ºèª
sinfo
```

### ç¢ºèªã•ã‚ŒãŸå‹•ä½œä¾‹
```bash
# æ‰‹å‹•è¨­å®šå¾Œã® sinfo å‡ºåŠ›ä¾‹
sinfo
PARTITION     AVAIL  TIMELIMIT  NODES  STATE NODELIST
dev*             up   infinite      2  alloc ip-10-4-33-25,ip-10-4-198-29
dev*             up   infinite      2   idle ip-10-3-96-49,ip-10-3-135-7
ml.c5.4xlarge    up   infinite      2  alloc ip-10-4-33-25,ip-10-4-198-29
ml.g5.xlarge     up   infinite      2   idle ip-10-3-96-49,ip-10-3-135-7
```

æ‰‹å‹•è¨­å®šã«ã‚ˆã‚Šã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—ã«å¯¾å¿œã—ãŸå°‚ç”¨ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ãŒæ­£å¸¸ã«ä½œæˆã•ã‚Œã€é©åˆ‡ãªãƒãƒ¼ãƒ‰ãŒå‰²ã‚Šå½“ã¦ã‚‰ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ã„ã¾ã™ã€‚

ä»¥ä¸‹ã®ã‚ˆã†ã« g5 ã‚°ãƒ«ãƒ¼ãƒ—ã¯æ­£å¸¸ã« `nvidia-smi` ãŒå®Ÿè¡Œã§ãã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚

```bash
# CPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§ Driver ãŒãªã„ã®ã§ã‚³ãƒãƒ³ãƒ‰å¤±æ•—
srun --partition=ml.c5.4xlarge nvidia-smi
NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.

srun: error: ip-10-4-33-25: task 0: Exited with exit code 9

# GPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§ã‚³ãƒãƒ³ãƒ‰æˆåŠŸ
ubuntu@ip-10-4-109-244:~$ srun --partition=ml.g5.xlarge nvidia-smi
Mon Dec 22 17:29:56 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A10G                    On  |   00000000:00:1E.0 Off |                    0 |
|  0%   17C    P8             11W /  300W |       0MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
```
::::

## SageMaker Studio Integration ã®è¨­å®š

[Amazon SageMaker Studio](https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html) ã¯ã€æ©Ÿæ¢°å­¦ç¿’ã®çµ±åˆé–‹ç™ºç’°å¢ƒï¼ˆIDEï¼‰ã§ã™ã€‚Web ãƒ™ãƒ¼ã‚¹ã® ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã«ã‚ˆã‚Šã€ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã‚„ç ”ç©¶è€…ãŒå˜ä¸€ã®ç’°å¢ƒã§ ML ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«å…¨ä½“ã‚’ç®¡ç†ã§ãã¾ã™ã€‚æŸ”è»Ÿã«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—ã‚’é¸æŠã§ãã€FSx for Lustre ã®ãƒ•ã‚¡ã‚¤ãƒ«å…±æœ‰ã€MLflow çµ±åˆãªã©å¤šæ§˜ãªæ©Ÿèƒ½ã‚’æœ‰ã—ã¦ã„ã¾ã™ã€‚

::::details Studio è£œè¶³æƒ…å ±
Studio ã¨ HyperPod ã®çµ±åˆã«ã‚ˆã‚Šã€**é–‹ç™ºç’°å¢ƒã¨å®Ÿè¡Œç’°å¢ƒã®ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ãªé€£æº**ãŒå®Ÿç¾ã•ã‚Œã¾ã™ã€‚å…·ä½“çš„ã«ã¯ã€Studio ã® Code Editor ã§ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’é–‹ç™ºã—ã€åŒã˜ç’°å¢ƒã®ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‹ã‚‰ `sbatch` ã‚³ãƒãƒ³ãƒ‰ã§ HyperPod ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«ã‚¸ãƒ§ãƒ–ã‚’æŠ•å…¥ã§ãã¾ã™ã€‚FSx for Lustre ã®å…±æœ‰ã«ã‚ˆã‚Šã€Studio ã§ä½œæˆã—ãŸã‚³ãƒ¼ãƒ‰ã‚„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒ HyperPod ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å…¨ä½“ã§å³åº§ã«åˆ©ç”¨å¯èƒ½ã«ãªã‚Šã¾ã™ã€‚è‡ªèº«ã®ãƒ­ãƒ¼ã‚«ãƒ«ã®ã‚¨ãƒ‡ã‚£ã‚¿ã®æ–¹ãŒä½¿ã„ã‚„ã™ã„å ´åˆã¯ãƒ­ãƒ¼ã‚«ãƒ«ã‚¨ãƒ‡ã‚£ã‚¿ã‹ã‚‰ Studio ã® Code Editor ã«æ¥ç¶šã™ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã™ã€‚ãã—ã¦ [Presigned URL](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreatePresignedDomainUrl.html) ã§ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ãªã—ã§ CLI ã‹ã‚‰ Studio ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚

ãŸã ã—ã€Studio Code Editor ã§ã¯ Docker ãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ç„¡åŠ¹ã§ã‚ã‚Šã€[Docker in Docker ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-updated-local-get-started.html)ã¨ãªã£ã¦ã„ã‚‹ãŸã‚ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ¢ãƒ¼ãƒ‰ã‚„ namespace ã«åˆ¶ç´„ãŒã‹ã‹ã£ã¦ãŠã‚Šã‚³ãƒ³ãƒ†ãƒŠé–‹ç™ºã«ã¯ä¸å‘ãã§ã™ã€‚
::::

:::message
- [ ] 1. Studio Domain ã®ä½œæˆ
- [ ] 2. User Profile ã®ä½œæˆï¼ˆCLIï¼‰
- [ ] 3. SageMaker Studio ã®è¨­å®š
- [ ] 4. FSx for Lustre ã¨ã®çµ±åˆç¢ºèª
:::

::::details 1. Studio Domain ã®ä½œæˆ

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: awsome-distributed-training ãƒªãƒã‚¸ãƒˆãƒªã® CloudFormation ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¦ã€SageMaker Studio Domain ã¨ FSx for Lustre ã®å®Œå…¨çµ±åˆç’°å¢ƒã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: CloudFormation ã‚¹ã‚¿ãƒƒã‚¯ãŒ CREATE_COMPLETE çŠ¶æ…‹ã«ãªã‚Šã€Studio Domain ãŒä½œæˆã•ã‚Œã¦ã„ã‚‹ã“ã¨ã€‚
:::

[awsome-distributed-training ã® CloudFormation ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ](https://github.com/aws-samples/awsome-distributed-training/blob/main/1.architectures/5.sagemaker-hyperpod/slurm-studio/studio-slurm.yaml)ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€FSx for Lustre çµ±åˆã€Slurm ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®šã®è‡ªå‹•åŒ–ãŒå®Ÿç¾ã•ã‚Œã¾ã™ã€‚

## è‡ªå‹•åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆæ¨å¥¨æ–¹æ³•ï¼‰

æ—¢å­˜ã® HyperPod ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æƒ…å ±ã‹ã‚‰å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è‡ªå‹•å–å¾—ã—ã€CloudFormation ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’æä¾›ã—ã¾ã™ï¼š

```bash
curl -sSL https://raw.githubusercontent.com/littlemex/samples/main/ml_distributed_experiment_collection/amazon-sagemaker-hyperpod-slurm-observability/create_studio_domain.sh -o create_studio_domain.sh
chmod +x create_studio_domain.sh

# ã”è‡ªèº«ã®ç’°å¢ƒã«åˆã‚ã›ã¦å¤‰æ›´ã—ã¦ãã ã•ã„
# sed -i 's/cpu-slurm-cluster/your-actual-cluster-name/' create_studio_domain.sh
# sed -i 's/us-east-1/your-region/' create_studio_domain.sh

# ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ
./create_studio_domain.sh
```

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã«ã‚ˆã‚Šã€FSx for Lustre ã®å®Œå…¨çµ±åˆã€Slurm ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®è‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®šã®è‡ªå‹•åŒ–ãŒå®Ÿç¾ã•ã‚Œã¾ã™ã€‚ä»¥ä¸‹ã®ã‚ˆã†ã« SageMaker AI ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã§æ­£å¸¸ã«ä½œæˆã•ã‚ŒãŸã“ã¨ãŒç¢ºèªã§ãã¾ã™ã€‚

![](/images/books/ml-distributed-experiment-collection/hyperpod-slurm-create-domain.png)
::::

::::details 2. User Profile ã®ä½œæˆï¼ˆCLIï¼‰

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: Studio Domain ä½œæˆå¾Œã« User Profile ã‚’è¿½åŠ ã—ã€FSx ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®è‡ªå‹•ä½œæˆã¨ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™è¨­å®šã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: User Profile ãŒ InService çŠ¶æ…‹ã«ãªã‚Šã€FSx ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ãŒè‡ªå‹•ä½œæˆã•ã‚Œã¦ã„ã‚‹ã“ã¨ã€‚
:::

CloudFormation ã«ã‚ˆã‚‹ Domain ä½œæˆãŒå®Œäº†ã—ãŸã‚‰ã€User Profile ã‚’ä½œæˆã—ã¾ã™ã€‚FSx for Lustre ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã¯ `/fsx/shared/` ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’é€šã˜ã¦ HyperPod ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã¨ Studio ç’°å¢ƒé–“ã§å…±æœ‰ã•ã‚Œã¾ã™ã€‚

## User Profile ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```bash
# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦å†…å®¹ç¢ºèª
curl -sSL https://raw.githubusercontent.com/littlemex/samples/main/ml_distributed_experiment_collection/amazon-sagemaker-hyperpod-slurm-observability/create_user_profile.sh -o create_user_profile.sh

# ã‚¹ã‚¯ãƒªãƒ—ãƒˆå†…å®¹ã®ç¢ºèª
head -20 create_user_profile.sh

chmod +x create_user_profile.sh
```

## å®Ÿè¡Œæ‰‹é †

```bash
# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã§ä½œæˆ
./create_user_profile.sh

# ã¾ãŸã¯ç‰¹å®šã®ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’æŒ‡å®š
./create_user_profile.sh data-scientist-1
```

User Profile ä½œæˆå¾Œã€SageMaker ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã‹ã‚‰è©²å½“ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã§ãƒ­ã‚°ã‚¤ãƒ³ã—ã€JupyterLab ã‚’èµ·å‹•ã§ãã¾ã™ã€‚

![](/images/books/ml-distributed-experiment-collection/hyperpod-slurm-create-user-profile.png)
::::

::::details 3. SageMaker Studio ã®è¨­å®š

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: ä½œæˆã—ãŸ Studio ç’°å¢ƒã§ Slurm ã‚³ãƒãƒ³ãƒ‰ã€HyperPod ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã¨ã®é€£æºãŒæ­£å¸¸ã«å‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: Studio å†…ã‹ã‚‰ FSx ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã€Slurm ã‚³ãƒãƒ³ãƒ‰ãŒå®Ÿè¡Œã§ãã‚‹ã“ã¨ã€‚
:::

## Code Editor Space ã®ä½œæˆ

:::message alert
User Profile ã‚’ä½œæˆã—ãŸã ã‘ã§ã¯ FSx ã‚„ Slurm ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“ã€‚ä»¥ä¸‹ã®æ‰‹é †ã§ Code Editor Space ã‚’æ‰‹å‹•ä½œæˆã—ã€FSx ã¨ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«è¨­å®šã‚’ã‚¢ã‚¿ãƒƒãƒã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
:::

1. **SageMaker Studio ã«ã‚¢ã‚¯ã‚»ã‚¹**
   - User Profile ã‹ã‚‰ Studio ã«ãƒ­ã‚°ã‚¤ãƒ³

2. **Code Editor Space ã®ä½œæˆ**
   - "Code Editor" ã‚’ã‚¯ãƒªãƒƒã‚¯
   - "Create Code Editor Space" ã‚’ã‚¯ãƒªãƒƒã‚¯
   - Space åã‚’å…¥åŠ›ï¼ˆä¾‹ï¼š`hyperpod-workspace`ï¼‰

3. **é‡è¦ï¼šFSx ã¨ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«è¨­å®šã®ã‚¢ã‚¿ãƒƒãƒ**
   - "Attach custom filesystem - optional" ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ã‹ã‚‰ **FSx for Lustre volume ã‚’é¸æŠ**
   - "Lifecycle configuration" ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ã‹ã‚‰ **åˆ©ç”¨å¯èƒ½ãªãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«è¨­å®šã‚’é¸æŠ**
   - "Run Space" ã‚’ã‚¯ãƒªãƒƒã‚¯

4. **Space ã®èµ·å‹•**
   - Code Editor ãŒèµ·å‹•ã™ã‚‹ã¾ã§æ•°åˆ†å¾…æ©Ÿ
   - æ­£å¸¸ã«ä½œæˆã•ã‚ŒãŸã‚‰ "Open Code Editor" ã§ Editor ã‚’èµ·å‹•

![](/images/books/ml-distributed-experiment-collection/hyperpod-slurm-studio-create-space.png)

## Slurm ã‚³ãƒãƒ³ãƒ‰ã®å®Ÿè¡Œæ–¹æ³•

Studio Code Editor ã§ã¯ã€**2 ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**ã§ Slurm ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚

### æ–¹æ³•1: Login ãƒãƒ¼ãƒ‰çµŒç”±ã®SSH

```bash
# GitHub ã‹ã‚‰ easy-ssh.sh ã‚’å–å¾—
curl -O https://raw.githubusercontent.com/aws-samples/awsome-distributed-training/main/1.architectures/5.sagemaker-hyperpod/easy-ssh.sh
chmod +x easy-ssh.sh

# SSH Key ç”Ÿæˆï¼ˆæœªä½œæˆã®å ´åˆï¼‰
ssh-keygen -t rsa -b 4096 -f "$HOME/.ssh/id_rsa" -N ""

# Login ãƒãƒ¼ãƒ‰ã«æ¥ç¶šè¨­å®š
./easy-ssh.sh -c login cpu-slurm-cluster

# SSH çµŒç”±ã§ã® Slurm æ“ä½œ
ssh cpu-slurm-cluster sinfo
```

### æ–¹æ³•2: Studio Code Editor å†…ã§ã®ç›´æ¥å®Ÿè¡Œ

:::message alert
**åˆ¶ç´„äº‹é …**: Studio Code Editor ã¯ã‚³ãƒ³ãƒ†ãƒŠç’°å¢ƒã§å‹•ä½œã—ã€MUNGE èªè¨¼ã«èª²é¡ŒãŒã‚ã‚Šã¾ã™ã€‚ä»¥ä¸‹ã®æ‰‹é †ã¯æŠ€è¡“æ¤œè¨¼ç”¨ã§ã‚ã‚Šã€æœ¬ç•ªåˆ©ç”¨ã«ã¯æ–¹æ³• 1 ã‚’æ¨å¥¨ã—ã¾ã™ã€‚
:::

[MUNGE (MUNGE Uid 'N' Gid Emporium)](https://dun.github.io/munge/) ã¯ã€Slurm ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«ãŠã‘ã‚‹èªè¨¼ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å†…ã®å…¨ãƒãƒ¼ãƒ‰é–“ã§å®‰å…¨ãªé€šä¿¡ã‚’ç¢ºä¿ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã‚¸ãƒ§ãƒ–ã‚’æŠ•å…¥ã™ã‚‹éš›ã®èªè¨¼ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚MUNGE ã®å‹•ä½œã«ã¯ã€ã™ã¹ã¦ã®ãƒãƒ¼ãƒ‰ã§åŒã˜ç§˜å¯†éµï¼ˆMUNGE ã‚­ãƒ¼ï¼‰ã‚’å…±æœ‰ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã€ã“ã®éµã®åŒæœŸãŒ Slurm ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œã®å‰ææ¡ä»¶ã¨ãªã‚Šã¾ã™ã€‚

Studio Code Editor å†…ã§ã® Slurm ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆç›´æ¥å®Ÿè¡Œã«ã¯ã€HyperPod ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‹ã‚‰ MUNGE ã‚­ãƒ¼ã‚’å–å¾—ã—ã€Studio ç’°å¢ƒã§ MUNGE ãƒ‡ãƒ¼ãƒ¢ãƒ³ã‚’èµ·å‹•ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

```bash
curl -sSL https://raw.githubusercontent.com/littlemex/samples/main/ml_distributed_experiment_collection/amazon-sagemaker-hyperpod-slurm-observability/setup_slurm_munge_studio.sh -o setup_slurm_munge_studio.sh
chmod +x setup_slurm_munge_studio.sh

./setup_slurm_munge_studio.sh

# slurm ã‚³ãƒãƒ³ãƒ‰ã®å‹•ä½œç¢ºèª
sinfo
```
::::

::::details 4. FSx for Lustre ã¨ã®çµ±åˆç¢ºèª

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: Studio ç’°å¢ƒã‹ã‚‰ FSx for Lustre ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’ç¢ºèªã—ã€å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚„å­¦ç¿’çµæœã®åŠ¹ç‡çš„ãªç®¡ç†æ–¹æ³•ã‚’ç¿’å¾—ã—ã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: Studio ã‹ã‚‰ FSx ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã€ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿æ›¸ãã¨å…±æœ‰ãŒæ­£å¸¸ã«å‹•ä½œã™ã‚‹ã“ã¨ã€‚
:::

FSx for Lustre ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã¯ã€Slurm ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å†…ã®å…¨ãƒãƒ¼ãƒ‰ã§ `/fsx` ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨ã—ã¦ãƒã‚¦ãƒ³ãƒˆã—ã¾ã—ãŸã€‚User Profile ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã®è¨­å®šã«ã‚ˆã£ã¦ Studio ç’°å¢ƒã‹ã‚‰ã‚‚å…±æœ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã§ã™ã€‚Slurm Login ãƒãƒ¼ãƒ‰ã«æ¥ç¶šã—ã¦ `/fsx` ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æ¨©é™è¨­å®šã‚’è¡Œã„ã¾ã—ã‚‡ã†ã€‚ã“ã‚Œã«ã‚ˆã£ã¦ Studio å´ã‹ã‚‰ã‚‚æ¨©é™ãŒè¨±å¯ã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã§ã™ã€‚

```bash
# HyperPod ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã« SSH æ¥ç¶š
ssh cpu-slurm-cluster

# FSx ã®æ¨©é™ç¢ºèª
ls -la /fsx

# Studio ãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘å…±æœ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆã¨æ¨©é™è¨­å®š
sudo mkdir -p /fsx/shared/studio-workspace
sudo chmod 755 /fsx/shared/studio-workspace 
sudo chown 10001:1001 /fsx/shared/studio-workspace

# ç¢ºèª
ls -la /fsx/shared
```

æ¨©é™è¨­å®šå¾Œã€Studio Code Editor ã‹ã‚‰å†åº¦ã‚¢ã‚¯ã‚»ã‚¹ã‚’è©¦è¡Œã—ã¾ã™ã€‚

```bash
# Studio ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‹ã‚‰å†è©¦è¡Œ
FSX_MOUNT=$(df -h | grep fsx_lustre | awk '{print $NF}')
touch $FSX_MOUNT/studio-workspace/testfile && ls -la $FSX_MOUNT/studio-workspace
```
::::

## Observability ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰

:::message
**æ›´æ–°ã•ã‚ŒãŸæ¨å¥¨æ‰‹é †**
- [ ] 1. Open Source Grafana + Amazon Managed Prometheus ç’°å¢ƒã®ãƒ‡ãƒ—ãƒ­ã‚¤
- [ ] 2. Grafana ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹
- [ ] 3. Grafana ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®è¨­å®š
- [ ] 4. HyperPod ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã¸ã® Observability ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
- [ ] 5. ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
:::

HyperPod Slurm Observability ã¯ã€Amazon Managed Service for Prometheus ã¨ Grafana ã‚’çµ„ã¿åˆã‚ã›ã¦å®Ÿç¾ã—ã¾ã™ã€‚

![](https://awslabs.github.io/ai-on-sagemaker-hyperpod/assets/images/observability_architecture-1f511d1934afb3d2ebf7c89c41a31a17.png)

:::message alert
**é‡è¦**: Amazon Managed Grafana ã¯ AWS Organizations ã®**ç®¡ç†ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ**ã§ã®ã¿å®Œå…¨ã«åˆ©ç”¨ã§ãã¾ã™ã€‚ãƒ¡ãƒ³ãƒãƒ¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã§ã¯ IAM Identity Center ã®åˆ¶é™ã«ã‚ˆã‚Š Amazon Managed Grafana ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã®ä½œæˆãŒã§ãã¾ã›ã‚“ã€‚
:::

ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—ç”¨ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãªã© Organization ã®ç®¡ç†ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãŒåˆ©ç”¨ã§ããªã„ã‚±ãƒ¼ã‚¹ã‚’æƒ³å®šã—ã¦æœ¬ç« ã§ã¯ OSS ãƒ™ãƒ¼ã‚¹ã® Grafana ã‚’ EC2 ã«ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã¦åˆ©ç”¨ã—ã¾ã™ã€‚ç®¡ç†ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’åˆ©ç”¨ã§ãã‚‹ã‚±ãƒ¼ã‚¹ã§ã¯[ã“ã¡ã‚‰](https://awslabs.github.io/ai-on-sagemaker-hyperpod/docs/add-ons/Observability/observability-slurm)ã®æ‰‹é †ã§ãƒ•ãƒ«ãƒãƒãƒ¼ã‚¸ãƒ‰ã® Grafana + Prometheus æ§‹æˆã‚’å®Ÿç¾ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚æœ¬ç•ªç’°å¢ƒã§ã¯ã“ã¡ã‚‰ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚

::::details  1. Open Source Grafana + Amazon Managed Prometheus ç’°å¢ƒã®ãƒ‡ãƒ—ãƒ­ã‚¤

CloudFormation ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¦ OSS Grafana ã¨ Amazon Managed Prometheus ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚

:::message
EC2 ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚°ãƒ«ãƒ¼ãƒ—ã® inbound ã‚¢ã‚¯ã‚»ã‚¹åˆ¶é™ã‚’ã‹ã‘ã‚‹ãŸã‚ã«è¨±å¯ã™ã‚‹ IP ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
:::

```bash
# ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ä¾å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
curl -sSL https://raw.githubusercontent.com/littlemex/samples/main/ml_distributed_experiment_collection/amazon-sagemaker-hyperpod-slurm-observability/setup_hyperpod_observability_oss.sh -o setup_hyperpod_observability_oss.sh
curl -sSL https://raw.githubusercontent.com/littlemex/samples/main/ml_distributed_experiment_collection/amazon-sagemaker-hyperpod-slurm-observability/ecr-policy.json -o ecr-policy.json
curl -sSL https://raw.githubusercontent.com/littlemex/samples/main/ml_distributed_experiment_collection/amazon-sagemaker-hyperpod-slurm-observability/cluster-observability-with-os-grafana.yaml -o cluster-observability-with-os-grafana.yaml

# å®Ÿè¡Œæ¨©é™ã‚’ä»˜ä¸
chmod +x setup_hyperpod_observability_oss.sh

# åŸºæœ¬å®Ÿè¡Œï¼ˆIP ã‚¢ãƒ‰ãƒ¬ã‚¹è‡ªå‹•æ¤œå‡ºï¼‰
./setup_hyperpod_observability_oss.sh

# ã¾ãŸã¯ç‰¹å®šã® IP ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’æŒ‡å®š
./setup_hyperpod_observability_oss.sh --ip 192.168.1.100
```

ä½œæˆã®å®Œäº†ã‚’å¾…ã£ã¦ Grafana ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ URL ã‚’ç¢ºèªã—ã¾ã—ã‚‡ã†ã€‚

```bash
# ä½œæˆçŠ¶æ³ã®ç¢ºèª
./setup_hyperpod_observability_oss.sh --check-status
==============================================
HyperPod Slurm Open Source Grafana Setup
==============================================
....
ğŸŒ Grafana Access Information:
   URL: http://XXX:3000
   Default Login: admin/admin
   Security: Access restricted to XXX/32

ğŸ”— Direct Browser Access:
   Copy this URL to your browser: http://XXX:3000

ğŸ“‹ Next Steps:
   1. Access Grafana and change default password
   2. Configure Prometheus data source
   3. Import observability dashboards

===============================================
```

## è£œè¶³: SigV4 èªè¨¼ã®è¨­å®š

ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ã¯ã€Grafana ã® SigV4 èªè¨¼ãŒäº‹å‰è¨­å®šã•ã‚Œã¾ã™ã€‚

```bash
# CloudFormation UserData ã§è¨­å®šã•ã‚Œã‚‹ç’°å¢ƒå¤‰æ•°
export AWS_SDK_LOAD_CONFIG=true
export GF_AUTH_SIGV4_AUTH_ENABLED=true

# Grafana è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§ã® SigV4 æœ‰åŠ¹åŒ–
[auth.sigv4]
enabled = true
verbose_logging = false
```
::::

::::details 2. Grafana ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹

![](/images/books/ml-distributed-experiment-collection/grafana-login.png)

- [ ] ãƒ–ãƒ©ã‚¦ã‚¶ã§å–å¾—ã—ãŸ URL ã«ã‚¢ã‚¯ã‚»ã‚¹
- [ ] ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆèªè¨¼æƒ…å ±ã§ãƒ­ã‚°ã‚¤ãƒ³ï¼š`admin / admin`
- [ ] Grafana ãƒ›ãƒ¼ãƒ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª

![](/images/books/ml-distributed-experiment-collection/grafana-home.png)
::::

::::details  3. Grafana ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®è¨­å®š

:::message alert
**é‡è¦**: Grafana ã®é€šå¸¸ã® Prometheus ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã® SigV4 èªè¨¼ã¯**éæ¨å¥¨**ã¨ãªã‚Šã¾ã—ãŸã€‚Amazon Managed Service for Prometheus å°‚ç”¨ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
:::

Grafana ã¨ Amazon Managed Prometheus workspace ã‚’æ¥ç¶šã™ã‚‹ãŸã‚ã® Prometheus ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ URL ã‚’å–å¾—ã—ã¾ã—ã‚‡ã†ã€‚

```bash
# Prometheus ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ URL ã‚’å–å¾—ï¼ˆqueryç”¨URLï¼‰
aws cloudformation describe-stacks \
  --stack-name HyperpodSlurmOSObservability \
  --region us-east-1 \
  --query 'Stacks[0].Outputs[?OutputKey==`PrometheusQueryURL`].OutputValue' \
  --output text

# å‡ºåŠ›ä¾‹: https://aps-workspaces.us-east-1.amazonaws.com/workspaces/ws-97d9f0f9-95bb-4779-9908-d42c2cd2a288/
```

![](/images/books/ml-distributed-experiment-collection/grafana-data-source.png)

- [ ] 1. **Connections > Data Sources** ã‚’é¸æŠ
- [ ] 2. **Add data source** ã‚’ã‚¯ãƒªãƒƒã‚¯

![](/images/books/ml-distributed-experiment-collection/grafana-add-data-source.png)

- [ ] 3. âš ï¸ **Amazon Managed Service for Prometheus** ã‚’é¸æŠï¼ˆé€šå¸¸ã® Prometheus ã§ã¯ãªã„ï¼‰

![](/images/books/ml-distributed-experiment-collection/grafana-add-data-source-connection.png)
![](/images/books/ml-distributed-experiment-collection/grafana-add-data-source-auth.png)
![](/images/books/ml-distributed-experiment-collection/grafana-add-data-source-save.png)

- [ ] 4. **ä»¥ä¸‹ã®è¨­å®šã‚’å…¥åŠ›**ï¼š
   - [ ] **Prometheus server URL**: ä¸Šè¨˜ã§å–å¾—ã—ãŸ URL
   - [ ] **Authentication Provider**: `AWS SDK Default` ã‚’é¸æŠ
   - [ ] **Default Region**: `us-east-1` ã‚’å…¥åŠ›
- [ ] 5. **Save & test** ã§ãƒ†ã‚¹ãƒˆæˆåŠŸã‚’ç¢ºèª
::::

::::details 4. HyperPod ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã¸ã® Observability ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: HyperPod ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®å…¨ãƒãƒ¼ãƒ‰ã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã€Amazon Managed Prometheus ã¸ã®é€ä¿¡ã‚’é–‹å§‹ã—ã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: å…¨ãƒãƒ¼ãƒ‰ã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¿ãƒ¼ï¼ˆNode Exporterã€DCGM Exporterã€Slurm Exporterï¼‰ãŒç¨¼åƒã—ã€Prometheus ã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒé€ä¿¡ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã€‚
:::

:::message alert
**OSS ç‰ˆã§ã®ç°¡ç•¥åŒ–**: `setup_hyperpod_observability_oss.sh` ã‚¹ã‚¯ãƒªãƒ—ãƒˆã«ã‚ˆã‚Šã€IAM æ¨©é™ã®è¿½åŠ ã¨ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æ›´æ–°ã¯è‡ªå‹•å®Ÿè¡Œæ¸ˆã¿ã§ã™ã€‚æ‰‹å‹•ã§ã®è¨­å®šã¯ä¸è¦ã§ã™ã€‚
:::

## HyperPod ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã§ã®æ‰‹å‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

æ—¢å­˜ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã« Observability ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’è¿½åŠ ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å ´åˆã®æ‰‹é †ã§ã™ã€‚

```bash
# 1. HyperPod ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ãƒ˜ãƒƒãƒ‰ãƒãƒ¼ãƒ‰ã«ã‚¢ã‚¯ã‚»ã‚¹
ssh cpu-slurm-cluster

# 2. ç’°å¢ƒå¤‰æ•°ã®è¨­å®šï¼ˆOSSç‰ˆCloudFormationã‚¹ã‚¿ãƒƒã‚¯ã‹ã‚‰å–å¾—ï¼‰
export NUM_WORKERS=2  # å®Ÿéš›ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰æ•°ã«èª¿æ•´

# OSSç‰ˆã‚¹ã‚¿ãƒƒã‚¯ã‹ã‚‰Prometheus remote write URLã‚’å–å¾—
export PROMETHEUS_REMOTE_WRITE_URL=$(aws cloudformation describe-stacks \
  --stack-name HyperpodSlurmOSObservability \
  --region us-east-1 \
  --query 'Stacks[0].Outputs[?OutputKey==`PrometheusRemoteWriteURL`].OutputValue' \
  --output text)

# é«˜åº¦ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã‚’æœ‰åŠ¹åŒ–
export ARG_ADVANCED=--advanced

# è¨­å®šç¢ºèª
echo "Worker nodes: $NUM_WORKERS"
echo "Prometheus URL: $PROMETHEUS_REMOTE_WRITE_URL"
```

## Observability ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æº–å‚™

```bash
# 3. å…±æœ‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ä¸Šã«ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³
mkdir -p ~/observability-setup
cd ~/observability-setup
git clone https://github.com/aws-samples/awsome-distributed-training.git
cd awsome-distributed-training/1.architectures/5.sagemaker-hyperpod/LifecycleScripts/base-config/observability

# 4. sudo æ¨©é™ã®ç¢ºèª
sudo hostname
srun -N $NUM_WORKERS sudo hostname
```

## æ—¢å­˜ã‚µãƒ¼ãƒ“ã‚¹ã®åœæ­¢ã¨æ–°è¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
# 5. æ—¢å­˜ã® Observability ã‚µãƒ¼ãƒ“ã‚¹ã‚’åœæ­¢ã€åˆå›ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ™‚ã«ã¯ã‚¨ãƒ©ãƒ¼ã™ã‚‹ãŒå•é¡Œãªã—
sudo python3 stop_observability.py --node-type controller || true
srun -N $NUM_WORKERS sudo python3 stop_observability.py --node-type compute || true

# 6. Controller ãƒãƒ¼ãƒ‰ï¼ˆãƒ˜ãƒƒãƒ‰ãƒãƒ¼ãƒ‰ï¼‰ã« Observability ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
sudo python3 install_observability.py \
  --node-type controller \
  --prometheus-remote-write-url $PROMETHEUS_REMOTE_WRITE_URL \
  $ARG_ADVANCED

# 7. å…¨ Worker ãƒãƒ¼ãƒ‰ã« Observability ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
srun -N $NUM_WORKERS sudo python3 install_observability.py \
  --node-type compute \
  --prometheus-remote-write-url $PROMETHEUS_REMOTE_WRITE_URL \
  $ARG_ADVANCED
```

## ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª

```bash
# Controller ãƒãƒ¼ãƒ‰ã§ã®ç¢ºèªï¼ˆDocker ã‚³ãƒ³ãƒ†ãƒŠæ–¹å¼ï¼‰
docker ps

# Worker ãƒãƒ¼ãƒ‰ã§ã®ç¢ºèª
srun -N $NUM_WORKERS docker ps
```

**æˆåŠŸæ™‚ã®å‡ºåŠ›ä¾‹**

**Controller ãƒãƒ¼ãƒ‰ï¼ˆãƒ˜ãƒƒãƒ‰ãƒãƒ¼ãƒ‰ï¼‰**:
```
CONTAINER ID   IMAGE                                                                                 COMMAND                  CREATED              STATUS              PORTS     NAMES
a0e5f004b876   602401143452.dkr.ecr.us-east-1.amazonaws.com/hyperpod/otel_collector:v1754424030352   "/app/otelcollector â€¦"   About a minute ago   Up About a minute             otel-collector
ef0511862528   602401143452.dkr.ecr.us-east-1.amazonaws.com/hyperpod/node_exporter:v1.9.1            "/bin/node_exporter â€¦"   About a minute ago   Up About a minute             node-exporter
```

**Worker ãƒãƒ¼ãƒ‰**:
```
CONTAINER ID   IMAGE                                                                                  COMMAND                  CREATED          STATUS          PORTS     NAMES
5fc5d5ca2167   602401143452.dkr.ecr.us-east-1.amazonaws.com/hyperpod/otel_collector:v1754424030352    "/app/otelcollector â€¦"   36 seconds ago   Up 36 seconds             otel-collector
05d54ddc42ff   602401143452.dkr.ecr.us-east-1.amazonaws.com/hyperpod/efa_exporter:1.0.0               "./node_exporter --pâ€¦"   40 seconds ago   Up 39 seconds             efa-exporter
9ef32f2ec5c3   602401143452.dkr.ecr.us-east-1.amazonaws.com/hyperpod/dcgm_exporter:4.1.1-4.0.4-ubi9   "/usr/local/dcgm/dcgâ€¦"   45 seconds ago   Up 41 seconds             dcgm-exporter
f685adb74e4e   602401143452.dkr.ecr.us-west-2.amazonaws.com/hyperpod/node_exporter:v1.9.1             "/bin/node_exporter â€¦"   53 seconds ago   Up 53 seconds             node-exporter
```

:::message
**Slurm Exporter ã«ã¤ã„ã¦**: æœ€æ–°ã®å®Ÿè£…ã§ã¯ã€Slurm Exporter ã¯ systemd ã‚µãƒ¼ãƒ“ã‚¹ã§ã¯ãªã Docker ã‚³ãƒ³ãƒ†ãƒŠã¾ãŸã¯åˆ¥ã®æ–¹å¼ã§å‹•ä½œã™ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®é€ä¿¡ãŒæ­£å¸¸ã«è¡Œã‚ã‚Œã¦ã„ã‚‹ã“ã¨ã¯ `docker ps` å‡ºåŠ›ã§ `otel_collector` ã‚³ãƒ³ãƒ†ãƒŠãŒ Up çŠ¶æ…‹ã§ã‚ã‚‹ã“ã¨ã§ç¢ºèªã§ãã¾ã™ã€‚
:::

:::message
**è‡ªå‹•åŒ–ã«ã‚ˆã‚‹ç°¡ç•¥åŒ–**: `setup_hyperpod_observability_oss.sh` ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã®å‡¦ç†ã‚’è‡ªå‹•å®Ÿè¡Œã—ã¾ã™ã€‚
- IAM æ¨©é™ã®è‡ªå‹•è¿½åŠ ï¼ˆAmazonPrometheusRemoteWriteAccessã€ECR ã‚¢ã‚¯ã‚»ã‚¹ï¼‰
- ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆè‡ªå‹•æ›´æ–°ï¼ˆenable_observability=Trueã€Prometheus URL è¨­å®šï¼‰
- S3 ã¸ã®è‡ªå‹•ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
:::

## Slurm Exporter å‹•ä½œç¢ºèª

:::message alert
**é‡è¦**: ä¸Šè¨˜ã®æ‰‹é †ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†å¾Œã€Slurm Exporter ãŒæ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã‚‹ã“ã¨ã‚’å¿…ãšç¢ºèªã—ã¦ãã ã•ã„ã€‚Slurm Exporter ãŒå‹•ä½œã—ã¦ã„ãªã„å ´åˆã€Grafana ã® Slurm ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ "No data" ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚
:::

**ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«å®Œäº†å¾Œã®ç¢ºèªæ‰‹é †**ï¼š
```bash
# Slurm Exporter ã‚µãƒ¼ãƒ“ã‚¹çŠ¶æ…‹ç¢ºèª
ssh cpu-slurm-cluster "systemctl status slurm_exporter.service"

# æœŸå¾…ã•ã‚Œã‚‹çŠ¶æ…‹
â— slurm_exporter.service - Prometheus SLURM Exporter
     Loaded: loaded (/etc/systemd/system/slurm_exporter.service; enabled)
     Active: active (running)
```

**Slurm Exporter ã‚µãƒ¼ãƒ“ã‚¹ãŒå­˜åœ¨ã—ãªã„å ´åˆã®å¯¾å‡¦æ³•**ï¼š

:::message
**åŸå› **: Controller ãƒãƒ¼ãƒ‰å‘ã‘ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ï¼ˆã‚¹ãƒ†ãƒƒãƒ— 6ï¼‰ãŒä½•ã‚‰ã‹ã®ç†ç”±ã§ä¸å®Œå…¨ã ã£ãŸå ´åˆã€Slurm Exporter ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚
:::

```bash
# Slurm Exporter ãƒã‚¤ãƒŠãƒªã®å­˜åœ¨ç¢ºèª
ssh cpu-slurm-cluster "which slurm_exporter"

# å­˜åœ¨ã—ãªã„å ´åˆã€Controller ãƒãƒ¼ãƒ‰å‘ã‘ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’å†å®Ÿè¡Œ
ssh cpu-slurm-cluster "cd ~/observability-setup/awsome-distributed-training/1.architectures/5.sagemaker-hyperpod/LifecycleScripts/base-config/observability && export PROMETHEUS_REMOTE_WRITE_URL=\$(aws cloudformation describe-stacks --stack-name HyperpodSlurmOSObservability --region us-east-1 --query 'Stacks[0].Outputs[?OutputKey==\`PrometheusRemoteWriteURL\`].OutputValue' --output text) && sudo python3 install_observability.py --node-type controller --prometheus-remote-write-url \$PROMETHEUS_REMOTE_WRITE_URL --advanced"
```

**Slurm ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ç”Ÿæˆç¢ºèª**ï¼š
```bash
# ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ç¢ºèª
ssh cpu-slurm-cluster "curl -s http://localhost:9341/metrics | grep -E '^slurm_' | head -10"

# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›ä¾‹ï¼ˆè±Šå¯Œãªã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å›ºæœ‰ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼‰
slurm_cpus_idle 40
slurm_cpus_total 40  
slurm_gpus_idle 2
slurm_gpus_total 2
slurm_nodes_total 4
slurm_partition_cpus_idle{partition="dev"} 40
slurm_node_status{node="ip-10-4-33-25",partition="dev*",status="idle"} 1
slurm_rpc_stats{operation="REQUEST_NODE_INFO"} 26426
```
::::

::::details 5. ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

Prometheus ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¨­å®šå®Œäº†å¾Œã€ä»¥ä¸‹ã®å…¬å¼ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚

:::message
FSx ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ã«ã¯è¿½åŠ ã§ Amazon CloudWatch ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®è¨­å®šãŒå¿…è¦ã§ã™ã€‚ä»Šå›ã¯å‰²æ„›ã—ã¾ã™ã€‚
:::

![](/images/books/ml-distributed-experiment-collection/grafana-dashboard-new.png)

- [ ] 1. **Dashboards > New > Import** ã‚’é¸æŠ

![](/images/books/ml-distributed-experiment-collection/grafana-dashboard-import.png)

- [ ] 2. ä»¥ä¸‹ã® URL ã‚’é †æ¬¡ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
  - [ ] **Slurm Exporter Dashboard**:
    https://grafana.com/grafana/dashboards/4323-slurm-dashboard/
  - [ ] **Node Exporter Dashboard**:
    https://grafana.com/grafana/dashboards/1860-node-exporter-full/
  - [ ] **DCGM Exporter Dashboard**ï¼ˆGPU ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼‰:
    https://grafana.com/grafana/dashboards/12239-nvidia-dcgm-exporter-dashboard/
  - [ ] **FSx for Lustre Dashboard**:
    https://grafana.com/grafana/dashboards/20906-fsx/

![](/images/books/ml-distributed-experiment-collection/grafana-dashboard-import-2.png)
![](/images/books/ml-distributed-experiment-collection/grafana-dashboard-node.png)
::::

## ã¾ã¨ã‚

æœ¬ç« ã§ã¯ã€SageMaker Studio çµ±åˆã¨ Amazon SageMaker HyperPod ã® Slurm ç’°å¢ƒã«ãŠã‘ã‚‹ observability æ©Ÿèƒ½ã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚

Amazon Managed Prometheus ã¨ Grafana ã‚’ç”¨ã„ãŸ**çµ±åˆãƒ¡ãƒˆãƒªã‚¯ã‚¹**ã«ã‚ˆã‚Šã€éšœå®³ã®äºˆå…†æ¤œå‡ºã‹ã‚‰å¾©æ—§ãƒ—ãƒ­ã‚»ã‚¹ã®å¯è¦–åŒ–ã¾ã§ã€åŒ…æ‹¬çš„ãª observability ãŒå®Ÿç¾ã•ã‚Œã¾ã™ã€‚æ¬¡å›ã¯ç¶šãã¨ã—ã¦ä»Šå›å°å…¥ã—ãŸ observability æ©Ÿèƒ½ã‚’æ´»ç”¨ã—ã¦ãƒ¬ã‚¸ãƒªã‚¨ãƒ³ã‚·ãƒ¼ã®ç¢ºèªã‚’ã—ã¦ã„ãã¾ã—ã‚‡ã†ã€‚
