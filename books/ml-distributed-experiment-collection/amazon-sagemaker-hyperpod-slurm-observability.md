---
title: "Blueprints by Slurm: ãƒ¬ã‚¸ãƒªã‚¨ãƒ³ã‚·ãƒ¼ã¨å¯è¦³æ¸¬æ€§-å‰ç·¨"
emoji: "ğŸ”§"
type: "tech"
topics: ["aws", "sagemaker", "hyperpod", "slurm", "resiliency", "observability"]
free: false
---

::::details å‰æ
:::message
**å¯¾è±¡èª­è€…**: Amazon SageMaker HyperPod Slurm ç’°å¢ƒã‚’æ§‹ç¯‰æ¸ˆã¿ã§ã€å®Ÿéš›ã® resiliency æ©Ÿèƒ½ã¨ observability ã®å‹•ä½œã‚’ç¢ºèªã—ãŸã„æ–¹ã€‚åˆ†æ•£å­¦ç¿’ã®é‹ç”¨é¢ã«èˆˆå‘³ãŒã‚ã‚‹æ–¹ã€‚
:::
:::message
**ãƒ©ã‚¤ã‚»ãƒ³ã‚¹**: Â© 2025 littlemex.
æœ¬æ–‡ãŠã‚ˆã³è‡ªä½œå›³è¡¨: CC BY 4.0
â€»å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ã®å¼•ç”¨ã‚„ç¿»è¨³éƒ¨åˆ†ã¯åŸå…¸ã®è‘—ä½œæ¨©ã«å¾“ã„ã¾ã™ã€‚
å¼•ç”¨ç”»åƒ: å„ç”»åƒã®å‡ºå…¸ã«è¨˜è¼‰ã•ã‚ŒãŸãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã«å¾“ã„ã¾ã™ã€‚
:::
:::message
ä¸€éƒ¨ AI ã‚’ç”¨ã„ã¦æ–‡ç« ã‚’ä½œæˆã—ã¾ã™ã€‚ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¯å®Ÿæ–½ã—ã¾ã™ãŒã€è¦‹é€ƒã›ãªã„é‡å¤§ãªé–“é•ã„ãªã©ãŒã‚ã‚Œã°[ã“ã¡ã‚‰ã® Issue](https://github.com/littlemex/samples/issues) ã‹ã‚‰é€£çµ¡ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚
:::
::::

:::message
å®Ÿè£…ãŒå¤‰æ›´ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚å¿…è¦ã«å¿œã˜ã¦[å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://awslabs.github.io/ai-on-sagemaker-hyperpod/docs/validation-and-testing/resiliency/overview)ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚
:::

**æœ¬ç« ã§ã¯ Amazon SageMaker HyperPod Slurm ç’°å¢ƒã«ãŠã‘ã‚‹éšœå®³å¯¾å¿œåŠ›ã®æ¤œè¨¼ã¨å¯è¦–åŒ–ã«ã¤ã„ã¦å®Ÿè·µã—ã¾ã™ã€‚**

---

[HyperPod Resiliency ãƒ†ã‚¹ãƒˆã‚¬ã‚¤ãƒ‰](https://awslabs.github.io/ai-on-sagemaker-hyperpod/docs/validation-and-testing/resiliency/slurm-resiliency)ã¨ [Observability è¨­å®šæ‰‹é †](https://awslabs.github.io/ai-on-sagemaker-hyperpod/docs/add-ons/Observability/observability-slurm)ã€ãŠã‚ˆã³[ç’°å¢ƒæ¤œè¨¼ã‚¬ã‚¤ãƒ‰](https://awslabs.github.io/ai-on-sagemaker-hyperpod/docs/validation-and-testing/environment-validation/pytorch-environment-validation)ã‚’å‚ç…§ã—ãªãŒã‚‰ã€æ„å›³çš„ãªéšœå®³æ³¨å…¥ã«ã‚ˆã‚‹ãƒ¬ã‚¸ãƒªã‚¨ãƒ³ã‚·ãƒ¼ã®å®Ÿé¨“ã‚’å®Ÿæ–½ã—ã¾ã™ã€‚

# éšœå®³å¯¾å¿œåŠ›æ¤œè¨¼

Amazon SageMaker HyperPod ã§ã¯ã€å¤§è¦æ¨¡ãªåˆ†æ•£å­¦ç¿’ã«ãŠã‘ã‚‹éšœå®³ã‹ã‚‰ã®è‡ªå‹•å¾©æ—§ãŒé‡è¦ãªæ©Ÿèƒ½ã¨ã—ã¦å®Ÿè£…ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã¯ã™ã§ã«ã“ã‚Œã¾ã§ã®ç« ã§è§£èª¬ã—ã¾ã—ãŸã€‚

æœ¬ç« ã§ã¯ Amazon SageMaker HyperPod ã® Slurm ç’°å¢ƒã«ãŠã„ã¦ã€æ„å›³çš„ãªéšœå®³æ³¨å…¥ã«ã‚ˆã‚Šéšœå®³å¯¾å¿œåŠ›ã‚’å®Ÿéš›ã«æ¤œè¨¼ã—ã€observability ã‚·ã‚¹ãƒ†ãƒ ã‚’é€šã˜ã¦å¾©æ—§ãƒ—ãƒ­ã‚»ã‚¹ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚åˆ¶å¾¡ã•ã‚ŒãŸç’°å¢ƒã§ã®éšœå®³ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€Auto-Resume æ©Ÿèƒ½ã«ã‚ˆã‚‹è‡ªå‹•å¾©æ—§ã€ãã—ã¦ Grafana ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ã«ã‚ˆã‚Šã€å¤§è¦æ¨¡å­¦ç¿’ç’°å¢ƒã«ãŠã‘ã‚‹éšœå®³å¯¾å¿œãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®å®ŸåŠ¹æ€§ã‚’ç¢ºèªã—ã¾ã™ã€‚


## Node Recovery ã®å‹•ä½œãƒ•ãƒ­ãƒ¼

[HyperPod ã® Automatic Node Recovery](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-resiliency-slurm-auto-resume.html) ã¯ã€Health Monitoring Agentï¼ˆHMAï¼‰ã«ã‚ˆã‚‹éšœå®³æ¤œå‡ºã‹ã‚‰å§‹ã¾ã‚Šã¾ã™ã€‚HMA ãŒ GPU ã®æ¸©åº¦ç•°å¸¸ã€ãƒ¡ãƒ¢ãƒªã‚¨ãƒ©ãƒ¼ã€NVLink éšœå®³ãªã©ã®ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢å•é¡Œã‚’æ¤œå‡ºã™ã‚‹ã¨ã€è©²å½“ãƒãƒ¼ãƒ‰ã¯è‡ªå‹•çš„ã«ãƒ‰ãƒ¬ã‚¤ãƒ³çŠ¶æ…‹ã«ãƒãƒ¼ã‚¯ã•ã‚Œã¾ã™ã€‚å®Ÿè¡Œä¸­ã®ã‚¸ãƒ§ãƒ–ãŒã™ã¹ã¦çµ‚äº†ã—ãŸå¾Œã€å•é¡Œã®ã‚ã‚‹ãƒãƒ¼ãƒ‰ã¯æ–°ã—ã„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«è‡ªå‹•çš„ã«äº¤æ›ã•ã‚Œã¾ã™ã€‚

é‡è¦ãªç‚¹ã¨ã—ã¦ã€Slurm ç’°å¢ƒã§ã® auto-resume æ©Ÿèƒ½ä½¿ç”¨æ™‚ã¯ã€å•é¡Œã®ã‚ã‚‹ãƒãƒ¼ãƒ‰ã‚’å¸¸ã«äº¤æ›ã—ã€ãƒªãƒ–ãƒ¼ãƒˆã¯å®Ÿè¡Œã•ã‚Œã¾ã›ã‚“ã€‚

```mermaid
graph TB
    subgraph "HyperPod Slurm Resiliency ãƒ•ãƒ­ãƒ¼"
        subgraph "ç›£è¦–ãƒ»æ¤œå‡ºãƒ•ã‚§ãƒ¼ã‚º"
            HMA[Health Monitoring Agent<br/>ç¶™ç¶šçš„ãªç›£è¦–]
            DETECT[éšœå®³æ¤œå‡º<br/>GPU/æ¸©åº¦/ãƒ¡ãƒ¢ãƒªã‚¨ãƒ©ãƒ¼]
            DRAIN[ãƒãƒ¼ãƒ‰ãƒ‰ãƒ¬ã‚¤ãƒ³<br/>æ–°è¦ã‚¸ãƒ§ãƒ–åœæ­¢]
        end
        
        subgraph "å¾©æ—§ãƒ•ã‚§ãƒ¼ã‚º"
            WAIT[å®Ÿè¡Œä¸­ã‚¸ãƒ§ãƒ–ã®<br/>æ­£å¸¸çµ‚äº†å¾…æ©Ÿ]
            REPLACE[ãƒãƒ¼ãƒ‰äº¤æ›<br/>æ–°ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èµ·å‹•]
            VALIDATE[æ–°ãƒãƒ¼ãƒ‰æ¤œè¨¼<br/>Health Check å®Ÿè¡Œ]
        end
        
        subgraph "å†é–‹ãƒ•ã‚§ãƒ¼ã‚º"
            RESUME[Auto-Resume<br/>ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹]
            NORMAL[é€šå¸¸é‹ç”¨å¾©å¸°]
        end
        
        HMA --> DETECT
        DETECT --> DRAIN
        DRAIN --> WAIT
        WAIT --> REPLACE
        REPLACE --> VALIDATE
        VALIDATE --> RESUME
        RESUME --> NORMAL
        
        NORMAL --> HMA
    end
    
    style HMA fill:#2d5986,color:#fff
    style DETECT fill:#8b4513,color:#fff
    style DRAIN fill:#6b4513,color:#fff
    style WAIT fill:#4a6b35,color:#fff
    style REPLACE fill:#2d6b35,color:#fff
    style VALIDATE fill:#5a7a96,color:#fff
    style RESUME fill:#4a5986,color:#fff
    style NORMAL fill:#2d5986,color:#fff
```

## Auto-Resume ã¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®é–¢ä¿‚

Auto-resume æ©Ÿèƒ½ã¯ã€[`--auto-resume=1` ãƒ•ãƒ©ã‚°ã‚’ä»˜ã‘ã¦æŠ•å…¥ã•ã‚ŒãŸã‚¸ãƒ§ãƒ–](https://awslabs.github.io/ai-on-sagemaker-hyperpod/docs/validation-and-testing/resiliency/slurm-resiliency)ã«å¯¾ã—ã¦è‡ªå‹•çš„ã«å‹•ä½œã—ã¾ã™ã€‚ã“ã®æ©Ÿèƒ½ãŒæœ‰åŠ¹ãªã‚¸ãƒ§ãƒ–ã§ã¯ã€ãƒãƒ¼ãƒ‰éšœå®³ãŒç™ºç”Ÿã—ãŸéš›ã«æœ€å¾Œã«ä¿å­˜ã•ã‚ŒãŸãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰è‡ªå‹•çš„ã«å­¦ç¿’ãŒå†é–‹ã•ã‚Œã¾ã™ã€‚

ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ä¿å­˜ã¯ã€å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆå†…ã§å®šæœŸçš„ã«å®Ÿè¡Œã•ã‚Œã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚PyTorch ã® `torch.save()` é–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã® state_dictã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã®çŠ¶æ…‹ã€ç¾åœ¨ã®ã‚¨ãƒãƒƒã‚¯æ•°ã‚’ä¿å­˜ã™ã‚‹ã“ã¨ã§ã€éšœå®³ç™ºç”Ÿæ™‚ã®å­¦ç¿’é€²æ—ã®æå¤±ã‚’æœ€å°é™ã«æŠ‘ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

::::details ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å´ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå®Ÿè£…ã®è©³ç´°

:::message
**é‡è¦**: HyperPod ã® Auto-Resume æ©Ÿèƒ½ã¯ã€ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ãƒ¬ãƒ™ãƒ«ã§ã®ãƒãƒ¼ãƒ‰äº¤æ›ã¨ã‚¸ãƒ§ãƒ–å†é–‹ã‚’è‡ªå‹•åŒ–ã—ã¾ã™ãŒã€å­¦ç¿’çŠ¶æ…‹ã®ä¿å­˜ã¨å¾©å…ƒã¯ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³å´ã§å®Ÿè£…ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ©Ÿèƒ½ãŒé©åˆ‡ã«å®Ÿè£…ã•ã‚Œã¦ã„ãªã„å ´åˆã€ãƒãƒ¼ãƒ‰ãŒå¾©æ—§ã—ã¦ã‚‚ã‚¼ãƒ­ã‹ã‚‰å­¦ç¿’ã‚’é–‹å§‹ã™ã‚‹ã“ã¨ã«ãªã‚Šã¾ã™ã€‚
:::

[PyTorch DistributedDataParallel (DDP)](https://docs.pytorch.org/tutorials/beginner/ddp_series_multigpu.html) ã§ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ã¨å¾©æ—§ã®å®Ÿè£…ä¾‹ã‚’ç´¹ä»‹ã—ã¾ã™ãŒã€FSDP ã®å ´åˆã¯[ã“ã¡ã‚‰](https://docs.pytorch.org/tutorials/recipes/distributed_checkpoint_recipe.html)ã‚’ç¢ºèªã—ã¾ã—ã‚‡ã†ã€‚

```mermaid
graph TB
    subgraph "DDP ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ"
        subgraph S["ä¿å­˜æ®µéš"]
            SAVE_TRIGGER[ä¿å­˜ãƒˆãƒªã‚¬ãƒ¼<br/>å®šæœŸå®Ÿè¡Œãƒ»ã‚·ã‚°ãƒŠãƒ«å—ä¿¡]
            RANK_CHECK[ãƒ©ãƒ³ã‚¯ç¢ºèª<br/>rank 0 ã®ã¿ä¿å­˜]
            STATE_COLLECT[çŠ¶æ…‹åé›†<br/>model.module.state_dict]
            ATOMIC_WRITE[åŸå­çš„æ›¸ãè¾¼ã¿<br/>.tmp â†’ rename]
        end
        
        subgraph R["å¾©æ—§æ®µéš"]
            DETECT[ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ¤œå‡º<br/>latest_checkpoint.pth]
            MAP_LOAD[ãƒ‡ãƒã‚¤ã‚¹å¯¾å¿œèª­ã¿è¾¼ã¿<br/>map_location æŒ‡å®š]
            STATE_RESTORE[çŠ¶æ…‹å¾©å…ƒ<br/>load_state_dict]
            RESUME[å­¦ç¿’å†é–‹<br/>ä¿å­˜ã‚¨ãƒãƒƒã‚¯ã‹ã‚‰ç¶™ç¶š]
        end
        
        SAVE_TRIGGER --> RANK_CHECK
        RANK_CHECK --> STATE_COLLECT
        STATE_COLLECT --> ATOMIC_WRITE
        
        DETECT --> MAP_LOAD
        MAP_LOAD --> STATE_RESTORE
        STATE_RESTORE --> RESUME
        S --> R
    end
    
    style SAVE_TRIGGER fill:#2d5986,color:#fff
    style RANK_CHECK fill:#4a6b35,color:#fff
    style STATE_COLLECT fill:#5a7a96,color:#fff
    style ATOMIC_WRITE fill:#6b4513,color:#fff
    style DETECT fill:#8b4513,color:#fff
    style MAP_LOAD fill:#4a5986,color:#fff
    style STATE_RESTORE fill:#2d6b35,color:#fff
    style RESUME fill:#2d5986,color:#fff
```

**1. DDP ã§ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜å®Ÿè£…**

[PyTorch å…¬å¼ DDP ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](https://docs.pytorch.org/tutorials/beginner/ddp_series_multigpu.html)ã«å¾“ã£ãŸæ¨™æº–çš„ãªãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã™ã€‚

```python
import torch
import torch.distributed as dist
import os

def save_checkpoint(model, optimizer, epoch, loss, checkpoint_dir):
    """DDP å¯¾å¿œãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜"""
    # rank 0 ã®ã¿ãŒãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜
    if dist.get_rank() == 0:
        checkpoint_state = {
            'epoch': epoch,
            'model': model.module.state_dict(),  # DDP wrapperå¯¾å¿œ
            'optimizer': optimizer.state_dict(),
            'loss': loss,
            'world_size': dist.get_world_size(),
        }
        
        # åŸå­çš„ä¿å­˜ã§ç ´æãƒ•ã‚¡ã‚¤ãƒ«ã‚’é˜²ãï¼ˆé‡è¦ãªæ¦‚å¿µï¼‰
        checkpoint_path = os.path.join(checkpoint_dir, f"checkpoint_epoch_{epoch}.pth")
        tmp_path = f"{checkpoint_path}.tmp"
        
        # 1. ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«å®Œå…¨ã«æ›¸ãè¾¼ã¿
        torch.save(checkpoint_state, tmp_path)
        
        # 2. åŸå­çš„æ“ä½œã§ç¬æ™‚ã«ç½®æ›
        # os.rename() ã¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ãƒ¬ãƒ™ãƒ«ã§ã€ŒAll or Nothingã€ã‚’ä¿è¨¼
        # â†’ å®Œå…¨ãªãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿ãŒå­˜åœ¨ã—ã€éƒ¨åˆ†çš„ã«æ›¸ãè¾¼ã¾ã‚ŒãŸç ´æãƒ•ã‚¡ã‚¤ãƒ«ã¯ä½œã‚‰ã‚Œãªã„
        os.rename(tmp_path, checkpoint_path)
        
        print(f"Checkpoint saved: {checkpoint_path}")
```

**2. DDP ã§ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿å®Ÿè£…**

[PyTorch DDP ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹](https://docs.pytorch.org/tutorials/beginner/ddp_series_multigpu.html#save-and-load-checkpoints)ã«å¾“ã£ãŸå¾©æ—§å‡¦ç†ã§ã™ã€‚

```python
def load_checkpoint(model, optimizer, rank, checkpoint_path):
    """DDP å¯¾å¿œãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆèª­ã¿è¾¼ã¿"""
    # å„rankã«é©åˆ‡ãªãƒ‡ãƒã‚¤ã‚¹ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’æŒ‡å®š
    map_location = {'cuda:%d' % 0: 'cuda:%d' % rank}
    checkpoint_state = torch.load(checkpoint_path, map_location=map_location)
    
    # ãƒ¢ãƒ‡ãƒ«ã¨ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã®çŠ¶æ…‹å¾©å…ƒ
    model.module.load_state_dict(checkpoint_state['model'])
    optimizer.load_state_dict(checkpoint_state['optimizer'])
    
    start_epoch = checkpoint_state['epoch'] + 1
    loss = checkpoint_state['loss']
    
    print(f"Resumed from epoch {start_epoch} on rank {rank}")
    return start_epoch, loss
```

**3. Auto-Resume ã¨ã®çµ±åˆãƒã‚¤ãƒ³ãƒˆ**

HyperPod ã® [`--auto-resume=1` ãƒ•ãƒ©ã‚°](https://docs.aws.amazon.com/sagemaker/latest/dg/sagemaker-hyperpod-resiliency-slurm-auto-resume.html)ã¨ DDP ã‚’çµ„ã¿åˆã‚ã›ã‚‹ãŸã‚ã®å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã™ã€‚

```python
import signal

def setup_checkpoint_handler(model, optimizer, checkpoint_dir):
    """HyperPod Auto-Resume å¯¾å¿œã®ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    def emergency_checkpoint_save(signum, frame):
        if dist.get_rank() == 0:
            emergency_path = os.path.join(checkpoint_dir, "emergency_checkpoint.pth")
            emergency_state = {
                'model': model.module.state_dict(),
                'optimizer': optimizer.state_dict(),
                'emergency': True
            }
            torch.save(emergency_state, emergency_path)
            print(f"Emergency checkpoint saved: {emergency_path}")
        exit(0)
    
    signal.signal(signal.SIGTERM, emergency_checkpoint_save)
    signal.signal(signal.SIGINT, emergency_checkpoint_save)

# å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã§ã®ä½¿ç”¨ä¾‹ï¼ˆå®Ÿéš›ã®ãƒã‚¦ãƒ³ãƒˆãƒã‚¤ãƒ³ãƒˆã‚’ç¢ºèªï¼‰
# FSx_MOUNT=$(df -h | grep fsx_lustre | awk '{print $NF}')
setup_checkpoint_handler(ddp_model, optimizer, f"{FSX_MOUNT}/checkpoints")
```

**4. å®Ÿé‹ç”¨ã§ã®æ¨å¥¨è¨­å®š**

- **ä¿å­˜é »åº¦**: 5-15 åˆ†é–“éš”ã¾ãŸã¯ 100-500 ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨
- **ä¿å­˜å ´æ‰€**: FSx for Lustre ã®å…±æœ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (`/fsx/checkpoints/`)
- **ãƒ‡ãƒã‚¤ã‚¹ãƒãƒƒãƒ”ãƒ³ã‚°**: å„ rank ã«é©åˆ‡ãª map_location æŒ‡å®š
- **DDP å¯¾å¿œ**: `model.module.state_dict()` ã«ã‚ˆã‚‹æ­£ã—ã„çŠ¶æ…‹å–å¾—

DDP ã®è©³ç´°ãªå®Ÿè£…æ–¹æ³•ã«ã¤ã„ã¦ã¯ [PyTorch DDP ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](https://docs.pytorch.org/tutorials/beginner/ddp_series_multigpu.html)ã€HyperPod ç’°å¢ƒã§ã®å®Ÿè·µã«ã¤ã„ã¦ã¯æœ¬æ›¸ã® [PyTorch DDP ç« ](./pytorch-ddp.md) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚
::::

## Observability ã®éšå±¤æ§‹é€ 

[HyperPod Slurm ç’°å¢ƒã§ã® observability](https://awslabs.github.io/ai-on-sagemaker-hyperpod/docs/add-ons/Observability/observability-slurm) ã¯ã€Amazon Managed Service for Prometheus ã¨ Amazon Managed Grafana ã‚’æ‰‹å‹•ã§çµ±åˆã™ã‚‹ã“ã¨ã§å®Ÿç¾ã•ã‚Œã¾ã™ã€‚EKS ç’°å¢ƒã®ãƒ¯ãƒ³ã‚¯ãƒªãƒƒã‚¯çµ±åˆã¨ã¯ç•°ãªã‚Šã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¿ãƒ¼ã®æ‰‹å‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã¨è¨­å®šãŒå¿…è¦ã¨ãªã‚Šã¾ã™ã€‚

Observability ã®éšå±¤ã¯ã€å‰ã®ç« ã§èª¬æ˜ã—ãŸ**çµ±åˆãƒ†ãƒ¬ãƒ¡ãƒˆãƒª**ã®æ¦‚å¿µã‚’å…·ç¾åŒ–ã—ãŸã‚‚ã®ã§ã™ã€‚ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãƒ¬ãƒ™ãƒ«ã§ã¯ Slurm ã®ã‚¸ãƒ§ãƒ–ã‚­ãƒ¥ãƒ¼ã®çŠ¶æ…‹ã€ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æƒ…å ±ã€ãƒãƒ¼ãƒ‰ã®åˆ©ç”¨ç‡ã‚’ç›£è¦–ã—ã¾ã™ã€‚ãƒãƒ¼ãƒ‰ãƒ¬ãƒ™ãƒ«ã§ã¯ GPU ã®æ¸©åº¦ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã€é›»åŠ›æ¶ˆè²»é‡ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒˆãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚’è¿½è·¡ã—ã¾ã™ã€‚ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¬ãƒ™ãƒ«ã§ã¯å­¦ç¿’ã®é€²æ—ã€æå¤±é–¢æ•°ã®å€¤ã€ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã‚’è¨˜éŒ²ã—ã¾ã™ã€‚

ã“ã‚Œã‚‰ã®å¤šå±¤çš„ãªç›£è¦–ã«ã‚ˆã‚Šã€éšœå®³ã®æ ¹æœ¬åŸå› ã‚’è¿…é€Ÿã«ç‰¹å®šã—ã€äºˆé˜²çš„ãªå¯¾ç­–ã‚’è¬›ã˜ã‚‹ã“ã¨ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚ä¾‹ãˆã°ã€ç‰¹å®šã® GPU ã§æ¸©åº¦ä¸Šæ˜‡ãŒç¶™ç¶šçš„ã«è¦³æ¸¬ã•ã‚Œã‚‹å ´åˆã€ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢éšœå®³ã®äºˆå…†ã¨ã—ã¦äº‹å‰ã«ãƒãƒ¼ãƒ‰ã‚’äº¤æ›ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚

---

# Amazon SageMaker HyperPod Slurm ã§ã®å®Ÿè£…

ã“ã“ã‹ã‚‰ã¯ã€å®Ÿéš›ã« HyperPod Slurm ç’°å¢ƒã§ resiliency ã¨ observability ã‚’ç¢ºèªã—ã¾ã™ã€‚å‰ç« ã§æ§‹ç¯‰ã—ãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’åŸºç›¤ã¨ã—ã¦ã€å®Ÿéš›ã®éšœå®³æ³¨å…¥ã‹ã‚‰å¾©æ—§ã¾ã§ã®ä¸€é€£ã®å‹•ä½œã‚’æ¤œè¨¼ã—ã¾ã—ã‚‡ã†ã€‚

## å‰ææ¡ä»¶

::::details ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£è¦ä»¶

:::message
**Slurm ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®æº–å‚™**

æœ¬ç« ã®å®Ÿè·µã«ã¯ã€å‰ç« ã§æ§‹ç¯‰ã—ãŸ Amazon SageMaker HyperPod Slurm ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãŒç¨¼åƒã—ã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãŒå‰Šé™¤ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€[Amazon SageMaker HyperPod Getting Started by SLURM](./amazon-sagemaker-hyperpod-slurm-tutorial) ã‚’å‚ç…§ã—ã¦ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’å†ä½œæˆã—ã¦ãã ã•ã„ã€‚
:::

:::message
AWS CLI v2 ã¨SSM Session Manager ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ãŒé©åˆ‡ã«è¨­å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ã¾ãŸã€Amazon Managed Service for Prometheus ã¨ Amazon Managed Grafana ã®ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã‚’ä½œæˆã™ã‚‹æ¨©é™ãŒå¿…è¦ã§ã™ã€‚
:::


## æ¨å¥¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ§‹æˆï¼ˆresiliency ãƒ†ã‚¹ãƒˆç”¨ï¼‰

å®Ÿéš›ã® resiliency ãƒ†ã‚¹ãƒˆã«ã¯ GPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®è¿½åŠ ã‚’æ¨å¥¨ã—ã¾ã™ã€‚GPU å›ºæœ‰ã®éšœå®³ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ãã®å¾©æ—§å‹•ä½œã‚’ç¢ºèªã™ã‚‹ãŸã‚ã§ã™ã€‚å‰ç« ã® CPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹æ§‹æˆã«åŠ ãˆã¦ã€Worker ã‚°ãƒ«ãƒ¼ãƒ—ã« `ml.g5.xlarge` ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ 2 å°è¿½åŠ ã™ã‚‹ã“ã¨ã§ã€ã‚ˆã‚Šå®Ÿè·µçš„ãªãƒ†ã‚¹ãƒˆç’°å¢ƒã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚

### provisioning_parameters.json è‡ªå‹•æ›´æ–°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã“ã® `provisioning_parameters.json` ãƒ•ã‚¡ã‚¤ãƒ«ã¯ `slurm.conf` ã¨ã„ã† slurm ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ Hyperpod ãŒè‡ªå‹•ç”Ÿæˆã™ã‚‹éš›ã«åˆ©ç”¨ã•ã‚Œã¾ã™ã€‚Quick Setup æ™‚ã«ã¯å‹æ‰‹ã«ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒä½œæˆã•ã‚Œã‚‹ãŸã‚æ„è­˜ã—ã¾ã›ã‚“ã§ã—ãŸãŒã€GPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’è¿½åŠ ã™ã‚‹å ´åˆã«ã¯**ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã™ã‚‹**å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ä»¥ä¸‹ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ GPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ã‚’è¨­å®šã«è¿½åŠ ã—ã€S3 ä¸Šã® json ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•æ›´æ–°ã§ãã¾ã™ã€‚

```bash
curl -sSL https://raw.githubusercontent.com/littlemex/samples/main/ml_distributed_experiment_collection/amazon-sagemaker-hyperpod-slurm-observability/update_provisioning_params.sh -o update_provisioning_params.sh
chmod +x update_provisioning_params.sh
```

ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã™ã‚‹å‰ã«ã€`BUCKET_NAME` ã‚’å®Ÿéš›ã® S3 ãƒã‚±ãƒƒãƒˆåã«å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚æœ¬ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã¨ç•°ãªã‚‹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—ã‚’æŒ‡å®šã™ã‚‹å ´åˆã‚‚æ‰‹å‹•ã§ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚

```bash
# ã‚¹ã‚¯ãƒªãƒ—ãƒˆå†…ã®ãƒã‚±ãƒƒãƒˆåã‚’æ›´æ–°ã—ã¦ã‹ã‚‰å®Ÿè¡Œ
sed -i 's/your-hyperpod-bucket-name/actual-bucket-name-here/' update_provisioning_params.sh
./update_provisioning_params.sh
```

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã«ã‚ˆã‚Šã€GPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—è¨­å®šãŒé©ç”¨ã•ã‚Œã¾ã™ã€‚
::::

::::details GPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®è¿½åŠ æ–¹æ³•

:::message alert
`ml.g5.xlarge for cluster usage` ãªã©ã® Service Quotas ã‚’ç¢ºèªã—ã¦å¿…è¦ã«å¿œã˜ã¦è¿½åŠ ãƒªã‚¯ã‚¨ã‚¹ãƒˆã—ã¾ã—ã‚‡ã†ã€‚
:::

å‰ç« ã§ä½œæˆã—ãŸ CPU ãƒ™ãƒ¼ã‚¹ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã« GPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’è¿½åŠ ã™ã‚‹å ´åˆã¯ã€ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ›´æ–°æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚æ—¢å­˜ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’å‰Šé™¤ã™ã‚‹ã“ã¨ãªãã€æ–°ã—ã„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ã‚’è¿½åŠ ã§ãã¾ã™ã€‚

![](/images/books/ml-distributed-experiment-collection/hyperpod-slurm-add-instance-group.png)

![](/images/books/ml-distributed-experiment-collection/hyperpod-slurm-add-gpu.png)

![](/images/books/ml-distributed-experiment-collection/hyperpod-slurm-updating-gpu.png)

SageMaker HyperPod ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ç®¡ç†ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã‹ã‚‰å¯¾è±¡ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’é¸æŠã—ã€ã€ŒEditã€ã‚’é¸æŠã—ã¾ã™ã€‚ã€ŒCreate instance groupã€ã‚’é¸æŠã—ã€ã€Œgpu-workerã€ã¨ã„ã†åå‰ã§ `ml.g5.xlarge` ã‚’ 2 å°è¿½åŠ ã—ã¾ã™ã€‚æ›´æ–°ã«ã¯ç´„ 10 æ•°åˆ†ã‹ã‹ã‚Šã€æ—¢å­˜ã® CPU ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹ã“ã¨ãªã GPU ãƒãƒ¼ãƒ‰ãŒè¿½åŠ ã•ã‚Œã¾ã™ã€‚GPU ãƒãƒ¼ãƒ‰ã§ã¯ CUDA ãƒ‰ãƒ©ã‚¤ãƒã¨ NCCL ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè‡ªå‹•çš„ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã€åˆ†æ•£å­¦ç¿’ã«å¿…è¦ãªç’°å¢ƒãŒæ•´å‚™ã•ã‚Œã¾ã™ã€‚
::::

::::details èª¿æŸ»ä¸­: ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³è‡ªå‹•ä½œæˆã®ä¸å…·åˆ

ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã¯ Slurm ã«ãŠã„ã¦**è¨ˆç®—ãƒãƒ¼ãƒ‰ã®è«–ç†çš„ãªã‚°ãƒ«ãƒ¼ãƒ—**ã§ã™ã€‚ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å†…ã®è¨ˆç®—ãƒªã‚½ãƒ¼ã‚¹ã‚’ç›®çš„ã‚„ç‰¹æ€§ã«å¿œã˜ã¦åˆ†é¡ãƒ»ç®¡ç†ã™ã‚‹ãŸã‚ã®ä»•çµ„ã¿ã§ã™ã€‚ç‰¹å®šã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®ã¿ã«ã‚¸ãƒ§ãƒ–ã‚’å‰²ã‚ŠæŒ¯ã‚ŠãŸã„ã‚ˆã†ãªã‚±ãƒ¼ã‚¹ã§åˆ©ç”¨ã§ãã¾ã™ã€‚

```
# ç‰¹å®šã®ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã§ã‚¸ãƒ§ãƒ–å®Ÿè¡Œ
sbatch --partition=gpu myjob.sh

# ã¾ãŸã¯ slurm ã‚¹ã‚¯ãƒªãƒ—ãƒˆå†…ã§æŒ‡å®š
#SBATCH --partition=gpu
```

:::message alert
`provisioning_parameters.json` ã§ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã¦ã‚‚ã€HyperPod Agent ãŒ GPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”¨ã®å°‚ç”¨ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ï¼ˆä¾‹ï¼š`ml.g5.xlarge`ï¼‰ã‚’ slurm.conf ã«åæ˜ ã—ãªã„å ´åˆãŒã‚ã‚Šã¾ã™ã€‚GPU ãƒãƒ¼ãƒ‰ã¯èªè­˜ã•ã‚Œã€Slurm ã§åˆ©ç”¨å¯èƒ½ã§ã™ãŒã€ã™ã¹ã¦ `dev` ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«é…ç½®ã•ã‚Œã‚‹çŠ¶æ³ãŒç™ºç”Ÿã—ã¾ã™ã€‚
:::

ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§æ‰‹å‹•ã§ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³è¨­å®šã‚’è¿½åŠ ã§ãã¾ã™ã€‚Nodes ã® IP ã‚¢ãƒ‰ãƒ¬ã‚¹ã¯ã”è‡ªèº«ã®ç’°å¢ƒã«åˆã‚ã›ã¦å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚ãŸã ã—æœ¬æ¥ã§ã‚ã‚Œã° HyperPod Agent ãŒè‡ªå‹•ç®¡ç†ã™ã‚‹ slurm.conf ã‚’ç›´æ¥ç·¨é›†ã™ã‚‹ã“ã¨ã®æ¨å¥¨åº¦ã¯ä¸æ˜ã§ã™ã€‚HyperPod Agent ã«ã‚ˆã‚‹è¨­å®šæ›´æ–°ã®ã‚¿ã‚¤ãƒŸãƒ³ã‚°ã‚„é »åº¦ã¯ç¾åœ¨èª¿æŸ»ä¸­ã§ã™ã€‚è¨­å®šãŒã‚¯ãƒªã‚¢ã•ã‚Œã‚‹å¯èƒ½æ€§ã‚‚ã‚ã‚‹ãŸã‚è‡ªå·±åˆ¤æ–­ã§è¨­å®šã—ã€é‡è¦ãªãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã§ã¯ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‹ã‚‰ä½œã‚Šç›´ã™ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚

```bash
# GPU ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã‚’æ‰‹å‹•è¿½åŠ 
echo "PartitionName=ml.g5.xlarge Nodes=ip-10-3-135-7,ip-10-3-96-49 Default=NO MaxTime=INFINITE State=UP" | sudo tee -a /opt/slurm/etc/slurm.conf
sudo scontrol reconfigure

# è¨­å®šç¢ºèª
sinfo
```

### ç¢ºèªã•ã‚ŒãŸå‹•ä½œä¾‹
```bash
# æ‰‹å‹•è¨­å®šå¾Œã® sinfo å‡ºåŠ›ä¾‹
sinfo
PARTITION     AVAIL  TIMELIMIT  NODES  STATE NODELIST
dev*             up   infinite      2  alloc ip-10-4-33-25,ip-10-4-198-29
dev*             up   infinite      2   idle ip-10-3-96-49,ip-10-3-135-7
ml.c5.4xlarge    up   infinite      2  alloc ip-10-4-33-25,ip-10-4-198-29
ml.g5.xlarge     up   infinite      2   idle ip-10-3-96-49,ip-10-3-135-7
```

æ‰‹å‹•è¨­å®šã«ã‚ˆã‚Šã€ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—ã«å¯¾å¿œã—ãŸå°‚ç”¨ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ãŒæ­£å¸¸ã«ä½œæˆã•ã‚Œã€é©åˆ‡ãªãƒãƒ¼ãƒ‰ãŒå‰²ã‚Šå½“ã¦ã‚‰ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ã„ã¾ã™ã€‚

ä»¥ä¸‹ã®ã‚ˆã†ã« g5 ã‚°ãƒ«ãƒ¼ãƒ—ã¯æ­£å¸¸ã« `nvidia-smi` ãŒå®Ÿè¡Œã§ãã¦ã„ã‚‹ã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ã€‚

```bash
# CPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§ Driver ãŒãªã„ã®ã§ã‚³ãƒãƒ³ãƒ‰å¤±æ•—
srun --partition=ml.c5.4xlarge nvidia-smi
NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.

srun: error: ip-10-4-33-25: task 0: Exited with exit code 9

# GPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§ã‚³ãƒãƒ³ãƒ‰æˆåŠŸ
ubuntu@ip-10-4-109-244:~$ srun --partition=ml.g5.xlarge nvidia-smi
Mon Dec 22 17:29:56 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A10G                    On  |   00000000:00:1E.0 Off |                    0 |
|  0%   17C    P8             11W /  300W |       0MiB /  23028MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
```
::::

## SageMaker Studio Integration ã®è¨­å®š

[Amazon SageMaker Studio](https://docs.aws.amazon.com/sagemaker/latest/dg/studio.html) ã¯ã€æ©Ÿæ¢°å­¦ç¿’ã®çµ±åˆé–‹ç™ºç’°å¢ƒï¼ˆIDEï¼‰ã§ã™ã€‚Web ãƒ™ãƒ¼ã‚¹ã® ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã«ã‚ˆã‚Šã€ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆã‚„ç ”ç©¶è€…ãŒå˜ä¸€ã®ç’°å¢ƒã§ ML ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«å…¨ä½“ã‚’ç®¡ç†ã§ãã¾ã™ã€‚æŸ”è»Ÿã«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—ã‚’é¸æŠã§ãã€FSx for Lustre ã®ãƒ•ã‚¡ã‚¤ãƒ«å…±æœ‰ã€MLflow çµ±åˆãªã©å¤šæ§˜ãªæ©Ÿèƒ½ã‚’æœ‰ã—ã¦ã„ã¾ã™ã€‚

::::details Studio è£œè¶³æƒ…å ±
Studio ã¨ HyperPod ã®çµ±åˆã«ã‚ˆã‚Šã€**é–‹ç™ºç’°å¢ƒã¨å®Ÿè¡Œç’°å¢ƒã®ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ãªé€£æº**ãŒå®Ÿç¾ã•ã‚Œã¾ã™ã€‚å…·ä½“çš„ã«ã¯ã€Studio ã® Code Editor ã§ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’é–‹ç™ºã—ã€åŒã˜ç’°å¢ƒã®ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‹ã‚‰ `sbatch` ã‚³ãƒãƒ³ãƒ‰ã§ HyperPod ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«ã‚¸ãƒ§ãƒ–ã‚’æŠ•å…¥ã§ãã¾ã™ã€‚FSx for Lustre ã®å…±æœ‰ã«ã‚ˆã‚Šã€Studio ã§ä½œæˆã—ãŸã‚³ãƒ¼ãƒ‰ã‚„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒ HyperPod ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å…¨ä½“ã§å³åº§ã«åˆ©ç”¨å¯èƒ½ã«ãªã‚Šã¾ã™ã€‚è‡ªèº«ã®ãƒ­ãƒ¼ã‚«ãƒ«ã®ã‚¨ãƒ‡ã‚£ã‚¿ã®æ–¹ãŒä½¿ã„ã‚„ã™ã„å ´åˆã¯ãƒ­ãƒ¼ã‚«ãƒ«ã‚¨ãƒ‡ã‚£ã‚¿ã‹ã‚‰ Studio ã® Code Editor ã«æ¥ç¶šã™ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã™ã€‚ãã—ã¦ [Presigned URL](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_CreatePresignedDomainUrl.html) ã§ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã‚¢ã‚¯ã‚»ã‚¹ãªã—ã§ CLI ã‹ã‚‰ Studio ã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚

ãŸã ã—ã€Studio Code Editor ã§ã¯ Docker ãŒãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ç„¡åŠ¹ã§ã‚ã‚Šã€[Docker in Docker ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£](https://docs.aws.amazon.com/sagemaker/latest/dg/studio-updated-local-get-started.html)ã¨ãªã£ã¦ã„ã‚‹ãŸã‚ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãƒ¢ãƒ¼ãƒ‰ã‚„ namespace ã«åˆ¶ç´„ãŒã‹ã‹ã£ã¦ãŠã‚Šã‚³ãƒ³ãƒ†ãƒŠé–‹ç™ºã«ã¯ä¸å‘ãã§ã™ã€‚
::::

:::message
- [ ] 1. Studio Domain ã®ä½œæˆ
- [ ] 2. User Profile ã®ä½œæˆï¼ˆCLIï¼‰
- [ ] 3. SageMaker Studio ã®è¨­å®š
- [ ] 4. FSx for Lustre ã¨ã®çµ±åˆç¢ºèª
:::

::::details 1. Studio Domain ã®ä½œæˆ

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: awsome-distributed-training ãƒªãƒã‚¸ãƒˆãƒªã® CloudFormation ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¦ã€SageMaker Studio Domain ã¨ FSx for Lustre ã®å®Œå…¨çµ±åˆç’°å¢ƒã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: CloudFormation ã‚¹ã‚¿ãƒƒã‚¯ãŒ CREATE_COMPLETE çŠ¶æ…‹ã«ãªã‚Šã€Studio Domain ãŒä½œæˆã•ã‚Œã¦ã„ã‚‹ã“ã¨ã€‚
:::

[awsome-distributed-training ã® CloudFormation ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ](https://github.com/aws-samples/awsome-distributed-training/blob/main/1.architectures/5.sagemaker-hyperpod/slurm-studio/studio-slurm.yaml)ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€FSx for Lustre çµ±åˆã€Slurm ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®šã®è‡ªå‹•åŒ–ãŒå®Ÿç¾ã•ã‚Œã¾ã™ã€‚

## è‡ªå‹•åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆæ¨å¥¨æ–¹æ³•ï¼‰

æ—¢å­˜ã® HyperPod ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æƒ…å ±ã‹ã‚‰å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è‡ªå‹•å–å¾—ã—ã€CloudFormation ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’æä¾›ã—ã¾ã™ï¼š

```bash
curl -sSL https://raw.githubusercontent.com/littlemex/samples/main/ml_distributed_experiment_collection/amazon-sagemaker-hyperpod-slurm-observability/create_studio_domain.sh -o create_studio_domain.sh
chmod +x create_studio_domain.sh

# ã”è‡ªèº«ã®ç’°å¢ƒã«åˆã‚ã›ã¦å¤‰æ›´ã—ã¦ãã ã•ã„
# sed -i 's/cpu-slurm-cluster/your-actual-cluster-name/' create_studio_domain.sh
# sed -i 's/us-east-1/your-region/' create_studio_domain.sh

# ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œ
./create_studio_domain.sh
```

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã«ã‚ˆã‚Šã€FSx for Lustre ã®å®Œå…¨çµ±åˆã€Slurm ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®è‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®šã®è‡ªå‹•åŒ–ãŒå®Ÿç¾ã•ã‚Œã¾ã™ã€‚ä»¥ä¸‹ã®ã‚ˆã†ã« SageMaker AI ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã§æ­£å¸¸ã«ä½œæˆã•ã‚ŒãŸã“ã¨ãŒç¢ºèªã§ãã¾ã™ã€‚

![](/images/books/ml-distributed-experiment-collection/hyperpod-slurm-create-domain.png)
::::

::::details 2. User Profile ã®ä½œæˆï¼ˆCLIï¼‰

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: Studio Domain ä½œæˆå¾Œã« User Profile ã‚’è¿½åŠ ã—ã€FSx ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®è‡ªå‹•ä½œæˆã¨ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™è¨­å®šã‚’å®Ÿè¡Œã—ã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: User Profile ãŒ InService çŠ¶æ…‹ã«ãªã‚Šã€FSx ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ãŒè‡ªå‹•ä½œæˆã•ã‚Œã¦ã„ã‚‹ã“ã¨ã€‚
:::

CloudFormation ã«ã‚ˆã‚‹ Domain ä½œæˆãŒå®Œäº†ã—ãŸã‚‰ã€User Profile ã‚’ä½œæˆã—ã¾ã™ã€‚FSx for Lustre ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã¯ `/fsx/shared/` ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’é€šã˜ã¦ HyperPod ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã¨ Studio ç’°å¢ƒé–“ã§å…±æœ‰ã•ã‚Œã¾ã™ã€‚

## User Profile ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```bash
# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦å†…å®¹ç¢ºèª
curl -sSL https://raw.githubusercontent.com/littlemex/samples/main/ml_distributed_experiment_collection/amazon-sagemaker-hyperpod-slurm-observability/create_user_profile.sh -o create_user_profile.sh

# ã‚¹ã‚¯ãƒªãƒ—ãƒˆå†…å®¹ã®ç¢ºèª
head -20 create_user_profile.sh

chmod +x create_user_profile.sh
```

## å®Ÿè¡Œæ‰‹é †

```bash
# ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã§ä½œæˆ
./create_user_profile.sh

# ã¾ãŸã¯ç‰¹å®šã®ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’æŒ‡å®š
./create_user_profile.sh data-scientist-1
```

User Profile ä½œæˆå¾Œã€SageMaker ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã‹ã‚‰è©²å½“ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«ã§ãƒ­ã‚°ã‚¤ãƒ³ã—ã€JupyterLab ã‚’èµ·å‹•ã§ãã¾ã™ã€‚

![](/images/books/ml-distributed-experiment-collection/hyperpod-slurm-create-user-profile.png)
::::

::::details 3. SageMaker Studio ã®è¨­å®š

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: ä½œæˆã—ãŸ Studio ç’°å¢ƒã§ Slurm ã‚³ãƒãƒ³ãƒ‰ã€HyperPod ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã¨ã®é€£æºãŒæ­£å¸¸ã«å‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: Studio å†…ã‹ã‚‰ FSx ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã€Slurm ã‚³ãƒãƒ³ãƒ‰ãŒå®Ÿè¡Œã§ãã‚‹ã“ã¨ã€‚
:::

## Code Editor Space ã®ä½œæˆ

:::message alert
User Profile ã‚’ä½œæˆã—ãŸã ã‘ã§ã¯ FSx ã‚„ Slurm ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã¾ã›ã‚“ã€‚ä»¥ä¸‹ã®æ‰‹é †ã§ Code Editor Space ã‚’æ‰‹å‹•ä½œæˆã—ã€FSx ã¨ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«è¨­å®šã‚’ã‚¢ã‚¿ãƒƒãƒã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
:::

1. **SageMaker Studio ã«ã‚¢ã‚¯ã‚»ã‚¹**
   - User Profile ã‹ã‚‰ Studio ã«ãƒ­ã‚°ã‚¤ãƒ³

2. **Code Editor Space ã®ä½œæˆ**
   - "Code Editor" ã‚’ã‚¯ãƒªãƒƒã‚¯
   - "Create Code Editor Space" ã‚’ã‚¯ãƒªãƒƒã‚¯
   - Space åã‚’å…¥åŠ›ï¼ˆä¾‹ï¼š`hyperpod-workspace`ï¼‰

3. **é‡è¦ï¼šFSx ã¨ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«è¨­å®šã®ã‚¢ã‚¿ãƒƒãƒ**
   - "Attach custom filesystem - optional" ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ã‹ã‚‰ **FSx for Lustre volume ã‚’é¸æŠ**
   - "Lifecycle configuration" ãƒ‰ãƒ­ãƒƒãƒ—ãƒ€ã‚¦ãƒ³ã‹ã‚‰ **åˆ©ç”¨å¯èƒ½ãªãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«è¨­å®šã‚’é¸æŠ**
   - "Run Space" ã‚’ã‚¯ãƒªãƒƒã‚¯

4. **Space ã®èµ·å‹•**
   - Code Editor ãŒèµ·å‹•ã™ã‚‹ã¾ã§æ•°åˆ†å¾…æ©Ÿ
   - æ­£å¸¸ã«ä½œæˆã•ã‚ŒãŸã‚‰ "Open Code Editor" ã§ Editor ã‚’èµ·å‹•

![](/images/books/ml-distributed-experiment-collection/hyperpod-slurm-studio-create-space.png)

## Slurm ã‚³ãƒãƒ³ãƒ‰ã®å®Ÿè¡Œæ–¹æ³•

Studio Code Editor ã§ã¯ã€**2 ã¤ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**ã§ Slurm ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚

### æ–¹æ³•1: Login ãƒãƒ¼ãƒ‰çµŒç”±ã®SSH

```bash
# GitHub ã‹ã‚‰ easy-ssh.sh ã‚’å–å¾—
curl -O https://raw.githubusercontent.com/aws-samples/awsome-distributed-training/main/1.architectures/5.sagemaker-hyperpod/easy-ssh.sh
chmod +x easy-ssh.sh

# SSH Key ç”Ÿæˆï¼ˆæœªä½œæˆã®å ´åˆï¼‰
ssh-keygen -t rsa -b 4096 -f "$HOME/.ssh/id_rsa" -N ""

# Login ãƒãƒ¼ãƒ‰ã«æ¥ç¶šè¨­å®š
./easy-ssh.sh -c login cpu-slurm-cluster

# SSH çµŒç”±ã§ã® Slurm æ“ä½œ
ssh cpu-slurm-cluster sinfo
```

### æ–¹æ³•2: Studio Code Editor å†…ã§ã®ç›´æ¥å®Ÿè¡Œ

:::message alert
**åˆ¶ç´„äº‹é …**: Studio Code Editor ã¯ã‚³ãƒ³ãƒ†ãƒŠç’°å¢ƒã§å‹•ä½œã—ã€MUNGE èªè¨¼ã«èª²é¡ŒãŒã‚ã‚Šã¾ã™ã€‚ä»¥ä¸‹ã®æ‰‹é †ã¯æŠ€è¡“æ¤œè¨¼ç”¨ã§ã‚ã‚Šã€æœ¬ç•ªåˆ©ç”¨ã«ã¯æ–¹æ³• 1 ã‚’æ¨å¥¨ã—ã¾ã™ã€‚
:::

[MUNGE (MUNGE Uid 'N' Gid Emporium)](https://dun.github.io/munge/) ã¯ã€Slurm ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«ãŠã‘ã‚‹èªè¨¼ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å†…ã®å…¨ãƒãƒ¼ãƒ‰é–“ã§å®‰å…¨ãªé€šä¿¡ã‚’ç¢ºä¿ã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã‚¸ãƒ§ãƒ–ã‚’æŠ•å…¥ã™ã‚‹éš›ã®èªè¨¼ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚MUNGE ã®å‹•ä½œã«ã¯ã€ã™ã¹ã¦ã®ãƒãƒ¼ãƒ‰ã§åŒã˜ç§˜å¯†éµï¼ˆMUNGE ã‚­ãƒ¼ï¼‰ã‚’å…±æœ‰ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã€ã“ã®éµã®åŒæœŸãŒ Slurm ã‚³ãƒãƒ³ãƒ‰å®Ÿè¡Œã®å‰ææ¡ä»¶ã¨ãªã‚Šã¾ã™ã€‚

Studio Code Editor å†…ã§ã® Slurm ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆç›´æ¥å®Ÿè¡Œã«ã¯ã€HyperPod ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‹ã‚‰ MUNGE ã‚­ãƒ¼ã‚’å–å¾—ã—ã€Studio ç’°å¢ƒã§ MUNGE ãƒ‡ãƒ¼ãƒ¢ãƒ³ã‚’èµ·å‹•ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

```bash
curl -sSL https://raw.githubusercontent.com/littlemex/samples/main/ml_distributed_experiment_collection/amazon-sagemaker-hyperpod-slurm-observability/setup_slurm_munge_studio.sh -o setup_slurm_munge_studio.sh
chmod +x setup_slurm_munge_studio.sh

./setup_slurm_munge_studio.sh

# slurm ã‚³ãƒãƒ³ãƒ‰ã®å‹•ä½œç¢ºèª
sinfo
```
::::

::::details 4. FSx for Lustre ã¨ã®çµ±åˆç¢ºèª

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: Studio ç’°å¢ƒã‹ã‚‰ FSx for Lustre ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’ç¢ºèªã—ã€å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚„å­¦ç¿’çµæœã®åŠ¹ç‡çš„ãªç®¡ç†æ–¹æ³•ã‚’ç¿’å¾—ã—ã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: Studio ã‹ã‚‰ FSx ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã€ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿æ›¸ãã¨å…±æœ‰ãŒæ­£å¸¸ã«å‹•ä½œã™ã‚‹ã“ã¨ã€‚
:::

FSx for Lustre ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã¯ã€Slurm ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å†…ã®å…¨ãƒãƒ¼ãƒ‰ã§ `/fsx` ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨ã—ã¦ãƒã‚¦ãƒ³ãƒˆã—ã¾ã—ãŸã€‚User Profile ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆã®è¨­å®šã«ã‚ˆã£ã¦ Studio ç’°å¢ƒã‹ã‚‰ã‚‚å…±æœ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã§ã™ã€‚Slurm Login ãƒãƒ¼ãƒ‰ã«æ¥ç¶šã—ã¦ `/fsx` ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®æ¨©é™è¨­å®šã‚’è¡Œã„ã¾ã—ã‚‡ã†ã€‚ã“ã‚Œã«ã‚ˆã£ã¦ Studio å´ã‹ã‚‰ã‚‚æ¨©é™ãŒè¨±å¯ã•ã‚ŒãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã§ã™ã€‚

```bash
# HyperPod ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã« SSH æ¥ç¶š
ssh cpu-slurm-cluster

# FSx ã®æ¨©é™ç¢ºèª
ls -la /fsx

# Studio ãƒ¦ãƒ¼ã‚¶ãƒ¼å‘ã‘å…±æœ‰ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆã¨æ¨©é™è¨­å®š
sudo mkdir -p /fsx/shared/studio-workspace
sudo chmod 755 /fsx/shared/studio-workspace 
sudo chown 10001:1001 /fsx/shared/studio-workspace

# ç¢ºèª
ls -la /fsx/shared
```

æ¨©é™è¨­å®šå¾Œã€Studio Code Editor ã‹ã‚‰å†åº¦ã‚¢ã‚¯ã‚»ã‚¹ã‚’è©¦è¡Œã—ã¾ã™ã€‚

```bash
# Studio ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‹ã‚‰å†è©¦è¡Œ
FSX_MOUNT=$(df -h | grep fsx_lustre | awk '{print $NF}')
touch $FSX_MOUNT/studio-workspace/testfile && ls -la $FSX_MOUNT/studio-workspace
```
::::

## Observability ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰

:::message
**æ›´æ–°ã•ã‚ŒãŸæ¨å¥¨æ‰‹é †**
- [ ] 1. Open Source Grafana + Amazon Managed Prometheus ç’°å¢ƒã®ãƒ‡ãƒ—ãƒ­ã‚¤
- [ ] 2. Grafana ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹
- [ ] 3. Grafana ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®è¨­å®š
- [ ] 4. ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
- [ ] 3. ãƒ†ã‚¹ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ•å…¥ç’°å¢ƒã®æ§‹ç¯‰ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
- [ ] 4. HyperPod ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã¸ã® Observability ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
- [ ] 5. å‹•ä½œç¢ºèªã¨ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­å®š
:::

HyperPod Slurm Observability ã¯ã€Amazon Managed Service for Prometheus ã¨ Grafana ã‚’çµ„ã¿åˆã‚ã›ã¦å®Ÿç¾ã—ã¾ã™ã€‚

![](https://awslabs.github.io/ai-on-sagemaker-hyperpod/assets/images/observability_architecture-1f511d1934afb3d2ebf7c89c41a31a17.png)

:::message alert
**é‡è¦**: Amazon Managed Grafana ã¯ AWS Organizations ã®**ç®¡ç†ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ**ã§ã®ã¿å®Œå…¨ã«åˆ©ç”¨ã§ãã¾ã™ã€‚ãƒ¡ãƒ³ãƒãƒ¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã§ã¯ IAM Identity Center ã®åˆ¶é™ã«ã‚ˆã‚Š Amazon Managed Grafana ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã®ä½œæˆãŒã§ãã¾ã›ã‚“ã€‚
:::

ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—ç”¨ã®ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãªã© Organization ã®ç®¡ç†ã‚¢ã‚«ã‚¦ãƒ³ãƒˆãŒåˆ©ç”¨ã§ããªã„ã‚±ãƒ¼ã‚¹ã‚’æƒ³å®šã—ã¦æœ¬ç« ã§ã¯ OSS ãƒ™ãƒ¼ã‚¹ã® Grafana ã‚’ EC2 ã«ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã¦åˆ©ç”¨ã—ã¾ã™ã€‚ç®¡ç†ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’åˆ©ç”¨ã§ãã‚‹ã‚±ãƒ¼ã‚¹ã§ã¯[ã“ã¡ã‚‰](https://awslabs.github.io/ai-on-sagemaker-hyperpod/docs/add-ons/Observability/observability-slurm)ã®æ‰‹é †ã§ãƒ•ãƒ«ãƒãƒãƒ¼ã‚¸ãƒ‰ã® Grafana + Prometheus æ§‹æˆã‚’å®Ÿç¾ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚æœ¬ç•ªç’°å¢ƒã§ã¯ã“ã¡ã‚‰ã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚

::::details  1. Open Source Grafana + Amazon Managed Prometheus ç’°å¢ƒã®ãƒ‡ãƒ—ãƒ­ã‚¤

CloudFormation ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¦ OSS Grafana ã¨ Amazon Managed Prometheus ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚

:::message
EC2 ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚°ãƒ«ãƒ¼ãƒ—ã® inbound ã‚¢ã‚¯ã‚»ã‚¹åˆ¶é™ã‚’ã‹ã‘ã‚‹ãŸã‚ã«è¨±å¯ã™ã‚‹ IP ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚
:::

```bash
# ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ä¾å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
curl -sSL https://raw.githubusercontent.com/littlemex/samples/main/ml_distributed_experiment_collection/amazon-sagemaker-hyperpod-slurm-observability/setup_hyperpod_observability_oss.sh -o setup_hyperpod_observability_oss.sh
curl -sSL https://raw.githubusercontent.com/littlemex/samples/main/ml_distributed_experiment_collection/amazon-sagemaker-hyperpod-slurm-observability/ecr-policy.json -o ecr-policy.json
curl -sSL https://raw.githubusercontent.com/littlemex/samples/main/ml_distributed_experiment_collection/amazon-sagemaker-hyperpod-slurm-observability/cluster-observability-with-os-grafana.yaml -o cluster-observability-with-os-grafana.yaml

# å®Ÿè¡Œæ¨©é™ã‚’ä»˜ä¸
chmod +x setup_hyperpod_observability_oss.sh

# åŸºæœ¬å®Ÿè¡Œï¼ˆIP ã‚¢ãƒ‰ãƒ¬ã‚¹è‡ªå‹•æ¤œå‡ºï¼‰
./setup_hyperpod_observability_oss.sh

# ã¾ãŸã¯ç‰¹å®šã® IP ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’æŒ‡å®š
./setup_hyperpod_observability_oss.sh --ip 192.168.1.100
```

ä½œæˆã®å®Œäº†ã‚’å¾…ã£ã¦ Grafana ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ URL ã‚’ç¢ºèªã—ã¾ã—ã‚‡ã†ã€‚

```bash
# ä½œæˆçŠ¶æ³ã®ç¢ºèª
./setup_hyperpod_observability_oss.sh --check-status
==============================================
HyperPod Slurm Open Source Grafana Setup
==============================================
....
ğŸŒ Grafana Access Information:
   URL: http://XXX:3000
   Default Login: admin/admin
   Security: Access restricted to XXX/32

ğŸ”— Direct Browser Access:
   Copy this URL to your browser: http://XXX:3000

ğŸ“‹ Next Steps:
   1. Access Grafana and change default password
   2. Configure Prometheus data source
   3. Import observability dashboards

===============================================
```

## è£œè¶³: SigV4 èªè¨¼ã®è¨­å®š

ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ã¯ã€Grafana ã® SigV4 èªè¨¼ãŒäº‹å‰è¨­å®šã•ã‚Œã¾ã™ã€‚

```bash
# CloudFormation UserData ã§è¨­å®šã•ã‚Œã‚‹ç’°å¢ƒå¤‰æ•°
export AWS_SDK_LOAD_CONFIG=true
export GF_AUTH_SIGV4_AUTH_ENABLED=true

# Grafana è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã§ã® SigV4 æœ‰åŠ¹åŒ–
[auth.sigv4]
enabled = true
verbose_logging = false
```
::::

::::details 2. Grafana ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹

![](/images/books/ml-distributed-experiment-collection/grafana-login.png)

- [ ] ãƒ–ãƒ©ã‚¦ã‚¶ã§å–å¾—ã—ãŸ URL ã«ã‚¢ã‚¯ã‚»ã‚¹
- [ ] ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆèªè¨¼æƒ…å ±ã§ãƒ­ã‚°ã‚¤ãƒ³ï¼š`admin / admin`
- [ ] Grafana ãƒ›ãƒ¼ãƒ ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèª

![](/images/books/ml-distributed-experiment-collection/grafana-home.png)
::::

::::details  3. Grafana ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®è¨­å®š

:::message alert
**é‡è¦**: Grafana ã®é€šå¸¸ã® Prometheus ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã® SigV4 èªè¨¼ã¯**éæ¨å¥¨**ã¨ãªã‚Šã¾ã—ãŸã€‚Amazon Managed Service for Prometheus å°‚ç”¨ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
:::

Grafana ã¨ Amazon Managed Prometheus workspace ã‚’æ¥ç¶šã™ã‚‹ãŸã‚ã® Prometheus ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ URL ã‚’å–å¾—ã—ã¾ã—ã‚‡ã†ã€‚

```bash
# Prometheus ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ URL ã‚’å–å¾—ï¼ˆqueryç”¨URLï¼‰
aws cloudformation describe-stacks \
  --stack-name HyperpodSlurmOSObservability \
  --region us-east-1 \
  --query 'Stacks[0].Outputs[?OutputKey==`PrometheusQueryURL`].OutputValue' \
  --output text

# å‡ºåŠ›ä¾‹: https://aps-workspaces.us-east-1.amazonaws.com/workspaces/ws-97d9f0f9-95bb-4779-9908-d42c2cd2a288/
```

![](/images/books/ml-distributed-experiment-collection/grafana-data-source.png)

- [ ] 1. **Connections > Data Sources** ã‚’é¸æŠ
- [ ] 2. **Add data source** ã‚’ã‚¯ãƒªãƒƒã‚¯

![](/images/books/ml-distributed-experiment-collection/grafana-add-data-source.png)

- [ ] 3. âš ï¸ **Amazon Managed Service for Prometheus** ã‚’é¸æŠï¼ˆé€šå¸¸ã® Prometheus ã§ã¯ãªã„ï¼‰

![](/images/books/ml-distributed-experiment-collection/grafana-add-data-source-connection.png)
![](/images/books/ml-distributed-experiment-collection/grafana-add-data-source-auth.png)
![](/images/books/ml-distributed-experiment-collection/grafana-add-data-source-save.png)

- [ ] 4. **ä»¥ä¸‹ã®è¨­å®šã‚’å…¥åŠ›**ï¼š
   - [ ] **Prometheus server URL**: ä¸Šè¨˜ã§å–å¾—ã—ãŸ URL
   - [ ] **Authentication Provider**: `AWS SDK Default` ã‚’é¸æŠ
   - [ ] **Default Region**: `us-east-1` ã‚’å…¥åŠ›
- [ ] 5. **Save & test** ã§ãƒ†ã‚¹ãƒˆæˆåŠŸã‚’ç¢ºèª
::::

::::details 4. ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ

Prometheus ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¨­å®šå®Œäº†å¾Œã€ä»¥ä¸‹ã®å…¬å¼ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚

![](/images/books/ml-distributed-experiment-collection/grafana-dashboard-new.png)

- [ ] 1. **Dashboards > New > Import** ã‚’é¸æŠ

![](/images/books/ml-distributed-experiment-collection/grafana-dashboard-import.png)

- [ ] 2. ä»¥ä¸‹ã® URL ã‚’é †æ¬¡ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
  - [ ] **Slurm Exporter Dashboard**:
    https://grafana.com/grafana/dashboards/4323-slurm-dashboard/
  - [ ] **Node Exporter Dashboard**:
    https://grafana.com/grafana/dashboards/1860-node-exporter-full/
  - [ ] **DCGM Exporter Dashboard**ï¼ˆGPU ãƒ¡ãƒˆãƒªã‚¯ã‚¹ï¼‰:
    https://grafana.com/grafana/dashboards/12239-nvidia-dcgm-exporter-dashboard/
  - [ ] **FSx for Lustre Dashboard**:
    https://grafana.com/grafana/dashboards/20906-fsx/

![](/images/books/ml-distributed-experiment-collection/grafana-dashboard-import-2.png)
![](/images/books/ml-distributed-experiment-collection/grafana-dashboard-slurm.png)
::::

::::details 5. 


:::message
FSx ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ç”¨ã«ã¯è¿½åŠ ã§ Amazon CloudWatch ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®è¨­å®šãŒå¿…è¦ã§ã™ã€‚åŒæ§˜ã®æ‰‹é †ã§ CloudWatch ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚
:::

**5. ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†çŠ¶æ³ã®ç¢ºèª**

HyperPod ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å´ã§ Observability ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆãŒæ­£å¸¸ã«å‹•ä½œã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚

```bash
# HyperPod ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã« SSH ã§ã‚¢ã‚¯ã‚»ã‚¹
ssh cpu-slurm-cluster

# Slurm Exporter ã‚µãƒ¼ãƒ“ã‚¹çŠ¶æ³ç¢ºèª
systemctl status slurm_exporter.service --no-pager -l

# Docker ã‚³ãƒ³ãƒ†ãƒŠçŠ¶æ³ç¢ºèª  
docker ps

# ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰ã®çŠ¶æ³ç¢ºèª
srun -N 2 docker ps
```

**æ­£å¸¸ãªå‡ºåŠ›ä¾‹**ï¼š
```
â— slurm_exporter.service - Prometheus SLURM Exporter
   Loaded: loaded (/etc/systemd/system/slurm_exporter.service; enabled; vendor preset: enabled)
   Active: active (running) since Thu 2025-09-11 04:27:30 UTC; 1 day 20h ago
Main PID: 2408455 (slurm_exporter)

CONTAINER ID   IMAGE                                                     COMMAND           CREATED       STATUS        NAMES
da773247a262   602401143452.dkr.ecr.us-west-2.amazonaws.com/hyperpod/otel_collector:v1754424030352   "/app/otelcollector"   6 hours ago   Up 6 hours    otel-collector
8c18b89cc1a3   602401143452.dkr.ecr.us-west-2.amazonaws.com/hyperpod/node_exporter:v1.9.1            "/bin/node_exporter"   45 hours ago  Up 45 hours   node-exporter
48396ed3e3ef   602401143452.dkr.ecr.us-west-2.amazonaws.com/hyperpod/dcgm_exporter:4.1.1-4.0.4-ubi9   "/usr/local/dcgm/dcg"  45 hours ago  Up 45 hours   dcgm-exporter
```

**ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£æ©Ÿèƒ½ã®è©³ç´°**:
- **å‹•çš„ IP åˆ¶é™**: ç¾åœ¨ã®ã‚¢ã‚¯ã‚»ã‚¹å…ƒ IP ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’è‡ªå‹•å–å¾—ã—ã€Grafana ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’ç‰¹å®š IP/32 ã«åˆ¶é™
- **CloudFormation Parameter æ–¹å¼**: IP ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’ Parameter ã¨ã—ã¦å®‰å…¨ã«æ¸¡ã—ã€ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆæ”¹å¤‰ã®å¿…è¦æ€§ã‚’æ’é™¤
- **IP ã‚¢ãƒ‰ãƒ¬ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³**: æ‰‹å‹•æŒ‡å®šã•ã‚ŒãŸ IP ã‚¢ãƒ‰ãƒ¬ã‚¹ã®å½¢å¼æ¤œè¨¼
- **SigV4 äº‹å‰è¨­å®š**: Grafana ã® SigV4 èªè¨¼ã‚’ CloudFormation ã§è‡ªå‹•æœ‰åŠ¹åŒ–

**æŠ€è¡“æ”¹è‰¯ç‚¹**:
- **ãƒ­ãƒ¼ã‚«ãƒ«ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ**: å¤–éƒ¨ä¾å­˜ã‚’æ’é™¤ã—ã€æ”¹è‰¯ã•ã‚ŒãŸãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã«é…ç½®
- **é«˜æ€§èƒ½ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹**: m5.xlarge ã¸ã®å¤‰æ›´ã§ Grafana ã®å¿œç­”æ€§èƒ½ã‚’å‘ä¸Š
- **æš—å·åŒ–ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸**: 50GB GP3 æš—å·åŒ– EBS ã«ã‚ˆã‚‹å®‰å…¨ãªãƒ‡ãƒ¼ã‚¿ä¿å­˜
- **å …ç‰¢ãªãƒ‡ãƒ—ãƒ­ã‚¤**: CloudFormation ã‚·ã‚°ãƒŠãƒ«ã«ã‚ˆã‚‹ãƒ‡ãƒ—ãƒ­ã‚¤å®Œäº†ç¢ºèª


### Amazon Managed Grafana è‡ªå‹•åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆç®¡ç†ã‚¢ã‚«ã‚¦ãƒ³ãƒˆç”¨ï¼‰

AWS Organizations ç®¡ç†ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã§åˆ©ç”¨ã™ã‚‹å ´åˆã®å®Œå…¨è‡ªå‹•åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã™ã€‚

```bash
# Amazon Managed Grafana ç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
curl -sSL https://raw.githubusercontent.com/littlemex/samples/main/ml_distributed_experiment_collection/amazon-sagemaker-hyperpod-slurm-observability/setup_hyperpod_observability.sh -o setup_hyperpod_observability.sh
chmod +x setup_hyperpod_observability.sh

# å®Ÿè¡Œ
./setup_hyperpod_observability.sh
```

**ä¸¡ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å…±é€šæ©Ÿèƒ½**:
- **å†ªç­‰æ€§ä¿è¨¼**: å®‰å…¨ã«è¤‡æ•°å›å®Ÿè¡Œå¯èƒ½
- **IAMæ¨©é™ã®è‡ªå‹•è¿½åŠ **: AmazonPrometheusRemoteWriteAccess ã¨ECRã‚¢ã‚¯ã‚»ã‚¹ãƒãƒªã‚·ãƒ¼ã®è¿½åŠ 
- **CloudFormation è‡ªå‹•ãƒ‡ãƒ—ãƒ­ã‚¤**: Prometheus workspace ã¨ Grafana ã®è‡ªå‹•ä½œæˆ
- **ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆæ›´æ–°**: S3ã‹ã‚‰ã®å–å¾—ã€Observabilityè¨­å®šã®æœ‰åŠ¹åŒ–ã€ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
- **ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**: æ—¢å­˜ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã¸ã®Observabilityã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆé…å¸ƒ

**æ®‹ã‚‹æ‰‹å‹•ä½œæ¥­**: Grafana ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã¨ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­å®šãŒå¿…è¦ã§ã™ã€‚

## Prometheus API 404ã‚¨ãƒ©ãƒ¼ã®ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

Amazon SageMaker HyperPod ã® observability å®Ÿè£…ã«ãŠã„ã¦ã€Prometheus API ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ™‚ã«404ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã™ã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚æœ¬ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ã“ã®å•é¡Œã®æ ¹æœ¬åŸå› ã¨è§£æ±ºæ–¹æ³•ã‚’è©³è¿°ã—ã¾ã™ã€‚

### å•é¡Œã®ç—‡çŠ¶

ä»¥ä¸‹ã®ã‚ˆã†ãªç—‡çŠ¶ãŒç™ºç”Ÿã—ã¦ã„ã‚‹å ´åˆã€ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è¨­å®šã«å•é¡ŒãŒã‚ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ï¼š

- Grafana ã§ã€ŒPlease enter a valid URLã€ã‚¨ãƒ©ãƒ¼ãŒè¡¨ç¤º
- Prometheus API ã‚¯ã‚¨ãƒªãŒ404 Not Foundã§å¤±æ•—
- ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ–ãƒ©ã‚¦ã‚¶ãƒ¼ã§ã€ŒNo options foundã€ãŒè¡¨ç¤º
- Explore ç”»é¢ã§ã€ŒNo dataã€ãŒç¶™ç¶š

### æ ¹æœ¬åŸå› ã®åˆ†æ

**AWS å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®è¦æ±‚äº‹é …**
[Amazon Managed Service for Prometheus ã§ä½¿ç”¨ã™ã‚‹ Grafana ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã¾ãŸã¯ Grafana Enterprise ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—](https://docs.aws.amazon.com/ja_jp/prometheus/latest/userguide/AMP-onboard-query-standalone-grafana.html)ã§ã¯ä»¥ä¸‹ãŒæ˜ç¢ºã«è¦æ±‚ã•ã‚Œã¦ã„ã¾ã™ï¼š

1. **å°‚ç”¨ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ä½¿ç”¨**: é€šå¸¸ã®ã€ŒPrometheusã€ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã§ã¯ãªãã€ŒAmazon Managed Service for Prometheusã€ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’ä½¿ç”¨
2. **URL å½¢å¼**: ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ `/api/v1/query` ãƒ‘ã‚¹ã‚’å‰Šé™¤ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
3. **SigV4 èªè¨¼**: AWS SDK Default èªè¨¼ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã‚’ä½¿ç”¨

**æ—¢å­˜ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å•é¡Œç‚¹**
- ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã® URL è¨­å®šãŒæ‰‹å‹•ã§ã‚ã‚Šã€è¨­å®šæ¼ã‚ŒãŒç™ºç”Ÿ
- æ­£ã—ã„ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ã®æŒ‡å®šãŒä¸æ˜ç¢º
- 404ã‚¨ãƒ©ãƒ¼æ™‚ã®è‡ªå‹•å¾©æ—§æ©Ÿèƒ½ãŒæœªå®Ÿè£…

### è‡ªå‹•è§£æ±ºãƒ„ãƒ¼ãƒ«ã®æä¾›

#### 1. Grafana ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è‡ªå‹•è¨­å®šã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```bash
# ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
curl -sSL https://raw.githubusercontent.com/littlemex/samples/main/ml_distributed_experiment_collection/amazon-sagemaker-hyperpod-slurm-observability/configure_grafana_datasource.sh -o configure_grafana_datasource.sh
chmod +x configure_grafana_datasource.sh

# è‡ªå‹•è¨­å®šå®Ÿè¡Œ
./configure_grafana_datasource.sh
```

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã‚’è‡ªå‹•å®Ÿè¡Œã—ã¾ã™ï¼š
- CloudFormation ã‚¹ã‚¿ãƒƒã‚¯ã‹ã‚‰æ­£ã—ã„ Prometheus Query URL ã‚’å–å¾—
- Grafana API çµŒç”±ã§ã®ã€ŒAmazon Managed Service for Prometheusã€ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ä½œæˆ
- AWS SDK Default èªè¨¼ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®è¨­å®š
- ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æ¥ç¶šãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ

#### 2. ãƒ†ã‚¹ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ•å…¥ç’°å¢ƒ

å®Ÿéš›ã«ã‚¯ã‚¨ãƒªå¯èƒ½ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒå­˜åœ¨ã—ãªã„å ´åˆã€ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ•å…¥ç’°å¢ƒã‚’æ§‹ç¯‰ã§ãã¾ã™ï¼š

```bash
# ãƒ†ã‚¹ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ç’°å¢ƒã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
curl -sSL https://raw.githubusercontent.com/littlemex/samples/main/ml_distributed_experiment_collection/amazon-sagemaker-hyperpod-slurm-observability/setup_test_metrics.sh -o setup_test_metrics.sh
chmod +x setup_test_metrics.sh

# ãƒ†ã‚¹ãƒˆç’°å¢ƒèµ·å‹•
./setup_test_metrics.sh start

# ç’°å¢ƒçŠ¶æ³ç¢ºèª
./setup_test_metrics.sh status

# ç’°å¢ƒåœæ­¢
./setup_test_metrics.sh stop
```

**ãƒ†ã‚¹ãƒˆç’°å¢ƒã®æ§‹æˆ**:
- **ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚µãƒ¼ãƒãƒ¼**: http://localhost:8000/metricsï¼ˆPrometheuså½¢å¼ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹é…ä¿¡ï¼‰
- **Prometheus**: http://localhost:9090ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«Prometheusã€AMP remote_writeè¨­å®šæ¸ˆã¿ï¼‰
- **ã‚µãƒ³ãƒ—ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹**: CPUã€ãƒ¡ãƒ¢ãƒªã€ãƒ‡ã‚£ã‚¹ã‚¯ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã€HyperPodé¢¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹

#### 3. åˆ©ç”¨å¯èƒ½ãªãƒ†ã‚¹ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹

æŠ•å…¥ã•ã‚Œã‚‹ã‚µãƒ³ãƒ—ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ä¸€è¦§ï¼š

**ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹**:
```promql
cpu_usage_percent{instance="test-server",cpu="cpu0",job="sample"}
memory_usage_percent{instance="test-server",job="sample"}
disk_read_bytes_total{device="nvme0n1",instance="test-server"}
network_receive_bytes_total{interface="eth0",instance="test-server"}
```

**HyperPod é¢¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹**:
```promql
slurm_jobs_running{cluster="cpu-slurm-cluster",partition="cpu"}
slurm_nodes_total{cluster="cpu-slurm-cluster",state="allocated"}
gpu_utilization_percent{instance="worker-01",gpu_id="0"}
```

### è§£æ±ºæ‰‹é †ã®å®Ÿè¡Œä¾‹

```bash
# Step 1: æ—¢å­˜ç’°å¢ƒç¢ºèª
aws cloudformation describe-stacks --stack-name HyperpodSlurmOSObservability --region us-east-1 --query 'Stacks[0].Outputs'

# Step 2: ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è‡ªå‹•è¨­å®š
./configure_grafana_datasource.sh

# Step 3: ãƒ†ã‚¹ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ•å…¥
./setup_test_metrics.sh start

# Step 4: Grafana ã§ã®ç¢ºèª
# ãƒ–ãƒ©ã‚¦ã‚¶ã§ http://YOUR-GRAFANA-IP:3000/explore ã«ã‚¢ã‚¯ã‚»ã‚¹
# Amazon Managed Service for Prometheus ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’é¸æŠ
# cpu_usage_percent ãªã©ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ã‚¯ã‚¨ãƒª
```

### æ¤œè¨¼çµæœã®ä¾‹

æ­£å¸¸ã«è¨­å®šã•ã‚ŒãŸå ´åˆã€ä»¥ä¸‹ã®ã‚ˆã†ãªçµæœãŒå¾—ã‚‰ã‚Œã¾ã™ï¼š

**ãƒ­ãƒ¼ã‚«ãƒ« Prometheus ã§ã®ç¢ºèª**:
```json
{
  "status":"success",
  "data":{
    "resultType":"vector",
    "result":[
      {
        "metric":{
          "__name__":"cpu_usage_percent",
          "cpu":"cpu0",
          "instance":"host.docker.internal:8000",
          "job":"sample-metrics-test"
        },
        "value":[1766750694.011,"76.53"]
      }
    ]
  }
}
```

**Grafana ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ–ãƒ©ã‚¦ã‚¶ãƒ¼**:
- âœ… `cpu_usage_percent`, `disk_read_bytes_total`, `gpu_utilization_percent` ãªã©
- âœ… ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã‚°ãƒ©ãƒ•è¡¨ç¤ºãŒæ­£å¸¸å‹•ä½œ
- âœ… Amazon Managed Service for Prometheus ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æ¥ç¶šæˆåŠŸ

### æ‰‹å‹•è¨­å®šï¼ˆè©³ç´°åˆ¶å¾¡ãŒå¿…è¦ãªå ´åˆï¼‰

è‡ªå‹•åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ç”¨ã—ãªã„å ´åˆã®æ‰‹å‹•è¨­å®šæ‰‹é †ã§ã™ã€‚

Observability ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã£ã¦ã„ã¾ã™ã€‚

![](https://awslabs.github.io/ai-on-sagemaker-hyperpod/assets/images/observability_architecture-1f511d1934afb3d2ebf7c89c41a31a17.png)

### æ¨å¥¨ãƒ‡ãƒ—ãƒ­ã‚¤æ–¹æ³•

ä»Šå›å®Ÿè¨¼ã—ãŸæˆåŠŸæ‰‹é †ã«åŸºã¥ãæ¨å¥¨æ§‹æˆã§ã™ï¼š

**Open Source Grafana + Amazon Managed Prometheus æ§‹æˆï¼ˆæ¨å¥¨ï¼‰**
- âœ… AWS Organizations ãƒ¡ãƒ³ãƒãƒ¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã§å®Œå…¨å‹•ä½œ
- âœ… IAM Identity Center ä¸è¦
- âœ… å®Œå…¨è‡ªå‹•åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã«ã‚ˆã‚‹æ§‹ç¯‰
- âœ… 404ã‚¨ãƒ©ãƒ¼è‡ªå‹•è§£æ±ºæ©Ÿèƒ½ä»˜ã

**Amazon Managed Grafana + Amazon Managed Prometheus æ§‹æˆï¼ˆç®¡ç†ã‚¢ã‚«ã‚¦ãƒ³ãƒˆé™å®šï¼‰**
- AWS Organizations ç®¡ç†ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã§ã®ã¿åˆ©ç”¨å¯èƒ½
- IAM Identity Center ãŒå¿…è¦
- ãƒ•ãƒ«ãƒãƒãƒ¼ã‚¸ãƒ‰ç’°å¢ƒ

::::details 1. Open Source Grafana + Amazon Managed Prometheus ç’°å¢ƒã®ãƒ‡ãƒ—ãƒ­ã‚¤

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: AWS Organizations ãƒ¡ãƒ³ãƒãƒ¼ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã§ã‚‚åˆ©ç”¨å¯èƒ½ãª Open Source Grafana ã¨ Amazon Managed Prometheus ã®çµ±åˆç’°å¢ƒã‚’è‡ªå‹•æ§‹ç¯‰ã—ã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: CloudFormation ã‚¹ã‚¿ãƒƒã‚¯ãŒ CREATE_COMPLETE çŠ¶æ…‹ã«ãªã‚Šã€Grafana ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã¨ Amazon Managed Prometheus workspace ãŒåˆ©ç”¨å¯èƒ½ã«ãªã‚‹ã“ã¨ã€‚
:::

**è‡ªå‹•åŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã«ã‚ˆã‚‹ä¸€æ‹¬æ§‹ç¯‰**

```bash
# Open Source Grafana ç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
curl -sSL https://raw.githubusercontent.com/littlemex/samples/main/ml_distributed_experiment_collection/amazon-sagemaker-hyperpod-slurm-observability/setup_hyperpod_observability_oss.sh -o setup_hyperpod_observability_oss.sh
chmod +x setup_hyperpod_observability_oss.sh

# äº‹å‰æ¤œè¨¼ï¼ˆæ¨å¥¨ï¼‰
./setup_hyperpod_observability_oss.sh --dry-run

# åŸºæœ¬å®Ÿè¡Œï¼ˆIP ã‚¢ãƒ‰ãƒ¬ã‚¹è‡ªå‹•æ¤œå‡ºï¼‰
./setup_hyperpod_observability_oss.sh

# ç‰¹å®š IP ã‚¢ãƒ‰ãƒ¬ã‚¹æŒ‡å®š
./setup_hyperpod_observability_oss.sh --ip 192.168.1.100
```

**ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒè‡ªå‹•å®Ÿè¡Œã™ã‚‹å†…å®¹**:
- HyperPod ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å­˜åœ¨ç¢ºèª
- IAM æ¨©é™è‡ªå‹•è¿½åŠ ï¼ˆAmazonPrometheusRemoteWriteAccessã€ECR ã‚¢ã‚¯ã‚»ã‚¹ï¼‰
- CloudFormation ã«ã‚ˆã‚‹ Amazon Managed Prometheus workspace ä½œæˆ
- Grafana EC2 ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èµ·å‹•ï¼ˆSigV4 èªè¨¼äº‹å‰è¨­å®šæ¸ˆã¿ï¼‰
- IP ã‚¢ãƒ‰ãƒ¬ã‚¹åˆ¶é™ã«ã‚ˆã‚‹ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¨­å®š
- ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®è‡ªå‹•æ›´æ–°

**å–å¾—ã•ã‚Œã‚‹æ¥ç¶šæƒ…å ±**:
```bash
# CloudFormation ã‚¹ã‚¿ãƒƒã‚¯çŠ¶æ³ç¢ºèª
aws cloudformation describe-stacks \
  --stack-name HyperpodSlurmOSObservability \
  --region us-east-1 \
  --query 'Stacks[0].Outputs'

# Grafana URL ã®è¡¨ç¤ºä¾‹
# http://3.81.149.57:3000
```
::::

::::details 2. Grafana ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®è‡ªå‹•è¨­å®š

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: Grafana ã¨ Amazon Managed Prometheus ã®æ¥ç¶šã‚’è‡ªå‹•è¨­å®šã—ã€404ã‚¨ãƒ©ãƒ¼ã®ç™ºç”Ÿã‚’é˜²æ­¢ã—ã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: Grafana ã§ Amazon Managed Service for Prometheus ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ãŒæ­£å¸¸ã«å‹•ä½œã—ã€æ¥ç¶šãƒ†ã‚¹ãƒˆãŒæˆåŠŸã™ã‚‹ã“ã¨ã€‚
:::

**ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹è‡ªå‹•è¨­å®šã‚¹ã‚¯ãƒªãƒ—ãƒˆ**

```bash
# è‡ªå‹•è¨­å®šã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
curl -sSL https://raw.githubusercontent.com/littlemex/samples/main/ml_distributed_experiment_collection/amazon-sagemaker-hyperpod-slurm-observability/configure_grafana_datasource.sh -o configure_grafana_datasource.sh
chmod +x configure_grafana_datasource.sh

# è‡ªå‹•è¨­å®šå®Ÿè¡Œ
./configure_grafana_datasource.sh
```

**ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒå®Ÿè¡Œã™ã‚‹å‡¦ç†**:
1. CloudFormation ã‹ã‚‰ Prometheus Query URL ã®è‡ªå‹•å–å¾—
2. Grafana API çµŒç”±ã§ã®ã€ŒAmazon Managed Service for Prometheusã€ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ä½œæˆ
3. AWS SDK Default èªè¨¼ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®è¨­å®š
4. ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æ¥ç¶šãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œã¨çµæœç¢ºèª

**æ‰‹å‹•ç¢ºèªæ–¹æ³•**:
```bash
# Grafana ã«ãƒ–ãƒ©ã‚¦ã‚¶ã§ã‚¢ã‚¯ã‚»ã‚¹
# 1. http://YOUR-GRAFANA-IP:3000 ã«ã‚¢ã‚¯ã‚»ã‚¹
# 2. admin/admin ã§ãƒ­ã‚°ã‚¤ãƒ³
# 3. Connections > Data sources ã§è¨­å®šç¢ºèª
# 4. Amazon Managed Service for Prometheus ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã®å­˜åœ¨ç¢ºèª
```
::::

::::details 3. ãƒ†ã‚¹ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ•å…¥ç’°å¢ƒã®æ§‹ç¯‰ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: HyperPod ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‹ã‚‰ã®å®Ÿéš›ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹æŠ•å…¥å‰ã«ã€Grafana ã§ã®ã‚¯ã‚¨ãƒªå‹•ä½œã‚’ç¢ºèªã™ã‚‹ãŸã‚ã®ãƒ†ã‚¹ãƒˆç’°å¢ƒã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: Grafana ã§ã‚µãƒ³ãƒ—ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã‚°ãƒ©ãƒ•è¡¨ç¤ºãŒæˆåŠŸã—ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ–ãƒ©ã‚¦ã‚¶ãƒ¼ã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¸€è¦§ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã“ã¨ã€‚
:::

**çµ±åˆãƒ†ã‚¹ãƒˆç’°å¢ƒã®èµ·å‹•**

```bash
# ãƒ†ã‚¹ãƒˆãƒ¡ãƒˆãƒªã‚¯ã‚¹ç’°å¢ƒã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
curl -sSL https://raw.githubusercontent.com/littlemex/samples/main/ml_distributed_experiment_collection/amazon-sagemaker-hyperpod-slurm-observability/setup_test_metrics.sh -o setup_test_metrics.sh
chmod +x setup_test_metrics.sh

# ãƒ†ã‚¹ãƒˆç’°å¢ƒèµ·å‹•ï¼ˆDocker ä½¿ç”¨ï¼‰
./setup_test_metrics.sh start

# ç’°å¢ƒçŠ¶æ³ç¢ºèª
./setup_test_metrics.sh status
```

**ãƒ†ã‚¹ãƒˆç’°å¢ƒã®æ§‹æˆ**:
- **ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚µãƒ¼ãƒãƒ¼**: http://localhost:8000/metrics
- **Prometheus**: http://localhost:9090
- **Grafana**: CloudFormation ã§ä½œæˆã•ã‚ŒãŸ URL

**ç”Ÿæˆã•ã‚Œã‚‹ã‚µãƒ³ãƒ—ãƒ«ãƒ¡ãƒˆãƒªã‚¯ã‚¹**:
```promql
# ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹
cpu_usage_percent{instance="test-server",cpu="cpu0",job="sample"}
memory_usage_percent{instance="test-server",job="sample"}
disk_read_bytes_total{device="nvme0n1",instance="test-server"}
network_receive_bytes_total{interface="eth0",instance="test-server"}

# HyperPod é¢¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹
slurm_jobs_running{cluster="cpu-slurm-cluster",partition="cpu"}
slurm_nodes_total{cluster="cpu-slurm-cluster",state="allocated"}
gpu_utilization_percent{instance="worker-01",gpu_id="0"}
```

**å‹•ä½œç¢ºèªæ–¹æ³•**:
```bash
# ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç”ŸæˆçŠ¶æ³ç¢ºèª
curl http://localhost:8000/metrics | head -10

# ãƒ­ãƒ¼ã‚«ãƒ« Prometheus ã§ã®ã‚¯ã‚¨ãƒªãƒ†ã‚¹ãƒˆ
curl -s "http://localhost:9090/api/v1/query?query=cpu_usage_percent" | jq .

# ãƒ†ã‚¹ãƒˆç’°å¢ƒåœæ­¢
./setup_test_metrics.sh stop
```
::::

::::details 4. HyperPod ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã¸ã® Observability ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: HyperPod ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®å…¨ãƒãƒ¼ãƒ‰ã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã€Amazon Managed Prometheus ã¸ã®é€ä¿¡ã‚’é–‹å§‹ã—ã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: å…¨ãƒãƒ¼ãƒ‰ã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¿ãƒ¼ï¼ˆNode Exporterã€DCGM Exporterã€Slurm Exporterï¼‰ãŒç¨¼åƒã—ã€Prometheus ã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒé€ä¿¡ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã€‚
:::

**HyperPod ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã§ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**

```bash
# 1. HyperPod ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ãƒ˜ãƒƒãƒ‰ãƒãƒ¼ãƒ‰ã«ã‚¢ã‚¯ã‚»ã‚¹
ssh cpu-slurm-cluster

# 2. ç’°å¢ƒå¤‰æ•°ã®è¨­å®š
export NUM_WORKERS=2  # å®Ÿéš›ã®ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰æ•°ã«èª¿æ•´
export PROMETHEUS_REMOTE_WRITE_URL=$(aws cloudformation describe-stacks \
  --stack-name HyperpodSlurmOSObservability \
  --region us-east-1 \
  --query 'Stacks[0].Outputs[?OutputKey==`PrometheusRemoteWriteURL`].OutputValue' \
  --output text)
export ARG_ADVANCED=--advanced

# 3. Observability ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æº–å‚™
mkdir -p ~/observability-setup
cd ~/observability-setup
git clone https://github.com/aws-samples/awsome-distributed-training.git
cd awsome-distributed-training/1.architectures/5.sagemaker-hyperpod/LifecycleScripts/base-config/observability

# 4. æ—¢å­˜ã‚µãƒ¼ãƒ“ã‚¹ã®åœæ­¢ï¼ˆå†ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ™‚ï¼‰
sudo python3 stop_observability.py --node-type controller || true
srun -N $NUM_WORKERS sudo python3 stop_observability.py --node-type compute || true

# 5. Observability ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
sudo python3 install_observability.py --node-type controller --prometheus-remote-write-url $PROMETHEUS_REMOTE_WRITE_URL $ARG_ADVANCED
srun -N $NUM_WORKERS sudo python3 install_observability.py --node-type compute --prometheus-remote-write-url $PROMETHEUS_REMOTE_WRITE_URL $ARG_ADVANCED
```

**ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª**:
```bash
# Controller ãƒãƒ¼ãƒ‰ï¼ˆãƒ˜ãƒƒãƒ‰ãƒãƒ¼ãƒ‰ï¼‰ã§ã®ç¢ºèª
systemctl status slurm_exporter.service --no-pager -l
docker ps

# Worker ãƒãƒ¼ãƒ‰ã§ã®ç¢ºèª
srun -N $NUM_WORKERS docker ps
srun -N $NUM_WORKERS systemctl is-active node_exporter
```

**æˆåŠŸæ™‚ã®å‡ºåŠ›ä¾‹**:
```
â— slurm_exporter.service - Prometheus SLURM Exporter
   Active: active (running)

CONTAINER ID   IMAGE                    COMMAND              STATUS
da773247a262   hyperpod/otel_collector  "/app/otelcollector" Up 6 hours
8c18b89cc1a3   hyperpod/node_exporter   "/bin/node_exporter" Up 45 hours
```
::::

::::details 5. å‹•ä½œç¢ºèªã¨ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰è¨­å®š

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: æ§‹ç¯‰ã—ãŸ Observability ç’°å¢ƒã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®åé›†ãƒ»è¡¨ç¤ºãŒæ­£å¸¸ã«å‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã€ç›£è¦–ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã‚’è¨­å®šã—ã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: Grafana ã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¯ã‚¨ãƒªãŒæˆåŠŸã—ã€å„ç¨®ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ãŒå¯èƒ½ã«ãªã‚‹ã“ã¨ã€‚
:::

**Grafana ã§ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç¢ºèª**

1. **Grafana ã‚¢ã‚¯ã‚»ã‚¹**
```bash
# Grafana URL ã®å–å¾—
GRAFANA_URL=$(aws cloudformation describe-stacks \
  --stack-name HyperpodSlurmOSObservability \
  --region us-east-1 \
  --query 'Stacks[0].Outputs[?OutputKey==`GrafanaInstanceAddress`].OutputValue' \
  --output text)

echo "Grafana URL: $GRAFANA_URL"
# ãƒ–ãƒ©ã‚¦ã‚¶ã§è©²å½“ URL ã«ã‚¢ã‚¯ã‚»ã‚¹ï¼ˆadmin/admin ã§ãƒ­ã‚°ã‚¤ãƒ³ï¼‰
```

2. **ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚¯ã‚¨ãƒªãƒ†ã‚¹ãƒˆ**
- **Explore ãƒšãƒ¼ã‚¸**ï¼ˆ/exploreï¼‰ã«ã‚¢ã‚¯ã‚»ã‚¹
- **Amazon Managed Service for Prometheus** ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‚’é¸æŠ
- **ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ–ãƒ©ã‚¦ã‚¶ãƒ¼**ã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¸€è¦§ç¢ºèª
- **ã‚µãƒ³ãƒ—ãƒ«ã‚¯ã‚¨ãƒªå®Ÿè¡Œ**:
  ```promql
  # CPU ä½¿ç”¨ç‡
  cpu_usage_percent
  
  # Slurm ã‚¸ãƒ§ãƒ–çŠ¶æ³
  slurm_jobs_running
  
  # GPU ä½¿ç”¨ç‡ï¼ˆGPU ãƒãƒ¼ãƒ‰å­˜åœ¨æ™‚ï¼‰
  gpu_utilization_percent
  ```

3. **ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ**

**æ¨å¥¨ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰**:
```bash
# Grafana UI ã§ä»¥ä¸‹ã‚’é †æ¬¡ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
# Dashboards > New > Import ã§ URL ã‚’å…¥åŠ›

# Node Exporter Dashboard (ID: 1860)
https://grafana.com/grafana/dashboards/1860-node-exporter-full/

# Slurm Dashboard (ID: 4323)
https://grafana.com/grafana/dashboards/4323-slurm-dashboard/

# DCGM Exporter Dashboard (ID: 12239) - GPU ãƒãƒ¼ãƒ‰ç”¨
https://grafana.com/grafana/dashboards/12239-nvidia-dcgm-exporter-dashboard/
```

4. **ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã®ç¢ºèª**

```bash
# Amazon Managed Prometheus ã§ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ç¢ºèª
aws amp query-metrics \
  --workspace-id $(aws cloudformation describe-stacks \
    --stack-name HyperpodSlurmOSObservability \
    --region us-east-1 \
    --query 'Stacks[0].Outputs[?OutputKey==`PrometheusWorkspaceId`].OutputValue' \
    --output text | sed 's|.*workspace/||') \
  --query "up" \
  --start-time $(date -u -d '5 minutes ago' +%Y-%m-%dT%H:%M:%S.%3NZ) \
  --end-time $(date -u +%Y-%m-%dT%H:%M:%S.%3NZ) \
  --region us-east-1
```

**æˆåŠŸæ™‚ã®ç¢ºèªé …ç›®**:
- âœ… Grafana ã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ–ãƒ©ã‚¦ã‚¶ãƒ¼ã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¸€è¦§è¡¨ç¤º
- âœ… Explore ã§ã®ã‚¯ã‚¨ãƒªå®Ÿè¡Œã¨ã‚°ãƒ©ãƒ•è¡¨ç¤ºæˆåŠŸ
- âœ… ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ãƒ‡ãƒ¼ã‚¿è¡¨ç¤º
- âœ… HyperPod ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‹ã‚‰ Amazon Managed Prometheus ã¸ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹é€ä¿¡ç¢ºèª

**ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°**:
ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒè¡¨ç¤ºã•ã‚Œãªã„å ´åˆã¯ã€ä»¥ä¸‹ã‚’ç¢ºèªï¼š
```bash
# HyperPod ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã§ã®ã‚µãƒ¼ãƒ“ã‚¹çŠ¶æ³
ssh cpu-slurm-cluster "systemctl status slurm_exporter.service"
ssh cpu-slurm-cluster "docker ps"

# Prometheus ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã¸ã®é€ä¿¡ç¢ºèª
ssh cpu-slurm-cluster "docker logs otel-collector | grep -i error"
```
::::

## ã¾ã¨ã‚

æœ¬ç« ã§ã¯ã€Amazon SageMaker HyperPod ã® Slurm ç’°å¢ƒã«ãŠã‘ã‚‹ observability æ©Ÿèƒ½ã‚’è¿½åŠ ã—ã¾ã—ãŸã€‚

Amazon Managed Prometheus ã¨ Grafana ã‚’ç”¨ã„ãŸçµ±åˆç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚Šã€éšœå®³ã®äºˆå…†æ¤œå‡ºã‹ã‚‰å¾©æ—§ãƒ—ãƒ­ã‚»ã‚¹ã®å¯è¦–åŒ–ã¾ã§ã€åŒ…æ‹¬çš„ãª observability ãŒå®Ÿç¾ã•ã‚Œã¾ã™ã€‚ç‰¹ã« GPU æ¸©åº¦ç›£è¦–ã«ã‚ˆã‚‹äºˆé˜²çš„ã‚¢ãƒ©ãƒ¼ãƒˆã¯ã€æ·±åˆ»ãªéšœå®³ã‚’æœªç„¶ã«é˜²ãæœ‰åŠ¹ãªæ‰‹æ®µã¨ã—ã¦æ©Ÿèƒ½ã—ã¾ã™ã€‚å¤šå±¤çš„ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã«ã‚ˆã‚Šã€ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã€ãƒãƒ¼ãƒ‰ã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å„ãƒ¬ãƒ™ãƒ«ã§ã®å•é¡Œã‚’è¿…é€Ÿã«ç‰¹å®šã§ãã¾ã™ã€‚æ¬¡å›ã¯ç¶šãã¨ã—ã¦ä»Šå›å°å…¥ã—ãŸ observability æ©Ÿèƒ½ã‚’æ´»ç”¨ã—ã¦ã„ãã¾ã—ã‚‡ã†ã€‚
