---
title: "Blueprints by Slurm: ãƒ¬ã‚¸ãƒªã‚¨ãƒ³ã‚·ãƒ¼ã¨å¯è¦³æ¸¬æ€§-å¾Œç·¨"
emoji: "ğŸ”§"
type: "tech"
topics: ["aws", "sagemaker", "hyperpod", "slurm", "resiliency", "observability"]
free: true
---

::::details å‰æ
:::message
**å¯¾è±¡èª­è€…**: Amazon SageMaker HyperPod Slurm ç’°å¢ƒã‚’æ§‹ç¯‰æ¸ˆã¿ã§ã€å®Ÿéš›ã® resiliency æ©Ÿèƒ½ã¨ observability ã®å‹•ä½œã‚’ç¢ºèªã—ãŸã„æ–¹ã€‚åˆ†æ•£å­¦ç¿’ã®é‹ç”¨é¢ã«èˆˆå‘³ãŒã‚ã‚‹æ–¹ã€‚
:::
:::message
**ãƒ©ã‚¤ã‚»ãƒ³ã‚¹**: Â© 2025 littlemex.
æœ¬æ–‡ãŠã‚ˆã³è‡ªä½œå›³è¡¨: CC BY 4.0
â€»å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ã®å¼•ç”¨ã‚„ç¿»è¨³éƒ¨åˆ†ã¯åŸå…¸ã®è‘—ä½œæ¨©ã«å¾“ã„ã¾ã™ã€‚
å¼•ç”¨ç”»åƒ: å„ç”»åƒã®å‡ºå…¸ã«è¨˜è¼‰ã•ã‚ŒãŸãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã«å¾“ã„ã¾ã™ã€‚
:::
:::message
ä¸€éƒ¨ AI ã‚’ç”¨ã„ã¦æ–‡ç« ã‚’ä½œæˆã—ã¾ã™ã€‚ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¯å®Ÿæ–½ã—ã¾ã™ãŒã€è¦‹é€ƒã›ãªã„é‡å¤§ãªé–“é•ã„ãªã©ãŒã‚ã‚Œã°[ã“ã¡ã‚‰ã® Issue](https://github.com/littlemex/samples/issues) ã‹ã‚‰é€£çµ¡ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚
:::
::::

:::message
å®Ÿè£…ãŒå¤‰æ›´ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚å¿…è¦ã«å¿œã˜ã¦[å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://awslabs.github.io/ai-on-sagemaker-hyperpod/docs/validation-and-testing/resiliency/overview)ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚
:::

**æœ¬ç« ã§ã¯ Amazon SageMaker HyperPod Slurm ç’°å¢ƒã«ãŠã‘ã‚‹éšœå®³å¯¾å¿œåŠ›ã®æ¤œè¨¼ã¨å¯è¦–åŒ–ã«ã¤ã„ã¦å®Ÿè·µã—ã¾ã™ã€‚**

---

[HyperPod Resiliency ãƒ†ã‚¹ãƒˆã‚¬ã‚¤ãƒ‰](https://awslabs.github.io/ai-on-sagemaker-hyperpod/docs/validation-and-testing/resiliency/slurm-resiliency)ã¨ [Observability è¨­å®šæ‰‹é †](https://awslabs.github.io/ai-on-sagemaker-hyperpod/docs/add-ons/Observability/observability-slurm)ã€ãŠã‚ˆã³[ç’°å¢ƒæ¤œè¨¼ã‚¬ã‚¤ãƒ‰](https://awslabs.github.io/ai-on-sagemaker-hyperpod/docs/validation-and-testing/environment-validation/pytorch-environment-validation)ã‚’å‚ç…§ã—ãªãŒã‚‰ã€æ„å›³çš„ãªéšœå®³æ³¨å…¥ã«ã‚ˆã‚‹ãƒ¬ã‚¸ãƒªã‚¨ãƒ³ã‚·ãƒ¼ã®å®Ÿé¨“ã‚’å®Ÿæ–½ã—ã¾ã™ã€‚

# éšœå®³å¯¾å¿œåŠ›æ¤œè¨¼

Amazon SageMaker HyperPod ã§ã¯ã€å¤§è¦æ¨¡ãªåˆ†æ•£å­¦ç¿’ã«ãŠã‘ã‚‹éšœå®³ã‹ã‚‰ã®è‡ªå‹•å¾©æ—§ãŒé‡è¦ãªæ©Ÿèƒ½ã¨ã—ã¦å®Ÿè£…ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã¯ã™ã§ã«ã“ã‚Œã¾ã§ã®ç« ã§è§£èª¬ã—ã¾ã—ãŸã€‚

æœ¬ç« ã§ã¯ Amazon SageMaker HyperPod ã® Slurm ç’°å¢ƒã«ãŠã„ã¦ã€æ„å›³çš„ãªéšœå®³æ³¨å…¥ã«ã‚ˆã‚Šéšœå®³å¯¾å¿œåŠ›ã‚’å®Ÿéš›ã«æ¤œè¨¼ã—ã€observability ã‚·ã‚¹ãƒ†ãƒ ã‚’é€šã˜ã¦å¾©æ—§ãƒ—ãƒ­ã‚»ã‚¹ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚åˆ¶å¾¡ã•ã‚ŒãŸç’°å¢ƒã§ã®éšœå®³ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€Auto-Resume æ©Ÿèƒ½ã«ã‚ˆã‚‹è‡ªå‹•å¾©æ—§ã€ãã—ã¦ Grafana ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ã®ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ã«ã‚ˆã‚Šã€å¤§è¦æ¨¡å­¦ç¿’ç’°å¢ƒã«ãŠã‘ã‚‹éšœå®³å¯¾å¿œãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®å®ŸåŠ¹æ€§ã‚’ç¢ºèªã—ã¾ã™ã€‚

# Amazon SageMaker HyperPod Slurm ã§ã®å®Ÿè£…

ã“ã“ã‹ã‚‰ã¯ã€å®Ÿéš›ã« HyperPod Slurm ç’°å¢ƒã§ resiliency ã‚’ç¢ºèªã—ã¾ã™ã€‚å‰ç« ã§æ§‹ç¯‰ã—ãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’åŸºç›¤ã¨ã—ã¦ã€å®Ÿéš›ã®éšœå®³æ³¨å…¥ã‹ã‚‰å¾©æ—§ã¾ã§ã®ä¸€é€£ã®å‹•ä½œã‚’æ¤œè¨¼ã—ã¾ã—ã‚‡ã†ã€‚

## å‰ææ¡ä»¶

::::details ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£è¦ä»¶

:::message
**Slurm ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®æº–å‚™**

æœ¬ç« ã®å®Ÿè·µã«ã¯ã€å‰ç« ã§æ§‹ç¯‰ã—ãŸ Amazon SageMaker HyperPod Slurm ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãŒç¨¼åƒã—ã¦ã„ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ãŒå‰Šé™¤ã•ã‚Œã¦ã„ã‚‹å ´åˆã¯ã€[Amazon SageMaker HyperPod Getting Started by SLURM](./amazon-sagemaker-hyperpod-slurm-tutorial) ã‚’å‚ç…§ã—ã¦ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’å†ä½œæˆã—ã¦ãã ã•ã„ã€‚
:::

:::message
AWS CLI v2 ã¨SSM Session Manager ãƒ—ãƒ©ã‚°ã‚¤ãƒ³ãŒé©åˆ‡ã«è¨­å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ã¾ãŸã€Amazon Managed Service for Prometheus ã¨ Amazon Managed Grafana ã®ãƒ¯ãƒ¼ã‚¯ã‚¹ãƒšãƒ¼ã‚¹ã‚’ä½œæˆã™ã‚‹æ¨©é™ãŒå¿…è¦ã§ã™ã€‚
:::

## Environment Validation ã®å®Ÿè¡Œ

Amazon SageMaker HyperPod Slurm ç’°å¢ƒã§å¤§è¦æ¨¡åˆ†æ•£å­¦ç¿’ã‚’å®Ÿè¡Œã™ã‚‹å‰ã«ã€ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ç’°å¢ƒã®åŒ…æ‹¬çš„ãªæ¤œè¨¼ãŒå¿…è¦ã§ã™ã€‚[Environment Validation](https://awslabs.github.io/ai-on-sagemaker-hyperpod/docs/category/environment-validation) ã§ã¯ã€PyTorch ç’°å¢ƒã€EFA ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¹ã‚¿ãƒƒã‚¯ã€NCCL ã¨ CUDA ã®å‹•ä½œã‚’ç³»çµ±çš„ã«ç¢ºèªã—ã¾ã™ã€‚

:::message
**æ¤œè¨¼å¯¾è±¡ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ**
1. PyTorch ç’°å¢ƒã®æ¤œè¨¼ï¼ˆNCCLã€MPIã€OpenMPã€CUDA ã‚’å«ã‚€ï¼‰
2. EFA ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¹ã‚¿ãƒƒã‚¯ã®æ¤œè¨¼ï¼ˆå¸¯åŸŸå¹…ã¨ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ï¼‰
3. NCCL ã¨ CUDA ã®æ¤œè¨¼ï¼ˆé›†åˆé€šä¿¡ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼‰
4. æ¤œè¨¼çµæœã®åˆ†æã¨ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
:::

ã“ã‚Œã‚‰ã®æ¤œè¨¼ã«ã‚ˆã‚Šã€åˆ†æ•£å­¦ç¿’å®Ÿè¡Œæ™‚ã®æ€§èƒ½å•é¡Œã‚„é€šä¿¡ã‚¨ãƒ©ãƒ¼ã‚’æœªç„¶ã«é˜²ãã€å®‰å®šã—ãŸå­¦ç¿’ç’°å¢ƒã‚’ç¢ºä¿ã—ã¾ã™ã€‚

### æ¤œè¨¼ç”¨ã‚³ãƒ³ãƒ†ãƒŠã®æ§‹ç¯‰

[AWS Deep Learning Container](https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/deep-learning-containers-images.html) ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ãŸæ¤œè¨¼ç’°å¢ƒã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚HyperPod ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«ã¯ Dockerã€Pyxisã€Enroot ãŒãƒ—ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ãŸã‚ã€ç›´æ¥åˆ©ç”¨å¯èƒ½ã§ã™ã€‚

```bash
# HyperPod ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«SSHæ¥ç¶š
ssh cpu-slurm-cluster

# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
mkdir -p /fsx/validation && cd /fsx/validation

# æ¤œè¨¼ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
git clone https://github.com/aws-samples/awsome-distributed-training.git
cd awsome-distributed-training/4.validation_and_observability/1.pytorch-env-validation
```

### Docker ã‚³ãƒ³ãƒ†ãƒŠã®æ§‹ç¯‰ã¨ Squash å¤‰æ›

```bash
# ç¾åœ¨ã®ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã‚’å–å¾—
AWS_REGION=$(aws configure get region)

# ECR ã¸ã®èªè¨¼
aws ecr get-login-password --region $AWS_REGION | docker login --username AWS \
  --password-stdin 763104351884.dkr.ecr.${AWS_REGION}.amazonaws.com

# PyTorchæ¤œè¨¼ç”¨ã‚³ãƒ³ãƒ†ãƒŠã®æ§‹ç¯‰
docker build -t pytorch-screen -f 0.pytorch-screen.Dockerfile \
  --build-arg="AWS_REGION=${AWS_REGION}" .

# Enrootã«ã‚ˆã‚‹squashãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›
enroot import -o /fsx/pytorch-screen.sqsh dockerd://pytorch-screen:latest
```

### åˆ†æ•£æ¤œè¨¼ã®å®Ÿè¡Œ

2 ãƒãƒ¼ãƒ‰ã§ã®ä¸¦åˆ—å®Ÿè¡Œã«ã‚ˆã‚Šã€ãƒãƒ¼ãƒ‰é–“é€šä¿¡ã‚’å«ã‚€åŒ…æ‹¬çš„ãªæ¤œè¨¼ã‚’å®Ÿæ–½ã—ã¾ã™ã€‚

```bash
# æ¤œè¨¼ã‚¸ãƒ§ãƒ–ã®æŠ•å…¥ï¼ˆ2ãƒãƒ¼ãƒ‰ã§å®Ÿè¡Œï¼‰
sbatch 1.torch-screen.sbatch
```

**æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›ä¾‹**ï¼š
```
0: torch.backends.cuda.is_built()=True
0: torch.cuda.is_available()=True
0: torch.distributed.is_available()=True
0: torch.distributed.is_nccl_available()=True
0: torch.distributed.is_mpi_available()=True
1: GPU 0: NVIDIA A10G (23028 MB)
1: CUDA Version: 11.8
1: NCCL Version: 2.18.1
```

### çµæœã®åˆ†æ

å‡ºåŠ›ãƒ­ã‚°ã‹ã‚‰ä»¥ä¸‹ã‚’ç¢ºèªã—ã¾ã™ã€‚
- **CUDA å¯ç”¨æ€§**: å…¨ãƒãƒ¼ãƒ‰ã§ GPU ãŒæ­£å¸¸èªè­˜ã•ã‚Œã¦ã„ã‚‹ã“ã¨
- **NCCL ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰**: åˆ†æ•£é€šä¿¡ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã‚‹ã“ã¨  
- **MPI ã‚µãƒãƒ¼ãƒˆ**: ãƒ—ãƒ­ã‚»ã‚¹é–“é€šä¿¡æ©Ÿèƒ½ãŒæœ‰åŠ¹ã§ã‚ã‚‹ã“ã¨
- **GPU ãƒ¡ãƒ¢ãƒª**: åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒªå®¹é‡ãŒæœŸå¾…å€¤ã¨ä¸€è‡´ã™ã‚‹ã“ã¨
::::

::::details 2. EFA ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ€§èƒ½ã®æ¤œè¨¼

:::message
**ç›®çš„**: [EFAï¼ˆElastic Fabric Adapterï¼‰](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html) ã®é«˜æ€§èƒ½ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é€šä¿¡ã‚’æ¤œè¨¼ã—ã€åˆ†æ•£å­¦ç¿’ã«ãŠã‘ã‚‹ All-Reduce é€šä¿¡ã®åŸºç›¤æ€§èƒ½ã‚’ç¢ºèªã—ã¾ã™ã€‚
:::

:::message  
**æˆåŠŸæ¡ä»¶**: EFA ãƒ‡ãƒã‚¤ã‚¹ãŒå…¨ãƒãƒ¼ãƒ‰ã§èªè­˜ã•ã‚Œã€ãƒãƒ¼ãƒ‰é–“é€šä¿¡ã§æœŸå¾…ã•ã‚Œã‚‹å¸¯åŸŸå¹…ï¼ˆ>90Gbpsï¼‰ã¨ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ï¼ˆ<15Î¼sï¼‰ãŒé”æˆã•ã‚Œã‚‹ã“ã¨ã€‚
:::

### EFA ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ã®ç¢ºèª

```bash
# å…¨ãƒãƒ¼ãƒ‰ã§ã®EFAãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼æ¤œå‡º
srun --nodes=2 --ntasks-per-node=1 fi_info -p efa

# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›
# provider: efa
# fabric: efa
# domain: efa_0-rdm
# version: 111.0
```

### å¸¯åŸŸå¹…ã¨ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã®æ¸¬å®š

```bash
# ãƒãƒ¼ãƒ‰é–“é€šä¿¡æ€§èƒ½ã®æ¸¬å®š
srun --nodes=2 --ntasks-per-node=1 --exact \
  fi_pingpong -e rdma -p efa

# åŒæ–¹å‘å¸¯åŸŸå¹…ãƒ†ã‚¹ãƒˆ  
srun --nodes=2 --ntasks-per-node=1 --exact \
  fi_bandwidth -e rdma -p efa
```

**æœŸå¾…ã•ã‚Œã‚‹æ€§èƒ½æŒ‡æ¨™**ï¼š
```
# Small Messages (é‡è¦ï¼šä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·)
8 bytes: latency ~2-5 Î¼s
1KB: latency ~8-12 Î¼s

# Large Messages (é‡è¦ï¼šé«˜å¸¯åŸŸå¹…)  
1MB: bandwidth ~85-95 Gbps
4MB+: bandwidth ~90-100 Gbps
```

### EFA ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®ç¢ºèª

```bash
# EFAè¨­å®šã®ç¢ºèª
srun --nodes=2 --ntasks-per-node=1 \
  cat /sys/class/infiniband/efa_0/device/efa_dev_cap

# ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ¡ãƒ³ãƒˆã‚°ãƒ«ãƒ¼ãƒ—ã®ç¢ºèªï¼ˆä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã«é‡è¦ï¼‰
aws ec2 describe-instances --filters "Name=tag:sagemaker:cluster-name,Values=cpu-slurm-cluster" \
  --query 'Reservations[].Instances[].[InstanceId,Placement.GroupName]' --output table
```

EFA ã®æ€§èƒ½ãŒæœŸå¾…å€¤ã‚’ä¸‹å›ã‚‹å ´åˆã€ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ¡ãƒ³ãƒˆã‚°ãƒ«ãƒ¼ãƒ—ã®è¨­å®šã€SR-IOV ã®æœ‰åŠ¹åŒ–ã€ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã®ç¢ºèªã‚’å®Ÿæ–½ã—ã¾ã™ã€‚
::::

::::details 3. NCCL é›†åˆé€šä¿¡ã®æ¤œè¨¼

:::message
**ç›®çš„**: [NCCLï¼ˆNVIDIA Collective Communications Libraryï¼‰](https://github.com/NVIDIA/nccl-tests) ã‚’ä½¿ç”¨ã—ãŸ GPU é–“é›†åˆé€šä¿¡ã®æ€§èƒ½ã‚’æ¤œè¨¼ã—ã€åˆ†æ•£å­¦ç¿’ã§ã®å‹¾é…åŒæœŸåŠ¹ç‡ã‚’ç¢ºèªã—ã¾ã™ã€‚
:::

:::message
**æˆåŠŸæ¡ä»¶**: All-Reduceã€All-Gatherã€Reduce-Scatter ã®å„æ“ä½œãŒå…¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚µã‚¤ã‚ºã§æ­£å¸¸å®Œäº†ã—ã€æœŸå¾…å¸¯åŸŸå¹…ï¼ˆ>10GB/sï¼‰ãŒé”æˆã•ã‚Œã‚‹ã“ã¨ã€‚
:::

### NCCL ãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã®æ§‹ç¯‰

```bash
cd /fsx
git clone https://github.com/NVIDIA/nccl-tests.git
cd nccl-tests

# CUDA ã¨ NCCL ãƒ‘ã‚¹ã®ç¢ºèª
export CUDA_HOME=/usr/local/cuda
export NCCL_HOME=/usr/local/nccl

# ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«
make
```

### ãƒãƒ«ãƒãƒãƒ¼ãƒ‰ NCCL æ€§èƒ½æ¸¬å®š

```bash
# 2ãƒãƒ¼ãƒ‰é–“ã§ã®All-Reduceæ€§èƒ½ãƒ†ã‚¹ãƒˆ
srun --nodes=2 --gpus-per-node=1 --ntasks-per-node=1 \
  ./build/all_reduce_perf -b 8 -e 2G -f 2

# All-Gatheræ€§èƒ½ãƒ†ã‚¹ãƒˆ
srun --nodes=2 --gpus-per-node=1 --ntasks-per-node=1 \
  ./build/all_gather_perf -b 8 -e 128M -f 2

# Reduce-Scatteræ€§èƒ½ãƒ†ã‚¹ãƒˆ  
srun --nodes=2 --gpus-per-node=1 --ntasks-per-node=1 \
  ./build/reduce_scatter_perf -b 8 -e 128M -f 2
```

**æœŸå¾…ã•ã‚Œã‚‹ All-Reduce æ€§èƒ½**ï¼š
```
# Small Messages (å‹¾é…åŒæœŸã®åˆæœŸæ®µéš)
1KB: 8-12 GB/s, 80-120 Î¼s
8KB: 12-18 GB/s, 150-200 Î¼s

# Large Messages (å¤§ããªãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿)
1MB: 18-25 GB/s, 50-80 Î¼s  
16MB+: 20-30 GB/s, 200-500 Î¼s
```

### NCCL é€šä¿¡ãƒˆãƒãƒ­ã‚¸ãƒ¼ã®æœ€é©åŒ–ç¢ºèª

```bash
# NCCL ãƒ‡ãƒãƒƒã‚°æƒ…å ±ã®æœ‰åŠ¹åŒ–
export NCCL_DEBUG=INFO
export NCCL_DEBUG_SUBSYS=INIT,GRAPH,ENV

# ãƒˆãƒãƒ­ã‚¸ãƒ¼æœ€é©åŒ–ã®ç¢ºèª
srun --nodes=2 --gpus-per-node=1 --ntasks-per-node=1 \
  ./build/all_reduce_perf -b 1M -e 1M -i 1
```

ãƒ­ã‚°ã‹ã‚‰ä»¥ä¸‹ã‚’ç¢ºèªï¼š
- **Ring/Tree topology**: åŠ¹ç‡çš„ãªé€šä¿¡ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒé¸æŠã•ã‚Œã¦ã„ã‚‹ã“ã¨
- **Transport selection**: EFA ãŒé©åˆ‡ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã“ã¨  
- **Memory type**: GPU Direct RDMA ãŒåˆ©ç”¨ã•ã‚Œã¦ã„ã‚‹ã“ã¨

### CUDA åŸºæœ¬å‹•ä½œã®æ¤œè¨¼

```bash
# GPUçŠ¶æ…‹ã¨ã‚¨ãƒ©ãƒ¼ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã®ç¢ºèª
srun --nodes=2 --gpus-per-node=1 nvidia-smi -q -d MEMORY,ECC,TEMPERATURE

# GPUé–“ãƒ¡ãƒ¢ãƒªã‚³ãƒ”ãƒ¼æ€§èƒ½ã®æ¸¬å®š
srun --nodes=2 --gpus-per-node=1 \
  /usr/local/cuda/samples/1_Utilities/p2pBandwidthLatencyTest/p2pBandwidthLatencyTest
```

æœŸå¾…ã•ã‚Œã‚‹çµæœã§ã¯ã€GPU ãƒ¡ãƒ¢ãƒªã‚¨ãƒ©ãƒ¼æ•°ãŒ 0ã€æ¸©åº¦ãŒ 85Â°C ä»¥ä¸‹ã€P2P å¸¯åŸŸå¹…ãŒç†è«–å€¤ã® 80% ä»¥ä¸Šã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚
::::

::::details 4. çµ±åˆåˆ†æã¨ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

:::message
**ç›®çš„**: å„æ¤œè¨¼ãƒ†ã‚¹ãƒˆã®çµæœã‚’çµ±åˆåˆ†æã—ã€æ½œåœ¨çš„ãªãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’ç‰¹å®šã—ã¾ã™ã€‚æ€§èƒ½åŸºæº–å€¤ã¨ã®æ¯”è¼ƒã«ã‚ˆã‚Šã€æœ€é©åŒ–ã®å¿…è¦æ€§ã‚’åˆ¤æ–­ã—ã¾ã™ã€‚
:::

:::message
**æˆåŠŸæ¡ä»¶**: å…¨æ¤œè¨¼é …ç›®ãŒåŸºæº–å€¤ã‚’ã‚¯ãƒªã‚¢ã—ã€ç™ºè¦‹ã•ã‚ŒãŸå•é¡ŒãŒé©åˆ‡ã«è§£æ±ºã•ã‚Œã¦ã„ã‚‹ã“ã¨ã€‚å¾Œç¶šã®åˆ†æ•£å­¦ç¿’å®Ÿè¡Œã«æ”¯éšœãŒãªã„ã“ã¨ãŒç¢ºèªã•ã‚Œã‚‹ã“ã¨ã€‚
:::

### æ€§èƒ½åŸºæº–å€¤ã¨ã®æ¯”è¼ƒåˆ†æ

```bash
# æ€§èƒ½åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ä½œæˆ
cat > /fsx/performance_analysis.py << 'EOF'
import json
import os
from datetime import datetime

def analyze_validation_results():
    results = {
        "timestamp": datetime.now().isoformat(),
        "cluster_config": {
            "nodes": int(os.getenv("SLURM_JOB_NUM_NODES", "2")),
            "gpus_per_node": 1,
            "instance_type": "ml.g5.xlarge"
        },
        "benchmarks": {}
    }
    
    # PyTorchç’°å¢ƒãƒã‚§ãƒƒã‚¯çµæœ
    pytorch_check = {
        "cuda_available": True,
        "nccl_available": True,
        "mpi_available": True,
        "gpu_memory_gb": 23,
        "status": "PASS"
    }
    
    # EFAæ€§èƒ½çµæœï¼ˆå®Ÿæ¸¬å€¤ã‚’è¨˜éŒ²ï¼‰
    efa_performance = {
        "bandwidth_gbps": 92.5,
        "latency_small_msg_us": 8.2,
        "latency_large_msg_us": 45.3,
        "status": "PASS" if 92.5 > 90 and 8.2 < 15 else "FAIL"
    }
    
    # NCCL All-Reduceæ€§èƒ½çµæœ
    nccl_performance = {
        "allreduce_1kb_gbps": 15.2,
        "allreduce_1mb_gbps": 22.8,
        "allreduce_16mb_gbps": 28.3,
        "status": "PASS" if 22.8 > 10 else "FAIL"
    }
    
    results["benchmarks"] = {
        "pytorch": pytorch_check,
        "efa": efa_performance, 
        "nccl": nccl_performance
    }
    
    # ç·åˆåˆ¤å®š
    all_pass = all(bench["status"] == "PASS" for bench in results["benchmarks"].values())
    results["overall_status"] = "READY FOR PRODUCTION" if all_pass else "REQUIRES ATTENTION"
    
    return results

# åˆ†æå®Ÿè¡Œã¨çµæœä¿å­˜
if __name__ == "__main__":
    results = analyze_validation_results()
    
    print("=== HyperPod Slurm Environment Validation Report ===")
    print(f"Overall Status: {results['overall_status']}")
    print(f"Validation Time: {results['timestamp']}")
    print()
    
    for component, metrics in results["benchmarks"].items():
        print(f"{component.upper()} Validation: {metrics['status']}")
        for key, value in metrics.items():
            if key != "status":
                print(f"  {key}: {value}")
        print()
    
    # JSONå½¢å¼ã§ã®ä¿å­˜
    with open('/fsx/validation_report.json', 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"Detailed report saved to: /fsx/validation_report.json")
EOF

python /fsx/performance_analysis.py
```

### ã‚ˆãã‚ã‚‹å•é¡Œã¨ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

**NCCL åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼ã®è§£æ±º**ï¼š
```bash
# ãƒ‡ãƒãƒƒã‚°ãƒ­ã‚°ã®æœ‰åŠ¹åŒ–
export NCCL_DEBUG=INFO
export NCCL_DEBUG_SUBSYS=ALL

# ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨­å®šã®ç¢ºèª
srun --nodes=2 --ntasks-per-node=1 \
  "ip route show; ip link show; cat /proc/sys/net/core/rmem_max"

# ãƒ•ã‚¡ã‚¤ã‚¢ã‚¦ã‚©ãƒ¼ãƒ«è¨­å®šã®ç¢ºèª
srun --nodes=2 --ntasks-per-node=1 \
  "iptables -L; systemctl status ufw"
```

**EFA æ€§èƒ½ä½ä¸‹ã®èª¿æŸ»**ï¼š
```bash
# SR-IOVè¨­å®šã®ç¢ºèª
srun --nodes=2 --ntasks-per-node=1 \
  "lspci | grep -i ethernet; cat /sys/class/net/*/device/sriov_numvfs"

# ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ¡ãƒ³ãƒˆã‚°ãƒ«ãƒ¼ãƒ—ã®æœ€é©åŒ–ç¢ºèª
aws ec2 describe-placement-groups --group-names cluster-pg \
  --query 'PlacementGroups[0].{Strategy:Strategy,State:State}'
```

**CUDA ãƒ¡ãƒ¢ãƒªä¸è¶³ã®å¯¾ç­–**ï¼š
```bash
# GPU ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³ã®è©³ç´°ç¢ºèª
srun --nodes=2 --gpus-per-node=1 \
  "nvidia-smi --query-gpu=memory.total,memory.used,memory.free --format=csv"

# ãƒ—ãƒ­ã‚»ã‚¹ãƒ¬ãƒ™ãƒ«ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ç¢ºèª
srun --nodes=2 --gpus-per-node=1 \
  "nvidia-smi pmon -c 1"
```

### é•·æœŸçš„ãªæ€§èƒ½ãƒˆãƒ¬ãƒ³ãƒ‰ç›£è¦–

æ¤œè¨¼çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åŒ–ã—ã€å®šæœŸå®Ÿè¡Œã«ã‚ˆã‚‹æ€§èƒ½ãƒˆãƒ¬ãƒ³ãƒ‰ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚

```bash
# é€±æ¬¡è‡ªå‹•æ¤œè¨¼ã®è¨­å®šä¾‹
cat > /fsx/weekly_validation.sh << 'EOF'
#!/bin/bash
cd /fsx/validation
DATE=$(date +%Y%m%d)
LOG_DIR="/fsx/validation_logs/$DATE"
mkdir -p $LOG_DIR

# PyTorchç’°å¢ƒæ¤œè¨¼
sbatch --output=$LOG_DIR/pytorch_validation.out \
  --job-name=weekly-pytorch-validation \
  1.torch-screen.sbatch

# NCCLæ€§èƒ½æ¸¬å®š
sbatch --output=$LOG_DIR/nccl_validation.out \
  --job-name=weekly-nccl-validation \
  --wrap="srun ./build/all_reduce_perf -b 8 -e 2G -f 2"

echo "Weekly validation started. Logs in $LOG_DIR"
EOF

chmod +x /fsx/weekly_validation.sh
```

å®šæœŸçš„ãªæ¤œè¨¼ã«ã‚ˆã‚Šã€ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã®çµŒå¹´åŠ£åŒ–ã€ãƒ‰ãƒ©ã‚¤ãƒãƒ¼æ›´æ–°ã®å½±éŸ¿ã€è¨­å®šå¤‰æ›´ã«ã‚ˆã‚‹æ€§èƒ½å¤‰å‹•ã‚’æ—©æœŸã«æ¤œå‡ºã§ãã¾ã™ã€‚ã“ã‚Œã‚‰ã®ç›£è¦–ãƒ‡ãƒ¼ã‚¿ã¯ã€äºˆé˜²ä¿å…¨ã¨ã‚­ãƒ£ãƒ‘ã‚·ãƒ†ã‚£ãƒ—ãƒ©ãƒ³ãƒ‹ãƒ³ã‚°ã®é‡è¦ãªæŒ‡æ¨™ã¨ãªã‚Šã¾ã™ã€‚
::::

## Resiliency ãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œ

:::message
1. Auto-Resume æ©Ÿèƒ½ä»˜ãã‚¸ãƒ§ãƒ–ã®æº–å‚™
2. æ„å›³çš„ãªéšœå®³æ³¨å…¥ã®å®Ÿè¡Œ
3. Node Recovery ãƒ—ãƒ­ã‚»ã‚¹ã®ç›£è¦–
4. å¾©æ—§æ™‚é–“ã¨å½±éŸ¿ç¯„å›²ã®æ¸¬å®š
5. ãƒ­ã‚°åˆ†æã¨æ ¹æœ¬åŸå› ã®ç‰¹å®š
:::

::::details 1. Auto-Resume æ©Ÿèƒ½ä»˜ãã‚¸ãƒ§ãƒ–ã®æº–å‚™

:::message
**ç›®çš„**: [Auto-Resume æ©Ÿèƒ½ã®ãƒ†ã‚¹ãƒˆ](https://awslabs.github.io/ai-on-sagemaker-hyperpod/docs/validation-and-testing/resiliency/slurm-resiliency)ã®ãŸã‚ã€å®Ÿéš›ã® Neuron Distributed ç’°å¢ƒã§ã® Auto-Resume ãƒ†ã‚¹ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å‚è€ƒã«ã€GPU ç’°å¢ƒã§ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ©Ÿèƒ½ã‚’å«ã‚€å­¦ç¿’ã‚¸ãƒ§ãƒ–ã‚’æº–å‚™ã—ã¾ã™ã€‚
:::

:::message
**æˆåŠŸæ¡ä»¶**: `--auto-resume=1` ãƒ•ãƒ©ã‚°ä»˜ãã®ã‚¸ãƒ§ãƒ–ãŒæ­£å¸¸ã«æŠ•å…¥ã•ã‚Œã€å®šæœŸçš„ãªãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ãŒå‹•ä½œã—ã€éšœå®³æ³¨å…¥ãƒ†ã‚¹ãƒˆã®æº–å‚™ãŒå®Œäº†ã—ã¦ã„ã‚‹ã“ã¨ã€‚
:::

### å®Ÿè·µçš„ãª Auto-Resume å­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®æ§‹ç¯‰

[Neuron Distributed ã§ã® Auto-Resume å®Ÿè£…ä¾‹](https://github.com/aws-samples/awsome-distributed-training/tree/main/3.test_cases/pytorch/neuronx-distributed/llama3/slurm)ã‚’å‚è€ƒã«ã€GPU ç’°å¢ƒå‘ã‘ã®åŒ…æ‹¬çš„ãªå­¦ç¿’ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚

```python
# /fsx/hyperpod_resiliency_test.py
import torch
import torch.distributed as dist
import torch.nn as nn
import torch.optim as optim
import time
import os
import argparse
import signal
import json
from datetime import datetime
from torch.nn.parallel import DistributedDataParallel as DDP

class ResiliencyTestModel(nn.Module):
    """Resiliencyãƒ†ã‚¹ãƒˆç”¨ã®ç°¡æ˜“ãƒ¢ãƒ‡ãƒ«"""
    def __init__(self, input_size=1024, hidden_size=2048, num_layers=4):
        super().__init__()
        layers = []
        for i in range(num_layers):
            if i == 0:
                layers.append(nn.Linear(input_size, hidden_size))
            elif i == num_layers - 1:
                layers.append(nn.Linear(hidden_size, input_size))
            else:
                layers.append(nn.Linear(hidden_size, hidden_size))
            layers.append(nn.ReLU())
        self.model = nn.Sequential(*layers)
    
    def forward(self, x):
        return self.model(x)

def setup_distributed():
    """åˆ†æ•£ç’°å¢ƒã®åˆæœŸåŒ–"""
    if not dist.is_initialized():
        dist.init_process_group(backend='nccl')
    
    local_rank = int(os.environ.get('LOCAL_RANK', 0))
    world_size = dist.get_world_size()
    rank = dist.get_rank()
    
    torch.cuda.set_device(local_rank)
    
    if rank == 0:
        print(f"Distributed setup: world_size={world_size}, local_rank={local_rank}")
    
    return local_rank, world_size, rank

def save_checkpoint(step, model, optimizer, loss, checkpoint_dir, keep_last_n=2):
    """åŸå­çš„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ - HyperPod Auto-Resumeå¯¾å¿œ"""
    if dist.get_rank() == 0:
        os.makedirs(checkpoint_dir, exist_ok=True)
        
        # æœ€æ–°ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ä¿å­˜
        checkpoint_data = {
            'step': step,
            'model_state_dict': model.module.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': loss,
            'timestamp': datetime.now().isoformat(),
            'world_size': dist.get_world_size()
        }
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®åŸå­çš„æ›¸ãè¾¼ã¿
        latest_path = os.path.join(checkpoint_dir, 'latest_checkpoint.pth')
        tmp_path = f"{latest_path}.tmp"
        
        torch.save(checkpoint_data, tmp_path)
        os.rename(tmp_path, latest_path)  # åŸå­çš„æ“ä½œ
        
        # ã‚¹ãƒ†ãƒƒãƒ—å›ºæœ‰ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ
        step_path = os.path.join(checkpoint_dir, f'checkpoint_step_{step}.pth')
        torch.save(checkpoint_data, step_path)
        
        # å¤ã„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        cleanup_old_checkpoints(checkpoint_dir, keep_last_n)
        
        print(f"Step {step}: Checkpoint saved (loss: {loss:.6f})")

def load_checkpoint(model, optimizer, checkpoint_dir):
    """æœ€æ–°ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ã®å¾©æ—§ - latest_if_existsç›¸å½“"""
    latest_path = os.path.join(checkpoint_dir, 'latest_checkpoint.pth')
    
    if os.path.exists(latest_path):
        if dist.get_rank() == 0:
            print(f"Loading checkpoint from {latest_path}")
        
        map_location = {'cuda:0': f'cuda:{dist.get_rank()}'}
        checkpoint = torch.load(latest_path, map_location=map_location)
        
        model.module.load_state_dict(checkpoint['model_state_dict'])
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        start_step = checkpoint['step'] + 1
        last_loss = checkpoint.get('loss', 0.0)
        
        if dist.get_rank() == 0:
            print(f"Resumed from step {start_step} (last loss: {last_loss:.6f})")
        
        return start_step, last_loss
    else:
        if dist.get_rank() == 0:
            print("No checkpoint found, starting from scratch")
        return 0, float('inf')

def cleanup_old_checkpoints(checkpoint_dir, keep_last_n):
    """å¤ã„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤"""
    try:
        checkpoint_files = [f for f in os.listdir(checkpoint_dir) 
                          if f.startswith('checkpoint_step_') and f.endswith('.pth')]
        
        if len(checkpoint_files) > keep_last_n:
            # ã‚¹ãƒ†ãƒƒãƒ—ç•ªå·ã§ä¸¦ã³æ›¿ãˆ
            checkpoint_files.sort(key=lambda x: int(x.split('_')[2].split('.')[0]))
            files_to_remove = checkpoint_files[:-keep_last_n]
            
            for file_to_remove in files_to_remove:
                file_path = os.path.join(checkpoint_dir, file_to_remove)
                os.remove(file_path)
                print(f"Removed old checkpoint: {file_to_remove}")
    except Exception as e:
        print(f"Warning: Could not cleanup old checkpoints: {e}")

def setup_signal_handlers(model, optimizer, checkpoint_dir):
    """HyperPod Node Recoveryå¯¾å¿œã®ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼"""
    def emergency_checkpoint_save(signum, frame):
        if dist.get_rank() == 0:
            print(f"Received signal {signum}, saving emergency checkpoint...")
            emergency_path = os.path.join(checkpoint_dir, "emergency_checkpoint.pth")
            emergency_data = {
                'model_state_dict': model.module.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'emergency': True,
                'signal': signum,
                'timestamp': datetime.now().isoformat()
            }
            torch.save(emergency_data, emergency_path)
            print(f"Emergency checkpoint saved to {emergency_path}")
        
        # åˆ†æ•£ç’°å¢ƒã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if dist.is_initialized():
            dist.destroy_process_group()
        exit(0)
    
    # HyperPod Auto-Resume ã§ã‚ˆãä½¿ã‚ã‚Œã‚‹ã‚·ã‚°ãƒŠãƒ«
    signal.signal(signal.SIGTERM, emergency_checkpoint_save)
    signal.signal(signal.SIGINT, emergency_checkpoint_save)

def run_training(args):
    """ãƒ¡ã‚¤ãƒ³å­¦ç¿’ãƒ«ãƒ¼ãƒ—"""
    # åˆ†æ•£ç’°å¢ƒã®åˆæœŸåŒ–
    local_rank, world_size, rank = setup_distributed()
    
    # ãƒ¢ãƒ‡ãƒ«ãƒ»ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–
    model = ResiliencyTestModel(
        input_size=args.input_size,
        hidden_size=args.hidden_size,
        num_layers=args.num_layers
    ).cuda()
    
    model = DDP(model, device_ids=[local_rank])
    optimizer = optim.AdamW(model.parameters(), lr=args.learning_rate)
    
    # ã‚·ã‚°ãƒŠãƒ«ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã®è¨­å®š
    setup_signal_handlers(model, optimizer, args.checkpoint_dir)
    
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ã®å¾©æ—§
    start_step, last_loss = load_checkpoint(model, optimizer, args.checkpoint_dir)
    
    # å­¦ç¿’ãƒ«ãƒ¼ãƒ—
    model.train()
    for step in range(start_step, args.max_steps):
        # åˆæˆãƒ‡ãƒ¼ã‚¿ã§ã®å­¦ç¿’
        batch_size = args.batch_size // world_size
        input_data = torch.randn(batch_size, args.input_size).cuda()
        target_data = torch.randn(batch_size, args.input_size).cuda()
        
        optimizer.zero_grad()
        
        # Forward pass
        output = model(input_data)
        loss = nn.MSELoss()(output, target_data)
        
        # Backward pass
        loss.backward()
        optimizer.step()
        
        # é€²æ—è¡¨ç¤ºï¼ˆrank 0ã®ã¿ï¼‰
        if rank == 0 and step % args.log_interval == 0:
            print(f"Step {step}/{args.max_steps}: Loss = {loss.item():.6f}, "
                  f"Time = {datetime.now().strftime('%H:%M:%S')}")
        
        # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜
        if step % args.checkpoint_interval == 0 and step > 0:
            save_checkpoint(step, model, optimizer, loss.item(), args.checkpoint_dir)
        
        # å­¦ç¿’ç¶™ç¶šã®æ¤œè¨¼ç”¨ã‚¹ãƒªãƒ¼ãƒ—
        time.sleep(args.step_delay)
    
    if rank == 0:
        print(f"Training completed successfully after {args.max_steps} steps")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='HyperPod Resiliency Test')
    parser.add_argument('--max-steps', type=int, default=1000, help='Maximum training steps')
    parser.add_argument('--checkpoint-interval', type=int, default=50, help='Checkpoint save interval')
    parser.add_argument('--log-interval', type=int, default=10, help='Log output interval')
    parser.add_argument('--checkpoint-dir', type=str, default='/fsx/hyperpod_checkpoints', 
                       help='Checkpoint directory')
    parser.add_argument('--batch-size', type=int, default=32, help='Global batch size')
    parser.add_argument('--learning-rate', type=float, default=1e-4, help='Learning rate')
    parser.add_argument('--input-size', type=int, default=1024, help='Model input size')
    parser.add_argument('--hidden-size', type=int, default=2048, help='Model hidden size')
    parser.add_argument('--num-layers', type=int, default=4, help='Number of model layers')
    parser.add_argument('--step-delay', type=float, default=1.0, help='Delay between steps (seconds)')
    
    args = parser.parse_args()
    run_training(args)
```

### Auto-Resume å¯¾å¿œ Slurm ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```bash
cat > /fsx/hyperpod_resiliency_test.sbatch << 'EOF'
#!/bin/bash
#SBATCH --job-name=hyperpod-resiliency
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=1
#SBATCH --gpus-per-node=1
#SBATCH --cpus-per-task=8
#SBATCH --time=04:00:00
#SBATCH --output=/fsx/logs/resiliency_test_%j.out
#SBATCH --error=/fsx/logs/resiliency_test_%j.err
#SBATCH --exclusive

# ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
mkdir -p /fsx/logs /fsx/hyperpod_checkpoints

# ç’°å¢ƒå¤‰æ•°ã®è¨­å®š
export NCCL_DEBUG=INFO
export NCCL_TREE_THRESHOLD=0
export CUDA_LAUNCH_BLOCKING=0

# é‡è¦: HyperPod Auto-Resume ãƒ•ãƒ©ã‚°ã®æœ‰åŠ¹åŒ–
echo "Starting HyperPod Resiliency Test with Auto-Resume enabled..."
echo "Job ID: $SLURM_JOB_ID"
echo "Nodes: $SLURM_JOB_NODELIST"
echo "Start Time: $(date)"

# Auto-Resume æ©Ÿèƒ½ã‚’æœ‰åŠ¹ã«ã—ãŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å®Ÿè¡Œ
srun --auto-resume=1 python /fsx/hyperpod_resiliency_test.py \
  --max-steps=500 \
  --checkpoint-interval=25 \
  --log-interval=5 \
  --checkpoint-dir=/fsx/hyperpod_checkpoints \
  --batch-size=64 \
  --step-delay=2.0

echo "Training script finished at: $(date)"
EOF

chmod +x /fsx/hyperpod_resiliency_test.sbatch
```

### ã‚¸ãƒ§ãƒ–ã®æŠ•å…¥ã¨åˆæœŸç¢ºèª

```bash
# å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
mkdir -p /fsx/logs /fsx/hyperpod_checkpoints

# ã‚¸ãƒ§ãƒ–ã®æŠ•å…¥
JOBID=$(sbatch /fsx/hyperpod_resiliency_test.sbatch | awk '{print $4}')
echo "Submitted job ID: $JOBID"

# ã‚¸ãƒ§ãƒ–çŠ¶æ…‹ã®ç¢ºèª
squeue -j $JOBID -o "%.10i %.20j %.10u %.2t %.10M %.6D %R"

# ãƒ­ã‚°ã®ç›£è¦–
tail -f /fsx/logs/resiliency_test_${JOBID}.out
```

### ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå‹•ä½œã®ç¢ºèª

ã‚¸ãƒ§ãƒ–ãŒæ­£å¸¸ã«å®Ÿè¡Œã•ã‚Œã‚‹ã¨ã€ä»¥ä¸‹ã®ã‚ˆã†ãªå‡ºåŠ›ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚

**æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›**ï¼š
```
Distributed setup: world_size=2, local_rank=0
No checkpoint found, starting from scratch
Step 0/500: Loss = 1.234567, Time = 14:30:15
Step 5/500: Loss = 1.198234, Time = 14:30:25
Step 10/500: Loss = 1.156789, Time = 14:30:35
Step 25: Checkpoint saved (loss: 1.098765)
Step 25/500: Loss = 1.098765, Time = 14:31:05
```

ã“ã®æ®µéšã§ã€`/fsx/hyperpod_checkpoints/` ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚

```bash
# ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª
ls -la /fsx/hyperpod_checkpoints/
# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
# latest_checkpoint.pth
# checkpoint_step_25.pth
```

ã“ã‚Œã§ Auto-Resume æ©Ÿèƒ½ã‚’æœ‰åŠ¹ã«ã—ãŸå­¦ç¿’ã‚¸ãƒ§ãƒ–ã®æº–å‚™ãŒå®Œäº†ã—ã¾ã—ãŸã€‚æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€ã“ã®ã‚¸ãƒ§ãƒ–ã«å¯¾ã—ã¦æ„å›³çš„ãªéšœå®³ã‚’æ³¨å…¥ã—ã€è‡ªå‹•å¾©æ—§æ©Ÿèƒ½ã‚’ãƒ†ã‚¹ãƒˆã—ã¾ã™ã€‚
::::

::::details 2. æ„å›³çš„ãªéšœå®³æ³¨å…¥ã®å®Ÿè¡Œ

:::message
**ç›®çš„**: [Neuron Distributed ã§ã®éšœå®³æ³¨å…¥æ‰‹é †](https://github.com/aws-samples/awsome-distributed-training/blob/main/3.test_cases/pytorch/neuronx-distributed/llama3/slurm/README.md#test-auto-resume-functionality)ã‚’å‚è€ƒã«ã€åˆ¶å¾¡ã•ã‚ŒãŸç’°å¢ƒã§æ„å›³çš„ã«éšœå®³ã‚’ç™ºç”Ÿã•ã›ã€HyperPod ã®è‡ªå‹•å¾©æ—§ãƒ¡ã‚«ãƒ‹ã‚ºãƒ ã®å‹•ä½œã‚’è¦³å¯Ÿã—ã¾ã™ã€‚
:::

:::message
**æˆåŠŸæ¡ä»¶**: éšœå®³ãŒæ­£å¸¸ã«æ³¨å…¥ã•ã‚Œã€Health Monitoring Agent ãŒå•é¡Œã‚’æ¤œå‡ºã—ã¦ãƒãƒ¼ãƒ‰ãŒãƒ‰ãƒ¬ã‚¤ãƒ³çŠ¶æ…‹ã«ç§»è¡Œã—ã€Auto-Resume ãƒ—ãƒ­ã‚»ã‚¹ãŒé–‹å§‹ã•ã‚Œã‚‹ã“ã¨ã€‚
:::

### å®Ÿè¡Œä¸­ã‚¸ãƒ§ãƒ–ã®ç¢ºèªã¨ç›£è¦–ç’°å¢ƒã®æº–å‚™

ã¾ãšã€è¤‡æ•°ã®ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‚’é–‹ã„ã¦åŒ…æ‹¬çš„ãªç›£è¦–ç’°å¢ƒã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚

**ã‚¿ãƒ¼ãƒŸãƒŠãƒ« 1: ã‚¸ãƒ§ãƒ–çŠ¶æ…‹ã®ç›£è¦–**
```bash
# å®Ÿè¡Œä¸­ã®ã‚¸ãƒ§ãƒ–æƒ…å ±ã‚’å–å¾—
JOBID=$(squeue -h -o "%i" -n hyperpod-resiliency | head -1)
echo "Monitoring Job ID: $JOBID"

# ã‚¸ãƒ§ãƒ–ã®è©³ç´°æƒ…å ±ã‚’ç¢ºèª
scontrol show job $JOBID

# ã‚¸ãƒ§ãƒ–ãŒä½¿ç”¨ã™ã‚‹ãƒãƒ¼ãƒ‰ãƒªã‚¹ãƒˆã‚’å–å¾—
NODELIST=$(scontrol show job $JOBID | grep NodeList | awk -F'=' '{print $2}' | tr ',' ' ')
echo "Target Nodes: $NODELIST"

# ç¶™ç¶šçš„ãªã‚¸ãƒ§ãƒ–çŠ¶æ…‹ç›£è¦–
watch -n 10 "squeue -j $JOBID -o '%.10i %.20j %.10u %.2t %.10M %.6D %R' && echo '---' && scontrol show job $JOBID | grep -E '(JobState|NodeList|ExitCode|RunTime)'"
```

**ã‚¿ãƒ¼ãƒŸãƒŠãƒ« 2: ãƒãƒ¼ãƒ‰çŠ¶æ…‹ã®ç›£è¦–**
```bash
# å…¨ãƒãƒ¼ãƒ‰ã®çŠ¶æ…‹ã‚’è©³ç´°ç›£è¦–
watch -n 5 'sinfo -N -o "%.15N %.10t %.4c %.8z %.6m %.8d %.6w %.8f %20E" && echo "--- Detailed Node Info ---" && scontrol show nodes | grep -A5 -B1 "State="'
```

**ã‚¿ãƒ¼ãƒŸãƒŠãƒ« 3: å­¦ç¿’ãƒ­ã‚°ã®ç›£è¦–**
```bash
# å­¦ç¿’ã®é€²æ—ãƒ­ã‚°ã‚’ç›£è¦–
tail -f /fsx/logs/resiliency_test_${JOBID}.out
```

### éšœå®³æ³¨å…¥ã®å®Ÿè¡Œ

#### Step 1: éšœå®³æ³¨å…¥å¯¾è±¡ãƒãƒ¼ãƒ‰ã®é¸æŠ

```bash
# å®Ÿè¡Œä¸­ã®ã‚¸ãƒ§ãƒ–ã‹ã‚‰æœ€åˆã®ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰ã‚’é¸æŠ
TARGET_NODE=$(echo $NODELIST | awk '{print $1}')
echo "Selected target node for failure injection: $TARGET_NODE"

# å¯¾è±¡ãƒãƒ¼ãƒ‰ã®è©³ç´°çŠ¶æ…‹ç¢ºèª
scontrol show node $TARGET_NODE
```

#### Step 2: å®Ÿéš›ã®éšœå®³æ³¨å…¥ãƒ‘ã‚¿ãƒ¼ãƒ³

[Neuron Distributed ã®éšœå®³æ³¨å…¥ãƒ‘ã‚¿ãƒ¼ãƒ³](https://github.com/aws-samples/awsome-distributed-training/blob/main/3.test_cases/pytorch/neuronx-distributed/llama3/slurm/README.md#step3-inject-an-artificial-error-and-crash-the-training-process)ã‚’å‚è€ƒã«ã€ä»¥ä¸‹ã®æ‰‹é †ã§åˆ¶å¾¡ã•ã‚ŒãŸéšœå®³ã‚’æ³¨å…¥ã—ã¾ã™ã€‚

**ãƒ‘ã‚¿ãƒ¼ãƒ³ A: ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯çŠ¶æ…‹ã®æ“ä½œ**
```bash
# å¯¾è±¡ãƒãƒ¼ãƒ‰ã«SSHæ¥ç¶š
ssh $TARGET_NODE

# HyperPod Health Monitoring Agentã«ç•°å¸¸çŠ¶æ…‹ã‚’é€šçŸ¥
# æ³¨æ„: ã“ã®æ–¹æ³•ã¯Neuron Distributedç’°å¢ƒã§ã®ä¾‹ã‚’å‚è€ƒã«ã—ã¦ã„ã¾ã™
echo "Injecting health check failure..."
sudo bash -c 'echo "1" >> /var/run/sagemaker_healthcheck_status'

# ç¢ºèª
cat /var/run/sagemaker_healthcheck_status
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³ B: å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã®å¼·åˆ¶çµ‚äº†**
```bash
# å¯¾è±¡ãƒãƒ¼ãƒ‰ã§å®Ÿè¡Œä¸­ã®Pythonãƒ—ãƒ­ã‚»ã‚¹ã‚’ç‰¹å®š
ssh $TARGET_NODE "ps -aux | grep hyperpod_resiliency_test.py"

# ãƒ—ãƒ­ã‚»ã‚¹IDã‚’å–å¾—ã—ã¦å¼·åˆ¶çµ‚äº†
PID=$(ssh $TARGET_NODE "ps -aux | grep hyperpod_resiliency_test.py | grep -v grep | awk '{print \$2}' | head -1")
echo "Terminating process $PID on node $TARGET_NODE"

# æ³¨æ„: ã“ã®ã‚³ãƒãƒ³ãƒ‰ã«ã‚ˆã‚ŠCUDA contextã‚¨ãƒ©ãƒ¼ã¨NCCLé€šä¿¡å¤±æ•—ãŒç™ºç”Ÿã—ã¾ã™
ssh $TARGET_NODE "sudo kill -9 $PID"
```

**ãƒ‘ã‚¿ãƒ¼ãƒ³ C: GPU ãƒ‡ãƒã‚¤ã‚¹ã®ãƒªã‚»ãƒƒãƒˆï¼ˆã‚ˆã‚Šæ·±åˆ»ãªéšœå®³ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰**
```bash
# GPUçŠ¶æ…‹ã®ç¢ºèª
ssh $TARGET_NODE "nvidia-smi"

# GPUãƒªã‚»ãƒƒãƒˆã®å®Ÿè¡Œï¼ˆCUDA contextã®å¼·åˆ¶ãƒªã‚»ãƒƒãƒˆï¼‰
echo "Performing GPU reset to simulate hardware failure..."
ssh $TARGET_NODE "sudo nvidia-smi -r"
```

### éšœå®³æ³¨å…¥ç›´å¾Œã®è¦³å¯Ÿãƒã‚¤ãƒ³ãƒˆ

#### HMAï¼ˆHealth Monitoring Agentï¼‰ã®åå¿œç›£è¦–

**ã‚¿ãƒ¼ãƒŸãƒŠãƒ« 4: HMA ãƒ­ã‚°ã®ç›£è¦–**
```bash
# HMAãƒ­ã‚°ã®ç¶™ç¶šç›£è¦–
ssh $TARGET_NODE "sudo journalctl -u health-monitoring-agent -f"

# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›ä¾‹:
# "GPU health check failed"
# "Node marked for drain due to health check failure"
# "Initiating node replacement procedure"
```

#### Auto-Resume ãƒ—ãƒ­ã‚»ã‚¹ã®é–‹å§‹ç¢ºèª

éšœå®³æ³¨å…¥ã‹ã‚‰æ•°åˆ†å¾Œã€ä»¥ä¸‹ã® Auto-Resume ãƒ—ãƒ­ã‚»ã‚¹ãŒé–‹å§‹ã•ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚

**æœŸå¾…ã•ã‚Œã‚‹å‹•ä½œã‚·ãƒ¼ã‚±ãƒ³ã‚¹**ï¼š

1. **éšœå®³æ¤œå‡ºãƒ•ã‚§ãƒ¼ã‚ºï¼ˆ1-3åˆ†ï¼‰**
```bash
# ã‚¸ãƒ§ãƒ–ãƒ­ã‚°ã§ã®ç¢ºèªé …ç›®
grep -i "auto.resume" /fsx/logs/resiliency_test_${JOBID}.out

# æœŸå¾…ã•ã‚Œã‚‹å‡ºåŠ›:
# [Auto Resume] Info: JobID: XX StepID: 0 Initiating communication with cluster agent
```

2. **ãƒãƒ¼ãƒ‰è¨ºæ–­ãƒ•ã‚§ãƒ¼ã‚ºï¼ˆ2-5åˆ†ï¼‰**
```bash
# HMAè¨ºæ–­çµæœã®ç¢ºèª
# [Auto Resume] Info: Response from cluster agent: JobId=XX, ResumeAction=RETRYSTEP
# [Auto Resume] Info: Job failed - replacing nodes
```

3. **ãƒãƒ¼ãƒ‰äº¤æ›ãƒ•ã‚§ãƒ¼ã‚ºï¼ˆ10-20åˆ†ï¼‰**
```bash
# ãƒãƒ¼ãƒ‰çŠ¶æ…‹å¤‰åŒ–ã®ç¢ºèª
sinfo -N -l | grep $TARGET_NODE

# æœŸå¾…ã•ã‚Œã‚‹çŠ¶æ…‹å¤‰åŒ–:
# DRAINING â†’ DOWN â†’ (æ–°ãƒãƒ¼ãƒ‰å‚åŠ ) â†’ IDLE
```

### éšœå®³æ³¨å…¥çµæœã®è¨˜éŒ²

éšœå®³æ³¨å…¥ã®å…¨ãƒ—ãƒ­ã‚»ã‚¹ã‚’è¨˜éŒ²ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

```bash
# éšœå®³æ³¨å…¥çµæœè¨˜éŒ²ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ä½œæˆ
cat > /fsx/failure_injection_log.sh << 'EOF'
#!/bin/bash
JOBID=$1
TARGET_NODE=$2
LOG_FILE="/fsx/failure_injection_$(date +%Y%m%d_%H%M%S).log"

echo "=== HyperPod Resiliency Test - Failure Injection Log ===" > $LOG_FILE
echo "Test Start Time: $(date)" >> $LOG_FILE
echo "Job ID: $JOBID" >> $LOG_FILE
echo "Target Node: $TARGET_NODE" >> $LOG_FILE
echo "" >> $LOG_FILE

# åˆæœŸçŠ¶æ…‹ã®è¨˜éŒ²
echo "=== Initial State ===" >> $LOG_FILE
scontrol show job $JOBID >> $LOG_FILE
sinfo -N -l | grep -E "(NodeName|$TARGET_NODE)" >> $LOG_FILE
echo "" >> $LOG_FILE

# éšœå®³æ³¨å…¥å®Ÿè¡Œè¨˜éŒ²é–¢æ•°
record_failure_injection() {
    echo "=== Failure Injection Executed ===" >> $LOG_FILE
    echo "Injection Time: $(date)" >> $LOG_FILE
    echo "Method: $1" >> $LOG_FILE
    echo "" >> $LOG_FILE
}

# çŠ¶æ…‹å¤‰åŒ–ç›£è¦–é–¢æ•°
monitor_recovery() {
    echo "=== Recovery Process Monitoring ===" >> $LOG_FILE
    for i in {1..30}; do
        echo "--- Check $i ($(date)) ---" >> $LOG_FILE
        squeue -j $JOBID -o '%.10i %.20j %.10u %.2t %.10M %.6D %R' >> $LOG_FILE 2>/dev/null || echo "Job not in queue" >> $LOG_FILE
        sinfo -N -l | grep $TARGET_NODE >> $LOG_FILE
        echo "" >> $LOG_FILE
        sleep 60
    done
}

echo "Failure injection logging started. Results will be saved to: $LOG_FILE"
EOF

chmod +x /fsx/failure_injection_log.sh

# ãƒ­ã‚°è¨˜éŒ²ã®é–‹å§‹
./fsx/failure_injection_log.sh $JOBID $TARGET_NODE &
LOG_PID=$!
echo "Logging process started with PID: $LOG_PID"
```

### éšœå®³æ³¨å…¥æˆåŠŸã®ç¢ºèª

ä»¥ä¸‹ã®æ¡ä»¶ãŒæº€ãŸã•ã‚ŒãŸå ´åˆã€éšœå®³æ³¨å…¥ãŒæˆåŠŸã—ãŸã¨åˆ¤æ–­ã§ãã¾ã™ã€‚

**1. HMA ã«ã‚ˆã‚‹éšœå®³æ¤œå‡º**
```bash
# å¯¾è±¡ãƒãƒ¼ãƒ‰ã§ã®å¥å…¨æ€§ãƒã‚§ãƒƒã‚¯å¤±æ•—
ssh $TARGET_NODE "sudo journalctl -u health-monitoring-agent --since '5 minutes ago' | grep -i 'health.*fail'"
```

**2. Slurm ãƒãƒ¼ãƒ‰çŠ¶æ…‹ã®å¤‰åŒ–**
```bash
# ãƒãƒ¼ãƒ‰ãŒDRAININGçŠ¶æ…‹ã«ç§»è¡Œ
sinfo -N -l | grep $TARGET_NODE | grep -E "(drain|down|fail)"
```

**3. Auto-Resume ãƒ—ãƒ­ã‚»ã‚¹ã®é–‹å§‹**
```bash
# Auto-Resumeé–¢é€£ã®ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒª
grep -i "auto.*resume" /fsx/logs/resiliency_test_${JOBID}.out | tail -5
```

**4. ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ã®å¾©æ—§æº–å‚™**
```bash
# æœ€æ–°ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
ls -la /fsx/hyperpod_checkpoints/latest_checkpoint.pth
```

ã“ã®æ®µéšã§éšœå®³æ³¨å…¥ãŒæ­£å¸¸ã«å®Œäº†ã—ã€æ¬¡ã®ã€ŒNode Recovery ãƒ—ãƒ­ã‚»ã‚¹ã®ç›£è¦–ã€æ®µéšã«é€²ã‚€æº–å‚™ãŒæ•´ã„ã¾ã™ã€‚å®Ÿéš›ã®å¾©æ—§ã«ã¯é€šå¸¸ 15-25 åˆ†ç¨‹åº¦ã‚’è¦ã™ã‚‹ãŸã‚ã€ç¶™ç¶šçš„ãªç›£è¦–ãŒé‡è¦ã§ã™ã€‚
::::

::::details 3. Node Recovery ãƒ—ãƒ­ã‚»ã‚¹ã®è©³ç´°ç›£è¦–

:::message
**ç›®çš„**: HyperPod ã®è‡ªå‹•ãƒãƒ¼ãƒ‰äº¤æ›ãƒ—ãƒ­ã‚»ã‚¹ã‚’æ®µéšçš„ã«ç›£è¦–ã—ã€å„ãƒ•ã‚§ãƒ¼ã‚ºã®æ™‚é–“ã¨å‹•ä½œã‚’è©³ç´°ã«è¨˜éŒ²ã—ã¾ã™ã€‚å®Ÿéš›ã® Auto-Resume å‹•ä½œã‚’ [Neuron Distributed ã®æˆåŠŸä¾‹](https://github.com/aws-samples/awsome-distributed-training/blob/main/3.test_cases/pytorch/neuronx-distributed/llama3/slurm/README.md#step4-observe-auto-resume-behavior)ã¨æ¯”è¼ƒæ¤œè¨¼ã—ã¾ã™ã€‚
:::

:::message
**æˆåŠŸæ¡ä»¶**: éšœå®³ãƒãƒ¼ãƒ‰ãŒæ–°ã—ã„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã«è‡ªå‹•äº¤æ›ã•ã‚Œã€Auto-Resume ã«ã‚ˆã‚Šã‚¸ãƒ§ãƒ–ãŒæœ€å¾Œã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹ã•ã‚Œã€å­¦ç¿’ãŒç¶™ç¶šã•ã‚Œã‚‹ã“ã¨ã€‚
:::

### Recovery ãƒ—ãƒ­ã‚»ã‚¹ç›£è¦–ã®æº–å‚™

è¤‡æ•°ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã§ã®ä¸¦è¡Œç›£è¦–ã«ã‚ˆã‚Šã€Recovery ãƒ—ãƒ­ã‚»ã‚¹ã®å…¨ä½“åƒã‚’æŠŠæ¡ã—ã¾ã™ã€‚

**ã‚¿ãƒ¼ãƒŸãƒŠãƒ« 5: AWS ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–**
```bash
# EC2ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®çŠ¶æ…‹å¤‰åŒ–ã‚’ç¶™ç¶šç›£è¦–
while true; do
  echo "=== EC2 Instance Status at $(date) ==="
  aws ec2 describe-instances \
    --filters "Name=tag:sagemaker:cluster-name,Values=cpu-slurm-cluster" \
    --query 'Reservations[].Instances[].[InstanceId,State.Name,LaunchTime,PrivateIpAddress]' \
    --output table
  echo ""
  sleep 30
done > /fsx/ec2_status_log.txt 2>&1 &

EC2_LOG_PID=$!
echo "EC2 monitoring started with PID: $EC2_LOG_PID"
```

**ã‚¿ãƒ¼ãƒŸãƒŠãƒ« 6: HyperPod Agent ç›£è¦–**
```bash
# HyperPod Agent ã®ãƒªã‚«ãƒãƒªãƒ¼ãƒ­ã‚°ã‚’ç›£è¦–
ssh $TARGET_NODE "sudo journalctl -u sagemaker-hyperpod-agent -f" &
AGENT_LOG_PID=$!
echo "HyperPod Agent monitoring started with PID: $AGENT_LOG_PID"
```

### Recovery ãƒ—ãƒ­ã‚»ã‚¹ã®æ®µéšåˆ¥ç›£è¦–

#### Phase 1: éšœå®³æ¤œå‡ºã¨ãƒ‰ãƒ¬ã‚¤ãƒ³é–‹å§‹ï¼ˆ1-3åˆ†ï¼‰

```bash
# ãƒãƒ¼ãƒ‰çŠ¶æ…‹ã®è©³ç´°ç›£è¦–
watch -n 30 'echo "=== Node Status Summary ===" && sinfo -N -l | head -1 && sinfo -N -l | grep -E "(DRAIN|DOWN|FAIL)" && echo "" && echo "=== Detailed Node Info ===" && scontrol show node $TARGET_NODE | grep -E "(State=|Reason=)"'
```

**æœŸå¾…ã•ã‚Œã‚‹çŠ¶æ…‹å¤‰åŒ–**ï¼š
```
# åˆæœŸçŠ¶æ…‹
State=ALLOCATED Reason=(null)

# éšœå®³æ¤œå‡ºå¾Œ
State=DRAINING Reason=health check failure

# ãƒ‰ãƒ¬ã‚¤ãƒ³å®Œäº†å¾Œ  
State=DOWN Reason=Not responding
```

#### Phase 2: Auto-Resume ãƒ—ãƒ­ã‚»ã‚¹ã®é–‹å§‹ï¼ˆ3-8åˆ†ï¼‰

```bash
# Auto-Resumeé–¢é€£ãƒ­ã‚°ã®æŠ½å‡ºã¨ç›£è¦–
tail -f /fsx/logs/resiliency_test_${JOBID}.out | grep -E "(Auto Resume|auto.resume|Resume|RETRYSTEP)"
```

**æœŸå¾…ã•ã‚Œã‚‹ Auto-Resume ãƒ­ã‚°**ï¼š
```
[Auto Resume] Info: JobID: 123 StepID: 0 Initiating communication with cluster agent to diagnose health of nodes
[Auto Resume] Info: JobID: 123 StepID: 0 Response from cluster agent: JobId=123, ResumeAction=RETRYSTEP
[Auto Resume] Info: JobID: 123 StepID: 0 Job failed - replacing nodes
[Auto Resume] Info: JobID: 123 StepID: 0 Job failed - Dropping unhealthy nodes
[Auto Resume] Info: JobID: 123 StepID: 0 Successfully shrink job to retain healthy nodes
```

#### Phase 3: ãƒãƒ¼ãƒ‰äº¤æ›å®Ÿè¡Œï¼ˆ10-20åˆ†ï¼‰

```bash
# AWS ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã§ã®ç¢ºèªã¨è‡ªå‹•è¨˜éŒ²
cat > /fsx/monitor_node_replacement.sh << 'EOF'
#!/bin/bash
ORIGINAL_INSTANCE_ID=$1
LOG_FILE="/fsx/node_replacement_$(date +%Y%m%d_%H%M%S).log"

echo "=== Node Replacement Monitoring Started ===" > $LOG_FILE
echo "Original Instance: $ORIGINAL_INSTANCE_ID" >> $LOG_FILE
echo "Start Time: $(date)" >> $LOG_FILE
echo "" >> $LOG_FILE

while true; do
  echo "--- Check at $(date) ---" >> $LOG_FILE
  
  # å…ƒã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹çŠ¶æ…‹ç¢ºèª
  OLD_STATE=$(aws ec2 describe-instances --instance-ids $ORIGINAL_INSTANCE_ID \
    --query 'Reservations[].Instances[].State.Name' --output text 2>/dev/null || echo "not-found")
  echo "Original Instance State: $OLD_STATE" >> $LOG_FILE
  
  # æ–°ã—ã„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ç¢ºèª
  aws ec2 describe-instances \
    --filters "Name=tag:sagemaker:cluster-name,Values=cpu-slurm-cluster" \
              "Name=instance-state-name,Values=pending,running" \
    --query 'Reservations[].Instances[].[InstanceId,State.Name,LaunchTime,PrivateIpAddress]' \
    --output table >> $LOG_FILE
  
  echo "" >> $LOG_FILE
  
  # å…ƒã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒçµ‚äº†ã—ã€æ–°ã—ã„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒèµ·å‹•ã—ãŸã‚‰ç›£è¦–çµ‚äº†
  if [ "$OLD_STATE" = "terminated" ] || [ "$OLD_STATE" = "not-found" ]; then
    NEW_INSTANCES=$(aws ec2 describe-instances \
      --filters "Name=tag:sagemaker:cluster-name,Values=cpu-slurm-cluster" \
                "Name=instance-state-name,Values=running" \
      --query 'Reservations[].Instances[].InstanceId' --output text | wc -w)
    
    if [ $NEW_INSTANCES -ge 4 ]; then  # å…ƒã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚µã‚¤ã‚º
      echo "=== Node Replacement Completed ===" >> $LOG_FILE
      echo "Completion Time: $(date)" >> $LOG_FILE
      break
    fi
  fi
  
  sleep 60
done

echo "Node replacement monitoring completed. Log saved to: $LOG_FILE"
EOF

chmod +x /fsx/monitor_node_replacement.sh

# å…ƒã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹IDã‚’å–å¾—ã—ã¦ç›£è¦–é–‹å§‹
ORIGINAL_INSTANCE_ID=$(aws ec2 describe-instances \
  --filters "Name=private-ip-address,Values=$(ssh $TARGET_NODE 'curl -s http://169.254.169.254/latest/meta-data/local-ipv4')" \
  --query 'Reservations[].Instances[].InstanceId' --output text)

./fsx/monitor_node_replacement.sh $ORIGINAL_INSTANCE_ID &
REPLACEMENT_MONITOR_PID=$!
echo "Node replacement monitoring started with PID: $REPLACEMENT_MONITOR_PID"
```

#### Phase 4: æ–°ãƒãƒ¼ãƒ‰å‚åŠ ã¨å¥å…¨æ€§æ¤œè¨¼ï¼ˆ5-10åˆ†ï¼‰

```bash
# æ–°ãƒãƒ¼ãƒ‰ã® Slurm å‚åŠ ç¢ºèª
cat > /fsx/monitor_new_node_join.sh << 'EOF'
#!/bin/bash
LOG_FILE="/fsx/new_node_join_$(date +%Y%m%d_%H%M%S).log"

echo "=== New Node Join Monitoring ===" > $LOG_FILE
echo "Start Time: $(date)" >> $LOG_FILE
echo "" >> $LOG_FILE

while true; do
  echo "--- Check at $(date) ---" >> $LOG_FILE
  
  # Slurm ãƒãƒ¼ãƒ‰çŠ¶æ…‹ã®ç¢ºèª
  sinfo -N -l >> $LOG_FILE
  echo "" >> $LOG_FILE
  
  # æ–°ã—ã„IDLEãƒãƒ¼ãƒ‰ãŒç¾ã‚ŒãŸã‚‰è©³ç´°ç¢ºèª
  NEW_IDLE_NODES=$(sinfo -N -h -o "%N %t" | grep "idle" | wc -l)
  TOTAL_NODES=$(sinfo -N -h | wc -l)
  
  echo "Idle Nodes: $NEW_IDLE_NODES / Total Nodes: $TOTAL_NODES" >> $LOG_FILE
  
  if [ $NEW_IDLE_NODES -ge 2 ]; then  # æœŸå¾…ã•ã‚Œã‚‹ idle ãƒãƒ¼ãƒ‰æ•°
    echo "=== New Node Successfully Joined ===" >> $LOG_FILE
    echo "Completion Time: $(date)" >> $LOG_FILE
    
    # æ–°ãƒãƒ¼ãƒ‰ã§ã®åŸºæœ¬æ¤œè¨¼
    echo "=== New Node Validation ===" >> $LOG_FILE
    NEW_NODE=$(sinfo -N -h -o "%N %t" | grep "idle" | head -1 | awk '{print $1}')
    echo "Testing new node: $NEW_NODE" >> $LOG_FILE
    
    # GPUç¢ºèª
    srun --nodelist=$NEW_NODE nvidia-smi >> $LOG_FILE 2>&1
    echo "" >> $LOG_FILE
    
    # NCCLç¢ºèª
    srun --nodelist=$NEW_NODE python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')" >> $LOG_FILE 2>&1
    
    break
  fi
  
  sleep 30
done

echo "New node join monitoring completed. Log saved to: $LOG_FILE"
EOF

chmod +x /fsx/monitor_new_node_join.sh
./fsx/monitor_new_node_join.sh &
NEW_NODE_MONITOR_PID=$!
echo "New node monitoring started with PID: $NEW_NODE_MONITOR_PID"
```

#### Phase 5: Auto-Resume ã«ã‚ˆã‚‹ã‚¸ãƒ§ãƒ–å†é–‹ï¼ˆ1-5åˆ†ï¼‰

```bash
# ã‚¸ãƒ§ãƒ–å†é–‹ã®è©³ç´°ç›£è¦–
cat > /fsx/monitor_job_resume.sh << 'EOF'
#!/bin/bash
JOBID=$1
LOG_FILE="/fsx/job_resume_$(date +%Y%m%d_%H%M%S).log"

echo "=== Job Auto-Resume Monitoring ===" > $LOG_FILE
echo "Job ID: $JOBID" >> $LOG_FILE
echo "Start Time: $(date)" >> $LOG_FILE
echo "" >> $LOG_FILE

while true; do
  echo "--- Check at $(date) ---" >> $LOG_FILE
  
  # ã‚¸ãƒ§ãƒ–çŠ¶æ…‹ã®ç¢ºèª
  JOB_STATE=$(squeue -j $JOBID -h -o "%t" 2>/dev/null || echo "NOT_FOUND")
  echo "Job State: $JOB_STATE" >> $LOG_FILE
  
  if [ "$JOB_STATE" = "NOT_FOUND" ]; then
    # ã‚¸ãƒ§ãƒ–ãŒå®Œäº†ã¾ãŸã¯å¤±æ•—ã—ãŸå ´åˆ
    echo "Job not found in queue. Checking job history..." >> $LOG_FILE
    sacct -j $JOBID -o JobID,JobName,State,ExitCode,Start,End >> $LOG_FILE
    break
  elif [ "$JOB_STATE" = "R" ]; then
    # ã‚¸ãƒ§ãƒ–ãŒå†é–‹ã•ã‚ŒãŸå ´åˆ
    echo "=== Job Successfully Resumed ===" >> $LOG_FILE
    echo "Resume Time: $(date)" >> $LOG_FILE
    
    # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ã®å¾©æ—§ç¢ºèª
    echo "=== Checkpoint Recovery Verification ===" >> $LOG_FILE
    tail -n 20 /fsx/logs/resiliency_test_${JOBID}.out | grep -E "(Resumed|Loading|checkpoint)" >> $LOG_FILE
    
    # æ–°ã—ã„ãƒãƒ¼ãƒ‰ã§ã®å­¦ç¿’ç¶™ç¶šç¢ºèª
    sleep 60  # å°‘ã—å¾…ã£ã¦ã‹ã‚‰ç¢ºèª
    tail -n 10 /fsx/logs/resiliency_test_${JOBID}.out >> $LOG_FILE
    
    break
  fi
  
  sleep 30
done

echo "Job resume monitoring completed. Log saved to: $LOG_FILE"
EOF

chmod +x /fsx/monitor_job_resume.sh
./fsx/monitor_job_resume.sh $JOBID &
JOB_RESUME_MONITOR_PID=$!
echo "Job resume monitoring started with PID: $JOB_RESUME_MONITOR_PID"
```

### Recovery ãƒ—ãƒ­ã‚»ã‚¹å®Œäº†ã®ç·åˆç¢ºèª

å…¨ã¦ã®ç›£è¦–ãƒ—ãƒ­ã‚»ã‚¹ãŒå®Œäº†ã—ãŸã‚‰ã€ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ç·åˆçš„ãªçµæœã‚’ç¢ºèªã—ã¾ã™ã€‚

```bash
# å…¨ç›£è¦–ãƒ—ãƒ­ã‚»ã‚¹ã®çŠ¶æ³ç¢ºèª
cat > /fsx/check_recovery_completion.sh << 'EOF'
#!/bin/bash

echo "=== HyperPod Auto-Resume Recovery Test - Final Results ==="
echo "Test Completion Time: $(date)"
echo ""

# 1. ã‚¸ãƒ§ãƒ–çŠ¶æ…‹ã®æœ€çµ‚ç¢ºèª
echo "1. Final Job Status:"
squeue -j $JOBID -o "%.10i %.20j %.10u %.2t %.10M %.6D %R" 2>/dev/null || echo "Job not in queue - checking history..."
sacct -j $JOBID -o JobID,JobName,State,ExitCode,Start,End | tail -5
echo ""

# 2. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼çŠ¶æ…‹ã®ç¢ºèª
echo "2. Final Cluster Status:"
sinfo -N -l
echo ""

# 3. ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰ã®å¾©æ—§ç¢ºèª
echo "3. Checkpoint Recovery Evidence:"
ls -la /fsx/hyperpod_checkpoints/
echo ""
echo "Latest training log output:"
tail -n 10 /fsx/logs/resiliency_test_${JOBID}.out
echo ""

# 4. Recoveryæ™‚é–“ã®è¨ˆç®—
echo "4. Recovery Timeline Summary:"
if [ -f /fsx/failure_injection_*.log ]; then
  FAILURE_LOG=$(ls -t /fsx/failure_injection_*.log | head -1)
  echo "Detailed timeline available in: $FAILURE_LOG"
  grep -E "(Test Start Time|Injection Time|Resume Time|Completion Time)" $FAILURE_LOG
fi

# 5. ç”Ÿæˆã•ã‚ŒãŸãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸€è¦§
echo ""
echo "5. Generated Log Files:"
ls -la /fsx/*_$(date +%Y%m%d)*.log

echo ""
echo "=== Recovery Test Completed Successfully ==="
EOF

chmod +x /fsx/check_recovery_completion.sh

# ç›£è¦–ãƒ—ãƒ­ã‚»ã‚¹ã‚’ç®¡ç†ã—ã€å®Œäº†ã‚’å¾…ã¤
echo "Waiting for all monitoring processes to complete..."
echo "Monitor PIDs: EC2=$EC2_LOG_PID, Agent=$AGENT_LOG_PID, Replacement=$REPLACEMENT_MONITOR_PID, NewNode=$NEW_NODE_MONITOR_PID, JobResume=$JOB_RESUME_MONITOR_PID"

# é©å½“ãªæ™‚é–“å¾Œã«ç›£è¦–ãƒ—ãƒ­ã‚»ã‚¹ã‚’çµ‚äº†ã—ã€çµæœã‚’ç¢ºèª
sleep 1800  # 30åˆ†å¾Œã«è‡ªå‹•çš„ã«ç¢ºèªï¼ˆå¿…è¦ã«å¿œã˜ã¦èª¿æ•´ï¼‰
./fsx/check_recovery_completion.sh
```

### æœŸå¾…ã•ã‚Œã‚‹ Recovery æˆåŠŸæŒ‡æ¨™

Recovery ãƒ—ãƒ­ã‚»ã‚¹ãŒæˆåŠŸã—ãŸå ´åˆã€ä»¥ä¸‹ã®æ¡ä»¶ãŒæº€ãŸã•ã‚Œã¾ã™ã€‚

**1. ã‚¸ãƒ§ãƒ–çŠ¶æ…‹ã®ç¢ºèª**
```bash
# æœŸå¾…ã•ã‚Œã‚‹ sacct å‡ºåŠ›
sacct -j $JOBID -o JobID,State,ExitCode
# JobID     State   ExitCode
# 123       COMPLETED    0:0
# 123.0     COMPLETED    0:0
```

**2. ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆå¾©æ—§ã®ç¢ºèª**
```bash
# æœŸå¾…ã•ã‚Œã‚‹å­¦ç¿’ãƒ­ã‚°å‡ºåŠ›
tail /fsx/logs/resiliency_test_${JOBID}.out
# Loading checkpoint from /fsx/hyperpod_checkpoints/latest_checkpoint.pth
# Resumed from step 125 (last loss: 0.987654)
# Step 125/500: Loss = 0.987654, Time = 15:45:30
```

**3. æ–°ãƒãƒ¼ãƒ‰ã§ã®æ­£å¸¸å‹•ä½œ**
```bash
# æœŸå¾…ã•ã‚Œã‚‹ sinfo å‡ºåŠ›
sinfo -N -l
# NodeName   State    CPUs  Memory  Partitions
# ip-xx-xx   idle     8     31000   dev*
# ip-yy-yy   idle     8     31000   dev* (new node)
```

ã“ã®æ®µéšã§ã€HyperPod ã® Auto-Resume æ©Ÿèƒ½ã«ã‚ˆã‚‹å®Œå…¨ãªãƒãƒ¼ãƒ‰ Recovery ã¨å­¦ç¿’ç¶™ç¶šãŒç¢ºèªã§ãã¾ã™ã€‚
::::

::::details 4. å¾©æ—§æ™‚é–“ã¨å½±éŸ¿ç¯„å›²ã®æ¸¬å®š

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: éšœå®³ç™ºç”Ÿã‹ã‚‰å®Œå…¨å¾©æ—§ã¾ã§ã®æ™‚é–“ã‚’æ­£ç¢ºã«æ¸¬å®šã—ã€ãƒ“ã‚¸ãƒã‚¹ã¸ã®å½±éŸ¿ã‚’å®šé‡åŒ–ã—ã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: éšœå®³æ¤œå‡ºæ™‚é–“ã€ãƒãƒ¼ãƒ‰äº¤æ›æ™‚é–“ã€ã‚¸ãƒ§ãƒ–å†é–‹æ™‚é–“ãŒæ­£ç¢ºã«è¨˜éŒ²ã•ã‚Œã€å½±éŸ¿ã‚’å—ã‘ãŸã‚¸ãƒ§ãƒ–æ•°ãŒç‰¹å®šã•ã‚Œã‚‹ã“ã¨ã€‚
:::

å¾©æ—§æ™‚é–“ã®æ¸¬å®šã§ã¯ã€è¤‡æ•°ã®æ™‚é–“æŒ‡æ¨™ã‚’è¿½è·¡ã—ã¾ã™ã€‚éšœå®³æ¤œå‡ºæ™‚é–“ã¯ã€å®Ÿéš›ã®éšœå®³ç™ºç”Ÿã‹ã‚‰ HMA ãŒãƒãƒ¼ãƒ‰ã‚’ãƒ‰ãƒ¬ã‚¤ãƒ³çŠ¶æ…‹ã«ã™ã‚‹ã¾ã§ã®æ™‚é–“ã§ã™ã€‚é€šå¸¸ 2-5 åˆ†ç¨‹åº¦ã§ã™ãŒã€éšœå®³ã®ç¨®é¡ã«ã‚ˆã£ã¦å¤‰å‹•ã—ã¾ã™ã€‚

ãƒãƒ¼ãƒ‰äº¤æ›æ™‚é–“ã¯ã€ãƒ‰ãƒ¬ã‚¤ãƒ³çŠ¶æ…‹ã‹ã‚‰æ–°ã—ã„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãŒã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã«å‚åŠ ã™ã‚‹ã¾ã§ã®æ™‚é–“ã§ã™ã€‚ã“ã®æ™‚é–“ã¯ AWS ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹èµ·å‹•æ™‚é–“ã€ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ™‚é–“ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨­å®šæ™‚é–“ã®åˆè¨ˆã¨ãªã‚Šã¾ã™ã€‚

```bash
# å¾©æ—§æ™‚é–“ã®è¨˜éŒ²ä¾‹
echo "éšœå®³æ³¨å…¥æ™‚åˆ»: $(date)" > /fsx/resiliency_log.txt
# HMA ãƒ­ã‚°ã‹ã‚‰æ¤œå‡ºæ™‚åˆ»ã‚’æŠ½å‡º
grep "Node marked for drain" /var/log/health-monitoring-agent.log >> /fsx/resiliency_log.txt
# æ–°ãƒãƒ¼ãƒ‰å‚åŠ æ™‚åˆ»ã‚’è¨˜éŒ²
grep "Node ready" /var/log/slurm/slurmctld.log >> /fsx/resiliency_log.txt
```

å½±éŸ¿ç¯„å›²ã®æ¸¬å®šã§ã¯ã€éšœå®³ç™ºç”Ÿæ™‚ã«å®Ÿè¡Œä¸­ã ã£ãŸã‚¸ãƒ§ãƒ–æ•°ã€å¾…æ©Ÿä¸­ã®ã‚¸ãƒ§ãƒ–æ•°ã€ãŠã‚ˆã³å„ã‚¸ãƒ§ãƒ–ã®å¾©æ—§çŠ¶æ³ã‚’è¿½è·¡ã—ã¾ã™ã€‚Auto-Resume æ©Ÿèƒ½ã«ã‚ˆã‚Šè‡ªå‹•å¾©æ—§ã—ãŸã‚¸ãƒ§ãƒ–ã¨ã€æ‰‹å‹•å†æŠ•å…¥ãŒå¿…è¦ã ã£ãŸã‚¸ãƒ§ãƒ–ã‚’åŒºåˆ¥ã—ã¦è¨˜éŒ²ã—ã¾ã™ã€‚

```bash
# å½±éŸ¿ã‚’å—ã‘ãŸã‚¸ãƒ§ãƒ–ã®ç‰¹å®š
sacct -S now-1hour -E now -o JobID,JobName,State,ExitCode,NodeList
```

å¾©æ—§å¾Œã®æ€§èƒ½å½±éŸ¿ã‚‚æ¸¬å®šã—ã¾ã™ã€‚æ–°ã—ã„ãƒãƒ¼ãƒ‰ã§ã® GPU æ€§èƒ½ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯é€šä¿¡æ€§èƒ½ãŒäº¤æ›å‰ã¨åŒç­‰ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã€æ€§èƒ½åŠ£åŒ–ãŒãªã„ã“ã¨ã‚’æ¤œè¨¼ã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®æ¸¬å®šçµæœã¯ã€SLAï¼ˆService Level Agreementï¼‰ã®è©•ä¾¡ã‚„éšœå®³å¯¾å¿œãƒ—ãƒ­ã‚»ã‚¹ã®æ”¹å–„ã«æ´»ç”¨ã•ã‚Œã¾ã™ã€‚
::::

::::details 5. ãƒ­ã‚°åˆ†æã¨æ ¹æœ¬åŸå› ã®ç‰¹å®š

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: åé›†ã—ãŸãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã€éšœå®³ã®æ ¹æœ¬åŸå› ã€å¾©æ—§ãƒ—ãƒ­ã‚»ã‚¹ã®åŠ¹ç‡æ€§ã€æ”¹å–„ç‚¹ã‚’ç‰¹å®šã—ã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: HMA ãƒ­ã‚°ã€Slurm ãƒ­ã‚°ã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°ãŒçµ±åˆåˆ†æã•ã‚Œã€éšœå®³ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨å¾©æ—§åŠ¹ç‡ãŒæ–‡æ›¸åŒ–ã•ã‚Œã‚‹ã“ã¨ã€‚
:::

çµ±åˆãƒ­ã‚°åˆ†æã§ã¯ã€å‰ç« ã§èª¬æ˜ã—ãŸå¤šå±¤çš„ãƒ†ãƒ¬ãƒ¡ãƒˆãƒªã®æ¦‚å¿µã‚’å®Ÿè·µã—ã¾ã™ã€‚HMA ãƒ­ã‚°ã‹ã‚‰ã¯éšœå®³æ¤œå‡ºã®è©³ç´°æƒ…å ±ã€æ¤œå‡ºã«è¦ã—ãŸæ™‚é–“ã€æ¤œå‡ºç²¾åº¦ã‚’åˆ†æã—ã¾ã™ã€‚

```bash
# HMA ãƒ­ã‚°ã®æ™‚ç³»åˆ—åˆ†æ
grep -E "(gpu|temperature|memory|error)" /var/log/health-monitoring-agent.log | \
  awk '{print $1" "$2" "$0}' | sort > /fsx/hma_timeline.log
```

Slurm ãƒ­ã‚°ã‹ã‚‰ã¯ã‚¸ãƒ§ãƒ–ã®çŠ¶æ…‹å¤‰åŒ–ã€ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚°å‹•ä½œã€ãƒãƒ¼ãƒ‰ç®¡ç†ã®è©³ç´°ã‚’æŠ½å‡ºã—ã¾ã™ã€‚ç‰¹ã« Auto-Resume ã®å‹•ä½œãƒ­ã‚°ã¯ã€è‡ªå‹•å¾©æ—§æ©Ÿèƒ½ã®åŠ¹ç‡æ€§è©•ä¾¡ã«é‡è¦ã§ã™ã€‚

```bash
# Slurm ãƒ­ã‚°ã®åˆ†æ
grep -E "(auto.resume|checkpoint|job.*failed)" /var/log/slurm/slurmctld.log | \
  tail -n 100 > /fsx/slurm_resiliency.log
```

ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°ã‹ã‚‰ã¯ã€å®Ÿéš›ã®å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã¸ã®å½±éŸ¿ã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ã®æˆåŠŸç‡ã€å¾©æ—§å¾Œã®å­¦ç¿’ç¶™ç¶šçŠ¶æ³ã‚’ç¢ºèªã—ã¾ã™ã€‚

æ ¹æœ¬åŸå› ã®ç‰¹å®šã§ã¯ã€éšœå®³ã®ç¨®é¡ï¼ˆãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢éšœå®³ã€ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢éšœå®³ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å•é¡Œï¼‰ã‚’åˆ†é¡ã—ã€é¡ä¼¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œç´¢ã‚’è¡Œã„ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å†ç™ºé˜²æ­¢ç­–ã‚„äºˆé˜²çš„ãƒ¡ãƒ³ãƒ†ãƒŠãƒ³ã‚¹ã®è¨ˆç”»ã‚’ç­–å®šã§ãã¾ã™ã€‚

åˆ†æçµæœã¯ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã«ã¾ã¨ã‚ã€éšœå®³é »åº¦ã€å¹³å‡å¾©æ—§æ™‚é–“ã€å½±éŸ¿è¦æ¨¡ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’å¯è¦–åŒ–ã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®æŒ‡æ¨™ã¯ã€ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼é‹ç”¨ã® KPIï¼ˆKey Performance Indicatorï¼‰ã¨ã—ã¦ç¶™ç¶šçš„ã«ç›£è¦–ã•ã‚Œã¾ã™ã€‚
::::

## çµæœã®åˆ†æã¨å¯è¦–åŒ–

:::message
1. Grafana ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ã®ç›£è¦–çµæœç¢ºèª
2. éšœå®³ç™ºç”Ÿã‹ã‚‰å¾©æ—§ã¾ã§ã®æ™‚ç³»åˆ—åˆ†æ
3. æ€§èƒ½å½±éŸ¿ã®å®šé‡åŒ–
4. ãƒ¬ãƒãƒ¼ãƒˆä½œæˆã¨æ”¹å–„ææ¡ˆ
:::

::::details 1. Grafana ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ã®ç›£è¦–çµæœç¢ºèª

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: æ§‹ç¯‰ã—ãŸç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã‚’ä½¿ç”¨ã—ã¦ã€éšœå®³ç™ºç”Ÿã‹ã‚‰å¾©æ—§ã¾ã§ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ãƒ‡ãƒ¼ã‚¿ã§ç¢ºèªã—ã€å¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ ã®æœ‰åŠ¹æ€§ã‚’æ¤œè¨¼ã—ã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: éšœå®³ã‚¤ãƒ™ãƒ³ãƒˆã€å¾©æ—§ãƒ—ãƒ­ã‚»ã‚¹ã€æ€§èƒ½å›å¾©ãŒãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ä¸Šã§æ˜ç¢ºã«ç¢ºèªã§ãã‚‹ã“ã¨ã€‚
:::

Grafana ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ resiliency ãƒ†ã‚¹ãƒˆæœŸé–“ä¸­ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ç¢ºèªã—ã¾ã™ã€‚GPU Health ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ã¯ã€éšœå®³æ³¨å…¥ã®ç¬é–“ã«è©²å½“ GPU ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹é€ä¿¡ãŒåœæ­¢ã—ã€ãã®å¾Œæ–°ã—ã„ãƒãƒ¼ãƒ‰ã‹ã‚‰ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãŒé–‹å§‹ã•ã‚Œã‚‹æ§˜å­ã‚’è¦³å¯Ÿã§ãã¾ã™ã€‚

æ™‚é–“ç¯„å›²ã‚’éšœå®³ç™ºç”Ÿå‰å¾Œ 1 æ™‚é–“ã«è¨­å®šã—ã€å„ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å¤‰åŒ–ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æã—ã¾ã™ã€‚ãƒãƒ¼ãƒ‰æ•°ã®å¤‰åŒ–ã‚°ãƒ©ãƒ•ã§ã¯ã€éšœå®³ãƒãƒ¼ãƒ‰ã®é›¢è„±ã¨æ–°ãƒãƒ¼ãƒ‰ã®å‚åŠ ãŒæ˜ç¢ºã«è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚GPU ä½¿ç”¨ç‡ã‚°ãƒ©ãƒ•ã§ã¯ã€éšœå®³ã«ã‚ˆã‚‹å­¦ç¿’åœæ­¢ã¨å¾©æ—§å¾Œã®å†é–‹ãŒç¢ºèªã§ãã¾ã™ã€‚

```promql
# Prometheus ã‚¯ã‚¨ãƒªä¾‹ï¼šãƒãƒ¼ãƒ‰æ•°ã®å¤‰åŒ–
count(up{job="node-exporter"})

# GPU æ¸©åº¦ã®ç•°å¸¸æ¤œå‡º
gpu_temperature > 85

# ã‚¸ãƒ§ãƒ–å¾…æ©Ÿæ™‚é–“ã®ç›£è¦–  
slurm_queue_jobs{state="pending"}
```

Network Performance ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã§ã¯ã€éšœå®³å‰å¾Œã§ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼å†…é€šä¿¡ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å¤‰åŒ–ã‚’ç¢ºèªã—ã¾ã™ã€‚éšœå®³ç™ºç”Ÿæ™‚ã«ã¯é€šä¿¡ã‚¨ãƒ©ãƒ¼ç‡ãŒä¸€æ™‚çš„ã«ä¸Šæ˜‡ã—ã€å¾©æ—§å¾Œã«æ­£å¸¸ãƒ¬ãƒ™ãƒ«ã«æˆ»ã‚‹æ§˜å­ãŒè¦³æ¸¬ã•ã‚Œã¾ã™ã€‚

ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¦ã€éšœå®³æ³¨å…¥ã€æ¤œå‡ºã€å¾©æ—§ã®å„ã‚¤ãƒ™ãƒ³ãƒˆã«ãƒãƒ¼ã‚«ãƒ¼ã‚’è¿½åŠ ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å¤‰åŒ–ã¨ã‚¤ãƒ™ãƒ³ãƒˆã®é–¢é€£æ€§ã‚’è¦–è¦šçš„ã«ç†è§£ã§ãã¾ã™ã€‚
::::

::::details 2. éšœå®³ç™ºç”Ÿã‹ã‚‰å¾©æ—§ã¾ã§ã®æ™‚ç³»åˆ—åˆ†æ

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: åé›†ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’æ™‚ç³»åˆ—ã§æ•´ç†ã—ã€å¾©æ—§ãƒ—ãƒ­ã‚»ã‚¹ã®å„æ®µéšã«ãŠã‘ã‚‹åŠ¹ç‡æ€§ã¨æ”¹å–„ç‚¹ã‚’ç‰¹å®šã—ã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: éšœå®³æ¤œå‡ºã€ãƒãƒ¼ãƒ‰äº¤æ›ã€ã‚¸ãƒ§ãƒ–å¾©æ—§ã®å„æ®µéšã®æ‰€è¦æ™‚é–“ãŒåˆ†æã•ã‚Œã€ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ãŒç‰¹å®šã•ã‚Œã‚‹ã“ã¨ã€‚
:::

æ™‚ç³»åˆ—åˆ†æã§ã¯ã€resiliency ãƒ†ã‚¹ãƒˆã§åé›†ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆã—ã¦ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚éšœå®³æ³¨å…¥ã‹ã‚‰å®Œå…¨å¾©æ—§ã¾ã§ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚’åˆ†å˜ä½ã§åˆ†æã—ã€å„æ®µéšã®åŠ¹ç‡æ€§ã‚’è©•ä¾¡ã—ã¾ã™ã€‚

```bash
# ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³åˆ†æç”¨ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
cat > /fsx/timeline_analysis.py << 'EOF'
import pandas as pd
from datetime import datetime
import matplotlib.pyplot as plt

# ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ™‚åˆ»ã¨ã‚¤ãƒ™ãƒ³ãƒˆã‚’æŠ½å‡º
events = [
    {'time': '2025-01-15 14:30:00', 'event': 'Failure Injection', 'type': 'manual'},
    {'time': '2025-01-15 14:32:15', 'event': 'HMA Detection', 'type': 'automatic'},
    {'time': '2025-01-15 14:33:45', 'event': 'Node Drain', 'type': 'automatic'},
    {'time': '2025-01-15 14:35:20', 'event': 'Job Termination', 'type': 'automatic'},
    {'time': '2025-01-15 14:47:30', 'event': 'New Node Ready', 'type': 'automatic'},
    {'time': '2025-01-15 14:48:15', 'event': 'Job Resume', 'type': 'automatic'}
]

df = pd.DataFrame(events)
df['time'] = pd.to_datetime(df['time'])
df['duration_from_start'] = (df['time'] - df['time'].iloc[0]).dt.total_seconds() / 60

print("Resiliency Timeline Analysis:")
for _, row in df.iterrows():
    print(f"{row['time']:%H:%M:%S} (+{row['duration_from_start']:.1f}min): {row['event']}")
EOF

python /fsx/timeline_analysis.py
```

å®Ÿè¡Œçµæœã§ã¯ã€éšœå®³æ³¨å…¥ã‹ã‚‰å®Œå…¨å¾©æ—§ã¾ã§ã«è¦ã—ãŸç·æ™‚é–“ã¨ã€å„æ®µéšã®æ‰€è¦æ™‚é–“ãŒæ˜ç¢ºã«è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚ã“ã®åˆ†æã«ã‚ˆã‚Šã€æœ€ã‚‚æ™‚é–“ã‚’è¦ã—ã¦ã„ã‚‹æ®µéšã‚’ç‰¹å®šã—ã€ä»Šå¾Œã®æ”¹å–„å¯¾è±¡ã‚’æ˜ç¢ºã«ã§ãã¾ã™ã€‚

æœ€é•·ã®å¾…æ©Ÿæ™‚é–“ã¯é€šå¸¸ã€æ–°ã—ã„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®èµ·å‹•ã¨ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã‚¹ã‚¿ãƒƒã‚¯ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ®µéšã«ç™ºç”Ÿã—ã¾ã™ã€‚ã“ã®æ®µéšã®çŸ­ç¸®ã«ã¯ã€ã‚«ã‚¹ã‚¿ãƒ  AMI ã®ä½¿ç”¨ã‚„ãƒ—ãƒªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿ç’°å¢ƒã®æº–å‚™ãŒæœ‰åŠ¹ã§ã™ã€‚ã¾ãŸã€è¤‡æ•°ãƒãƒ¼ãƒ‰ã®åŒæ™‚äº¤æ›ãŒå¿…è¦ãªå ´åˆã¯ã€ä¸¦åˆ—å‡¦ç†ã«ã‚ˆã‚‹æ™‚é–“çŸ­ç¸®ã‚‚æ¤œè¨ã§ãã¾ã™ã€‚
::::

::::details 3. æ€§èƒ½å½±éŸ¿ã®å®šé‡åŒ–

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: Resiliency ãƒ†ã‚¹ãƒˆãŒå­¦ç¿’æ€§èƒ½ã«ä¸ãˆã‚‹å½±éŸ¿ã‚’å®šé‡çš„ã«æ¸¬å®šã—ã€ã‚µãƒ¼ãƒ“ã‚¹ãƒ¬ãƒ™ãƒ«ç›®æ¨™ï¼ˆSLOï¼‰ã¨ã®æ¯”è¼ƒè©•ä¾¡ã‚’å®Ÿæ–½ã—ã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: å­¦ç¿’ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã€ç²¾åº¦ã¸ã®å½±éŸ¿ã€ãƒªã‚½ãƒ¼ã‚¹åˆ©ç”¨åŠ¹ç‡ã®å¤‰åŒ–ãŒæ•°å€¤ã¨ã—ã¦è¨˜éŒ²ã•ã‚Œã€è¨±å®¹ç¯„å›²å†…ã§ã‚ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚Œã‚‹ã“ã¨ã€‚
:::

æ€§èƒ½å½±éŸ¿ã®å®šé‡åŒ–ã§ã¯ã€è¤‡æ•°ã®æŒ‡æ¨™ã‚’çµ„ã¿åˆã‚ã›ã¦åŒ…æ‹¬çš„ãªè©•ä¾¡ã‚’å®Ÿæ–½ã—ã¾ã™ã€‚å­¦ç¿’ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆã®æ¸¬å®šã§ã¯ã€éšœå®³ç™ºç”Ÿå‰å¾Œã§ã® 1 ç§’ã‚ãŸã‚Šã®å‡¦ç†ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚é€šå¸¸ã€éšœå®³ã‹ã‚‰ã®å¾©æ—§ç›´å¾Œã¯ä¸€æ™‚çš„ã«ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãŒä½ä¸‹ã—ã¾ã™ãŒã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰å†é–‹ã•ã‚Œã‚‹ãŸã‚å­¦ç¿’é€²æ—ã¸ã®å½±éŸ¿ã¯æœ€å°é™ã«ç•™ã¾ã‚Šã¾ã™ã€‚

```bash
# æ€§èƒ½æ¸¬å®šã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ä½œæˆ
cat > /fsx/performance_analysis.py << 'EOF'
import json
import pandas as pd
from datetime import datetime

# ãƒ­ã‚°ã‹ã‚‰ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
def extract_throughput_data(log_file):
    throughput_data = []
    with open(log_file, 'r') as f:
        for line in f:
            if 'samples/sec' in line:
                # ãƒ­ã‚°è§£æã—ã¦ã‚¹ãƒ«ãƒ¼ãƒ—ãƒƒãƒˆå€¤ã‚’æŠ½å‡º
                timestamp = line.split()[0] + " " + line.split()[1]
                throughput = float(line.split('samples/sec')[0].split()[-1])
                throughput_data.append({
                    'timestamp': timestamp, 
                    'throughput': throughput
                })
    return throughput_data

# éšœå®³å‰å¾Œã®æ€§èƒ½æ¯”è¼ƒ
baseline_throughput = 1250.0  # samples/sec
post_recovery_throughput = 1180.0  # samples/sec

performance_impact = ((baseline_throughput - post_recovery_throughput) / baseline_throughput) * 100
print(f"Performance Impact: {performance_impact:.2f}%")

# å¾©æ—§æ™‚é–“ã®è¨ˆç®—
failure_time = datetime.strptime('14:30:00', '%H:%M:%S')
recovery_time = datetime.strptime('14:48:15', '%H:%M:%S')
downtime_minutes = (recovery_time - failure_time).total_seconds() / 60
print(f"Total Downtime: {downtime_minutes:.1f} minutes")

# SLO é”æˆçŠ¶æ³ã®è©•ä¾¡
slo_availability = 99.9  # 99.9% availability target
monthly_minutes = 30 * 24 * 60  # 43,200 minutes per month
allowed_downtime = monthly_minutes * (100 - slo_availability) / 100  # 43.2 minutes
print(f"SLO Compliance: {'PASS' if downtime_minutes < allowed_downtime else 'FAIL'}")
EOF

python /fsx/performance_analysis.py
```

ãƒªã‚½ãƒ¼ã‚¹åˆ©ç”¨åŠ¹ç‡ã®åˆ†æã§ã¯ã€GPU ä½¿ç”¨ç‡ã€ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä½¿ç”¨é‡ã®å¤‰åŒ–ã‚’è¿½è·¡ã—ã¾ã™ã€‚é©åˆ‡ã«è¨­è¨ˆã•ã‚ŒãŸãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæ©Ÿèƒ½ã«ã‚ˆã‚Šã€å¾©æ—§å¾Œã®å­¦ç¿’å†é–‹ã¯é«˜åŠ¹ç‡ã§å®Ÿè¡Œã•ã‚Œã€ãƒªã‚½ãƒ¼ã‚¹ã®ç„¡é§„é£ã„ã¯æœ€å°é™ã«æŠ‘ãˆã‚‰ã‚Œã¾ã™ã€‚

å­¦ç¿’ç²¾åº¦ã¸ã®å½±éŸ¿è©•ä¾¡ã§ã¯ã€éšœå®³å‰å¾Œã§ã®æå¤±é–¢æ•°ã®å€¤ã€æ¤œè¨¼ç²¾åº¦ã€åæŸé€Ÿåº¦ã‚’æ¯”è¼ƒã—ã¾ã™ã€‚ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ™ãƒ¼ã‚¹ã®å¾©æ—§ã§ã¯ã€å­¦ç¿’çŠ¶æ…‹ãŒæ­£ç¢ºã«å¾©å…ƒã•ã‚Œã‚‹ãŸã‚ã€ç²¾åº¦ã¸ã®æ‚ªå½±éŸ¿ã¯ã»ã¨ã‚“ã©ç™ºç”Ÿã—ã¾ã›ã‚“ã€‚ãŸã ã—ã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé–“éš”ãŒé•·ã„å ´åˆã¯ã€ä¸€éƒ¨ã®å­¦ç¿’é€²æ—ãŒå¤±ã‚ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚

ã“ã‚Œã‚‰ã®æ¸¬å®šçµæœã‚’æœˆæ¬¡ãƒ¬ãƒãƒ¼ãƒˆã¨ã—ã¦ã¾ã¨ã‚ã€ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼é‹ç”¨ã® KPI ã¨ã—ã¦ç¶™ç¶šçš„ã«ç›£è¦–ã—ã¾ã™ã€‚æ€§èƒ½å½±éŸ¿ãŒè¨±å®¹ç¯„å›²ã‚’è¶…ãˆã‚‹å ´åˆã¯ã€ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆæˆ¦ç•¥ã®è¦‹ç›´ã—ã‚„ã€ã‚ˆã‚Šé«˜æ€§èƒ½ãªã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—ã¸ã®ç§»è¡Œã‚’æ¤œè¨ã—ã¾ã™ã€‚
::::

::::details 4. ãƒ¬ãƒãƒ¼ãƒˆä½œæˆã¨æ”¹å–„ææ¡ˆ

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: Resiliency ã¨ Observability ã®æ¤œè¨¼çµæœã‚’åŒ…æ‹¬çš„ãªãƒ¬ãƒãƒ¼ãƒˆã¨ã—ã¦ã¾ã¨ã‚ã€é‹ç”¨æ”¹å–„ã®ãŸã‚ã®å…·ä½“çš„ãªææ¡ˆã‚’ç­–å®šã—ã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: æ¤œè¨¼çµæœã€å•é¡Œç‚¹ã€æ”¹å–„ææ¡ˆãŒæ–‡æ›¸åŒ–ã•ã‚Œã€ã‚¹ãƒ†ãƒ¼ã‚¯ãƒ›ãƒ«ãƒ€ãƒ¼ã¸ã®å ±å‘Šæº–å‚™ãŒå®Œäº†ã™ã‚‹ã“ã¨ã€‚
:::

åŒ…æ‹¬çš„ãªãƒ¬ãƒãƒ¼ãƒˆä½œæˆã§ã¯ã€å®Ÿæ–½ã—ãŸå…¨ã¦ã®ãƒ†ã‚¹ãƒˆã¨æ¤œè¨¼ã®çµæœã‚’çµ±åˆã—ã€é‹ç”¨ãƒãƒ¼ãƒ ã¨ç ”ç©¶ãƒãƒ¼ãƒ ã®ä¸¡æ–¹ã«ã¨ã£ã¦æœ‰ç”¨ãªæƒ…å ±ã‚’æä¾›ã—ã¾ã™ã€‚Executive Summary ã§ã¯ã€Resiliency æ©Ÿèƒ½ã®æœ‰åŠ¹æ€§ã€Observability ã‚·ã‚¹ãƒ†ãƒ ã®ä¾¡å€¤ã€æ¤œå‡ºã•ã‚ŒãŸå•é¡Œã¨è§£æ±ºç­–ã‚’ç°¡æ½”ã«ã¾ã¨ã‚ã¾ã™ã€‚

```markdown
# HyperPod Slurm Resiliency & Observability æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ

## Executive Summary
- **ãƒ†ã‚¹ãƒˆæœŸé–“**: 2025 å¹´ 1 æœˆ 15 æ—¥ - 1 æœˆ 16 æ—¥
- **å¯¾è±¡ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼**: cpu-slurm-cluster (4 ãƒãƒ¼ãƒ‰, GPU 2 å°è¿½åŠ )
- **å®Ÿæ–½ãƒ†ã‚¹ãƒˆ**: Environment Validation, Intentional Failure Injection, Auto-Resume Verification
- **ä¸»è¦çµæœ**: 
  - éšœå®³æ¤œå‡ºæ™‚é–“: 2.3 åˆ†ï¼ˆç›®æ¨™ 5 åˆ†ä»¥å†…ï¼‰
  - å®Œå…¨å¾©æ—§æ™‚é–“: 18.2 åˆ†ï¼ˆç›®æ¨™ 30 åˆ†ä»¥å†…ï¼‰
  - Auto-Resume æˆåŠŸç‡: 100%ï¼ˆ2/2 ã‚¸ãƒ§ãƒ–ï¼‰
  - æ€§èƒ½å½±éŸ¿: 5.6%ï¼ˆè¨±å®¹ç¯„å›² 10% ä»¥å†…ï¼‰

## æ¤œè¨¼çµæœè©³ç´°

### Environment Validation
- PyTorch ç’°å¢ƒ: å…¨ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæ­£å¸¸å‹•ä½œç¢ºèª
- EFA ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯: å¸¯åŸŸå¹… 95Gbpsã€ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· 8.2Î¼sé”æˆ
- NCCL é€šä¿¡: All-Reduce æ€§èƒ½ 12.8GB/sé”æˆ

### Resiliency Testing  
- æ„å›³çš„éšœå®³æ³¨å…¥: GPU ãƒ—ãƒ­ã‚»ã‚¹å¼·åˆ¶çµ‚äº†ã«ã‚ˆã‚‹ CUDA ã‚¨ãƒ©ãƒ¼
- HMA æ¤œå‡º: 2.3 åˆ†ã§éšœå®³ãƒãƒ¼ãƒ‰ç‰¹å®šã¨ãƒ‰ãƒ¬ã‚¤ãƒ³é–‹å§‹
- ãƒãƒ¼ãƒ‰äº¤æ›: 15.9 åˆ†ã§æ–°ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å‚åŠ å®Œäº†
- ã‚¸ãƒ§ãƒ–å¾©æ—§: ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‹ã‚‰æ­£å¸¸å†é–‹ç¢ºèª

### Observability Effectiveness
- Grafana ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç›£è¦–ã§éšœå®³å¯è¦–åŒ–æˆåŠŸ
- ã‚¢ãƒ©ãƒ¼ãƒˆé€šçŸ¥: GPU æ¸©åº¦ç•°å¸¸ã®äº‹å‰æ¤œå‡ºï¼ˆãƒ†ã‚¹ãƒˆæ™‚ 87Â°C ã§ç™ºç«ï¼‰
- ãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†: 99.7% ã®å¯ç”¨æ€§ã§ç¶™ç¶šãƒ‡ãƒ¼ã‚¿å–å¾—
```

æŠ€è¡“çš„æ”¹å–„ææ¡ˆã§ã¯ã€ä»Šå›ã®æ¤œè¨¼ã§ç‰¹å®šã•ã‚ŒãŸèª²é¡Œã¨è§£æ±ºç­–ã‚’å…·ä½“çš„ã«æç¤ºã—ã¾ã™ã€‚ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆé »åº¦ã®æœ€é©åŒ–ã€ç›£è¦–é–¾å€¤ã®èª¿æ•´ã€ã‚¢ãƒ©ãƒ¼ãƒˆé€šçŸ¥å…ˆã®æ‹¡å……ã€è‡ªå‹•å¾©æ—§ãƒ—ãƒ­ã‚»ã‚¹ã®é«˜é€ŸåŒ–ãªã©ã‚’å«ã¿ã¾ã™ã€‚

é‹ç”¨ãƒ—ãƒ­ã‚»ã‚¹ã®æ”¹å–„ææ¡ˆã§ã¯ã€å®šæœŸçš„ãª Resiliency ãƒ†ã‚¹ãƒˆã®å®Ÿæ–½è¨ˆç”»ã€éšœå®³å¯¾å¿œãƒãƒ‹ãƒ¥ã‚¢ãƒ«ã®æ›´æ–°ã€ãƒãƒ¼ãƒ é–“ã®é€£æºå¼·åŒ–ç­–ã‚’ææ¡ˆã—ã¾ã™ã€‚ã¾ãŸã€é¡ä¼¼ç’°å¢ƒã§ã® best practice ã®å…±æœ‰ã‚„ã€æ¥­ç•Œæ¨™æº–ã¨ã®æ¯”è¼ƒè©•ä¾¡ã‚‚å«ã‚ã¾ã™ã€‚

ã‚³ã‚¹ãƒˆåŠ¹æœåˆ†æã§ã¯ã€è‡ªå‹•å¾©æ—§ã«ã‚ˆã‚‹äººçš„ã‚³ã‚¹ãƒˆå‰Šæ¸›åŠ¹æœã€ãƒ€ã‚¦ãƒ³ã‚¿ã‚¤ãƒ çŸ­ç¸®ã«ã‚ˆã‚‹æ©Ÿä¼šæå¤±å›é¿åŠ¹æœã‚’å®šé‡åŒ–ã—ã¾ã™ã€‚Observability ã‚·ã‚¹ãƒ†ãƒ ã®æ§‹ç¯‰ãƒ»é‹ç”¨ã‚³ã‚¹ãƒˆã¨ã€ãã‚Œã«ã‚ˆã£ã¦å¾—ã‚‰ã‚Œã‚‹ä¾¡å€¤ã‚’æ¯”è¼ƒã—ã€ROIï¼ˆæŠ•è³‡åç›Šç‡ï¼‰ã‚’ç®—å‡ºã—ã¾ã™ã€‚

ä»Šå¾Œã®å±•é–‹è¨ˆç”»ã§ã¯ã€ã‚ˆã‚Šå¤§è¦æ¨¡ãªã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã§ã®æ¤œè¨¼ã€ç•°ãªã‚‹éšœå®³ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã®ãƒ†ã‚¹ãƒˆã€æ©Ÿæ¢°å­¦ç¿’ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰å›ºæœ‰ã® resiliency è¦ä»¶ã¸ã®å¯¾å¿œã‚’ææ¡ˆã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®ææ¡ˆã¯ã€ç¶™ç¶šçš„ãªæ”¹å–„ã‚µã‚¤ã‚¯ãƒ«ã®åŸºç›¤ã¨ãªã‚Šã¾ã™ã€‚
::::

# ã¾ã¨ã‚

æœ¬ç« ã§ã¯ã€Amazon SageMaker HyperPod ã® Slurm ç’°å¢ƒã«ãŠã‘ã‚‹ resiliency æ©Ÿèƒ½ã¨ observability ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè·µçš„ãªæ¤œè¨¼ã‚’å®Ÿæ–½ã—ã¾ã—ãŸã€‚ç†è«–çš„ãªèª¬æ˜ã‹ã‚‰å§‹ã¾ã‚Šã€å®Ÿéš›ã®ãƒãƒ³ã‚ºã‚ªãƒ³ã‚’é€šã˜ã¦ã€å¤§è¦æ¨¡å­¦ç¿’ç’°å¢ƒã«ãŠã‘ã‚‹éšœå®³å¯¾å¿œã¨ç›£è¦–ã®é‡è¦æ€§ã‚’ç¢ºèªã§ãã¾ã—ãŸã€‚

**Resiliency æ©Ÿèƒ½ã®æœ‰åŠ¹æ€§**: Auto-Resume æ©Ÿèƒ½ã¨ Health Monitoring Agent ã®çµ„ã¿åˆã‚ã›ã«ã‚ˆã‚Šã€ãƒãƒ¼ãƒ‰éšœå®³ã‹ã‚‰ã®è‡ªå‹•å¾©æ—§ãŒç¢ºå®Ÿã«å‹•ä½œã™ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã—ãŸã€‚éšœå®³æ¤œå‡ºã‹ã‚‰å®Œå…¨å¾©æ—§ã¾ã§å¹³å‡ 18 åˆ†ã¨ã„ã†æ™‚é–“ã¯ã€å‰ç« ã§ç´¹ä»‹ã—ãŸ Meta Llama 3 ã®äº‹ä¾‹ã¨æ¯”è¼ƒã—ã¦ã‚‚å®Ÿç”¨çš„ãªæ°´æº–ã§ã™ã€‚ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ™ãƒ¼ã‚¹ã®å­¦ç¿’å†é–‹ã«ã‚ˆã‚Šã€éšœå®³ã«ã‚ˆã‚‹å­¦ç¿’é€²æ—ã®æå¤±ã‚’æœ€å°é™ã«æŠ‘åˆ¶ã§ãã¦ã„ã¾ã™ã€‚

**Observability ã®ä¾¡å€¤**: Amazon Managed Prometheus ã¨ Grafana ã‚’ç”¨ã„ãŸçµ±åˆç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ã«ã‚ˆã‚Šã€éšœå®³ã®äºˆå…†æ¤œå‡ºã‹ã‚‰å¾©æ—§ãƒ—ãƒ­ã‚»ã‚¹ã®å¯è¦–åŒ–ã¾ã§ã€åŒ…æ‹¬çš„ãª observability ãŒå®Ÿç¾ã•ã‚Œã¾ã—ãŸã€‚ç‰¹ã« GPU æ¸©åº¦ç›£è¦–ã«ã‚ˆã‚‹äºˆé˜²çš„ã‚¢ãƒ©ãƒ¼ãƒˆã¯ã€æ·±åˆ»ãªéšœå®³ã‚’æœªç„¶ã«é˜²ãæœ‰åŠ¹ãªæ‰‹æ®µã¨ã—ã¦æ©Ÿèƒ½ã—ã¾ã™ã€‚å¤šå±¤çš„ãªãƒ¡ãƒˆãƒªã‚¯ã‚¹åé›†ã«ã‚ˆã‚Šã€ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã€ãƒãƒ¼ãƒ‰ã€ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®å„ãƒ¬ãƒ™ãƒ«ã§ã®å•é¡Œã‚’è¿…é€Ÿã«ç‰¹å®šã§ãã¾ã™ã€‚

**Environment Validation ã®é‡è¦æ€§**: PyTorchã€EFAã€NCCL ã®å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ç³»çµ±çš„ã«æ¤œè¨¼ã™ã‚‹ã“ã¨ã§ã€åˆ†æ•£å­¦ç¿’ç’°å¢ƒã®å¥å…¨æ€§ã‚’å®¢è¦³çš„ã«è©•ä¾¡ã§ãã¾ã—ãŸã€‚ã“ã‚Œã‚‰ã®æ¤œè¨¼ã¯ã€å¤§è¦æ¨¡å­¦ç¿’ã‚’é–‹å§‹ã™ã‚‹å‰ã®å¿…é ˆæ‰‹é †ã¨ã—ã¦ä½ç½®ã¥ã‘ã‚‰ã‚Œã¾ã™ã€‚å®šæœŸçš„ãª validation å®Ÿè¡Œã«ã‚ˆã‚Šã€ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã®çµŒå¹´åŠ£åŒ–ã‚„è¨­å®šå¤‰æ›´ã®å½±éŸ¿ã‚’æ—©æœŸç™ºè¦‹ã§ãã¾ã™ã€‚

**å®Ÿè·µçš„ãªé‹ç”¨çŸ¥è­˜ã®ç¿’å¾—**: æ„å›³çš„ãªéšœå®³æ³¨å…¥ã‹ã‚‰å¾©æ—§ãƒ—ãƒ­ã‚»ã‚¹ã®è©³ç´°ç›£è¦–ã¾ã§ã€å®Ÿéš›ã®é‹ç”¨ã§é­é‡ã™ã‚‹çŠ¶æ³ã‚’æ¨¡æ“¬ä½“é¨“ã™ã‚‹ã“ã¨ã§ã€ç†è«–ã¨å®Ÿè·µã®ã‚®ãƒ£ãƒƒãƒ—ã‚’åŸ‹ã‚ã‚‹ã“ã¨ãŒã§ãã¾ã—ãŸã€‚SageMaker Studio ã¨ã®çµ±åˆã«ã‚ˆã‚Šã€å¾“æ¥ã®ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³æ“ä½œã«åŠ ãˆã¦ã€GUI ãƒ™ãƒ¼ã‚¹ã§ã®ç›´æ„Ÿçš„ãªã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ç®¡ç†ã‚‚å®Ÿç¾ã•ã‚Œã¾ã™ã€‚

ä»Šå›ã®æ¤œè¨¼ã«ã‚ˆã‚Šã€HyperPod Slurm ç’°å¢ƒãŒæä¾›ã™ã‚‹ resiliency æ©Ÿèƒ½ã¯ã€å¤§è¦æ¨¡åˆ†æ•£å­¦ç¿’ã®å®Ÿç”¨çš„ãªè¦æ±‚ã‚’æº€ãŸã™æ°´æº–ã«ã‚ã‚‹ã“ã¨ãŒç¢ºèªã•ã‚Œã¾ã—ãŸã€‚é©åˆ‡ãª observability ã‚·ã‚¹ãƒ†ãƒ ã¨ã®çµ„ã¿åˆã‚ã›ã«ã‚ˆã‚Šã€ç ”ç©¶è€…ã¯å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®é–‹ç™ºã«é›†ä¸­ã—ã€ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ã®éšœå®³å¯¾å¿œã¯è‡ªå‹•åŒ–ã•ã‚ŒãŸã‚·ã‚¹ãƒ†ãƒ ã«å§”ä»»ã§ãã¾ã™ã€‚

ç¶™ç¶šçš„ãªæ”¹å–„ã¨ã—ã¦ã€ã‚ˆã‚Šå¤§è¦æ¨¡ãªã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã§ã®æ¤œè¨¼ã€ç•°ãªã‚‹éšœå®³ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ã®ãƒ†ã‚¹ãƒˆã€æ©Ÿæ¢°å­¦ç¿’ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰å›ºæœ‰ã®è¦ä»¶ã¸ã®æœ€é©åŒ–ã‚’é€²ã‚ã‚‹ã“ã¨ã§ã€ã•ã‚‰ã«å …ç‰¢ã§åŠ¹ç‡çš„ãªå­¦ç¿’ç’°å¢ƒã‚’æ§‹ç¯‰ã§ãã‚‹ã§ã—ã‚‡ã†ã€‚
