---
title: "パイプライン並列化を理解する -- GPipe から DualPipe まで、スケジューリング戦略の全体像"
emoji: "⛓️"
type: "tech"
topics: ["MachineLearning", "DistributedTraining", "PyTorch", "GPU", "DeepLearning"]
free: true
---

# パイプライン並列化を理解する

## はじめに

前章では Transformer の分散並列化における 6 つの戦略と集団通信の全体像を解説しました。本章では、その中でも特に奥が深い**パイプライン並列化（Pipeline Parallelism, PP）**に焦点を当て、各スケジューリング戦略を詳しく解説します。

パイプライン並列化は、モデルを**層（レイヤー）方向に分割**し、複数のデバイスに配置する手法です。Tensor Parallel が 1 つの層の中の行列演算を分割するのに対し、Pipeline Parallel はモデル全体を層のブロック単位で分割します。

:::message
本記事は [ailzhang.github.io の記事](https://ailzhang.github.io/posts/pipeline-parallelism-demystified/) の内容を参考に、論文の検証を加えて日本語で再構成したものです。
:::

---

## パイプライン並列化の基本概念

### ステージとマイクロバッチ

パイプライン並列化の 2 つの基本概念を押さえましょう。

**ステージ（Stage）**: モデルを層方向に分割した各ブロックです。例えば 24 層のモデルを 4 GPU に分割する場合、各 GPU は 6 層ずつを担当します。

```
24 層モデル → 4 ステージに分割

Stage 0 (GPU 0): Layer 0-5
Stage 1 (GPU 1): Layer 6-11
Stage 2 (GPU 2): Layer 12-17
Stage 3 (GPU 3): Layer 18-23
```

**マイクロバッチ（Microbatch）**: ミニバッチ（1 回の重み更新で使用するデータの単位）をさらに小さく分割したものです。パイプラインの並列度を高めるために使用されます。

```
ミニバッチ (B=32) → 4 マイクロバッチに分割

Microbatch 0: サンプル 0-7
Microbatch 1: サンプル 8-15
Microbatch 2: サンプル 16-23
Microbatch 3: サンプル 24-31
```

**重み更新のタイミング**: すべてのパイプラインスケジュール（GPipe, 1F1B, Zero Bubble など）では、**勾配蓄積（gradient accumulation）** を使用します。各マイクロバッチで計算された勾配を足し合わせ、全マイクロバッチの処理完了後にまとめて 1 回の重み更新を行います。これにより、ミニバッチ全体を一度に処理したのと同じ効果が得られます。

### パイプラインバブル -- 最大の課題

パイプライン並列化の最大の課題が**パイプラインバブル（Pipeline Bubble）**です。バブルとは、GPU がアイドル状態になる時間のことを指します。

素朴にモデルを分割して逐次実行すると、ある GPU が計算している間、他の GPU は待機状態になります。

```
ステージ間の逐次実行（Naive Model Parallel）:

        時刻 →
GPU 0: [F0]                              [B0]     [F1]
GPU 1:      [F0]                    [B0]              [F1]
GPU 2:           [F0]          [B0]                        [F1]
GPU 3:                [F0][B0]                                  [F1][B1]

※ F = Forward, B = Backward, 数字 = マイクロバッチ番号
※ 空白部分が待機時間（パイプラインバブル）
※ Forward は GPU 0→1→2→3 と順に実行され、
   Backward は GPU 3→2→1→0 と逆順に実行される
```

この非効率を解消するために、様々なスケジューリング戦略が提案されてきました。

### バブル率の定義

バブル率は、パイプラインの効率を測る指標です。以下のように定義されます。

```
バブル率 = (全 GPU の総アイドル時間) / (全 GPU の総実行可能時間)
```

理想的にはバブル率 0（全 GPU が常に稼働）ですが、実際にはステージ間の依存関係により一定のバブルが発生します。以降の節で、各スケジューリング戦略がこのバブル率をどのように削減するかを見ていきます。

---

## スケジューリング戦略の 2 つの系統 -- BFS と DFS

パイプラインスケジューリングを理解する上で重要な概念が、**BFS（Breadth-First Scheduling）** と **DFS（Depth-First Scheduling）** の 2 つの系統です。

### BFS（幅優先スケジューリング）

BFS では、**全マイクロバッチの順伝播を先に完了**してから、逆伝播を開始します。GPipe がこの方式を採用しています。

```
BFS: 全マイクロバッチの Forward を先に実行

GPU 0: [F0][F1][F2][F3]                        [B3][B2][B1][B0]
GPU 1:      [F0][F1][F2][F3]              [B3][B2][B1][B0]
GPU 2:           [F0][F1][F2][F3]    [B3][B2][B1][B0]
GPU 3:                [F0][F1][F2][F3][B3][B2][B1][B0]
```

**特徴**:
- 実装がシンプル
- 全マイクロバッチの中間アクティベーションを保持する必要があるため、メモリ使用量が大きい

### DFS（深さ優先スケジューリング）

DFS では、**順伝播と逆伝播を交互に実行**し、1 つのマイクロバッチの処理をできるだけ早く完了させます。1F1B がこの方式を採用しています。

```
DFS: Forward と Backward を交互に実行

GPU 0: [F0][F1][F2][F3][B0][B1][B2][B3]
GPU 1:      [F0][F1][F2][B0][F3][B1][B2][B3]
GPU 2:           [F0][F1][B0][F2][B1][F3][B2][B3]
GPU 3:                [F0][B0][F1][B1][F2][B2][F3][B3]
```

**特徴**:
- 早期に逆伝播を開始するため、中間アクティベーションを早く解放でき、メモリ効率が良い
- 同時に保持するアクティベーション数が少ない

| 比較項目 | BFS (GPipe) | DFS (1F1B) |
|---------|-------------|------------|
| Forward の実行順序 | 全マイクロバッチを先に処理 | 逆伝播と交互に処理 |
| メモリ使用量 | 大（全アクティベーション保持） | 小（早期解放） |
| バブル率 | 同等 | 同等 |
| 代表的手法 | GPipe | PipeDream, Megatron 1F1B |

:::message
**注**: 上記の BFS/DFS の図は、スケジューリング方式の基本概念を示す簡略図です。実際のスケジュールでは、ウォームアップ（パイプラインへの投入）、定常状態、クールダウン（パイプラインからの排出）の 3 つのフェーズがあり、より複雑な配置になります。詳細は以降のセクション（GPipe, 1F1B など）で解説します。
:::

---

## GPipe -- BFS 方式の基本形

GPipe（Google, 2018）は、パイプライン並列化の基礎となるスケジューリング手法です。

### 動作原理

GPipe は BFS 方式を採用し、以下のように動作します。

1. ミニバッチを M 個のマイクロバッチに分割
2. 全マイクロバッチの順伝播を実行（パイプライン形式で順次投入）
3. 全順伝播が完了したら、逆伝播を実行
4. 勾配を蓄積し、全マイクロバッチの処理完了後に 1 回の重み更新を実行

```
GPipe スケジューリング（p=4 ステージ, m=4 マイクロバッチ）:

時間 →
GPU 0: [F0][F1][F2][F3][ idle  ][B3][B2][B1][B0][ Update ]
GPU 1:     [F0][F1][F2][F3][ idle  ][B3][B2][B1][B0]
GPU 2:         [F0][F1][F2][F3][ idle  ][B3][B2][B1][B0]
GPU 3:             [F0][F1][F2][F3][B3][B2][B1][B0]

※ idle = パイプラインバブル
```

### バブル率

GPipe のバブル率は以下の通りです。

**なぜ (p-1) が係数になるのか**: 最初のマイクロバッチが Stage 0 で Forward を完了してから、Stage 1, 2, ..., p-1 を順に通過して最終ステージに到達するまでに (p-1) ステージ分の処理時間が必要です。この間、最初のステージは次のマイクロバッチを待つため、(p-1) 個のタイムスロット分のバブルが発生します。

```
バブルサイズ = (p - 1)(F + B + W)
バブル率 = (p - 1)(F + B + W) / [m(F + B + W)] = (p - 1) / m

p: パイプラインステージ数
m: マイクロバッチ数
F, B, W: 各処理の実行時間（Forward, Backward 入力勾配, Backward 重み勾配）
```

例えば p=4, m=8 の場合、バブル率は (4-1)/8 = 37.5% です。マイクロバッチ数 m を増やすことでバブル率を下げられますが、マイクロバッチサイズが小さくなりすぎると GPU の演算効率が低下します。

:::message
**バブルサイズの構成要素と表記について**:

GPipe 自体は逆伝播を B（入力勾配）と W（重み勾配）に分離しません。GPipe の文脈では逆伝播は一体の処理として扱われます。

しかし、本記事では Zero Bubble との統一的な比較のために、F, B, W に分けて表記しています。`F + B + W` = Forward + Backward 全体 という関係です。GPipe では全マイクロバッチが順伝播を完了してから逆伝播を開始するため、Forward と Backward 全体がパイプラインの依存関係に含まれ、バブル時間に寄与します。

後述の 1F1B も同様に F+B+W 全体がバブルに含まれますが、Zero Bubble では B と W を分離し、W を空き時間に配置することでバブルを削減します。
:::


### メモリ使用量

GPipe では全マイクロバッチの中間アクティベーションを保持する必要があるため、メモリ使用量は O(m) に比例します。これが GPipe の主要な制約です。

---

## 1F1B -- DFS 方式の基本形

1F1B（One Forward One Backward）は、PipeDream（Microsoft, 2018）で提案された DFS 方式のスケジューリングです。Megatron-LM でも採用されています。

### 動作原理

1F1B は 3 つのフェーズで構成されます。

1. **ウォームアップフェーズ**: 順伝播のみを実行してパイプラインを埋める
2. **定常状態フェーズ**: 1 回の順伝播と 1 回の逆伝播を交互に実行
3. **クールダウンフェーズ**: 残りの逆伝播を完了

**注**: フェーズ区分線は GPU 0 の視点で記載しています。下位ステージ（GPU 1-3）では定常状態が延長され、ウォームアップで投入されなかったマイクロバッチの Forward が定常状態の一部として処理されます（例: GPU 1 の F7）。

```
1F1B スケジューリング（p=4 ステージ, m=8 マイクロバッチ）:

      ウォームアップ    定常状態                    クールダウン
      ──────────    ──────────────────────    ──────────
GPU 0: [F0][F1][F2][F3] [B0][F4][B1][F5][B2][F6][B3][F7] [B4][B5][B6][B7]
GPU 1:     [F0][F1][F2] [B0][F3][B1][F4][B2][F5][B3][F6] [B4][F7][B5][B6][B7]
GPU 2:         [F0][F1] [B0][F2][B1][F3][B2][F4][B3][F5] [B4][F6][B5][F7][B6][B7]
GPU 3:             [F0] [B0][F1][B1][F2][B2][F3][B3][F4] [B4][F5][B5][F6][B6][F7][B7]
```

### バブル率

1F1B のバブル率は GPipe と同じです。

```
バブル率 = (p - 1) / m
```

バブル率は同じですが、メモリ効率が大幅に改善されている点が重要です。

### メモリ使用量

1F1B の重要な利点はメモリ効率です。定常状態フェーズでは、各 GPU が同時に保持する中間アクティベーションの数は最大 p 個（ステージ数）です。GPipe の O(m) と比較して大幅に削減されます。

```
メモリ使用量の比較:
  GPipe: O(m) -- 全マイクロバッチのアクティベーション
  1F1B:  O(p) -- ステージ数分のアクティベーション

例: p=4, m=32 の場合
  GPipe: 32 マイクロバッチ分のアクティベーション
  1F1B:  4 マイクロバッチ分のアクティベーション（8 倍の削減）
```

---

## Interleaved 1F1B -- 仮想ステージによるバブル削減

Interleaved 1F1B は Megatron-LM（NVIDIA, 2021）で提案された手法で、**仮想パイプラインステージ（Virtual Pipeline Stages）**を導入してバブル率を削減します。

### 動作原理

通常の 1F1B では各 GPU が 1 つの連続した層ブロック（ステージ）を担当しますが、Interleaved 1F1B では各 GPU が**複数の非連続なステージ**を担当します。

```
通常の 1F1B（各 GPU = 1 ステージ）:

GPU 0: Layer 0-5    （Stage 0）
GPU 1: Layer 6-11   （Stage 1）
GPU 2: Layer 12-17  （Stage 2）
GPU 3: Layer 18-23  （Stage 3）

Interleaved 1F1B（各 GPU = 2 仮想ステージ, v=2）:

GPU 0: Layer 0-2, Layer 12-14  （Stage 0, Stage 4）
GPU 1: Layer 3-5, Layer 15-17  （Stage 1, Stage 5）
GPU 2: Layer 6-8, Layer 18-20  （Stage 2, Stage 6）
GPU 3: Layer 9-11, Layer 21-23 （Stage 3, Stage 7）
```

仮想ステージ数を v とすると、パイプラインの総ステージ数は p * v になります。各ステージが担当する層数は 1/v に減るため、1 ステージあたりの実行時間も 1/v に短縮されます。

### バブル率

```
バブル率 = (p - 1) / (v * m)

v: 仮想ステージ数（各デバイスが担当するチャンク数）
```

v=2 の場合、バブル率は通常の 1F1B の半分になります。

### トレードオフ

仮想ステージ数を増やすとバブル率は下がりますが、ステージ間の通信回数が v 倍に増加します。Tensor Parallel が NVLink のような高速接続を前提とするのと同様に、通信コストとバブル削減のバランスを考慮する必要があります。

また、マイクロバッチ数 m がパイプラインステージ数 p の整数倍である必要があるという制約もあります。

---

## Zero Bubble -- 逆伝播の分離によるバブル解消

Zero Bubble Pipeline Parallelism（Qi et al., 2024）は、逆伝播を 2 つの独立した計算に分離することで、パイプラインバブルの大幅な削減を実現します。

### 核心となるアイデア -- B と W の分離

従来のスケジューリングでは、逆伝播（Backward）を 1 つの単位として扱っていました。Zero Bubble はこれを 2 つに分離します。

- **B（dL/dx）**: 入力に対する勾配計算。**連鎖律（chain rule）により、前のステージに勾配を渡す必要がある**ため、ステージ間の依存関係がある
- **W（dL/dw）**: パラメータに対する勾配計算。**ローカルなパラメータの勾配なので、他のステージの結果を待たずに計算できる**ため、自由にスケジューリング可能

:::message
**なぜ B と W で依存関係が異なるのか**: 逆伝播では連鎖律によって「出力側から入力側へ」勾配が伝播します。B は次のステージ（入力側）に勾配 dL/dx を渡す必要があり、そのステージの逆伝播が B の結果を待つため、ステージ間の依存が発生します。一方、W はステージ内のパラメータの勾配を計算するだけで、他のステージとのデータのやり取りが不要なため、独立して実行できます。
:::

```
従来: Backward = [B + W] -- 一体として扱う
Zero Bubble: Backward = [B] + [W] -- 分離して個別にスケジューリング

重要な洞察:
  F と B は異なるステージ間で逐次依存関係がある
  W はどのステージにも依存しない → バブルを埋めるのに使える
```

### 3 つのスケジュール変種

Zero Bubble 論文では 3 つのスケジュールが提案されています。

**ZB-H1（メモリ効率型）**:
- 1F1B と同等のメモリ使用量を維持
- バブルを 1F1B の約 1/3 に削減
- W をウォームアップ後の空き時間に配置

**ZB-H2（ゼロバブル型）**:
- バブルをほぼ完全に解消
- ウォームアップ中に追加の Forward を実行し、W の並べ替えで空き時間を解消
- メモリ使用量は 1F1B の約 2 倍

**ZB-V（バランス型）**:
- モデルを 2p 個のチャンクに分割し、各デバイスに**V 字型**（先頭と末尾のチャンクを同じ GPU に配置するパターン）で 2 チャンクを配置
- この配置により、Forward と Backward が同じデバイスから始まり、早期にアクティベーションを解放可能
- 1F1B と同等のメモリ制約内でバブルを最小化
- Interleaved 1F1B と類似のアプローチだが、チャンク配置パターンが異なる

```
ZB-V のステージ配置（p=4）:

GPU 0: Chunk 0, Chunk 7   (V 字の左端と右端)
GPU 1: Chunk 1, Chunk 6
GPU 2: Chunk 2, Chunk 5
GPU 3: Chunk 3, Chunk 4   (V 字の底)

Forward:  0→1→2→3→4→5→6→7
Backward: 7→6→5→4→3→2→1→0

Forward と Backward が同じデバイスから始まるため
アクティベーションの早期解放が可能
```

### バブル率の比較

```
            バブル率               メモリ（アクティベーション数）
1F1B:       (p-1)(F+B+W) / m      p

ZB-H1:      (p-1)(F+B) / m        p
            (W を空き時間に配置)

ZB-H2:      ≈ 0                   約 2p
            (ILP ソルバーで最適化)

ZB-V:       大幅に削減             p（1F1B と同等）
            (V 字配置 + W 最適化)

F: Forward 1 回の実行時間
B: dL/dx の実行時間（Backward の入力勾配計算）
W: dL/dw の実行時間（Backward の重み勾配計算）
m: マイクロバッチ数
p: パイプラインステージ数

※ バブルサイズは (p-1) * (該当する実行時間の合計) で表されます。
  1F1B では F, B, W のすべてがパイプラインの依存関係に含まれるため、
  バブルに F+B+W の全時間が寄与します。Zero Bubble は W を空き時間に
  配置することでバブルを削減します。

※ ZB-H2 のバブル率: 論文では整数線形計画法（ILP）ソルバーを使って
  最適なスケジュールを見つけるため、閉形式のバブル率は明示的には
  与えられていません。実際には追加の Forward と W の並べ替えにより、
  理論的にバブルをほぼゼロに近づけられます（ZB-H2 が「Zero Bubble」
  の名を冠する理由）。

※ ZB-V のバブル率: V 字型配置により Forward と Backward が同じデバイス
  から開始・終了するため、アクティベーションの早期解放と W の効率的な
  配置が可能になります。これにより、1F1B と同等のメモリ制約内で
  バブルを大幅に削減できます（ZB-H1 相当の削減効果）。論文では
  Interleaved 1F1B と類似のアプローチとして説明されていますが、
  チャンク配置パターンが異なります。
```

Zero Bubble は、1F1B と比較して最大 23% のスループット向上を実現しています（メモリ制約が同等の場合）。

---

## DualPipe -- 双方向パイプラインの革新

DualPipe は DeepSeek-V3（DeepSeek, 2024）の訓練で使用された手法で、パイプラインの両端からマイクロバッチを同時に送り込む双方向スケジューリングを実現します。

### 動作原理

DualPipe の核心は、2 つの 1F1B スケジュールを**逆方向**に同時実行することです。

**「逆方向」とは**: 通常のパイプラインは Stage 0 → Stage 1 → ... → Stage N と進みますが、逆方向パイプラインは **Stage N → ... → Stage 1 → Stage 0 の順序でマイクロバッチを送り込みます**。つまり、パイプラインの最終ステージから最初のステージへ向かう方向です。

```
DualPipe の概念（p=4 ステージ）:

正方向（マイクロバッチ群 A）:
  Stage 0 → Stage 1 → Stage 2 → Stage 3

逆方向（マイクロバッチ群 B）:
  Stage 3 → Stage 2 → Stage 1 → Stage 0

両方向を同時に実行:
GPU 0: [Fa0]...[Fb0][Fa1][Fb1]...[Ba0]...[Bb0]
GPU 1: ...[Fa0]...[Fb0][Fa1][Fb1]...[Ba0]...
GPU 2: ...[Fa0]...[Fb0]...[Ba0]...[Bb0]...
GPU 3: [Fb0]...[Fa0][Fb1][Fa1]...[Bb0]...[Ba0]

Fa = 正方向 Forward, Fb = 逆方向 Forward
Ba = 正方向 Backward, Bb = 逆方向 Backward
```

### 計算と通信の重なり

DualPipe の重要な特徴は、計算と通信を重ねて実行できることです。DeepSeek-V3 の MoE アーキテクチャでは AllToAll 通信が頻繁に発生しますが、DualPipe により正方向と逆方向の計算を交互にスケジューリングすることで、通信の遅延を計算で隠蔽できます。

論文では「all-to-all と PP 通信の両方を実行中に完全に隠蔽できる」と述べられています。

### バブル率

DeepSeek-V3 の論文によると、DualPipe のバブル式は以下の通りです。

**記号の定義**:
- F&B: Forward と Backward が重なっている部分の実行時間（両方向の処理が同時に走る部分）
- F, B, W: 各処理の実行時間（Forward, Backward 入力勾配, Backward 重み勾配）

```
DualPipe:  (p/2 - 1) * (F&B + B - 3W)
1F1B:      (p - 1) * (F + B + W)
```

DualPipe では以下の理由でバブルが削減されます。

1. **ステージ係数が半減**: 双方向スケジューリングにより係数が `p-1` から `p/2-1` に削減
   - 通常の 1F1B では全ステージ（p 個）を一方向に通るため (p-1) 個のバブルが発生
   - DualPipe では各方向が p/2 ステージのみを通るため、バブルは (p/2-1) に削減
2. **W の重複実行**: 両方向のパイプラインが進行する間に W を空き時間に配置でき、実質的に `-3W` の削減効果
   - 正方向と逆方向の処理が交互に進むため、一方の計算中に他方の W を実行可能
   - これにより W がバブル時間から除外される
3. **F と B の重なり**: 正方向と逆方向の処理が重なることで、実効的な処理時間が短縮
   - 「F&B」は両方向の処理が同時に走る部分の実行時間を表す

**直感的理解**: DualPipe は「パイプラインを 2 つに分けて両端から送り込む」ことで、各パイプラインが短くなり（p → p/2）、さらに両方向が同時に進むことで空き時間に W を挿入できます。結果として、バブル時間が通常の 1F1B の半分以下に削減されます。

:::message
DeepSeek-V3 論文では、MoE の AllToAll 通信を含む実測環境で 1F1B と比較しています。1F1B のバブル式が `(p-1)(F+B)` と簡略化されているのは、この文脈での比較を示しています。理論的な完全なバブルサイズは `(p-1)(F+B+W)` です。
:::

### トレードオフ

DualPipe は各デバイスが正方向と逆方向の両方のステージを保持する必要があるため、**メモリ使用量が約 2 倍**になります。例えば通常 2 層を担当するデバイスが、DualPipe では 4 層分のパラメータとアクティベーションを保持する必要があります。

---

## Eager 1F1B -- 通信と計算の早期重なり

Eager 1F1B は、通信を早期に開始して計算と重ねることで、パイプラインの効率を改善する手法です。

### 動作原理

通常の 1F1B では、Forward の計算が完全に終了してから次のステージに送信します。Eager スケジューリングでは、**Forward の計算を複数の部分（チャンク）に分割し、最初の部分が完了した時点で送信を開始**します。これにより、次のステージでの受信と計算を重ねられます。

```
通常の 1F1B:
GPU 0: [F0  ][send]     [F1  ][send]
GPU 1:       [recv][F0  ]      [recv][F1  ]

Eager 1F1B:
GPU 0: [F0  ][send][F1  ][send]
GPU 1:    [recv+F0 ][recv+F1  ]

※ send/recv を計算と重ねることで待機時間を削減
```

### トレードオフ

Eager スケジューリングは Forward のスループットを改善しますが、追加のマイクロバッチが同時にパイプライン内に存在するため、メモリ使用量がやや増加します。

---

## スケジューリング戦略の比較まとめ

以下の表で各戦略を比較します。

| 戦略 | バブル率 | メモリ（アクティベーション） | 通信コスト | 実装の複雑さ |
|------|---------|----------------------|-----------|------------|
| GPipe | (p-1)/m | O(m) | 低 | 低 |
| 1F1B | (p-1)/m | O(p) | 低 | 中 |
| Interleaved 1F1B | (p-1)/(v*m) | O(p) | 中（v 倍） | 中 |
| ZB-H1 | 約 1/3 of 1F1B | O(p) | 低 | 高 |
| ZB-H2 | 約 0 | O(2p) | 低 | 高 |
| ZB-V | 大幅削減 | O(p) | 中（2 倍） | 高 |
| DualPipe | 大幅削減 | O(2p) | 中 | 高 |
| Eager 1F1B | (p-1)/m（通信隠蔽） | O(p) + alpha | 低 | 中 |

```
進化の系譜:

GPipe (2018)          1F1B / PipeDream (2018)
  |                       |
  BFS 方式                DFS 方式
                          |
                    Interleaved 1F1B (2021)
                          |
                    +-----+-----+
                    |           |
              Zero Bubble    Eager 1F1B
              (2024)
                    |
              DualPipe (2024, DeepSeek-V3)
```

---

## 実装の観点 -- PyTorch での対応状況

PyTorch の `torch.distributed.pipelining` モジュールでは、以下のスケジューリング戦略がサポートされています。

- `ScheduleGPipe`: GPipe 方式
- `Schedule1F1B`: 1F1B 方式
- `ScheduleInterleaved1F1B`: Interleaved 1F1B 方式
- `ScheduleLoopedBFS`: BFS 方式のループ型スケジュール

その他、Megatron-LM や DeepSpeed などのフレームワークでも各種スケジューリング戦略が実装されています。

---

## まとめ

本章では、パイプライン並列化の各スケジューリング戦略を解説しました。

**基本概念**:
- パイプライン並列化はモデルを層方向に分割し、マイクロバッチをパイプライン形式で処理する
- バブル（GPU アイドル時間）の削減が主要な課題
- BFS（幅優先）と DFS（深さ優先）の 2 つの系統がある

**各戦略の位置づけ**:
- **GPipe**: BFS の基本形。シンプルだがメモリ非効率
- **1F1B**: DFS の基本形。メモリ効率が大幅に改善
- **Interleaved 1F1B**: 仮想ステージでバブル率を 1/v に削減。通信コスト増
- **Zero Bubble**: 逆伝播の B/W 分離でバブルをほぼ解消
- **DualPipe**: 双方向パイプラインで計算と通信を重畳。DeepSeek-V3 で採用
- **Eager 1F1B**: 通信の早期開始による効率改善

パイプライン並列化は、Tensor Parallel や Data Parallel と組み合わせた 3D 並列化（あるいはさらに多くの並列化を組み合わせた nD 並列化）の重要な一角を担っています。モデルの規模、ハードウェア構成、メモリ制約に応じて、適切なスケジューリング戦略を選択することが重要です。

---

## 参考文献

- [Pipeline Parallelism Demystified (ailzhang)](https://ailzhang.github.io/posts/pipeline-parallelism-demystified/) -- 本記事の参考元
- [GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism](https://arxiv.org/abs/1811.06965) -- GPipe の原論文
- [PipeDream: Fast and Efficient Pipeline Parallel DNN Training](https://arxiv.org/abs/1806.03377) -- 1F1B を提案した PipeDream の原論文
- [Efficient Large-Scale Language Model Training on GPU Clusters Using Megatron-LM](https://arxiv.org/abs/2104.04473) -- Interleaved 1F1B を提案した Megatron-LM の論文
- [Zero Bubble Pipeline Parallelism](https://arxiv.org/abs/2401.10241) -- Zero Bubble の原論文
- [DeepSeek-V3 Technical Report](https://arxiv.org/abs/2412.19437) -- DualPipe を含む DeepSeek-V3 の技術報告
- [Breadth-First Pipeline Parallelism](https://arxiv.org/abs/2211.05953) -- BFS 方式のパイプラインスケジューリング
