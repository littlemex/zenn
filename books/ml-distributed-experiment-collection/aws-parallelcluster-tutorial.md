---
title: "AWS ParallelCluster ãƒãƒ³ã‚ºã‚ªãƒ³"
emoji: "ğŸ¶"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["aws", "sagemaker", "hyperpod", "distributed", "infrastructure"]
free: true
---

::::details å‰æ
:::message
**å¯¾è±¡èª­è€…**: å¤§è¦æ¨¡åŸºç›¤ãƒ¢ãƒ‡ãƒ«ãŒã©ã†ã„ã†ã‚‚ã®ã‹ã‚’ç†è§£ã—ã¦ã„ã‚‹æ–¹ã€ã“ã‚Œã‹ã‚‰ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã‚’è¡Œã†æ–¹
:::
:::message
**ãƒ©ã‚¤ã‚»ãƒ³ã‚¹**: Â© 2025 littlemex.
æœ¬æ–‡ãŠã‚ˆã³è‡ªä½œå›³è¡¨: CC BY 4.0
â€»å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ã®å¼•ç”¨ã‚„ç¿»è¨³éƒ¨åˆ†ã¯åŸå…¸ã®è‘—ä½œæ¨©ã«å¾“ã„ã¾ã™ã€‚
å¼•ç”¨ç”»åƒ: å„ç”»åƒã®å‡ºå…¸ã«è¨˜è¼‰ã•ã‚ŒãŸãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã«å¾“ã„ã¾ã™ã€‚
:::
:::message
ä¸€éƒ¨ AI ã‚’ç”¨ã„ã¦æ–‡ç« ã‚’ä½œæˆã—ã¾ã™ã€‚ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¯å®Ÿæ–½ã—ã¾ã™ãŒã€è¦‹é€ƒã›ãªã„é‡å¤§ãªé–“é•ã„ãªã©ãŒã‚ã‚Œã°[ã“ã¡ã‚‰ã®Issue](https://github.com/littlemex/samples/issues)ã‹ã‚‰é€£çµ¡ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚
:::
::::

**æœ¬ç« ã§ã¯ AWS ParallelCluster ã‚’å®Ÿéš›ã«è©¦ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚ä»¥ä¸‹ã®ãƒªãƒã‚¸ãƒˆãƒªã‚’ãƒã‚¹ã‚¿ãƒ¼ã¨ã—ã¦èª¬æ˜ã«ä¸€éƒ¨è£œè¶³ã‚’åŠ ãˆã¦å®Ÿæ–½ã—ã¾ã™ã€‚**

:::message
å®Ÿè£…ãŒå¤‰æ›´ã•ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚å¿…è¦ã«å¿œã˜ã¦ãƒªãƒã‚¸ãƒˆãƒªã® README ã‚’ç¢ºèªãã ã•ã„
:::

https://github.com/aws-samples/awsome-distributed-training/tree/main/1.architectures/2.aws-parallelcluster

è‹±èªã«æŠµæŠ—ãŒãªã„æ–¹ã¯ä»¥ä¸‹ã®ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ§ãƒƒãƒ—ã‚‚ãŠã™ã™ã‚ã§ã™ã€‚

https://catalog.workshops.aws/ml-on-aws-parallelcluster/en-US

ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸéš›ã«ã¯ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚’å‚è€ƒã«ã—ã¦ãã ã•ã„ã€‚

https://docs.aws.amazon.com/ja_jp/parallelcluster/latest/ug/troubleshooting-v3.html

---

# AWS ParallelCluster ã«ã‚ˆã‚‹åˆ†æ•£å­¦ç¿’ç’°å¢ƒã®æ§‹ç¯‰

æœ¬ç« ã§ã¯ã€AWS ParallelCluster ç’°å¢ƒã®æ§‹ç¯‰æ–¹æ³•ã‚’è§£èª¬ã—ã¾ã™ã€‚æœ¬ç« ã§ã¯æœ€ä½é™ã®å‹•ä½œç¢ºèªã‚’ç›®çš„ã¨ã—ãŸæ§‹æˆã‚’ç´¹ä»‹ã—ã¾ã™ï¼ˆHead Node: c5.9xlargeã€Compute Node: c5n.9xlarge Ã— 3 å°ï¼‰ã€‚å®Ÿéš›ã®åˆ†æ•£å­¦ç¿’ã§ã¯ GPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆp5.48xlarge ãªã©ï¼‰ã‚’ä½¿ç”¨ã—ã€ã‚ˆã‚Šå¤§è¦æ¨¡ãªæ§‹æˆãŒå¿…è¦ã«ãªã‚Šã¾ã™ã€‚

## ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£æ¦‚è¦

AWS ParallelCluster ã¯ 2 å±¤ã®ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ã§æ§‹æˆã•ã‚Œã¾ã™ã€‚

![](/images/books/ml-distributed-experiment-collection/pcluster.png)

::::details å„ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®è©³ç´°

### Head Node
- ãƒ­ã‚°ã‚¤ãƒ³ãƒãƒ¼ãƒ‰ã¨ã—ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ¥ç¶šã™ã‚‹éš›ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
- Slurm ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã®ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã¨ã—ã¦ã€ã‚¸ãƒ§ãƒ–ç®¡ç†ã¨ãƒªã‚½ãƒ¼ã‚¹å‰²ã‚Šå½“ã¦ã‚’å®Ÿè¡Œ
- ç®¡ç†ã‚µãƒ¼ãƒ“ã‚¹ï¼ˆç›£è¦–ã€ãƒ­ã‚°åé›†ãªã©ï¼‰ã‚’å®Ÿè¡Œ
- æ¨å¥¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—ã¯ m5.8xlargeï¼ˆ32 vCPUã€128 GiB ãƒ¡ãƒ¢ãƒªï¼‰

### Compute Node
- å®Ÿéš›ã®è¨ˆç®—ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œã™ã‚‹ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒãƒ¼ãƒ‰
- GPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆP4dã€P5ã€Trn ãªã©ï¼‰ã¾ãŸã¯ CPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½¿ç”¨
- Slurm ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã‹ã‚‰ã®ã‚¸ãƒ§ãƒ–è¦æ±‚ã«å¿œã˜ã¦å‹•çš„ã«ãƒ—ãƒ­ãƒ“ã‚¸ãƒ§ãƒ‹ãƒ³ã‚°
- EFAï¼ˆElastic Fabric Adapterï¼‰ã«ã‚ˆã‚Šä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·ã®ãƒãƒ¼ãƒ‰é–“é€šä¿¡ã‚’å®Ÿç¾

### Shared Storage

**FSx for Lustre**ï¼ˆ/fsx ãƒã‚¦ãƒ³ãƒˆãƒã‚¤ãƒ³ãƒˆï¼‰
- é«˜æ€§èƒ½ãªä¸¦åˆ—ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã§ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆä¿å­˜ã«ä½¿ç”¨
- S3 ãƒã‚±ãƒƒãƒˆã¨ã® Data Repository Associationï¼ˆDRAï¼‰ã«ã‚ˆã‚Šè‡ªå‹•çš„ãªãƒ‡ãƒ¼ã‚¿åŒæœŸãŒå¯èƒ½
- `PERSISTENT_2` ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ¡ãƒ³ãƒˆã‚¿ã‚¤ãƒ—ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€é«˜å¯ç”¨æ€§ã¨é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’ä¸¡ç«‹

**FSx for OpenZFS**ï¼ˆ/home ãƒã‚¦ãƒ³ãƒˆãƒã‚¤ãƒ³ãƒˆï¼‰
- ãƒ›ãƒ¼ãƒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä¿å­˜ã«ä½¿ç”¨
- ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆã€å°è¦æ¨¡ãªãƒ•ã‚¡ã‚¤ãƒ«ã®ç®¡ç†ã«æœ€é©
- NFS ãƒ—ãƒ­ãƒˆã‚³ãƒ«ã‚’ã‚µãƒãƒ¼ãƒˆã—ã€POSIX æº–æ‹ ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ 
::::

## äº‹å‰æº–å‚™

å¿…è¦ãªãƒ„ãƒ¼ãƒ«ã‚’ãƒ­ãƒ¼ã‚«ãƒ«ã‚‚ã—ãã¯ã‚¯ãƒ©ã‚¦ãƒ‰ IDE ç’°å¢ƒã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚

::::details å¿…è¦ãªãƒ„ãƒ¼ãƒ«

AWS CloudShell ã‹ã‚‰ä½œæ¥­ã™ã‚‹ã“ã¨ã‚‚å¯èƒ½ã§ã™ã€‚

![](/images/books/ml-distributed-experiment-collection/cloudshell.png)

ä»¥ä¸‹ã®ãƒ„ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

**Git**
- ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³ã¨ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†ã«ä½¿ç”¨
- ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: [Git ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒšãƒ¼ã‚¸](https://git-scm.com/downloads)
- AWS CloudShell ã®å ´åˆã¯ã™ã§ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿

**Python 3.8 ä»¥é™**
- AWS ParallelCluster CLI ã®å®Ÿè¡Œã«å¿…è¦
- ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: [Python ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒšãƒ¼ã‚¸](https://www.python.org/downloads/)
- ç¢ºèªã‚³ãƒãƒ³ãƒ‰: `python3 --version`
- AWS CloudShell ã®å ´åˆã¯ã™ã§ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ¸ˆã¿

**yq**
- YAML ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†ã«ä½¿ç”¨ã™ã‚‹è»½é‡ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ãƒ„ãƒ¼ãƒ«
- ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ–¹æ³•
  - macOS: `brew install yq`
  - Linux: `sudo snap install yq` ã¾ãŸã¯ `sudo apt-get install yq`
  - Windows: `choco install yq`
  - AWS CloudShell: `sudo yum install yq`
- ç¢ºèªã‚³ãƒãƒ³ãƒ‰: `yq --version`
::::

## 1. ç’°å¢ƒå¤‰æ•°ã®è¨­å®š

:::message
- [ ] 1-1. ã‚µãƒ³ãƒ—ãƒ«ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³
- [ ] 1-2. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ
:::

::::details 1-1. ã‚µãƒ³ãƒ—ãƒ«ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: AWS ParallelCluster ã®ã‚µãƒ³ãƒ—ãƒ«è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å–å¾—ã—ã¾ã™ã€‚ã“ã®ãƒªãƒã‚¸ãƒˆãƒªã«ã¯ã€ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼æ§‹ç¯‰ã«å¿…è¦ãª CloudFormation ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¨è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: `awsome-distributed-training/1.architectures/2.aws-parallelcluster` ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•ã§ãã€`cluster-templates` ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã€‚
:::

ã‚µãƒ³ãƒ—ãƒ«ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ã—ã¦ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•ã—ã¾ã™ã€‚

```bash
git clone https://github.com/aws-samples/awsome-distributed-training.git
cd awsome-distributed-training/1.architectures/2.aws-parallelcluster
```

ãƒªãƒã‚¸ãƒˆãƒªã®å†…å®¹ã‚’ç¢ºèªã—ã¾ã™ã€‚

```bash
ls -la
```
::::

::::details 1-2. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è¨­å®šã‚’ç®¡ç†ã™ã‚‹ãŸã‚ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚å¾Œç¶šã®æ‰‹é †ã§ä½¿ç”¨ã™ã‚‹ç’°å¢ƒå¤‰æ•°ã‚’ YAML ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã€è¨­å®šã®ä¸€å…ƒç®¡ç†ã‚’å®Ÿç¾ã—ã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: `${CONFIG_DIR}/config.yaml` ãƒ•ã‚¡ã‚¤ãƒ«ãŒä½œæˆã•ã‚Œã€åŸºæœ¬çš„ãªè¨­å®šï¼ˆCLUSTER_NAMEã€AWS_REGIONã€PCLUSTER_VERSIONï¼‰ãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã€‚`cat ${CONFIG_DIR}/config.yaml` ã§å†…å®¹ã‚’ç¢ºèªã§ãã‚‹ã“ã¨ã€‚
:::

ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã™ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã™ã€‚

```bash
export AWS_REGION=ap-northeast-1
export CLUSTER_NAME=ml-cluster
export PCLUSTER_VERSION=3.13.1
export CONFIG_DIR="${HOME}/${CLUSTER_NAME}_${AWS_REGION}_${PCLUSTER_VERSION}"

mkdir -p ${CONFIG_DIR}
touch ${CONFIG_DIR}/config.yaml
yq -i ".CLUSTER_NAME = \"$CLUSTER_NAME\"" ${CONFIG_DIR}/config.yaml
yq -i ".AWS_REGION = \"$AWS_REGION\"" ${CONFIG_DIR}/config.yaml
yq -i ".PCLUSTER_VERSION = \"$PCLUSTER_VERSION\"" ${CONFIG_DIR}/config.yaml
```

è¨­å®šå†…å®¹ã‚’ç¢ºèªã—ã¾ã™ã€‚

```bash
cat ${CONFIG_DIR}/config.yaml
```
::::

## 2. AWS ParallelCluster CLI ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

::::details AWS ParallelCluster CLI ã¨ã¯

`pcluster` ã¯ AWS ParallelCluster ã®ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ï¼ˆCLIï¼‰ãƒ„ãƒ¼ãƒ«ã§ã™ã€‚

## ä¸»ãªç”¨é€”

**HPC ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ç®¡ç†**
- AWS ã‚¯ãƒ©ã‚¦ãƒ‰ä¸Šã§ HPC ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®èµ·å‹•ã€ç®¡ç†ã€å‰Šé™¤ã‚’å®Ÿè¡Œ
- ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«å…¨ä½“ã‚’ CLI ã‹ã‚‰åˆ¶å¾¡å¯èƒ½

**ã‚«ã‚¹ã‚¿ãƒ  AMI ã‚¤ãƒ¡ãƒ¼ã‚¸ã®ä½œæˆã¨ç®¡ç†**
- ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã§ä½¿ç”¨ã™ã‚‹ã‚«ã‚¹ã‚¿ãƒ  AMI ã®æ§‹ç¯‰
- ç‰¹å®šã®ã‚½ãƒ•ãƒˆã‚¦ã‚§ã‚¢ã‚„ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’äº‹å‰ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ãŸã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’ä½œæˆå¯èƒ½

## ä¸»è¦ãªã‚³ãƒãƒ³ãƒ‰

**ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ç®¡ç†**
- `create-cluster`: æ–°ã—ã„ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’ä½œæˆ
- `delete-cluster`: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’å‰Šé™¤
- `update-cluster`: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®è¨­å®šã‚’æ›´æ–°
- `describe-cluster`: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®è©³ç´°æƒ…å ±ã‚’å–å¾—
- `list-clusters`: æ—¢å­˜ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ä¸€è¦§ã‚’è¡¨ç¤º

**ã‚¤ãƒ¡ãƒ¼ã‚¸ç®¡ç†**
- `build-image`: ã‚«ã‚¹ã‚¿ãƒ  AMI ã‚’ãƒ“ãƒ«ãƒ‰
- `delete-image`: ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’å‰Šé™¤
- `describe-image`: ã‚¤ãƒ¡ãƒ¼ã‚¸ã®è©³ç´°æƒ…å ±ã‚’å–å¾—
- `list-images`: ã‚¤ãƒ¡ãƒ¼ã‚¸ä¸€è¦§ã‚’è¡¨ç¤º

**æ¥ç¶šã¨ãƒ‡ãƒãƒƒã‚°**
- `ssh`: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã« SSH æ¥ç¶š
- `dcv-connect`: NICE DCV ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«æ¥ç¶š
- `export-cluster-logs`: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ãƒ­ã‚°ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
- `get-cluster-log-events`: ãƒ­ã‚°ã‚¤ãƒ™ãƒ³ãƒˆã‚’å–å¾—

**Compute Fleet ç®¡ç†**
- `describe-compute-fleet`: Compute ãƒãƒ¼ãƒ‰ã®çŠ¶æ…‹ã‚’ç¢ºèª
- `update-compute-fleet`: Compute ãƒãƒ¼ãƒ‰ã®èµ·å‹•/åœæ­¢ã‚’åˆ¶å¾¡

## å¿…è¦ãªæ¨©é™

pcluster CLI ã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€é©åˆ‡ãª IAM ãƒ­ãƒ¼ãƒ«ã¨æ¨©é™ãŒå¿…è¦ã§ã™ã€‚ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ä½œæˆã€æ›´æ–°ã€å‰Šé™¤ãªã©ã®æ“ä½œã«ã¯ã€VPCã€EC2ã€CloudFormationã€FSx ãªã©ã®ãƒªã‚½ãƒ¼ã‚¹ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãŒæ±‚ã‚ã‚‰ã‚Œã¾ã™ã€‚
::::

:::message
- [ ] 2-1. Python ä»®æƒ³ç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¨ CLI ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
:::

::::details 2-1. Python ä»®æƒ³ç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã¨ CLI ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: AWS ParallelCluster CLI ã‚’ Python ä»®æƒ³ç’°å¢ƒã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚ä»®æƒ³ç’°å¢ƒã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ã‚·ã‚¹ãƒ†ãƒ ã® Python ç’°å¢ƒã«å½±éŸ¿ã‚’ä¸ãˆãšã«å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ç®¡ç†ã§ãã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: `pcluster version` ã‚³ãƒãƒ³ãƒ‰ãŒæ­£å¸¸ã«å®Ÿè¡Œã§ãã€æŒ‡å®šã—ãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³ï¼ˆä¾‹: 3.13.1ï¼‰ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã“ã¨ã€‚
:::

AWS ParallelCluster CLI ã‚’ Python ä»®æƒ³ç’°å¢ƒã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ã€‚

```bash
export VIRTUAL_ENV_PATH=~/pcluster_${PCLUSTER_VERSION}_env
# pip ã¨ virtualenv ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®æ›´æ–°
python3 -m pip install --upgrade pip
python3 -m pip install --user --upgrade virtualenv
python3 -m virtualenv ${VIRTUAL_ENV_PATH}
source ${VIRTUAL_ENV_PATH}/bin/activate
pip3 install awscli
pip3 install aws-parallelcluster==${PCLUSTER_VERSION}
```

ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã‚’ç¢ºèªã—ã¾ã™ã€‚

```bash
pcluster version
```

å‡ºåŠ›ä¾‹ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

```
{
  "version": "3.13.1"
}
```
::::

## 3. ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã¨ã‚­ãƒ£ãƒ‘ã‚·ãƒ†ã‚£ã®è¨­å®š

:::message
- [ ] 3-1. Head Node ã¨ Compute Node ã®è¨­å®š
:::

::::details 3-1. Head Node ã¨ Compute Node ã®è¨­å®š

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: Head Node ã¨ Compute Node ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—ã€ãƒãƒ¼ãƒ‰æ•°ã€ã‚¢ãƒ™ã‚¤ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ¼ã‚¾ãƒ¼ãƒ³ã‚’è¨­å®šã—ã¾ã™ã€‚åˆ†æ•£å­¦ç¿’ã§ã¯ EFAï¼ˆElastic Fabric Adapterï¼‰ã‚’ä½¿ç”¨ã—ãŸä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·é€šä¿¡ãŒé‡è¦ãªãŸã‚ã€Compute Node ã«ã¯ EFA å¯¾å¿œã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’é¸æŠã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
:::

:::message alert
**Head Node ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚µã‚¤ã‚ºã«ã¤ã„ã¦**

Head Node ã§ã¯ã€ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼èµ·å‹•æ™‚ã«ä»¥ä¸‹ã®å‡¦ç†ãŒå®Ÿè¡Œã•ã‚Œã¾ã™ã€‚
- Chef ãƒ¬ã‚·ãƒ”ã®å®Ÿè¡Œã¨ã‚·ã‚¹ãƒ†ãƒ è¨­å®š
- Dockerã€NCCLã€Pyxis ãªã©ã® CustomActions ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å®Ÿè¡Œ
- Slurm ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã®åˆæœŸåŒ–

**é‡è¦ãªæ³¨æ„äº‹é …**
- c5n.large (2 vCPU, 5.25 GiB) ã®ã‚ˆã†ãªå°ã•ã„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§ã¯ã€ã“ã‚Œã‚‰ã®å‡¦ç†ã«æ™‚é–“ãŒã‹ã‹ã‚Š CloudFormation ã® WaitConditionï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ 35 åˆ†ï¼‰ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™
- **æ¨å¥¨ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚µã‚¤ã‚º**: å‹•ä½œç¢ºèªã§ã¯ c5.9xlarge (36 vCPU, 72 GiB) ä»¥ä¸Šã€æœ¬ç•ªç’°å¢ƒã§ã¯ c5n.18xlarge ã¾ãŸã¯ m5.24xlarge ãªã©ã€ã‚ˆã‚Šå¤§ãã„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
- å°ã•ã„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€CustomActions ã®æ•°ã‚’æ¸›ã‚‰ã™ã‹ã€ã‚«ã‚¹ã‚¿ãƒ  AMI ã‚’äº‹å‰ã«æ§‹ç¯‰ã—ã¦èµ·å‹•æ™‚é–“ã‚’çŸ­ç¸®ã™ã‚‹ã“ã¨ã‚’æ¤œè¨ã—ã¦ãã ã•ã„
:::

:::message alert
**EFA å¯¾å¿œã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®é¸æŠã«ã¤ã„ã¦**

AWS ParallelCluster ã§åˆ†æ•£å­¦ç¿’ã‚’è¡Œã†å ´åˆã€ãƒãƒ¼ãƒ‰é–“ã®ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·é€šä¿¡ã®ãŸã‚ã« EFAï¼ˆElastic Fabric Adapterï¼‰ã®ä½¿ç”¨ã‚’å¼·ãæ¨å¥¨ã—ã¾ã™ã€‚

**EFA å¯¾å¿œã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ç¢ºèªæ–¹æ³•**

ä»¥ä¸‹ã®ã‚³ãƒãƒ³ãƒ‰ã§ EFA å¯¾å¿œã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ä¸€è¦§ã‚’ç¢ºèªã§ãã¾ã™ã€‚

```bash
aws ec2 describe-instance-types \
    --region ${AWS_REGION} \
    --filters Name=network-info.efa-supported,Values=true \
    --query "InstanceTypes[*].[InstanceType]" \
    --output text | sort
```

**ä¸»è¦ãª EFA å¯¾å¿œã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãƒ•ã‚¡ãƒŸãƒªãƒ¼**
- GPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹: p4dã€p4deã€p5ã€trn1ã€trn1n
- CPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹: c5nã€c6iï¼ˆå¤§å‹ã‚µã‚¤ã‚ºï¼‰ã€c6inã€c7iï¼ˆå¤§å‹ã‚µã‚¤ã‚ºï¼‰ã€m6iï¼ˆå¤§å‹ã‚µã‚¤ã‚ºï¼‰ã€r6iï¼ˆå¤§å‹ã‚µã‚¤ã‚ºï¼‰ã€hpc7a

**å¯¾å¿œ OS ã¨ã‚«ãƒ¼ãƒãƒ«è¦ä»¶**
- Amazon Linux 2ã€Ubuntu 18.04/20.04/22.04ã€RHEL 7.6 ä»¥é™ã€CentOS 7.6 ä»¥é™
- ã‚«ãƒ¼ãƒãƒ« 3.10 ä»¥é™ï¼ˆæ¨å¥¨: 4.14 ä»¥é™ï¼‰
- è©³ç´°ã¯ [EFA å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://docs.aws.amazon.com/ja_jp/AWSEC2/latest/UserGuide/efa.html)ã‚’å‚ç…§

**æ³¨æ„äº‹é …**
- EFA éå¯¾å¿œã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’é¸æŠã—ãŸå ´åˆã€ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ä½œæˆæ™‚ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã™
- å‹•ä½œç¢ºèªç”¨ã«ã¯ c5n.9xlargeï¼ˆ36 vCPUã€96 GiB ãƒ¡ãƒ¢ãƒªï¼‰ã€æœ¬ç•ªã§ã¯ p5.48xlarge ãªã©ã® GPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’æ¨å¥¨
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: `cat ${CONFIG_DIR}/config.yaml` ã§ AZã€NUM_INSTANCESã€INSTANCE ã®è¨­å®šãŒç¢ºèªã§ãã‚‹ã“ã¨ã€‚GPU ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ CAPACITY_RESERVATION_ID ã‚‚è¨­å®šã•ã‚Œã¦ã„ã‚‹ã“ã¨ã€‚
:::

æœ€ä½é™ã®å‹•ä½œç¢ºèªã§ã¯ã€ä»¥ä¸‹ã®ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¾ã™ã€‚

```bash
# æœ€ä½é™ã®å‹•ä½œç¢ºèªç”¨æ§‹æˆ
export AZ=ap-northeast-1a  # ä½¿ç”¨ã™ã‚‹ã‚¢ãƒ™ã‚¤ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ¼ã‚¾ãƒ¼ãƒ³
export HEAD_NODE_INSTANCE_TYPE=c5.9xlarge  # Head Node ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—
export NUM_INSTANCES=3  # Compute Node æ•°
export INSTANCE=c5n.9xlarge  # Compute Node ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—ï¼ˆEFA å¯¾å¿œï¼‰

yq -i ".AZ = \"$AZ\"" ${CONFIG_DIR}/config.yaml
yq -i ".HEAD_NODE_INSTANCE_TYPE = \"$HEAD_NODE_INSTANCE_TYPE\"" ${CONFIG_DIR}/config.yaml
yq -i ".NUM_INSTANCES = \"$NUM_INSTANCES\"" ${CONFIG_DIR}/config.yaml
yq -i ".INSTANCE = \"$INSTANCE\"" ${CONFIG_DIR}/config.yaml
```

è¨­å®šã‚’ç¢ºèªã—ã¾ã™ã€‚

```bash
cat ${CONFIG_DIR}/config.yaml
```

GPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€è¿½åŠ ã§ Capacity Reservation ã‚’è¨­å®šã—ã¾ã™ã€‚

```bash
export CAPACITY_RESERVATION_ID=cr-0123456789abcdef0  # Capacity Reservation ID
export AZ=ap-northeast-1a  # ã‚¢ãƒ™ã‚¤ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ¼ã‚¾ãƒ¼ãƒ³
export HEAD_NODE_INSTANCE_TYPE=c5n.9xlarge  # Head Node ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—ï¼ˆã‚ˆã‚Šå¤§ãã„ã‚µã‚¤ã‚ºæ¨å¥¨ï¼‰
export NUM_INSTANCES=8  # ãƒãƒ¼ãƒ‰æ•°
export INSTANCE=p5.48xlarge  # Compute Node ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—ï¼ˆEFA å¯¾å¿œï¼‰

yq -i ".CAPACITY_RESERVATION_ID = \"$CAPACITY_RESERVATION_ID\"" ${CONFIG_DIR}/config.yaml
yq -i ".AZ = \"$AZ\"" ${CONFIG_DIR}/config.yaml
yq -i ".HEAD_NODE_INSTANCE_TYPE = \"$HEAD_NODE_INSTANCE_TYPE\"" ${CONFIG_DIR}/config.yaml
yq -i ".NUM_INSTANCES = \"$NUM_INSTANCES\"" ${CONFIG_DIR}/config.yaml
yq -i ".INSTANCE = \"$INSTANCE\"" ${CONFIG_DIR}/config.yaml
```
::::

## 4. SSH æ¥ç¶šã®æº–å‚™

:::message
- [ ] 4-1. EC2 Key Pair ã®ä½œæˆ
:::

::::details 4-1. EC2 Key Pair ã®ä½œæˆ

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã® Head Node ã« SSH æ¥ç¶šã™ã‚‹ãŸã‚ã® EC2 Key Pair ã‚’ä½œæˆã—ã¾ã™ã€‚æ—¢å­˜ã® Key Pair ãŒã‚ã‚‹å ´åˆã¯ã€ãã®åå‰ã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®šã™ã‚‹ã ã‘ã§ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’ã‚¹ã‚­ãƒƒãƒ—ã§ãã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: `aws ec2 describe-key-pairs --region ${AWS_REGION}` ã‚³ãƒãƒ³ãƒ‰ã§æŒ‡å®šã—ãŸ Key Pair ãŒç¢ºèªã§ãã€ç§˜å¯†éµãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ.pemï¼‰ãŒ `~/.ssh` ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ä¿å­˜ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã€‚
:::

Key Pair åã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®šã—ã¾ã™ã€‚

```bash
export KEYPAIR_NAME=my-keypair
yq -i ".KEYPAIR_NAME = \"$KEYPAIR_NAME\"" ${CONFIG_DIR}/config.yaml
```

æ—¢å­˜ã® Key Pair ãŒãªã„å ´åˆã¯ã€æ–°è¦ä½œæˆã—ã¾ã™ã€‚

```bash
# .ssh ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆï¼ˆå­˜åœ¨ã—ãªã„å ´åˆï¼‰
mkdir -p ~/.ssh

# Key Pair ã®ä½œæˆã¨ç§˜å¯†éµã®ä¿å­˜
aws ec2 create-key-pair \
    --key-name ${KEYPAIR_NAME} \
    --query KeyMaterial \
    --key-type ed25519 \
    --region ${AWS_REGION} \
    --output text > ~/.ssh/${KEYPAIR_NAME}.pem

# ç§˜å¯†éµã®ãƒ‘ãƒ¼ãƒŸãƒƒã‚·ãƒ§ãƒ³è¨­å®š
chmod 600 ~/.ssh/${KEYPAIR_NAME}.pem
```

ä½œæˆã‚’ç¢ºèªã—ã¾ã™ã€‚

```bash
aws ec2 describe-key-pairs --region ${AWS_REGION}
ls -la ~/.ssh/${KEYPAIR_NAME}.pem
```
::::

## 5. S3 ãƒã‚±ãƒƒãƒˆã®ä½œæˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

:::message alert
S3ã€DRA ã‚’è¨­å®šã™ã‚‹ã®ã« 10 åˆ†ä»¥ä¸Šã‹ã‹ã‚‹ãŸã‚ãƒãƒ³ã‚ºã‚ªãƒ³ç­‰ã®æ™‚é–“ã®é™ã‚‰ã‚Œã‚‹çŠ¶æ…‹ã§ã¯ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ã€‚
:::

:::message
- [ ] 5-1. S3 ãƒã‚±ãƒƒãƒˆã®ä½œæˆ
- [ ] 5-2. ãƒã‚±ãƒƒãƒˆåã®å–å¾—ã¨è¨­å®š
:::

::::details 5-1. S3 ãƒã‚±ãƒƒãƒˆã®ä½œæˆ

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚„ãƒ¢ãƒ‡ãƒ«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’æ°¸ç¶šåŒ–ã™ã‚‹ãŸã‚ã® S3 ãƒã‚±ãƒƒãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚å¾Œã§ FSx for Lustre ã¨ Data Repository Associationï¼ˆDRAï¼‰ã§é€£æºã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: CloudFormation ã‚¹ã‚¿ãƒƒã‚¯ãŒ CREATE_COMPLETE çŠ¶æ…‹ã«ãªã‚Šã€S3 ãƒã‚±ãƒƒãƒˆãŒæ­£å¸¸ã«ä½œæˆã•ã‚ŒãŸã“ã¨ã€‚`aws cloudformation describe-stacks` ã‚³ãƒãƒ³ãƒ‰ã§ã‚¹ã‚¿ãƒƒã‚¯ã®çŠ¶æ…‹ã‚’ç¢ºèªã§ãã‚‹ã“ã¨ã€‚
:::

CloudFormation ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¦ S3 ãƒã‚±ãƒƒãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚

```bash
# ã‚¹ã‚¿ãƒƒã‚¯åã¨ãƒã‚±ãƒƒãƒˆåã‚’è¨­å®š
export S3_STACK_NAME=cluster-data-bucket
export S3_BUCKET_NAME=${S3_STACK_NAME}-${AWS_REGION}-$(date +%s)

# CloudFormation ã‚¹ã‚¿ãƒƒã‚¯ã‚’ä½œæˆï¼ˆãƒã‚±ãƒƒãƒˆåã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§æŒ‡å®šï¼‰
aws cloudformation create-stack \
    --stack-name ${S3_STACK_NAME} \
    --template-url https://awsome-distributed-training.s3.amazonaws.com/templates/0.private-bucket.yaml \
    --parameters ParameterKey=S3BucketName,ParameterValue=${S3_BUCKET_NAME} \
    --region ${AWS_REGION}

echo "CloudFormation ã‚¹ã‚¿ãƒƒã‚¯ã®ä½œæˆã‚’é–‹å§‹ã—ã¾ã—ãŸ"

# ã‚¹ã‚¿ãƒƒã‚¯ä½œæˆã®å®Œäº†ã‚’å¾…æ©Ÿï¼ˆé€šå¸¸ 1 ã‹ã‚‰ 2 åˆ†ï¼‰
aws cloudformation wait stack-create-complete \
    --stack-name ${S3_STACK_NAME} \
    --region ${AWS_REGION}

echo "S3 ãƒã‚±ãƒƒãƒˆã®ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸ"
```

ä½œæˆã‚’ç¢ºèªã—ã¾ã™ã€‚

```bash
aws cloudformation describe-stacks \
    --stack-name ${S3_STACK_NAME} \
    --region ${AWS_REGION} \
    --query 'Stacks[0].StackStatus' \
    --output text
```
::::

::::details 5-2. ãƒã‚±ãƒƒãƒˆåã®å–å¾—ã¨è¨­å®š

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: ä½œæˆã—ãŸ S3 ãƒã‚±ãƒƒãƒˆã®åå‰ã‚’ CloudFormation ã®å‡ºåŠ›ã‹ã‚‰å–å¾—ã—ã€è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: `echo ${DATA_BUCKET_NAME}` ã§ãƒã‚±ãƒƒãƒˆåãŒè¡¨ç¤ºã•ã‚Œã€`config.yaml` ã«ãƒã‚±ãƒƒãƒˆåãŒä¿å­˜ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã€‚
:::

CloudFormation ã® Outputs ã‹ã‚‰ãƒã‚±ãƒƒãƒˆåã‚’å–å¾—ã—ã¾ã™ã€‚

```bash
export DATA_BUCKET_NAME=$(aws cloudformation describe-stacks \
    --stack-name cluster-data-bucket \
    --query 'Stacks[0].Outputs[?OutputKey==`S3BucketName`].OutputValue' \
    --region ${AWS_REGION} \
    --output text)

echo "Your data bucket name is: ${DATA_BUCKET_NAME}"
yq -i ".DATA_BUCKET_NAME = \"$DATA_BUCKET_NAME\"" ${CONFIG_DIR}/config.yaml
```
::::

## 6. VPC ã¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã®ä½œæˆ

:::message
- [ ] 6-1. VPC ã¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤
- [ ] 6-2. ã‚¹ã‚¿ãƒƒã‚¯åã®è¨­å®š
:::

::::details 6-1. VPC ã¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã®ãƒ‡ãƒ—ãƒ­ã‚¤

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: VPCã€ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚°ãƒ«ãƒ¼ãƒ—ã€FSx for Lustreã€FSx for OpenZFS ãªã©ã®åŸºç›¤ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ã‚’ CloudFormation ã§ä¸€æ‹¬ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã¯ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸åŸºç›¤ã¨ã—ã¦æ©Ÿèƒ½ã—ã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: CloudFormation ã‚¹ã‚¿ãƒƒã‚¯ãŒ CREATE_COMPLETE çŠ¶æ…‹ã«ãªã‚Šã€å…¨ã¦ã®ãƒªã‚½ãƒ¼ã‚¹ï¼ˆVPCã€Subnetã€FSx ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ãªã©ï¼‰ãŒæ­£å¸¸ã«ä½œæˆã•ã‚ŒãŸã“ã¨ã€‚ã‚¹ã‚¿ãƒƒã‚¯ä½œæˆã«ã¯ 15 ã‹ã‚‰ 20 åˆ†ã‹ã‹ã‚Šã¾ã™ã€‚
:::

CloudFormation ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’ä½¿ç”¨ã—ã¦ã‚¤ãƒ³ãƒ•ãƒ©ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã¾ã™ã€‚

```bash
# ã‚¹ã‚¿ãƒƒã‚¯åã‚’è¨­å®š
export STACK_ID_VPC=parallelcluster-prerequisites

# CloudFormation ã‚¹ã‚¿ãƒƒã‚¯ã‚’ä½œæˆï¼ˆã‚¢ãƒ™ã‚¤ãƒ©ãƒ“ãƒªãƒ†ã‚£ãƒ¼ã‚¾ãƒ¼ãƒ³ã‚’ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§æŒ‡å®šï¼‰
aws cloudformation create-stack \
    --stack-name ${STACK_ID_VPC} \
    --template-url https://awsome-distributed-training.s3.amazonaws.com/templates/parallelcluster-prerequisites.yaml \
    --parameters ParameterKey=PrimarySubnetAZ,ParameterValue=${AZ} \
    --region ${AWS_REGION}

echo "CloudFormation ã‚¹ã‚¿ãƒƒã‚¯ã®ä½œæˆã‚’é–‹å§‹ã—ã¾ã—ãŸ"
echo "ã‚¹ã‚¿ãƒƒã‚¯ä½œæˆã«ã¯ 15 ã‹ã‚‰ 20 åˆ†ã‹ã‹ã‚Šã¾ã™"

# ã‚¹ã‚¿ãƒƒã‚¯ä½œæˆã®å®Œäº†ã‚’å¾…æ©Ÿ
aws cloudformation wait stack-create-complete \
    --stack-name ${STACK_ID_VPC} \
    --region ${AWS_REGION}

echo "ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ã®ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸ"
```

ä½œæˆã‚’ç¢ºèªã—ã¾ã™ã€‚

```bash
aws cloudformation describe-stacks \
    --stack-name ${STACK_ID_VPC} \
    --region ${AWS_REGION} \
    --query 'Stacks[0].StackStatus' \
    --output text
```
::::

::::details 6-2. ã‚¹ã‚¿ãƒƒã‚¯åã®è¨­å®š

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: ä½œæˆã—ãŸ CloudFormation ã‚¹ã‚¿ãƒƒã‚¯ã®åå‰ã‚’ç’°å¢ƒå¤‰æ•°ã«ä¿å­˜ã—ã¾ã™ã€‚å¾Œç¶šã®æ‰‹é †ã§ã“ã®ã‚¹ã‚¿ãƒƒã‚¯ã‹ã‚‰ VPC ID ã‚„ FSx ID ãªã©ã®æƒ…å ±ã‚’å–å¾—ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã—ã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: `config.yaml` ã« STACK_ID_VPC ãŒä¿å­˜ã•ã‚Œã€`cat ${CONFIG_DIR}/config.yaml` ã§ç¢ºèªã§ãã‚‹ã“ã¨ã€‚
:::

CloudFormation ã‚¹ã‚¿ãƒƒã‚¯åã‚’ç’°å¢ƒå¤‰æ•°ã¨ã—ã¦è¨­å®šã—ã¾ã™ã€‚

```bash
export STACK_ID_VPC=parallelcluster-prerequisites
yq -i ".STACK_ID_VPC = \"$STACK_ID_VPC\"" ${CONFIG_DIR}/config.yaml
```

è¨­å®šã‚’ç¢ºèªã—ã¾ã™ã€‚

```bash
cat ${CONFIG_DIR}/config.yaml
```
::::

## 7. Data Repository Association ã®ä½œæˆï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

:::message
- [ ] 7-1. FSx for Lustre ã¨ S3 ã®é€£æºè¨­å®š
:::

::::details 7-1. FSx for Lustre ã¨ S3 ã®é€£æºè¨­å®š

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: FSx for Lustre ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã¨ S3 ãƒã‚±ãƒƒãƒˆã®é–“ã« Data Repository Associationï¼ˆDRAï¼‰ã‚’ä½œæˆã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€S3 ã«ä¿å­˜ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ãŒ FSx ã«è‡ªå‹•çš„ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã•ã‚Œã€FSx ã®å¤‰æ›´ãŒ S3 ã«è‡ªå‹•çš„ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã•ã‚Œã¾ã™ã€‚ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã¯ S3 ãƒã‚±ãƒƒãƒˆã‚’ä½œæˆã—ãŸå ´åˆã®ã¿å®Ÿæ–½ã—ã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: DRA ã® Lifecycle ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãŒ `AVAILABLE` ã«ãªã‚‹ã“ã¨ã€‚ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèªã‚³ãƒãƒ³ãƒ‰ã§ `AVAILABLE` ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã“ã¨ã€‚
:::

FSx for Lustre ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ  ID ã‚’å–å¾—ã—ã¾ã™ã€‚

```bash
export FSX_ID=$(aws cloudformation describe-stacks \
    --stack-name ${STACK_ID_VPC} \
    --query 'Stacks[0].Outputs[?OutputKey==`FSxLustreFilesystemId`].OutputValue' \
    --region ${AWS_REGION} \
    --output text)

echo "FSx ID: ${FSX_ID}"
```

Data Repository Association ã‚’ä½œæˆã—ã¾ã™ã€‚

```bash
aws fsx create-data-repository-association \
    --file-system-id ${FSX_ID} \
    --file-system-path "/data" \
    --data-repository-path s3://${DATA_BUCKET_NAME} \
    --s3 AutoImportPolicy='{Events=[NEW,CHANGED,DELETED]},AutoExportPolicy={Events=[NEW,CHANGED,DELETED]}' \
    --batch-import-meta-data-on-create \
    --region ${AWS_REGION}
```

DRA ã®ä½œæˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚’ç¢ºèªã—ã¾ã™ï¼ˆ`AVAILABLE` ã«ãªã‚‹ã¾ã§å¾…æ©Ÿï¼‰ã€‚

```bash
aws fsx describe-data-repository-associations \
    --filters "Name=file-system-id,Values=${FSX_ID}" \
    --query "Associations[0].Lifecycle" \
    --output text \
    --region ${AWS_REGION}
```
::::

## 8. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆ

:::message
- [ ] 8-1. CloudFormation å‡ºåŠ›ã®å–å¾—ã¨ãƒãƒ¼ã‚¸
- [ ] 8-2. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆ
:::

::::details 8-1. CloudFormation å‡ºåŠ›ã®å–å¾—ã¨ãƒãƒ¼ã‚¸

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: CloudFormation ã‚¹ã‚¿ãƒƒã‚¯ã‹ã‚‰ VPC IDã€Subnet IDã€FSx ID ãªã©ã®æƒ…å ±ã‚’å–å¾—ã—ã€æ—¢å­˜ã® config.yaml ã«ãƒãƒ¼ã‚¸ã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®æƒ…å ±ã¯ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆã«å¿…è¦ã§ã™ã€‚
:::

:::message alert
**è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®å†åˆ©ç”¨ã«ã¤ã„ã¦**

ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ä½œæˆã•ã‚Œã‚‹ `config.yaml` ã«ã¯ã€ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ï¼ˆVPCã€FSx ãªã©ï¼‰ã¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è¨­å®šï¼ˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—ã€ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ãªã©ï¼‰ã®å…¨ã¦ã®æƒ…å ±ãŒå«ã¾ã‚Œã¾ã™ã€‚

**ä¸€åº¦ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã™ã‚Œã°ã€ä»¥ä¸‹ã®çŠ¶æ³ã§å†åˆ©ç”¨ã§ãã¾ã™**
- åŒã˜ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ã§ç•°ãªã‚‹è¨­å®šã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’ä½œæˆã™ã‚‹å ´åˆ
- ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’å‰Šé™¤ã—ã¦å†åº¦ä½œæˆã™ã‚‹å ´åˆ
- ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—ã‚„ãƒãƒ¼ãƒ‰æ•°ãªã©ã€ä¸€éƒ¨ã®è¨­å®šã®ã¿ã‚’å¤‰æ›´ã™ã‚‹å ´åˆ

**å†åˆ©ç”¨æ™‚ã®æ‰‹é †**
1. ä»®æƒ³ç’°å¢ƒã‚’æœ‰åŠ¹åŒ–: `source ${VIRTUAL_ENV_PATH}/bin/activate`
2. ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿: `eval $(yq e 'to_entries | .[] | "export " + .key + "=\"" + .value + "\""' ${CONFIG_DIR}/config.yaml)`
3. å¿…è¦ã«å¿œã˜ã¦ç’°å¢ƒå¤‰æ•°ã‚’å¤‰æ›´ï¼ˆä¾‹: `export NUM_INSTANCES=5`ï¼‰
4. ã‚»ã‚¯ã‚·ãƒ§ãƒ³ 8-2 ä»¥é™ã®æ‰‹é †ã‚’å®Ÿè¡Œ

ã“ã‚Œã«ã‚ˆã‚Šã€ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ã®å†æ§‹ç¯‰ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ã€ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è¨­å®šã®ã¿ã‚’åŠ¹ç‡çš„ã«æ›´æ–°ã§ãã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: `config.yaml` ã« CloudFormation ã®å…¨å‡ºåŠ›ï¼ˆPublicSubnetã€PrimaryPrivateSubnetã€FSxLustreFilesystemIdã€FSxORootVolumeIdã€SecurityGroup ãªã©ï¼‰ãŒè¿½åŠ ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã€‚
:::

ä»®æƒ³ç’°å¢ƒãŒæœ‰åŠ¹ã«ãªã£ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚

```bash
source ${VIRTUAL_ENV_PATH}/bin/activate
```

CloudFormation ã‚¹ã‚¿ãƒƒã‚¯ã®å‡ºåŠ›ã‚’å–å¾—ã—ã€config.yaml ã«ãƒãƒ¼ã‚¸ã—ã¾ã™ã€‚

```bash
# CloudFormation ã‚¹ã‚¿ãƒƒã‚¯ã®å‡ºåŠ›ã‚’å–å¾—
aws cloudformation describe-stacks \
    --stack-name $STACK_ID_VPC \
    --query 'Stacks[0].Outputs[?contains(@.OutputKey, ``)].{OutputKey:OutputKey, OutputValue:OutputValue}' \
    --region ${AWS_REGION} \
    --output json | yq e '.[] | .OutputKey + ": " + .OutputValue' - > ${CONFIG_DIR}/stack_outputs.yaml

# config.yaml ã¨ãƒãƒ¼ã‚¸
yq eval-all 'select(fileIndex == 0) * select(fileIndex == 1)' ${CONFIG_DIR}/config.yaml ${CONFIG_DIR}/stack_outputs.yaml > ${CONFIG_DIR}/config_updated.yaml
mv ${CONFIG_DIR}/config_updated.yaml ${CONFIG_DIR}/config.yaml
rm ${CONFIG_DIR}/stack_outputs.yaml
```

ãƒãƒ¼ã‚¸ã•ã‚ŒãŸè¨­å®šã‚’ç¢ºèªã—ã¾ã™ã€‚

```bash
cat ${CONFIG_DIR}/config.yaml
```

å‡ºåŠ›ä¾‹ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

```yaml
CLUSTER_NAME: ml-cluster
AWS_REGION: ap-northeast-1
PCLUSTER_VERSION: 3.13.1
AZ: ap-northeast-1a
NUM_INSTANCES: "3"
INSTANCE: c5n.9xlarge
KEYPAIR_NAME: my-keypair
DATA_BUCKET_NAME: cluster-data-bucket-ap-northeast-1-1765267989
STACK_ID_VPC: parallelcluster-prerequisites
PrimaryPrivateSubnet: subnet-0fe2476bb2e98a579
FSxLustreFilesystemMountname: 6syhhbev
FSxORootVolumeId: fsvol-057e2427d7dcbc766
FSxLustreFilesystemDNSname: fs-002917c15943993c7.fsx.ap-northeast-1.amazonaws.com
VPC: vpc-0c2cf596e382deb91
FSxLustreFilesystemId: fs-002917c15943993c7
SecurityGroup: sg-09f73a7cc2a170abc
PublicSubnet: subnet-00738f6db68fa46d2
HEAD_NODE_INSTANCE_TYPE: c5n.large
```
::::

::::details 8-2. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆ

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¨ç’°å¢ƒå¤‰æ•°ã‚’ä½¿ç”¨ã—ã¦ã€å®Ÿéš›ã«ãƒ‡ãƒ—ãƒ­ã‚¤ã™ã‚‹ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆcluster.yamlï¼‰ã‚’ç”Ÿæˆã—ã¾ã™ã€‚ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¯ Head Nodeã€Compute Nodeã€Shared Storage ã®è©³ç´°ãªè¨­å®šãŒå«ã¾ã‚Œã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: `${CONFIG_DIR}/cluster.yaml` ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã€`cat ${CONFIG_DIR}/cluster.yaml` ã§è¨­å®šå†…å®¹ã‚’ç¢ºèªã§ãã‚‹ã“ã¨ã€‚ç’°å¢ƒå¤‰æ•°ãŒæ­£ã—ãå±•é–‹ã•ã‚Œã¦ã„ã‚‹ã“ã¨ï¼ˆä¾‹: `${AWS_REGION}` ãŒ `ap-northeast-1` ã«ç½®ãæ›ã‚ã£ã¦ã„ã‚‹ã“ã¨ï¼‰ã€‚
:::

ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚

```bash
eval $(yq e 'to_entries | .[] | "export " + .key + "=\"" + .value + "\""' ${CONFIG_DIR}/config.yaml)
```

envsubst ã‚³ãƒãƒ³ãƒ‰ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™ï¼ˆAWS CloudShell ã®å ´åˆï¼‰ã€‚

```bash
# envsubst ãŒåˆ©ç”¨å¯èƒ½ã‹ç¢ºèª
if ! command -v envsubst &> /dev/null; then
    echo "envsubst ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™"
    sudo yum install -y gettext
fi
```

ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‹ã‚‰è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

```bash
cat cluster-templates/cluster-vanilla.yaml | envsubst > ${CONFIG_DIR}/cluster.yaml

# Head Node ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—ã‚’è¨­å®š
if [ -n "${HEAD_NODE_INSTANCE_TYPE}" ]; then
    echo "Head Node ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚¿ã‚¤ãƒ—ã‚’ ${HEAD_NODE_INSTANCE_TYPE} ã«è¨­å®šã—ã¾ã™"
    yq eval ".HeadNode.InstanceType = \"${HEAD_NODE_INSTANCE_TYPE}\"" -i ${CONFIG_DIR}/cluster.yaml
fi

# CAPACITY_RESERVATION_ID ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯ CapacityReservationTarget ã‚’å‰Šé™¤
if [ -z "${CAPACITY_RESERVATION_ID}" ]; then
    echo "Capacity Reservation ãŒè¨­å®šã•ã‚Œã¦ã„ãªã„ãŸã‚ã€CapacityReservationTarget ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å‰Šé™¤ã—ã¾ã™"
    yq eval 'del(.Scheduling.SlurmQueues[].ComputeResources[].CapacityReservationTarget)' -i ${CONFIG_DIR}/cluster.yaml
fi
```

ç”Ÿæˆã•ã‚ŒãŸè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¾ã™ã€‚

```bash
cat ${CONFIG_DIR}/cluster.yaml
```
::::

::::details ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®è©³ç´°è§£èª¬

ç”Ÿæˆã•ã‚ŒãŸ `cluster.yaml` ã®ä¸»è¦ãªè¨­å®šé …ç›®ã‚’è§£èª¬ã—ã¾ã™ã€‚

### Head Node è¨­å®š

```yaml
HeadNode:
  InstanceType: c5.9xlarge  # 36 vCPU, 72 GiB ãƒ¡ãƒ¢ãƒªï¼ˆå‹•ä½œç¢ºèªç”¨ï¼‰
  Networking:
    SubnetId: ${PublicSubnet}
  Ssh:
    KeyName: ${KEYPAIR_NAME}
  LocalStorage:
    RootVolume:
      Size: 500  # GB
  Iam:
    AdditionalIamPolicies:
      - Policy: arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
      - Policy: arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess
      - Policy: arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
  CustomActions:
    OnNodeConfigured:
      Sequence:
        - Script: '.../docker/postinstall.sh'
        - Script: '.../nccl/postinstall.sh'
        - Script: '.../pyxis/postinstall.sh'
```

**è¨­å®šã®ãƒã‚¤ãƒ³ãƒˆ**
- `InstanceType`: å‹•ä½œç¢ºèªã§ã¯ c5.9xlargeï¼ˆ36 vCPUã€72 GiB ãƒ¡ãƒ¢ãƒªï¼‰ã‚’æ¨å¥¨ã€‚c5n.large ãªã©ã®å°ã•ã„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§ã¯ã€CustomActions ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆå®Ÿè¡Œæ™‚ã«ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™
- `AdditionalIamPolicies`: SSMã€S3ã€ECR ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ã‚’è¨±å¯
- `CustomActions.OnNodeConfigured`: Dockerã€NCCLã€Pyxis ã‚’è‡ªå‹•ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

### Compute Node è¨­å®š

```yaml
Scheduling:
  Scheduler: slurm
  SlurmSettings:
    ScaledownIdletime: 60  # ã‚¢ã‚¤ãƒ‰ãƒ« 60 ç§’å¾Œã«ã‚¹ã‚±ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³
  SlurmQueues:
    - Name: compute-gpu
      ComputeResources:
        - Name: distributed-ml
          InstanceType: c5n.9xlarge  # å‹•ä½œç¢ºèªç”¨ï¼ˆEFA å¯¾å¿œï¼‰
          MinCount: 3  # æœ€å°ãƒãƒ¼ãƒ‰æ•°
          MaxCount: 3  # æœ€å¤§ãƒãƒ¼ãƒ‰æ•°
          Efa:
            Enabled: true  # EFA ã‚’æœ‰åŠ¹åŒ–
```

**è¨­å®šã®ãƒã‚¤ãƒ³ãƒˆ**
- `ScaledownIdletime`: ãƒãƒ¼ãƒ‰ãŒã‚¢ã‚¤ãƒ‰ãƒ«çŠ¶æ…‹ã«ãªã£ã¦ã‹ã‚‰ã‚¹ã‚±ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ã™ã‚‹ã¾ã§ã®æ™‚é–“
- `MinCount`/`MaxCount`: åŒã˜å€¤ã«è¨­å®šã™ã‚‹ã¨ã‚­ãƒ£ãƒ‘ã‚·ãƒ†ã‚£ã‚’ç¶­æŒ
- `Efa.Enabled`: EFA å¯¾å¿œã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§ã¯ true ã«è¨­å®šã—ã¦ä½ãƒ¬ã‚¤ãƒ†ãƒ³ã‚·é€šä¿¡ã‚’æœ‰åŠ¹åŒ–
- GPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ `CapacityReservationId` ã‚‚è¨­å®š

### Shared Storage è¨­å®š

```yaml
SharedStorage:
  - Name: HomeDirs
    MountDir: /home
    StorageType: FsxOpenZfs
  - MountDir: /fsx
    Name: fsx
    StorageType: FsxLustre
```

**è¨­å®šã®ãƒã‚¤ãƒ³ãƒˆ**
- `/home`: ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ›ãƒ¼ãƒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆFSx for OpenZFSï¼‰
- `/fsx`: ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆï¼ˆFSx for Lustreï¼‰
::::

## 9. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ä½œæˆ

:::message
- [ ] 9-1. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ä½œæˆå®Ÿè¡Œ
- [ ] 9-2. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ä½œæˆã®ç›£è¦–
:::

::::details 9-1. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ä½œæˆå®Ÿè¡Œ

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: ç”Ÿæˆã—ãŸè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¦ AWS ParallelCluster ã‚’å®Ÿéš›ã«ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã¾ã™ã€‚ã“ã®ã‚³ãƒãƒ³ãƒ‰ã«ã‚ˆã‚Šã€Head Nodeã€Compute Nodeã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯è¨­å®šãªã©ãŒè‡ªå‹•çš„ã«æ§‹ç¯‰ã•ã‚Œã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: pcluster create-cluster ã‚³ãƒãƒ³ãƒ‰ãŒæ­£å¸¸ã«å®Ÿè¡Œã•ã‚Œã€clusterStatus ãŒ "CREATE_IN_PROGRESS" ã«ãªã‚‹ã“ã¨ã€‚
:::

ç”Ÿæˆã•ã‚ŒãŸè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’ä½œæˆã—ã¾ã™ã€‚

```bash
pcluster create-cluster \
    --cluster-name ${CLUSTER_NAME} \
    --cluster-configuration ${CONFIG_DIR}/cluster.yaml \
    --region ${AWS_REGION} \
    --rollback-on-failure false
```

å‡ºåŠ›ä¾‹ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

```json
{
  "cluster": {
    "clusterName": "ml-cluster",
    "cloudformationStackStatus": "CREATE_IN_PROGRESS",
    "cloudformationStackArn": "arn:aws:cloudformation:ap-northeast-1:123456789012:stack/ml-cluster/abcd1234-...",
    "region": "ap-northeast-1",
    "version": "3.13.1",
    "clusterStatus": "CREATE_IN_PROGRESS",
    "scheduler": {
      "type": "slurm"
    }
  }
}
```
::::

::::details 9-2. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ä½œæˆã®ç›£è¦–

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ä½œæˆé€²è¡ŒçŠ¶æ³ã‚’ç›£è¦–ã—ã€CREATE_COMPLETE çŠ¶æ…‹ã«ãªã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: `pcluster list-clusters` ã‚³ãƒãƒ³ãƒ‰ã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãŒ "CREATE_COMPLETE" ã«ãªã‚‹ã“ã¨ã€‚é€šå¸¸ 15 ã‹ã‚‰ 20 åˆ†ã‹ã‹ã‚Šã¾ã™ã€‚
:::

AWS ParallelCluster CLI ã‚’ä½¿ç”¨ã—ã¦ä½œæˆçŠ¶æ³ã‚’ç¢ºèªã—ã¾ã™ã€‚

```bash
pcluster list-clusters --region ${AWS_REGION}
```

```bash
(pcluster_3.13.1_env) 2.aws-parallelcluster $ pcluster list-clusters --region ${AWS_REGION}
/home/cloudshell-user/pcluster_3.13.1_env/lib/python3.9/site-packages/pcluster/api/controllers/common.py:20: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  from pkg_resources import packaging

{
  "clusters": [
    {
      "clusterName": "ml-cluster",
      "cloudformationStackStatus": "CREATE_IN_PROGRESS",
      "cloudformationStackArn": "arn:aws:cloudformation:ap-northeast-1:XXXXXXXXX:stack/ml-cluster/531f6f90-d4df-11f0-80fa-062c6da0463d",
      "region": "ap-northeast-1",
      "version": "3.13.1",
      "clusterStatus": "CREATE_IN_PROGRESS",
      "scheduler": {
        "type": "slurm"
      }
    }
  ]
}
```

ã¾ãŸã¯ CloudFormation ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã§ç›£è¦–ã§ãã¾ã™ã€‚

1. [CloudFormation ã‚³ãƒ³ã‚½ãƒ¼ãƒ«](https://console.aws.amazon.com/cloudformation)ã«ç§»å‹•
2. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ã‚¹ã‚¿ãƒƒã‚¯ã‚’é¸æŠ
3. "Events" ã‚¿ãƒ–ã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã®æ›´æ–°ã‚’ç›£è¦–
::::

## 10. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã¸ã®æ¥ç¶š

:::message
- [ ] 10-1. SSM Session Manager ã§ã®æ¥ç¶š
- [ ] 10-2. SSH ã§ã®æ¥ç¶šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
:::

::::details 10-1. SSM Session Manager ã§ã®æ¥ç¶š

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: AWS Systems Manager Session Manager ã‚’ä½¿ç”¨ã—ã¦ Head Node ã«æ¥ç¶šã—ã¾ã™ã€‚ãƒãƒ¼ãƒˆã‚’é–‹ãå¿…è¦ãŒãªãã€AWS ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã§ã®èªè¨¼ã®ã¿ã§å®‰å…¨ã«æ¥ç¶šã§ãã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: Head Node ã«æ¥ç¶šã§ãã€ubuntu ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«åˆ‡ã‚Šæ›¿ãˆã¦ `hostname` ã‚³ãƒãƒ³ãƒ‰ãŒå®Ÿè¡Œã§ãã‚‹ã“ã¨ã€‚
:::

SSM Session Manager ã‚’ä½¿ç”¨ã—ã¦æ¥ç¶šã—ã¾ã™ã€‚

1. [EC2 ã‚³ãƒ³ã‚½ãƒ¼ãƒ«](https://console.aws.amazon.com/ec2/)ã«ç§»å‹•
2. Head Node ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’è¦‹ã¤ã‘ã‚‹ï¼ˆã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åã« "HeadNode" ã‚’å«ã‚€ï¼‰
3. "Connect" ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
4. "Session Manager" ã‚¿ãƒ–ã‚’é¸æŠ
5. "Connect" ã‚’ã‚¯ãƒªãƒƒã‚¯

æ¥ç¶šå¾Œã€ubuntu ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«åˆ‡ã‚Šæ›¿ãˆã¾ã™ã€‚

```bash
sudo su - ubuntu
```

ãƒ›ãƒ¼ãƒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç¢ºèªã—ã¾ã™ã€‚

```bash
pwd
hostname
```
::::

::::details 10-2. SSH ã§ã®æ¥ç¶šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: SSH ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦ Head Node ã«æ¥ç¶šã—ã¾ã™ã€‚ãƒ­ãƒ¼ã‚«ãƒ«ã®ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ã‹ã‚‰ç›´æ¥æ¥ç¶šã—ãŸã„å ´åˆã«ä½¿ç”¨ã—ã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: SSH æ¥ç¶šãŒæˆåŠŸã—ã€Head Node ã®ã‚·ã‚§ãƒ«ã«ã‚¢ã‚¯ã‚»ã‚¹ã§ãã‚‹ã“ã¨ã€‚
:::

Head Node ã® IP ã‚¢ãƒ‰ãƒ¬ã‚¹ã‚’å–å¾—ã—ã¾ã™ã€‚

```bash
pcluster ssh --region ${AWS_REGION} --cluster-name ${CLUSTER_NAME} --identity_file ~/.ssh/${KEYPAIR_NAME}.pem --dryrun true
```

å‡ºåŠ›ä¾‹ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

```json
{
  "command": "ssh ubuntu@18.183.235.248 -i ~/.ssh/my-keypair.pem"
}
```

å‡ºåŠ›ã•ã‚ŒãŸã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¦æ¥ç¶šã—ã¾ã™ã€‚

```bash
ssh ubuntu@18.183.235.248 -i ~/.ssh/my-keypair.pem
```
::::

## 11. Slurm ã®åŸºæœ¬æ“ä½œ

Slurm ã¯ HPC ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã§åºƒãä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã‚¸ãƒ§ãƒ–ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã§ã™ã€‚

:::message
- [ ] 11-1. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®çŠ¶æ…‹ç¢ºèª
- [ ] 11-2. srun ã«ã‚ˆã‚‹å¯¾è©±çš„ãªã‚¸ãƒ§ãƒ–å®Ÿè¡Œ
- [ ] 11-3. sbatch ã«ã‚ˆã‚‹ãƒãƒƒãƒã‚¸ãƒ§ãƒ–ã®æŠ•å…¥
- [ ] 11-4. salloc ã«ã‚ˆã‚‹ãƒªã‚½ãƒ¼ã‚¹ã®å¯¾è©±çš„ãªå‰²ã‚Šå½“ã¦
:::

::::details 11-1. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®çŠ¶æ…‹ç¢ºèª

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: Slurm ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ãƒãƒ¼ãƒ‰çŠ¶æ…‹ã‚’ç¢ºèªã—ã€Compute Node ãŒæ­£å¸¸ã«èµ·å‹•ã—ã¦åˆ©ç”¨å¯èƒ½ãªçŠ¶æ…‹ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: `sinfo` ã‚³ãƒãƒ³ãƒ‰ã§ãƒãƒ¼ãƒ‰ã® STATE ãŒ "idle" ã¾ãŸã¯ "alloc" ã¨è¡¨ç¤ºã•ã‚Œã€è¨­å®šã—ãŸãƒãƒ¼ãƒ‰æ•°ãŒç¢ºèªã§ãã‚‹ã“ã¨ã€‚
:::

ãƒãƒ¼ãƒ‰ã®çŠ¶æ…‹ã‚’ç¢ºèªã—ã¾ã™ã€‚

```bash
sinfo
```

å‡ºåŠ›ä¾‹ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

```
PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
compute-gpu*  up   infinite      3   idle compute-gpu-distributed-ml-[1-3]
```

è©³ç´°ãªãƒãƒ¼ãƒ‰æƒ…å ±ã‚’ç¢ºèªã—ã¾ã™ã€‚

```bash
sinfo -N -l
```
::::

::::details 11-2. srun ã«ã‚ˆã‚‹å¯¾è©±çš„ãªã‚¸ãƒ§ãƒ–å®Ÿè¡Œ

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: `srun` ã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€ãƒªã‚½ãƒ¼ã‚¹ã‚’å³åº§ã«å‰²ã‚Šå½“ã¦ã¦ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚é–‹ç™ºã‚„ãƒ†ã‚¹ãƒˆæ™‚ã«ä¾¿åˆ©ãªã‚³ãƒãƒ³ãƒ‰ã§ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: `srun hostname` ã‚³ãƒãƒ³ãƒ‰ãŒæ­£å¸¸ã«å®Ÿè¡Œã•ã‚Œã€Compute Node ã®ãƒ›ã‚¹ãƒˆåãŒè¡¨ç¤ºã•ã‚Œã‚‹ã“ã¨ã€‚
:::

åŸºæœ¬çš„ãªä½¿ç”¨ä¾‹ã‚’è©¦ã—ã¾ã™ã€‚

```bash
# å˜ä¸€ãƒãƒ¼ãƒ‰ã§ hostname ã‚’å®Ÿè¡Œ
srun hostname

# 3 ãƒãƒ¼ãƒ‰ã§ hostname ã‚’å®Ÿè¡Œ
srun --nodes=3 --ntasks-per-node=1 hostname

# 2 ãƒãƒ¼ãƒ‰ã€å„ãƒãƒ¼ãƒ‰ 2 ã‚¿ã‚¹ã‚¯ã§å®Ÿè¡Œ
srun --nodes=2 --ntasks-per-node=2 hostname
```

**ä¸»è¦ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³**
- `--nodes`: ä½¿ç”¨ã™ã‚‹ãƒãƒ¼ãƒ‰æ•°
- `--ntasks-per-node`: ãƒãƒ¼ãƒ‰ã‚ãŸã‚Šã®ã‚¿ã‚¹ã‚¯æ•°
- `--mpi`: MPI ã®å®Ÿè£…ï¼ˆpmixã€pmi2 ãªã©ï¼‰
- `--cpu-bind`: CPU ãƒã‚¤ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ã®æ–¹æ³•ï¼ˆnoneã€coresã€threads ãªã©ï¼‰
::::

::::details 11-3. sbatch ã«ã‚ˆã‚‹ãƒãƒƒãƒã‚¸ãƒ§ãƒ–ã®æŠ•å…¥

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: `sbatch` ã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€ãƒãƒƒãƒã‚¸ãƒ§ãƒ–ã‚’ã‚­ãƒ¥ãƒ¼ã«æŠ•å…¥ã—ã¾ã™ã€‚ã‚¹ã‚¯ãƒªãƒ—ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã€ã‚¸ãƒ§ãƒ–ã‚’ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã«ç®¡ç†ã•ã›ã‚‹ã“ã¨ã§ã€é•·æ™‚é–“å®Ÿè¡Œã™ã‚‹ã‚¸ãƒ§ãƒ–ã‚’åŠ¹ç‡çš„ã«å‡¦ç†ã§ãã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: `sbatch` ã‚³ãƒãƒ³ãƒ‰ã§ã‚¸ãƒ§ãƒ–ãŒæ­£å¸¸ã«æŠ•å…¥ã•ã‚Œã€ã‚¸ãƒ§ãƒ– ID ãŒè¿”ã•ã‚Œã‚‹ã“ã¨ã€‚`squeue` ã‚³ãƒãƒ³ãƒ‰ã§ã‚¸ãƒ§ãƒ–ã®çŠ¶æ…‹ãŒç¢ºèªã§ãã‚‹ã“ã¨ã€‚
:::

ãƒãƒƒãƒã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆã—ã¾ã™ï¼ˆtest-job.sbatchï¼‰ã€‚

```bash
cat << 'EOF' > test-job.sbatch
#!/bin/bash

#SBATCH --job-name=test-job
#SBATCH --nodes=2
#SBATCH --ntasks-per-node=2
#SBATCH --output=%x_%j.out
#SBATCH --error=%x_%j.err

# ã‚¸ãƒ§ãƒ–ã®å†…å®¹
srun hostname
srun sleep 10
srun date
EOF
```

ã‚¸ãƒ§ãƒ–ã‚’æŠ•å…¥ã—ã¾ã™ã€‚

```bash
sbatch test-job.sbatch
```

å‡ºåŠ›ä¾‹ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

```
Submitted batch job 123
```

ã‚¸ãƒ§ãƒ–ã®çŠ¶æ…‹ã‚’ç¢ºèªã—ã¾ã™ã€‚

```bash
squeue
```

**ä¸»è¦ãª #SBATCH ãƒ‡ã‚£ãƒ¬ã‚¯ãƒ†ã‚£ãƒ–**
- `--job-name`: ã‚¸ãƒ§ãƒ–ã®åå‰
- `--nodes`: ä½¿ç”¨ã™ã‚‹ãƒãƒ¼ãƒ‰æ•°
- `--ntasks-per-node`: ãƒãƒ¼ãƒ‰ã‚ãŸã‚Šã®ã‚¿ã‚¹ã‚¯æ•°
- `--output`: æ¨™æº–å‡ºåŠ›ã®ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆ%x ã¯ã‚¸ãƒ§ãƒ–åã€%j ã¯ã‚¸ãƒ§ãƒ– IDï¼‰
- `--error`: æ¨™æº–ã‚¨ãƒ©ãƒ¼å‡ºåŠ›ã®ãƒ•ã‚¡ã‚¤ãƒ«å
::::

::::details 11-4. salloc ã«ã‚ˆã‚‹ãƒªã‚½ãƒ¼ã‚¹ã®å¯¾è©±çš„ãªå‰²ã‚Šå½“ã¦

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: `salloc` ã‚³ãƒãƒ³ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€å¯¾è©±çš„ãªã‚»ãƒƒã‚·ãƒ§ãƒ³ç”¨ã«ãƒªã‚½ãƒ¼ã‚¹ã‚’å‰²ã‚Šå½“ã¦ã¾ã™ã€‚ãƒªã‚½ãƒ¼ã‚¹ã‚’å‰²ã‚Šå½“ã¦ãŸã¾ã¾è¤‡æ•°ã®ã‚³ãƒãƒ³ãƒ‰ã‚’è©¦ã™ã“ã¨ãŒã§ãã€é–‹ç™ºã‚„ãƒ‡ãƒãƒƒã‚°æ™‚ã«ä¾¿åˆ©ã§ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: `salloc` ã‚³ãƒãƒ³ãƒ‰ã§ãƒªã‚½ãƒ¼ã‚¹ãŒå‰²ã‚Šå½“ã¦ã‚‰ã‚Œã€å‰²ã‚Šå½“ã¦ã‚‰ã‚ŒãŸãƒªã‚½ãƒ¼ã‚¹ä¸Šã§ `srun` ã‚³ãƒãƒ³ãƒ‰ãŒå®Ÿè¡Œã§ãã‚‹ã“ã¨ã€‚
:::

ãƒªã‚½ãƒ¼ã‚¹ã‚’å‰²ã‚Šå½“ã¦ã¾ã™ã€‚

```bash
# 2 ãƒãƒ¼ãƒ‰ã‚’å‰²ã‚Šå½“ã¦
salloc --nodes=2 --ntasks-per-node=2
```

å‰²ã‚Šå½“ã¦ã‚‰ã‚ŒãŸãƒªã‚½ãƒ¼ã‚¹ä¸Šã§ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

```bash
srun hostname
srun date
```

ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’çµ‚äº†ã—ã¾ã™ã€‚

```bash
exit
```
::::

::::details ã‚¸ãƒ§ãƒ–ç®¡ç†ã‚³ãƒãƒ³ãƒ‰ã®å‚è€ƒ

**ã‚¸ãƒ§ãƒ–ã®çŠ¶æ…‹ã‚’ç¢ºèª**ï¼ˆsqueueï¼‰

```bash
# å…¨ã‚¸ãƒ§ãƒ–ã®çŠ¶æ…‹ã‚’è¡¨ç¤º
squeue

# è‡ªåˆ†ã®ã‚¸ãƒ§ãƒ–ã®ã¿ã‚’è¡¨ç¤º
squeue -u $USER

# è©³ç´°ãªæƒ…å ±ã‚’è¡¨ç¤º
squeue -l
```

**ã‚¸ãƒ§ãƒ–ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«**ï¼ˆscancelï¼‰

```bash
# ã‚¸ãƒ§ãƒ– ID ã‚’æŒ‡å®šã—ã¦ã‚­ãƒ£ãƒ³ã‚»ãƒ«
scancel 123

# è‡ªåˆ†ã®å…¨ã‚¸ãƒ§ãƒ–ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«
scancel -u $USER
```

**ã‚¸ãƒ§ãƒ–ã®è©³ç´°æƒ…å ±ã‚’ç¢ºèª**ï¼ˆscontrolï¼‰

```bash
# ã‚¸ãƒ§ãƒ–ã®è©³ç´°ã‚’è¡¨ç¤º
scontrol show job 123

# ãƒãƒ¼ãƒ‰ã®è©³ç´°ã‚’è¡¨ç¤º
scontrol show node compute-gpu-distributed-ml-1
```
::::

## 12. ã‚³ãƒ³ãƒ†ãƒŠã‚’ä½¿ã£ãŸã‚¸ãƒ§ãƒ–å®Ÿè¡Œ

AWS ParallelCluster ã§ã¯ã€Enroot ã¨ Pyxis ã‚’ä½¿ç”¨ã—ã¦ã‚³ãƒ³ãƒ†ãƒŠåŒ–ã•ã‚ŒãŸãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã‚’ Slurm ä¸Šã§å®Ÿè¡Œã§ãã¾ã™ã€‚

:::message
- [ ] 12-1. enroot import ã«ã‚ˆã‚‹ã‚³ãƒ³ãƒ†ãƒŠã‚¤ãƒ¡ãƒ¼ã‚¸ã®æº–å‚™
- [ ] 12-2. srun --container-image ã«ã‚ˆã‚‹ã‚³ãƒ³ãƒ†ãƒŠã‚¸ãƒ§ãƒ–ã®å®Ÿè¡Œ
:::

::::details Enroot ã¨ Pyxis ã®æ¦‚è¦

**Enroot**
- NVIDIA ãŒé–‹ç™ºã—ãŸã‚³ãƒ³ãƒ†ãƒŠãƒ©ãƒ³ã‚¿ã‚¤ãƒ 
- Docker ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’ Squash ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã«å¤‰æ›
- é«˜æ€§èƒ½ãª HPC ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰å‘ã‘ã«æœ€é©åŒ–

**Pyxis**
- Enroot ã‚’ Slurm ã¨çµ±åˆã™ã‚‹ Slurm ãƒ—ãƒ©ã‚°ã‚¤ãƒ³
- `srun` ã‚³ãƒãƒ³ãƒ‰ã§ `--container-image` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¦ã‚³ãƒ³ãƒ†ãƒŠã‚’å®Ÿè¡Œå¯èƒ½

### ã‚³ãƒ³ãƒ†ãƒŠãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

```mermaid
graph LR
    A[Docker Image] -->|docker build| B[Local Image]
    B -->|enroot import| C[Squash File<br/>.sqsh]
    C -->|srun --container-image| D[Container Job]
    
    E[Docker Registry<br/>ECR/DockerHub] -->|enroot import dockerd://| C
    
    style A fill:#2196F3,color:#fff
    style B fill:#2196F3,color:#fff
    style C fill:#FF9800,color:#fff
    style D fill:#4CAF50,color:#fff
    style E fill:#9C27B0,color:#fff
```
::::

::::details 12-1. enroot import ã«ã‚ˆã‚‹ã‚³ãƒ³ãƒ†ãƒŠã‚¤ãƒ¡ãƒ¼ã‚¸ã®æº–å‚™

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: Docker ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’ Squash ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ï¼ˆ.sqshï¼‰ã«å¤‰æ›ã—ã¾ã™ã€‚Squash ãƒ•ã‚¡ã‚¤ãƒ«ã¯èª­ã¿å–ã‚Šå°‚ç”¨ã®åœ§ç¸®ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã‚¤ãƒ¡ãƒ¼ã‚¸ã§ã€HPC ç’°å¢ƒã§ã®é…å¸ƒã¨å®Ÿè¡Œã«æœ€é©åŒ–ã•ã‚Œã¦ã„ã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: `/fsx` ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã« .sqsh ãƒ•ã‚¡ã‚¤ãƒ«ãŒä½œæˆã•ã‚Œã€`ls -lh /fsx/*.sqsh` ã§ç¢ºèªã§ãã‚‹ã“ã¨ã€‚
:::

Docker ãƒ¬ã‚¸ã‚¹ãƒˆãƒªã‹ã‚‰ç›´æ¥ Squash ãƒ•ã‚¡ã‚¤ãƒ«ã«å¤‰æ›ã—ã¾ã™ã€‚

```bash
# Docker Hub ã‹ã‚‰å¤‰æ›
enroot import -o /fsx/pytorch.sqsh docker://nvcr.io/nvidia/pytorch:24.07-py3

# ãƒ‘ãƒ–ãƒªãƒƒã‚¯ ECR ã‹ã‚‰å¤‰æ›
enroot import -o /fsx/ubuntu.sqsh dockerd://public.ecr.aws/ubuntu/ubuntu:22.04
```

ãƒ­ãƒ¼ã‚«ãƒ«ã® Docker ã‚¤ãƒ¡ãƒ¼ã‚¸ã‹ã‚‰å¤‰æ›ã™ã‚‹å ´åˆã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ã—ã¾ã™ã€‚

```bash
# Docker ã‚¤ãƒ¡ãƒ¼ã‚¸ã‚’ãƒ“ãƒ«ãƒ‰
docker build -t my-image:latest .

# Squash ãƒ•ã‚¡ã‚¤ãƒ«ã«å¤‰æ›
enroot import -o /fsx/my-image.sqsh dockerd://my-image:latest
```

ä½œæˆã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¾ã™ã€‚

```bash
ls -lh /fsx/*.sqsh
```
::::

::::details 12-2. srun --container-image ã«ã‚ˆã‚‹ã‚³ãƒ³ãƒ†ãƒŠã‚¸ãƒ§ãƒ–ã®å®Ÿè¡Œ

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: `srun` ã‚³ãƒãƒ³ãƒ‰ã« `--container-image` ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’è¿½åŠ ã—ã¦ã€ã‚³ãƒ³ãƒ†ãƒŠå†…ã§ã‚¸ãƒ§ãƒ–ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚ãƒ›ã‚¹ãƒˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒã‚¦ãƒ³ãƒˆã—ãŸã‚Šã€ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ãŸã‚Šã§ãã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: ã‚³ãƒ³ãƒ†ãƒŠå†…ã§ã‚³ãƒãƒ³ãƒ‰ãŒæ­£å¸¸ã«å®Ÿè¡Œã•ã‚Œã€æœŸå¾…ã™ã‚‹å‡ºåŠ›ãŒå¾—ã‚‰ã‚Œã‚‹ã“ã¨ã€‚
:::

åŸºæœ¬çš„ãªä½¿ç”¨ä¾‹ã‚’è©¦ã—ã¾ã™ã€‚

```bash
# ã‚³ãƒ³ãƒ†ãƒŠå†…ã§ã‚³ãƒãƒ³ãƒ‰ã‚’å®Ÿè¡Œ
srun --nodes=1 \
     --ntasks=1 \
     --container-image=/fsx/ubuntu.sqsh \
     cat /etc/os-release
```

ãƒ›ã‚¹ãƒˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒã‚¦ãƒ³ãƒˆã—ã¦å®Ÿè¡Œã—ã¾ã™ã€‚

```bash
# /fsx ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒã‚¦ãƒ³ãƒˆ
srun --container-image=/fsx/ubuntu.sqsh \
     --container-mounts=/fsx:/fsx \
     ls -la /fsx

# ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒã‚¦ãƒ³ãƒˆ
srun --container-image=/fsx/ubuntu.sqsh \
     --container-mounts=$PWD:/work \
     ls -la /work
```

ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®šã—ã¦å®Ÿè¡Œã—ã¾ã™ã€‚

```bash
# ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
export MY_VAR="test value"

srun --container-image=/fsx/ubuntu.sqsh \
     --export=ALL \
     bash -c 'echo $MY_VAR'
```

**ä¸»è¦ãªã‚ªãƒ—ã‚·ãƒ§ãƒ³**
- `--container-image`: ä½¿ç”¨ã™ã‚‹ Squash ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
- `--container-mounts`: ãƒ›ã‚¹ãƒˆã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒŠã¸ã®ãƒã‚¦ãƒ³ãƒˆï¼ˆ`host_path:container_path` å½¢å¼ã€è¤‡æ•°æŒ‡å®šå¯èƒ½ï¼‰
- `--export`: ç’°å¢ƒå¤‰æ•°ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ–¹æ³•ï¼ˆALLã€NONEã€ã¾ãŸã¯ç‰¹å®šã®å¤‰æ•°åï¼‰
::::

## 13. å®Ÿè·µä¾‹ï¼šç°¡æ˜“çš„ãªåˆ†æ•£ã‚¸ãƒ§ãƒ–ã®å®Ÿè¡Œ

:::message
- [ ] 13-1. åˆ†æ•£ã‚¸ãƒ§ãƒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ä½œæˆã¨å®Ÿè¡Œ
:::

::::details 13-1. åˆ†æ•£ã‚¸ãƒ§ãƒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®ä½œæˆã¨å®Ÿè¡Œ

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: è¤‡æ•°ãƒãƒ¼ãƒ‰ã«ã¾ãŸãŒã‚‹ç°¡å˜ãªåˆ†æ•£ã‚¸ãƒ§ãƒ–ã‚’å®Ÿè¡Œã—ã¦ã€Slurm ã®å‹•ä½œã‚’ç¢ºèªã—ã¾ã™ã€‚ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã§ã¯ã€3 ãƒãƒ¼ãƒ‰ã«å¯¾ã—ã¦å„ãƒãƒ¼ãƒ‰ 2 ã‚¿ã‚¹ã‚¯ï¼ˆåˆè¨ˆ 6 ã‚¿ã‚¹ã‚¯ï¼‰ã‚’ä¸¦åˆ—å®Ÿè¡Œã—ã€åˆ†æ•£ç’°å¢ƒã§ã®ã‚¿ã‚¹ã‚¯å‰²ã‚Šå½“ã¦ã¨ãƒãƒ¼ãƒ‰é–“é€šä¿¡ã®åŸºæœ¬ã‚’ç†è§£ã—ã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: ã‚¸ãƒ§ãƒ–ãŒæ­£å¸¸ã«å®Ÿè¡Œã•ã‚Œã€å…¨ãƒãƒ¼ãƒ‰ã‹ã‚‰ã®å‡ºåŠ›ãŒç¢ºèªã§ãã‚‹ã“ã¨ã€‚
:::

åˆ†æ•£ã‚¸ãƒ§ãƒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½œæˆã—ã¾ã™ã€‚

```bash
cat << 'EOF' > distributed-job.sbatch
#!/bin/bash

#SBATCH --job-name=distributed-test
#SBATCH --nodes=3
#SBATCH --ntasks-per-node=2
#SBATCH --output=%x_%j.out
#SBATCH --error=%x_%j.err

echo "=== ã‚¸ãƒ§ãƒ–é–‹å§‹ ==="
echo "ã‚¸ãƒ§ãƒ– ID: $SLURM_JOB_ID"
echo "ãƒãƒ¼ãƒ‰æ•°: $SLURM_JOB_NUM_NODES"
echo "ç·ã‚¿ã‚¹ã‚¯æ•°: $SLURM_NTASKS"

# å„ãƒãƒ¼ãƒ‰ã§ãƒ›ã‚¹ãƒˆåã‚’è¡¨ç¤º
srun hostname

# å„ãƒãƒ¼ãƒ‰ã§ç°¡å˜ãªè¨ˆç®—ã‚’å®Ÿè¡Œ
srun bash -c 'echo "Node: $(hostname), Task: $SLURM_PROCID, Result: $((SLURM_PROCID * 100))"'

echo "=== ã‚¸ãƒ§ãƒ–çµ‚äº† ==="
EOF
```

**ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®å‹•ä½œèª¬æ˜**

ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã®å‹•ä½œã‚’å®Ÿè¡Œã—ã¾ã™ã€‚

1. **ãƒªã‚½ãƒ¼ã‚¹å‰²ã‚Šå½“ã¦**ï¼ˆ#SBATCH ãƒ‡ã‚£ãƒ¬ã‚¯ãƒ†ã‚£ãƒ–ï¼‰
   - `--nodes=3`: 3 ã¤ã® Compute Node ã‚’å‰²ã‚Šå½“ã¦
   - `--ntasks-per-node=2`: å„ãƒãƒ¼ãƒ‰ã§ 2 ã¤ã®ã‚¿ã‚¹ã‚¯ã‚’ä¸¦åˆ—å®Ÿè¡Œï¼ˆåˆè¨ˆ 6 ã‚¿ã‚¹ã‚¯ï¼‰
   - `--output=%x_%j.out`: æ¨™æº–å‡ºåŠ›ã‚’ `distributed-test_<ã‚¸ãƒ§ãƒ–ID>.out` ã«ä¿å­˜
   - `--error=%x_%j.err`: ã‚¨ãƒ©ãƒ¼å‡ºåŠ›ã‚’ `distributed-test_<ã‚¸ãƒ§ãƒ–ID>.err` ã«ä¿å­˜

2. **ã‚¸ãƒ§ãƒ–æƒ…å ±ã®è¡¨ç¤º**
   - `SLURM_JOB_ID`: Slurm ãŒå‰²ã‚Šå½“ã¦ãŸã‚¸ãƒ§ãƒ–ã®ä¸€æ„ãª ID
   - `SLURM_JOB_NUM_NODES`: ã“ã®ã‚¸ãƒ§ãƒ–ã§ä½¿ç”¨ã—ã¦ã„ã‚‹ãƒãƒ¼ãƒ‰æ•°ï¼ˆ3ï¼‰
   - `SLURM_NTASKS`: ç·ã‚¿ã‚¹ã‚¯æ•°ï¼ˆ6 = 3 ãƒãƒ¼ãƒ‰ Ã— 2 ã‚¿ã‚¹ã‚¯/ãƒãƒ¼ãƒ‰ï¼‰

3. **åˆ†æ•£å®Ÿè¡Œã®ãƒ†ã‚¹ãƒˆ**
   - `srun hostname`: å„ã‚¿ã‚¹ã‚¯ãŒå®Ÿè¡Œã•ã‚Œã¦ã„ã‚‹ãƒãƒ¼ãƒ‰ã®ãƒ›ã‚¹ãƒˆåã‚’è¡¨ç¤º
   - `srun bash -c '...'`: å„ã‚¿ã‚¹ã‚¯ã§ `SLURM_PROCID`ï¼ˆã‚¿ã‚¹ã‚¯ ID: 0-5ï¼‰ã‚’ä½¿ç”¨ã—ãŸè¨ˆç®—ã‚’å®Ÿè¡Œ

ã‚¸ãƒ§ãƒ–ã‚’æŠ•å…¥ã—ã¾ã™ã€‚

```bash
sbatch distributed-job.sbatch
```

å‡ºåŠ›ä¾‹ã¯ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

```bash
Submitted batch job 1
```

ã‚¸ãƒ§ãƒ–ã®çŠ¶æ…‹ã‚’ç¢ºèªã—ã¾ã™ã€‚

```bash
squeue
```

å®Ÿè¡Œä¸­ã®å ´åˆã¯ä»¥ä¸‹ã®ã‚ˆã†ã«è¡¨ç¤ºã•ã‚Œã¾ã™ã€‚

```
JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)
    1 compute-g distribu   ubuntu  R       0:05      3 compute-gpu-distributed-ml-[1-3]
```

**çŠ¶æ…‹ã®æ„å‘³**
- `R` (Running): å®Ÿè¡Œä¸­
- `PD` (Pending): å¾…æ©Ÿä¸­ï¼ˆãƒªã‚½ãƒ¼ã‚¹ãŒåˆ©ç”¨å¯èƒ½ã«ãªã‚‹ã¾ã§ï¼‰
- `CG` (Completing): å®Œäº†å‡¦ç†ä¸­

ã‚¸ãƒ§ãƒ–ãŒå®Œäº†ã—ãŸã‚‰ã€å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç¢ºèªã—ã¾ã™ã€‚

```bash
cat distributed-test_1.out
```

**å‡ºåŠ›ä¾‹ã¨è§£é‡ˆ**

```
=== ã‚¸ãƒ§ãƒ–é–‹å§‹ ===
ã‚¸ãƒ§ãƒ– ID: 1
ãƒãƒ¼ãƒ‰æ•°: 3
ç·ã‚¿ã‚¹ã‚¯æ•°: 6
compute-gpu-distributed-ml-1
compute-gpu-distributed-ml-1
compute-gpu-distributed-ml-2
compute-gpu-distributed-ml-2
compute-gpu-distributed-ml-3
compute-gpu-distributed-ml-3
Node: compute-gpu-distributed-ml-1, Task: 0, Result: 0
Node: compute-gpu-distributed-ml-1, Task: 1, Result: 100
Node: compute-gpu-distributed-ml-2, Task: 2, Result: 200
Node: compute-gpu-distributed-ml-2, Task: 3, Result: 300
Node: compute-gpu-distributed-ml-3, Task: 4, Result: 400
Node: compute-gpu-distributed-ml-3, Task: 5, Result: 500
=== ã‚¸ãƒ§ãƒ–çµ‚äº† ===
```

**å‡ºåŠ›ã®è§£é‡ˆ**

1. **ãƒ›ã‚¹ãƒˆåã®å‡ºåŠ›**: å„ãƒãƒ¼ãƒ‰ã‹ã‚‰ 2 å›ãšã¤ãƒ›ã‚¹ãƒˆåãŒè¡¨ç¤ºã•ã‚Œã€3 ãƒãƒ¼ãƒ‰ Ã— 2 ã‚¿ã‚¹ã‚¯ = 6 è¡Œã®å‡ºåŠ›ã‚’ç¢ºèª
2. **ã‚¿ã‚¹ã‚¯ ID ã¨è¨ˆç®—çµæœ**: å„ã‚¿ã‚¹ã‚¯ã« 0 ã‹ã‚‰ 5 ã® IDï¼ˆ`SLURM_PROCID`ï¼‰ãŒå‰²ã‚Šå½“ã¦ã‚‰ã‚Œã€ãã‚Œãã‚ŒãŒç‹¬ç«‹ã—ã¦è¨ˆç®—ã‚’å®Ÿè¡Œ
3. **ãƒãƒ¼ãƒ‰åˆ†æ•£**: ã‚¿ã‚¹ã‚¯ 0-1 ãŒãƒãƒ¼ãƒ‰ 1ã€ã‚¿ã‚¹ã‚¯ 2-3 ãŒãƒãƒ¼ãƒ‰ 2ã€ã‚¿ã‚¹ã‚¯ 4-5 ãŒãƒãƒ¼ãƒ‰ 3 ã§å®Ÿè¡Œã•ã‚Œã€ã‚¿ã‚¹ã‚¯ãŒå‡ç­‰ã«åˆ†æ•£ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª

ã“ã®ã‚ˆã†ã«ã€Slurm ã¯è‡ªå‹•çš„ã«ã‚¿ã‚¹ã‚¯ã‚’ãƒãƒ¼ãƒ‰ã«åˆ†æ•£ã—ã€å„ã‚¿ã‚¹ã‚¯ã«ä¸€æ„ãª ID ã‚’å‰²ã‚Šå½“ã¦ã¦ä¸¦åˆ—å®Ÿè¡Œã‚’ç®¡ç†ã—ã¾ã™ã€‚å®Ÿéš›ã®åˆ†æ•£å­¦ç¿’ã§ã¯ã€ã“ã®ä»•çµ„ã¿ã‚’ä½¿ç”¨ã—ã¦å„ãƒãƒ¼ãƒ‰ãŒç•°ãªã‚‹ãƒ‡ãƒ¼ã‚¿ãƒãƒƒãƒã‚’å‡¦ç†ã—ãŸã‚Šã€ãƒ¢ãƒ‡ãƒ«ã®ç•°ãªã‚‹éƒ¨åˆ†ã‚’è¨ˆç®—ã—ãŸã‚Šã—ã¾ã™ã€‚
::::

## 14. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®å‰Šé™¤

:::message
- [ ] 14-1. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®å‰Šé™¤
- [ ] 14-2. å‰ææ¡ä»¶ã‚¤ãƒ³ãƒ•ãƒ©ã®å‰Šé™¤ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
:::

::::details 14-1. ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®å‰Šé™¤

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: ä½¿ç”¨ãŒçµ‚ã‚ã£ãŸã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’å‰Šé™¤ã—ã¦ã€ã‚³ã‚¹ãƒˆã®ç™ºç”Ÿã‚’åœæ­¢ã—ã¾ã™ã€‚Head Node ã¨ Compute Node ãŒå‰Šé™¤ã•ã‚Œã¾ã™ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: `pcluster list-clusters` ã‚³ãƒãƒ³ãƒ‰ã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãŒ "DELETE_COMPLETE" ã«ãªã‚‹ã“ã¨ã€‚å‰Šé™¤ã«ã¯ 5 ã‹ã‚‰ 10 åˆ†ã‹ã‹ã‚Šã¾ã™ã€‚
:::

ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã‚’å‰Šé™¤ã—ã¾ã™ã€‚

```bash
pcluster delete-cluster \
    --cluster-name ${CLUSTER_NAME} \
    --region ${AWS_REGION}
```

å‰Šé™¤ã®é€²è¡ŒçŠ¶æ³ã‚’ç¢ºèªã—ã¾ã™ã€‚

```bash
pcluster list-clusters --region ${AWS_REGION}
```
::::

::::details 14-2. å‰ææ¡ä»¶ã‚¤ãƒ³ãƒ•ãƒ©ã®å‰Šé™¤ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

:::message
ãªã‚“ã®ãŸã‚ã®ä½œæ¥­ã‹: VPCã€FSx ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ãªã©ã®åŸºç›¤ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ã‚’å‰Šé™¤ã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®ãƒªã‚½ãƒ¼ã‚¹ã¯ä»–ã®ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã§ã‚‚å†åˆ©ç”¨ã§ãã‚‹ãŸã‚ã€å¿…è¦ã«å¿œã˜ã¦å‰Šé™¤ã—ã¦ãã ã•ã„ã€‚
:::

:::message
æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã«é€²ã‚€æ¡ä»¶: CloudFormation ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã§ã‚¹ã‚¿ãƒƒã‚¯ãŒ DELETE_COMPLETE çŠ¶æ…‹ã«ãªã‚‹ã“ã¨ã€‚
:::

CloudFormation ã‚¹ã‚¿ãƒƒã‚¯ã‚’å‰Šé™¤ã—ã¾ã™ã€‚

1. [CloudFormation ã‚³ãƒ³ã‚½ãƒ¼ãƒ«](https://console.aws.amazon.com/cloudformation)ã«ç§»å‹•
2. `parallelcluster-prerequisites` ã‚¹ã‚¿ãƒƒã‚¯ã‚’é¸æŠ
3. "Delete" ã‚’ã‚¯ãƒªãƒƒã‚¯
4. å‰Šé™¤ã®å®Œäº†ã‚’å¾…æ©Ÿï¼ˆ10 ã‹ã‚‰ 15 åˆ†ï¼‰

S3 ãƒã‚±ãƒƒãƒˆã‚’ä½œæˆã—ãŸå ´åˆã¯ã€åŒæ§˜ã« `cluster-data-bucket` ã‚¹ã‚¿ãƒƒã‚¯ã‚‚å‰Šé™¤ã§ãã¾ã™ï¼ˆãƒã‚±ãƒƒãƒˆå†…ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’äº‹å‰ã«å‰Šé™¤ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼‰ã€‚
::::

## ã¾ã¨ã‚

æœ¬ç« ã§ã¯ã€AWS ParallelCluster ç’°å¢ƒã®ãƒãƒ³ã‚ºã‚ªãƒ³ã‚’æä¾›ã—ã¾ã—ãŸã€‚åŸºæœ¬çš„ãª Head Nodeã€Compute Node ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã€FSx for Lustre ã«ã‚ˆã‚‹ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ãƒ¬ã‚¤ãƒ¤ã®ç†è§£ã€Slurm ã‚³ãƒãƒ³ãƒ‰ã®åˆ©ç”¨æ–¹æ³•ã‚„ã‚³ãƒ³ãƒ†ãƒŠçµ±åˆã€ãªã©ã‚’å®Ÿéš›ã«è©¦ã›ã‚‹ã‚ˆã†ã«ã—ã¾ã—ãŸã€‚AWS ParallelCluster ã¯ã€ã‚ªãƒ³ãƒ—ãƒ¬ãƒŸã‚¹ã® HPC ã‚¯ãƒ©ã‚¹ã‚¿ãƒ¼ã¨åŒæ§˜ã®ä½¿ç”¨æ„Ÿã‚’ä¿ã¡ãªãŒã‚‰ã€AWS ã®ã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã¨æŸ”è»Ÿæ€§ã‚’æ´»ç”¨ã§ãã‚‹å¼·åŠ›ãªãƒ„ãƒ¼ãƒ«ã§ã™ã€‚

æ¬¡ç« ã§ã¯å¤§è¦æ¨¡åŸºç›¤ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã®ãƒ¬ã‚¸ãƒªã‚¨ãƒ³ã‚·ãƒ¼ã®é‡è¦æ€§ã«ã¤ã„ã¦è§£èª¬ã—ã¾ã™ã€‚