---
title: "大規模基盤モデル学習におけるレジリエンシーの重要性"
emoji: "🎶"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["aws", "sagemaker", "hyperpod", "distributed", "infrastructure"]
free: true
---

::::details 前提
:::message
**対象読者**: 大規模基盤モデルがどういうものかを理解している方、これからモデル学習を行う方
:::
:::message
**ライセンス**: © 2025 littlemex.
本文および自作図表: CC BY 4.0
※公式ドキュメントからの引用や翻訳部分は原典の著作権に従います。
引用画像: 各画像の出典に記載されたライセンスに従います。
:::
:::message
一部 AI を用いて文章を作成します。レビューは実施しますが、見逃せない重大な間違いなどがあれば[こちらのIssue](https://github.com/littlemex/samples/issues)から連絡をお願いします。
:::
::::

**本章では大規模な GPU クラスターで基盤モデルを学習する際に発生する障害の頻度とその影響、そしてレジリエンシーの重要性について整理します。**

---

## スケールに伴う避けられない障害

GPU クラスターのスケールが大きくなるほど、障害の発生頻度は線形に増加します。単一の H100 GPU が平均 50,000 時間（約 6 年）に 1 回障害を起こすと仮定した場合、クラスター全体の平均障害間隔（MTBF）はクラスター内の GPU 数に反比例します。1,000 GPU のクラスターでは約 2 日ごとに障害が発生し、10,000 GPU では 5 時間ごとになります。さらにスケールが拡大すると、100,000 GPU のクラスターでは 30 分ごとに、1,000,000 GPU のクラスターではわずか 3 分ごとに何らかの GPU 障害が発生する計算になります。

この計算から明らかなように、数週間から数ヶ月にわたる大規模学習では、障害への対応メカニズムなしに学習を完了することは事実上不可能です。障害検出と自動復旧は、大規模学習における必須要件となっています。

## Meta Llama 3 405B が示す障害の実態

Meta が公開した Llama 3 405B モデルの学習データは、大規模学習における障害の現実を示す貴重な資料です。この学習は 16,384 基の H100 80GB GPU を使用したクラスターで 54 日間にわたって実施され、その間に 419 回の予期しない中断が記録されました。これは平均すると 3 時間ごとに 1 回、つまり 1 日あたり 7.76 回の障害が発生したことになります。

記録された 419 回の中断を分析すると、GPU 本体の障害が 148 回で全体の 30.1% を占め、GPU に搭載された HBM3 メモリの障害が 72 回で 17.2% となりました。GPU の SRAM メモリとシステムプロセッサの問題を合わせると 36 回で 8.6% を占めます。つまり、GPU とそのメモリ関連の障害だけで全体の 58.7% に達しています。

この高い GPU 障害率の背景には、H100 GPU が 700W という高い消費電力を持ち、大きな熱ストレスを受けることがあります。一方で、ネットワークスイッチやケーブルの問題は 35 回で 8.4% を占め、インフラストラクチャ全体の複雑さも障害の要因となっています。興味深いことに、CPU の障害はわずか 2 回のみで、GPU と比較して CPU の信頼性が極めて高いことが示されました。

Meta の学習で特筆すべき点は、419 回の中断のうち重大な手動介入が必要だったのはわずか 3 回のみで、残りの 416 回は自動化により処理されたことです。この高度な自動化により、Meta は 90% 以上の効果的な学習時間を維持することができました。手動介入に依存していた場合、研究者は 3 時間ごとに発生する障害に対応する必要があり、これは現実的ではありません。

## ダウンタイムを構成する要素

NVIDIA の研究によると、学習における実際のダウンタイムは複数の要素から構成されます。従来の MFU（Model FLOPS Utilization）や MTTF（Mean Time To Failure）といったメトリクスは、ハードウェアの利用効率や平均故障間隔に焦点を当てており、研究者が実際に経験する学習の中断やそれに伴う時間的コストを十分に反映していません。

研究者の観点からは、チェックポイントの保存に要する時間、エラー発生後の失われた作業、システムのシャットダウン時間、そして学習を再開するまでの時間、これら全てが生産的な学習時間を削減します。ダウンタイムは、チェックポイントのオーバーヘッドに加えて、エラーの発生回数に検出時間と復旧時間を掛け合わせたものとして表現できます。この公式から明らかなように、障害の検出と復旧を高速化することが、ダウンタイムの最小化に直結します。

Llama 3 405B の学習では、約 2.5 秒かけてチェックポイントを保存し、これを 4 分ごとに実行しました。最適なチェックポイント間隔は、チェックポイントの保存時間、GPU 数、そして GPU あたりの障害率から数学的に導出できます。Meta のケースでは、チェックポイントと障害復旧による時間損失は全学習時間の約 2.1% に抑えられました。

## 障害のタイプと検出の課題

NVIDIA の分析によると、研究者が遭遇するエラーは大きく 3 つのカテゴリーに分類されます。第一に、即座のクラッシュがあります。これは BIOS や電源、熱問題、訂正不可能な ECC エラー、サイレントデータ破損による NaN の発生、あるいはネットワークの不安定性など、ハードウェアの根本的な障害に起因します。これらの障害が発生すると学習は即座に停止し、再起動が必要となります。

第二に、通信ライブラリのハングがあります。これは PyTorch の NCCL watch dog エラーや Transformer Engine の通信ハングとして現れます。ハングの原因は、ファイルシステムからのデータ転送やネットワークを介したテンソル（勾配や中間アクティベーション）の転送における複雑な依存関係にあります。システム全体が応答しなくなり、タイムアウトまで待機が必要となるため、ライブラリとアプリケーション内での堅牢なフォールトトレランス、封じ込め、早期検出メカニズムが重要です。

第三に、速度低下があります。これには一時的な速度低下と持続的なボトルネックの両方が含まれます。一時的な速度低下は、ネットワークやストレージの一時的な問題、あるいは温度変動による GPU のクロック調整によって発生します。Meta の学習では、日中の温度変動により 1-2% のスループット変動が観測されました。一方、持続的なボトルネックは、大規模クラスター内の特定の GPU が常に遅い状態にあることで発生します。このようなストラグラー GPU は、数千の他の GPU に影響を及ぼし、全体の学習速度を低下させます。

これらの障害は、ハードウェア、インフラストラクチャ、ソフトウェアの問題に起因しますが、研究者の視点からは、突然の中断または大幅な速度低下として現れます。単一のエラーメッセージだけでは真の原因を特定するのは困難であり、複数のテレメトリソースを相関分析することで、検出と復旧の速度と精度を向上できます。

## DCGM による階層的な障害検出

NVIDIA Data Center GPU Manager (DCGM) は、GPU クラスターの健全性を監視し、障害を早期に検出するためのフレームワークです。DCGM Diagnostics は [4 つの Run Level](https://docs.nvidia.com/datacenter/dcgm/latest/user-guide/dcgm-diagnostics.html#run-levels-and-tests) を提供し、それぞれ異なる深さと所要時間でシステムを検証します。

Level 1 は Quick と呼ばれ、2.5 秒未満でシステムの基本的な健全性を確認します。このレベルでは、CUDA や NVML ライブラリの確認、PCIe リンクの基本チェック、GPU メモリエラーのチェックが実行されます。Level 1 はノード起動時やジョブ実行前の健全性確認として使用され、定期的なヘルスチェックにも適しています。

Level 2 は Medium と呼ばれ、8 GPU システムで 2.5 分から 10.5 分を要します。Level 1 の全てのテストに加えて、メモリ帯域幅の測定、GPU の基本的な計算テスト、PCIe 帯域幅の測定が実行されます。このレベルはジョブ失敗時のエピローグチェックや定期的な詳細チェックに使用され、より包括的なシステム検証を提供します。

Level 3 は Long と呼ばれ、10 分から 35 分を要するハードウェア診断です。Level 1 と Level 2 の全てのテストに加えて、GPU コアへのストレステスト、電力制限のテスト、メモリへの負荷テスト、NVLink 接続のテストが実行されます。このレベルは GPU 障害が疑われる時や、管理者による詳細調査、メンテナンス時の包括的チェックに使用されます。

Level 4 は Extended と呼ばれ、30 分から 120 分を要する最も徹底した診断です。Level 1 から Level 3 の全てのテストに加えて、より長時間のストレステスト、断続的な負荷を与えるパルステスト、電力配分の詳細テストが実行されます。このレベルは、RMA（返品承認）前の最終検証や、原因不明の障害の徹底調査、新規ハードウェアの受け入れテストに使用されます。

## 統合テレメトリによる根本原因の迅速な特定

大規模な GPU クラスターでは、クラスター、ノード、アプリケーションという 3 つの層からテレメトリデータを収集し、これらを相関分析することで根本原因を迅速に特定できます。

クラスターレベルのテレメトリでは、ストレージサーバーのメタデータ操作や読み書き操作、ネットワークスイッチの状態を監視します。これが重要なのは、1 つのノードの障害が通信呼び出しや破損した勾配の伝播、ストレージシステムの過負荷を通じて他のノードに波及する可能性があるためです。例えば、単一のジョブが過剰なメタデータ操作を生成してストレージノードを圧迫すると、他のジョブやノードがパフォーマンス問題に直面する可能性があります。

ノードレベルのテレメトリは、プロログ、定期的なヘルスチェック、エピローグという 3 つのフェーズで機能します。プロログスクリプトはジョブ開始前に実行され、ハードウェア状態の検証、ソフトウェア依存関係の確認、環境の設定を行います。ここでは DCGM の Level 1 診断が実行され、問題の早期検出により後続のデバッグ時間を削減し、全体的な信頼性を向上させます。

定期的なヘルスチェックでは、GPU、CPU、メモリ、ネットワーク、ストレージ、サービスなどの主要なハードウェアとソフトウェアコンポーネントが正常に機能していることを確認します。エピローグスクリプトはジョブ終了後または障害時に実行され、リソースの解放、ログの保存、システムのクリーン状態への復元を行います。ここでは DCGM の Level 2 診断が実行され、より詳細な障害分析が可能になります。

アプリケーションログは、重要な制御ポイント、不変条件、進捗の測定、システムエラーとパフォーマンスパターンに関する情報を提供します。これらのログは、中央リポジトリの履歴データと相関分析することで、ストラグラー、ハング、断続的または再発する障害を判断するための最も強力なシグナルとなります。

例えば、同じ NaN エラーが異なる物理ノードで同じイテレーションとランクに決定論的に現れる場合、これはアプリケーションエラーの可能性が高いと判断できます。一方、特定のノードで繰り返し発生する同じエラーは、深刻なハードウェア障害を示唆します。このような相関分析により、研究者と運用チームは共通のシステム動作と障害パターンの理解を共有できます。

さらに、単一ジョブ内でのパターン認識に加えて、複数ジョブにわたる障害パターンの分析も重要です。ジョブ間のテレメトリ分析により、再発する問題を特定し、ハードウェアの経年劣化を追跡し、問題が深刻化する前に予防的な対策を講じることが可能になります。

## 自動化が実現する高い稼働率

Meta の Llama 3 学習において、419 回の予期しない中断のうち重大な手動介入が必要だったのはわずか 3 回のみでした。残りの 416 回は自動化により処理され、これが 90% 以上の効果的な学習時間を維持する鍵となりました。

自動化システムは、DCGM による継続的な GPU 監視、PyTorch NCCL Flight Recorder によるハングの診断、ストラグラー検出ツールによる遅延 GPU の特定を通じて、障害を検出します。検出された障害に対しては、問題ノードの自動除外、チェックポイントからの自動再起動、ジョブの再統合という復旧プロセスが自動的に実行されます。さらに、運用チームと研究者への自動アラート、根本原因の分析と推奨事項の提示、可視化されたダッシュボードを通じて、システム全体の状況が共有されます。

自動化がなければ、研究者は 24 時間体制でジョブを監視し、3 時間ごとに発生する障害に手動で対応する必要があります。これは研究者の本来の目的であるモデル開発や科学の進歩から時間を奪い、開発サイクル全体を遅延させます。スケールが増加するにつれて、システムの組み合わせ的複雑性も増大し、問題をさらに悪化させます。

## チェックポイント戦略の数学的最適化

チェックポイントの頻度は、計算オーバーヘッドと障害時の失われる進捗の間でバランスを取る必要があります。頻繁にチェックポイントを保存すると障害時の進捗損失は最小化されますが、計算オーバーヘッドが増加します。逆にチェックポイントの頻度が低いと、オーバーヘッドは削減されますが、障害時により多くの進捗を失います。

数学的なモデルにより、最適なチェックポイント間隔は、チェックポイントの保存時間、GPU 数、GPU あたりの障害率から導出できます。Llama 3 405B の学習では、この最適化により約 4 分ごとにチェックポイントを実行し、全学習時間の約 2.1% のオーバーヘッドに抑えることができました。

従来のストレージベースのチェックポイントでは、GPU 数が増加するとストレージ帯域幅がボトルネックになる可能性があります。Meta の学習では、オプティマイザ状態を書き込む際のストレージ帯域幅は 2TB/s に制限されていました。復旧時にはデータ並列学習により複数の GPU が部分的に重複するオプティマイザ状態を読み取る必要があるため、さらに高い帯域幅が要求されます。

この問題に対する代替手法として、他の GPU のメモリにチェックポイントを分散保存する GPU メモリチェックポイントが注目されています。この手法は Google DeepMind の Gemini でも使用されており、ストレージ帯域幅の制約を回避し、より高速なチェックポイントと復旧を実現します。通常 3 から 4 つのレプリカを GPU メモリに保持し、バッファとして予備ノードを準備することで、ピア GPU メモリからの直接読み取りによる高速な復旧が可能になります。

## レジリエンシー戦略の実装

大規模学習でダウンタイムを最小化するには、多層的なアプローチが必要です。

まず、継続的な監視と階層的な診断が基盤となります。ノード起動時には DCGM Level 1 によるクイックチェックを実行し、システムが基本的に動作していることを確認します。学習実行中は、GPU の温度、電力、メモリエラーをリアルタイムで監視し、アプリケーションログから進捗とエラーパターンを追跡します。

障害が発生した際には、まず Level 2 のエピローグチェックにより障害の詳細を確認します。ここで障害が特定できれば自動復旧プロセスに進み、問題ノードを除外してジョブを再起動します。障害が特定できない場合は Level 3 の詳細診断に進み、それでも原因が不明な場合は Level 4 の徹底診断を実行します。最終的に手動介入が必要な場合でも、収集されたテレメトリデータにより、トラブルシューティングの時間を大幅に削減できます。

プロアクティブな対策も重要です。定期的な Level 2 または Level 3 診断により、障害予兆のある GPU を事前に特定し交換できます。環境要因である温度や電力の最適化、ストラグラー検出ツールによる遅延 GPU の早期発見により、問題が深刻化する前に対処できます。

チェックポイント戦略では、数学的最適化により間隔を決定し、可能であればストレージ帯域幅を拡張します。GPU 数が非常に大きい場合は、GPU メモリチェックポイントの採用を検討することで、ストレージのボトルネックを回避できます。

自動化については、問題ノードの自動検出と除外、チェックポイントからの自動再起動、リトライメカニズムを実装することで、手動介入を最小限に抑えます。Meta の実績が示すように、419 回の中断のうち 416 回を自動化で処理することで、研究者は学習に集中できます。

## NVIDIA DGX Cloud における実証された成果

NVIDIA DGX Cloud では、これらの統合テレメトリと自動化技術により、2K から 10K GPU 規模のトレーニングで 1% 未満のハードウェアダウンタイムを達成しました。この成果は 2024 年から 2025 年にかけての Nemotron モデルファミリーの学習で実証されています。

この低いダウンタイムを実現した要因は、複数のテレメトリソースの相関分析による高速な障害検出、問題ノードの自動除外と再起動による自動復旧、問題が深刻化する前に検出するプロアクティブな監視、そして運用チームと研究者が共通のシステム動作と障害パターンを理解するための情報共有です。

このアプローチにより、研究者はインフラストラクチャデータを活用してデバッグを改善でき、一方で運用チームはアプリケーションのインサイトを使用してシステムの自動化を改善し、ハードウェアダウンタイムを削減できます。この相互的な情報の流れが、エンドツーエンドのレジリエンシーを実現する鍵となっています。

## まとめ

大規模基盤モデルの学習において、レジリエンシーは必須要件です。100,000 GPU のクラスターでは 30 分ごとに障害が発生し、数週間から数ヶ月の学習では障害対応なしに完了することは不可能です。Meta の Llama 3 学習は、16,384 GPU で 3 時間ごとに障害が発生しながらも、自動化により 90% 以上の稼働率を維持できることを示しました。

障害の早期検出により、失われる学習の進捗を最小化できます。DCGM の階層的な診断レベルは、ノード起動時の 2.5 秒のクイックチェックから、徹底調査のための 2 時間の診断まで、状況に応じた適切な深さで迅速に問題を特定する手段を提供します。

自動化は大規模学習を実現可能にする最も重要な要素です。Meta は 419 回の中断のうち 416 回を自動化で処理しました。手動介入に依存すると、研究者は本来の目的であるモデル開発ではなく、システムの保守に時間を奪われます。クラスター、ノード、アプリケーションの複数のテレメトリを相関分析することで、根本原因を迅速に特定し、自動復旧システムが適切な対応を判断できます。

ジョブ間でのテレメトリ分析により、再発する問題を予防的に対処し、システム全体の信頼性を継続的に向上できます。DCGM による障害検出と自動復旧メカニズムは、大規模学習を実現可能にする基盤技術です。適切なレジリエンシー戦略なしに、数百万 GPU 規模の学習は実現できません。高い GPU 稼働率を維持することは重要ですが、それ以上に重要なのは、研究者がモデルの開発と科学の進歩に集中できる環境を提供することです。

## 参考資料

- [Ensuring Reliable Model Training on NVIDIA DGX Cloud](https://developer.nvidia.com/blog/ensuring-reliable-model-training-on-nvidia-dgx-cloud/) (NVIDIA, 2025)
- [Meta report details hundreds of GPU and HBM3 related interruptions to Llama 3 training run](https://www.datacenterdynamics.com/en/news/meta-report-details-hundreds-of-gpu-and-hbm3-related-interruptions-to-llama-3-training-run/) (Data Center Dynamics, 2024)
- [Hardware failures won't limit AI scaling](https://epoch.ai/blog/hardware-failures-wont-limit-ai-scaling) (Epoch AI, 2024)
- [DCGM Diagnostics — NVIDIA DCGM Documentation](https://docs.nvidia.com/datacenter/dcgm/latest/user-guide/dcgm-diagnostics.html) (NVIDIA)
